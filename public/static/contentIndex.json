{"index":{"title":"知识管理","links":["posts/km/知识管理","posts/nm/笔记管理","posts/wm/工作管理","posts/rm/阅读管理","posts/bm/博客管理","posts/cm/搜藏管理","posts/pm/计划管理","posts/em/考试管理"],"tags":["evergreen"],"content":"\n这里记录一些永久笔记，使用的是quartz搭建的永久笔记。\n我的笔记库：\n\n知识管理\n笔记管理\n工作管理\n阅读管理\n博客管理\n搜藏管理\n计划管理\n考试管理\n\n更多的数据，你也可以访问:\n\n我的学习笔记\n个人空间\n"},"books":{"title":"Booklist","links":["thoughts/Alexandre-Grothendieck","thoughts/The-Mythical-Man-Month","thoughts/Scientific-Freedom","thoughts/Where-is-My-Flying-Car","thoughts/visualization","thoughts/A-Pattern-Language","thoughts/Tomorrow,-and-Tomorrow,-and-Tomorrow","thoughts/Seeing-like-a-State","thoughts/Games-Agency-as-Art","thoughts/Weaving-the-Web","thoughts/The-Writing-Life","thoughts/Tools-for-Conviviality","thoughts/In-Over-Our-Heads","thoughts/The-Midnight-Library","thoughts/Archipelago","thoughts/How-to-do-Nothing","thoughts/A-Tale-for-the-Time-Being","thoughts/Atlas-of-AI","thoughts/The-Grasshopper,-Games,-Life-and-Utopia","thoughts/The-Anthropocene-Reviewed","thoughts/From-Counterculture-to-Cyberculture","thoughts/Design-Justice","thoughts/Mindstorms","thoughts/Making-and-Maintenance-of-OSS"],"tags":[],"content":"A (mostly) up-to-date list of books I at some point, have wanted to read, am reading, or finished reading. Links are to pages/blog posts/ideas that were inspired by the book that’s linked!\n\n “the end of a book’s wisdom appears to us as merely the start of our own,” Nussbaum writes\n\nTo Read\n\nWhat is an antilibrary? To put it simply, an antilibrary is a private collection of unread books. Instead of a celebration of everything you know, an antilibrary is an ode to everything you want to explore.\n\nFiction\n\nAccelerando by Charles Stross\nHomo Ludens by Johan Huizinga\nThe Ministry for the Future by Kim Stanley Robinson\nCat’s Cradle by Kurt Vonnegut\n\nNon-fiction\n\nDealers of Lighting by Michael A. Hiltzik\nCreation: Life and How to Make it by Steve Grand\nThe Utopia of Rules by David Graeber\nInventing the Medium by Janet J. Murray\nDesigning an Internet by David D. Clark\nPlaying Software by Miguel Sicart\nDesigns for the Pluriverse by Arturo Escobar\nNew Media Art by Mark Tribe/Reena Jana\nare we human? notes on an archaeology of design by Beatriz Colomina &amp; Mark Wigley\nThe Unbearable Lightness of Being by Milan Kundera\nDancing on the Ceiling: Art &amp; Zero Gravity by Kathleen Forde\n\nPoetry\n\nWithout by David Hall\n\nCurrent\n\nThe Dream Machine by M. Mitchell Waldrop\nSaving Time, Discovering a Life Beyond the Clock by Jenny Odell\n\nPast\n2024\n\nRécoltes et Semailles by Alexandre Grothendieck\n\n2023\n\nThe Mythical Man-Month by Frederick P. Brooks, Jr.\nDesign as Art by Bruno Munari\nScientific Freedom: The Elixir of Civilization by Donald W. Braben\nWhere Is My Flying Car? by J. Storrs Hall\nThe Visual Display of Quantitative Information by Edward R. Tufte\nA Pattern Language by Christopher Alexander\nUpstream: Selected Essays by Mary Oliver\nFoundation by Isaac Asimov\nTomorrow, and Tomorrow, and Tomorrow by Gabrielle Zevin\nProject Hail Mary by Andy Weir\n\n2022\n\nSeeing Like A State by James C. Scott\nGames Agency as Art by C. Thi Nguyen\nPermutation City by Greg Egan\nWeaving the Web by Tim Berners-Lee\nThe Writing Life by Annie Dillard\nTools for Conviviality by Ivan Illich\nEnvisioning Information by Edward R. Tufte\nThe Book of Form and Emptiness by Ruth Ozeki\nWalden Two by B. F. Skinner\nIn Over Our Heads: The Mental Demands of Modern Life by Robert Kegan\nGhost Work by Mary L. Gray and Siddharth Suri\nThe Midnight Library by Matt Haig\nArchipelago by Édouard Glissant\nUnflattening by Nick Sousanis\n\n2021\n\nHow to Do Nothing by Jenny Odell\nA Tale for the Time Being by Ruth Ozeki\nAtlas of AI by Kate Crawford\nKlara and the Sun by Kazuo Ishiguro\nThe Grasshopper: Games, Life and Utopia by Bernard Suits\nThe Anthropocene Reviewed by John Green\nNever Let Me Go by Kazuo Ishiguro\nFrom Counterculture to Cyberculture by Fred Turner\nDesign Justice by Sasha Costanza-Chock\nMindstorms by Seymour A. Papert\nHalf of a Yellow Sun by Chimamanda Ngozi Adichie\n1984 by George Orwell\nOn Earth We’re Briefly Gorgeous by Ocean Vuong\nThe Making and Maintenance of Open Source Software by Nadia Asparouhova\nWhen Breath Becomes Air by Paul Kalanithi\nKafka on the Shore by Haruki Murakami\nThe Art of Thinking Clearly by Rolf Dobelli\n21 Lessons for the 21st Century by Yuval Noah Harari\n\n2020\n\nSapiens by Yuval Noah Harari\nMeasure What Matters by John Doerr\nA Beautifully Foolish Endeavour by Hank Green\nThe Uninhabitable Earth by David Wallace-Wells\nThe Subtle Art of Not Giving a Fuck by Mark Manson\nThe Death of Ivan Ilyich by Tolstoy\n"},"posts/2020":{"title":"2020: An Imperfect Palindrome","links":["thoughts/reading","thoughts/exploit-explore","thoughts/writing","thoughts/telic-action","thoughts/paratelic-action","thoughts/feedback-loops"],"tags":["fruit","personal"],"content":"A year of ‘almosts’\n2020 is:\n\nabout 9.5 times longer than a Rick Roll in seconds\nabout 54 times longer than the number of minutes I listened to Spotify this year\nabout how many contributions I made on GitHub this year\nroughly the amount of pages I’ve read this year\nalmost a palindrome\n\nNow there’s nothing super fascinating about almost-palindromes but it is also very fitting given the number of ‘almosts’ in my year. Here’s an imperfect recollection of what I’ve been up to this year.\nnwPlus\nI help to do event logistics at nwPlus! This year, Allison and Anne somehow convinced me to take on a director role for HackCamp (or what was previously known as UBC Local Hack Day).\nTo be completely frank, I had no clue what that really entailed and jumped into it head-first. Along the way, I struggled with figuring out how to effectively lead a team of 4 older teammates, balancing nwPlus with all my other commitments, and being confident in my work as director. We had a bunch of logistical wrenches thrown our way, from figuring out how to bring our hackathon to the virtual stage to and navigating an entire event rebrand. It was honestly amazing to have even been able to hold the event at all, let alone attract over 900+ registrations, 500+ attendees, 3.2k+ livestream viewers, and $1200 in donations to charities. To say I was completely blown away by the turnout would be a massive understatement. We didn’t get to hold the in-person event we were hoping for, but the next-best thing was almost just as good.\nPost-HackCamp Feels\nA poorly played game of Tetris\nDue to a combination of poor planning, luck, and FOMO, I made the poor decision to take on an internship at Hootsuite along with the open-source fellowship at MLH, all the while trying to juggle my mental sanity and health. Spoiler alert: it was a bad idea.\nMy calendar ended up looking a lot like a poorly played game of Tetris as I tried to stack hackathon onto after-work social onto tech talk. Unfortunately in this version of Tetris, there was no line clearing when I fully packed a week. You don’t need more than two braincells to realize that a schedule like that just isn’t sustainable. I was often working upwards of 80+ hours a week and I felt like death. I think the lowest point of the summer was sitting on the couch after working an 8 hour day at Hootsuite followed by another 6 hours on fellowship things and realizing that I only had oatmeal in the morning.\nIn order to get through the summer, I had to develop healthier coping methods to deal with the stress, anxiety, and general lack-of-time-to-do-anything that came along with it. Though I by no means have claimed to have solved any of these things, I’ve found a few things to be really helpful in maintaining some semblance of mental sanity:\n\nTime Blocking — COVID had eliminated any physical boundaries I already had; no 15 minute transit to separate work and home, no physical walk required to go from home to school. As a result, everything started to meld together. If it only takes me 3 clicks to go from my club meeting to my work tickets, I can just work just a little longer, right? I found that time blocking really let me regain a little bit of control back in my life. I could tell myself to only spend x hours doing y. It had the added benefit of allowing me to actually block off time for myself and to enjoy life a little bit.\nExercise — although I’ve known this fact for a while now, this is the year I’ve really decided to actually apply it to my life. Feeling like garbage? Go for a run to feel more like garbage for a good hour before feeling better! I’ve had the privilege of being really close to nature so I get to enjoy the subtle crunching of leaves and brisk autumn air with each step.\nQuality time with quality people — 2020 was not a kind year to most, myself included and I’m not going to pretend otherwise. However, the presence of a select few individuals in my life really helped to make the year slightly less of a dumpster fire. You know who you are :‘)\n\nIndependent living\nI finally moved out from on-campus housing into my own place! I’ve definitely felt tinges of loneliness during the pandemic but I’ve learned to fill the silence with music, podcasts, reading, or questionable arts and crafts. I’ve also somehow managed to get by the past few months without giving myself food poisoning in my transition from campus food to homecooked food.\nNo fancy photos here but enjoy a few pictures of my workspace and the view from my balcony.\nHome sweet 127.0.0.1\nAn imperfect retrospective\nIt would be sort of naive to try to list out everything that went well/didn’t go well this year. Too much happened this year to fully give each bit of my year the respect it deserves, so in the spirit of this ‘almost’ year, enjoy this ‘almost’ retrospective.\nThings that sparked joy\n\nSurrounding myself with genuine people who care.\nPicking up old hobbies and books — Heraclitus said, “No man steps in the same river twice.” The second time around, both man and river are different than they were before. The paints and books are the same, but we change between reads and brushstrokes. The world changes, too. I’ve found it really rewarding to be able to pick up old hobbies like watercolour, crocheting, and reading and discover it for the second time.\nExercising more and enjoying nature — not everyday you get to read during sunset at the top of a peak with some of your closest friends.\n\nThings that didn’t spark to joy\n\nLearning to block out time better — although I did improve on this, I’ve definitely told myself “ah, another few hours couldn’t hurt” far too many times this year. I need to work on prioritizing my time more and saying no to things.\nBalancing intake and output — How does one balance learning/intake of information intake with processing and thinking about that information? This year felt like trying to drink information through a firehose and spitting it right back out using a paper straw. I’m trying to work on better processes for choosing what information to explore more and to spend more time thinking and marinating thoughts (see: exploit explore tradeoff). Tweet.\n\nGoal setting\nWhen writing up the first few drafts for this post, I realized that just having goals with cold hard numbers on them without any motivating reason feels very empty. I’m not setting these goals just to achieve some number, but rather to enjoy the act of doing it or to develop a habit. I had a conversation with Emre about this and he brought up Reversal Theory as a way to understand our motivations and goals. In particular, one of the four domains of Reversal Theory is the means-ends domain. Motivations in this domain lie somewhere on the telic to paratelic action scale.\nParatelic Goals\n\nAcknowledging my privilege and use it to help those who are less fortunate\nHave impact with my work, whether that be through writing, projects, or just conversations\nLead a more physically healthy lifestyle\nLearn more about the world through a feedback loops of reading, writing, and talking to people\n\nTelic Goals\n\nDonate 5% of pre-tax income to charity\nReach 50k people through projects and blog posts\nRead 10 books\nSet aside 10 hours a week for personal projects, learning, and writing\nBe able to run from my apartment to Stanley Park and back (~10km) in one go\nHave 1:1s with 50 new people in 2021\n\nI’ve been waiting almost 2 years to say this pun but,\n\nhindsight is 2020.\n\n(no I didn’t plan on releasing this right at midnight, I just take a long time to write)"},"posts/2021":{"title":"2021: A Letter to my Past and Future selves","links":["thoughts/writing","books","posts/2020","thoughts/Projects","thoughts/interaction-design","posts/networked-thought","thoughts/Bentoism","thoughts/play","thoughts/sleep"],"tags":["fruit","personal"],"content":"\nHave you been writing those letters to yourself?\n“Dear Evan Hansen, This is gonna be a good day and here’s why.”\n\nThis post comes to you in 3 parts: a letter to my past self, a reflection on goals, and a letter to my future self.\nOne reminiscent and nostalgic, one a neutral review, and the last a brutally honest dump of aspirations and feelings.\n\nTo my 2021 self,\nLittle do you know you’ll start off the year making a charcuterie board of tofu, snowshoe trips with friends, and making a masking tape Christmas tree that still stands to this day.\n\nMaybe not the most usual start to a year but fitting considering the year that is ahead of you.\nYou tried lots of new things! Some you liked and some you didn’t. It was full of meeting lots of really wonderful new people, writing lots, reading lots, getting incredibly digital-garden-pilled, and (admittedly) being on Twitter a bit too much.\nYou learned how to drive, maybe a few years later than usual, but hey you got it done. Not many people may trust your driving skills from New York but that’s reasonable given the only thing between you and a license was a 7-minute long stroll down the road. In learning to drive, you spent some extended time with the family for the first time in a while and all of the pains and joys that brings.\nAnd maybe the most surprising of them all, you decided to say ‘fuck it’ and decided to move to a co-living house with 15 other people in New York for the summer. Let’s just say you learned the importance of personal time pretty quickly. Yet, amidst the hustle-and-bustle of the millions in New York, you were able to find solace in the books, the parks, and the few close friends made. There is a really nice bench near the Guggenheim under a blooming Crabapple tree that sags just the right amount to be extra comfortable. I suggest bringing some nice bread and a book to spend a few afternoons here and really just bask in its serenity.\n\nLuckily, COVID calmed down enough in the second half of 2021 to be able to see old classmates again in-person for the first time. Being able to laugh at dumb jokes in the same room or just working in amiable silence may feel quite distant but I assure you it will happen and it will be just as good as you remembered it being.\nYou had cozy picnic dates, 20-person AirBnB rentals on Victoria Island, and a crazy hackathon all the way down in Austin, Texas. You and Anson finally celebrated your first anniversary together and, lo and behold, got each other the exact same thing. Disgustingly cheesy, you two.\n\nTo summarize, this was a year of understanding yourself and opening up to the world. You learned that, maybe, it wasn’t so bad to be vulnerable and to express yourself in full to the people around you. You learned the joys of meeting wonderfully passionate people who talk about their craft with stars in their eyes and the deep gratitude of getting to more fully know the people you choose to surround yourself with.\nThe biggest part of this year were the friends and loved ones that stuck with me through the best and worst of it.\n\n“Who are the people, ideas, and books that magnify your spirit? Find them, hold on to them, and visit them often.” — Maria Popova\n\nI hope you cherish these people and make sure they know just how much they are appreciated, and excited for you to live this year like I did.\nKindly,\nYour present self\n\nI set a few goals at the end of my last end-of-year reflection.\n\nDonate 5% of pre-tax income to charity\n\nI made a total pre-tax income of around ~30k and total donation amount tallying up to 2.2k to Project for Awesome, Asian Mental Health, Nonprofit Foundation, Heart and Stroke Foundation, and bootstrapping for the reflect apprenticeship.\nThere were a few times this year I felt stretched thin financially due to lack of financial planning. I don’t regret giving though. I love giving in a way that feels local and impactful. I would be interested in expanding this to start a mini grants program using Moth Minds or something, but this would involve first improving my own financial management and budgeting :))\n\n\nReach 50k people through projects and blog posts\n\nDone! Surprisingly prolific year in terms of projects and writing. Got really into digital gardening and CLIs for a bit over the summer and even ended up doing a few workshops too. Happy with the amount of technical growth that happened this year.\nAs a side note, it’s been so so cool watching a little community grow around Quartz and all the different types of people from all around the world giving it a chance to be a part of their daily workflow. (And all the growing pains that come along with open-source maintenance and community management)\n\n\nRead 10 books\n\nActually pretty proud of this one! Not just because I crushed the number (18 whole books!! A whole 3x more than last year) but I felt like I had a really good diet of the different types of content I was consuming. Did a lot of online link collecting and book reading of both fiction and non-fiction. I found out that I surprisingly liked both dense academic text as well as soft scifi. I love the nuance and depth of takes that longer texts afford as well as the elaborate and at times poetic world building that happens in heavier fiction reads. I always remember reading under the covers with a flashlight because I wanted to finish the last book in a trilogy but lost that itch quite a while ago. Missing your bus stop because a chapter was just so good is not quite the same but close.\n\n\nSet aside 10 hours a week for personal projects, learning, and writing\n\nOne particularly hard week in March ruined this streak unfortunately :((\nOtherwise, I would consider this a success! This year, I had the wonderful chance of meeting some people who don’t make me feel ashamed of but genuinely excited about niche interaction design reads, weirdly technical blogs, and digital gardening.\nI feel like I finally have a group of people to just ‘nerd snipe’ each other and be ok with just sharing cool things I am working on. Eternally grateful to curius.app, the whole gang at @verses, and the whole ‘snipe city’ group chat.\n\n\nBe able to run from my apartment to Stanley Park and back (~10km) in one go\n\nSadly, the one actual physical goal I had I did not meet. In reality, I didn’t take as good care of my body as I would’ve liked, often skimping on meals to get just a little bit more done.\nI often blamed my weird class schedule for my lack of consistent physical activity this term but I know that that is no real excuse. I’m going to do better on this next year.\n\n\nHave 1:1s with 50 new people in 2021\n\nThis is a goal that actually really surprised me as to how easy it became. When setting it, I surely thought that if there was one goal in this list I wouldn’t meet, it would be this one.\nAt the start of last year, meeting people was not something I generally looked forward to. Yet, I found myself actually really enjoying each conversation after a while. Each call and meeting was a chance to quickly glimpse into the life of another, an offering of their incredibly valuable time in order to just talk.\nTo all 56 who offered just a little bit of your time to talk, I thank you. Many of my thoughts build off of our conversations and I am forever grateful for your thought-gifts.\n\n\n\n\nTo my future self,\nI am writing to you with the highs of new years optimism wearing off. ‘Tis the time of new years resolutions!\nMaybe it’s the residual headache from the booster shot speaking or maybe its just me being a little tired but I hope you are doing well. More than anything I hope you are taking good care of yourself.\nI’ve never really written a letter to my self before, let alone one to my future self. I’m not quite sure what the tone of this piece is and quite certain that you will look back on this and laugh a little and just how bad it is.\nTo be honest, I know I’ve been saying that I’m working on being candidly excited about everything but I’m a little scared about the future.\nI care a lot about the people around me right now. These are an absolutely wonderful group of people who care about the world, endlessly curious, and inspire me to be better people. Yet it seems that everyone is headed in slightly different directions. I don’t want to lose these people to a few measly miles and timezones. I really hope that, despite life choosing to move us in different paths physically, we’ll be able to consciously choose to band together and stay close.\nI know it would be unreasonable to set super concrete goals given the whole waves hand. So I leave you not with a list of goals that I hope you will have accomplished, but a list of qualities I hope you’ll have grown into.\nMy long term Bento\nI recently found out about the philosophy of Bentoism, a way of planning with a wider view of interests than just what we want right now, like our future selves, the people we care about, and the future of our children.\nI really like this way of thinking about self-interest as not just our current selves but as a community that spans across time. I’ve been thinking a lot about about what long-term success for me would look like. I don’t think I’ve settled on anything concrete but there are certain aesthetics I would like it to embody:\n\nI would like to be emotionally and physically well. I would like to be able to reach deep focus in whatever work I do and have the resources to be able to choose the work I find enjoyable.\nI would like to be a great friend, family member, partner, and community member. I want to have the bandwidth to be generous to the people around me and the clarity to prioritize the important people in my life.\nI want to be authentic and unabashedly excited about the world. I want to be able to help others create spaces of local abundance.\nI hope you are ambitious in your dreams, projects, and writing. I want to embody a sort of quiet confidence in my own abilities and interests.\nI hope you are steadfast in the values you believe in, cherish the people around you, and grow into the person you’ve been wanting to become.\n\nTo concretize, I hope you end each day saying you’ve embodied these:\n\nIf it’s not a FUCK YES, it’s a no. We are not half-assing anything in 2022\nEat well, sleep consistently, and exercise often\nBe intentional about the people you care about\n\nI hope you are honest with yourself. Be kind.\nKindly,\nYour present self"},"posts/2022":{"title":"2022: A year lived earnestly","links":["posts/2021","posts/the-fools-who-dream","posts/casual-magic","thoughts/In-Over-Our-Heads","thoughts/50-pounds-of-pots"],"tags":["personal","sapling"],"content":"tw: abuse. This reflection serves as a sort of public journal entry. It talks about a lot of sensitive topics. If you choose to read, please do so with care and tenderness and treat it as if I shared it in confidence with you.\n\n\n“Yet I live earnestly, building the most beautiful sandcastles I can, knowing they will be washed away. And getting others on the beach to build with me, at times even suspending our belief of the fact that it will disappear; letting ourselves be fooled for a moment that it will last.” Source\n\nTo my 2022 self,\nYou’ll be pleased to know that I’m continuing the tradition of writing letters to you (myself?) every year! This letter in particular, took me a really long time to figure out how to write properly.\nYou get to spend the first part of the year in some truly beautiful places with some wonderful company. You start the year in Montreal with some close friends from Verses, doing everything from talking grand plans about what the web might look like in the future to soaking in an outdoor hot tub on a freezing day. Hold these people close, they are a wonderfully thoughtful and quirky bunch that will feel like one big extended family.\nBack in Vancouver, you convinced your friends to regularly make a trek down to the beach to enjoy sunset with you despite the hundreds of steps needed to get there. You became close with this little ragtag bunch and cried a bit when most of them graduated, knowing that it would be the last time you spend in close company for a while.\nFor perhaps the first time in your life, you felt a level of quiet confidence that allowed you to try something foolish and new. This conviction brought you to the South Bay to live with a motley crew of people. You decide to do a spontaneous writing trip down in Monterey Bay, road-tripped from SF to LA, and even hosted a small hackathon (I guess your days of event organizing never truly go away).\nThe theme, as you can probably tell, is good company matters. These people will inspire you to look deeper and notice all the casual magic in the world. They indulge your childlike curiosities and help bring out your favourite parts about yourself.\n\nBut, what’s a story without a little plot twist? Life has a certain way of throwing wrenches into your well crafted plans.\nVisiting relatives for the first time in a while highlighted that certain people can also bring out the worst parts of yourself, sticking them out like welts on rotting skin. September to November happened to be periods of intense self-reckoning around your own self-worth as a person, friend, and partner.\nPeople who claim they love you will tear at the very quiet confidence you worked so hard to build up over the year. They claim to do things out of love, but refuse to learn the way you want to be loved. It’s terrifying to realize that someone who claims to love you hurt you in ways that take advantage of that very fact. Sometimes the worst abuse happens under the guise of love and care.\nIt will exacerbate a lot of your anxieties around self-worth, trust, and will completely tank your physical and emotional wellbeing. I’ll give you a heads up here and tell you that going to therapy didn’t change things very much. There’s not much they tell you that you don’t already know. The real painful process here is learning to accept and move on from the fact that some people you care a lot about did some pretty fucked up things to you in the past.\nThere’s not much you can do to change their mind, or fix them or whatever. But what will help is realizing that trying to change their behaviour is not your responsibility. You will come to realize that there is no rhyme or reason to their actions at times and the mental gymnastics they do to justify their actions are not for you to untangle.\nThe key realization that will really help you start to move on is to treat their voice like a voice, but not the only one that matters. I’ll quote Kegan at length because I think he says it a lot better than I can.\n\nSuppose you have a dog. A big-hearted, high-energy dog who begins to bark, and won’t shut up, every time someone approaches your door. Now one day your dog starts into howling something first. He sounds a terrible alarm. You look out the window and it’s just your friendly neighbourhood mailman. So what do you do? You aren’t going to shoot your dog dead. He’s a pain but you wouldn’t think of it. Your dog loves you. He barks to warn you when anyone approaches. He wants nothing bad to happen to you. That’s just how he is. Problem is, he’s completely indiscriminate. He thinks everyone’s a danger, barks at anyone who approaches… You’re going to have a look for yourself. You’re going to bend over and stroke your dog. ‘Down boy,’ you say. ‘It’s just the postman. No harm here, silly guy’\n\nIt sounds almost silly when its said out loud like this, but you will learn it with time. It’s not a process that is super linear either. There will be days you break down and won’t be able to leave the house, but it does get better.\nYou’ll start to settle back into old rhythms with school and pick up the guitar. You also start going to the gym regularly again because, goddamn people were right when they said how much of a mood booster exercise was.\nYou will make it to the end of the year, finally at a point where you feel well enough to reflect on the past and start thinking about the future again.\nLearnings\n\nA yes means nothing if you never say no.\nLearning is learning. If you spend five hours on a problem and can’t figure out the answer, it doesn’t mean you haven’t learned, it means you spent five hours learning. Your brain has rewired and formed new attractor states, you’ve done necessary work, the activity you’ve done is called learning. (Source)\nMake 50 pounds of pots. Quantity is the journey to quality.\n“Choose joy. Choose it like a child chooses the shoe to put on the right foot, the crayon to paint a sky. Choose it at first consciously, effortfully, pressing against the weight of a world heavy with reasons for sorrow, restless with need for action.” (Source) Choose joy because what you look for in life is what comes your way.\n\nLooking forward\nUnlike last year, I think it would be a little silly to try to set rigid goals for myself. Some of my best moments this past year happened because I gave myself the flexibility, and free-time to do so.\nTo continue the trends of values I’d like myself to embody next year:\n\nI would like to be emotionally and physically well. I would like to be able to reach deep focus in whatever work I do and have the resources to be able to choose the work I find enjoyable.\nI hope you are ambitious in how you choose to cultivate spaces of joy and abundance in your life.\nI hope you give yourself the grace and time to do things that bring you into company with good people. I would like to be a great friend, family member, partner, and community member. I want to have the bandwidth to be generous to the people around me and the clarity to prioritize the important people in my life.\nI hope you continue to be someone who is mesmerized by the beauty of the world.\n\n\nKindly,\nYour present self"},"posts/2023":{"title":"2023: Polarity","links":["posts/2022","thoughts/Franz-Kafka","thoughts/The-Sword-of-Damocles","thoughts/50-pounds-of-pots"],"tags":["personal","sapling"],"content":"tw: abuse. This reflection serves as a sort of public journal entry. It talks about a lot of sensitive topics. If you choose to read, please do so with care and tenderness and treat it as if I shared it in confidence with you.\nHey there 2023 me,\nI’ve been struggling to write this letter for the past week and now that I am sitting at the airport waiting to go home, the emotions finally feel right to get it out of my system. I don’t think it’s going to be in any sort of structured format like it has been in previous years but alas, I write this for me and you only.\n\nTo start, the first half of 2023 was better than anything I could have asked for by pretty much all accounts.\nDespite this being my final term of my four years at UBC, I lived it more fully and presently than any other term I had prior. Though school itself was a very minor part of my time, the revolving cast made Vancouver a hard place to say goodbye to.\nYou’ll find this is a common theme but good company makes otherwise unbearably difficult things quite awesome, actually.\nThis is particularly funny because I know one of the big things you were really nervous about last year was whether you’d still be able to find joy and community despite all of your close friends graduating. I definitely remember having cried a few times about having felt incredibly lonely around last December but yet you held on to a deep-seated belief that there had to be incredibly good people around you who were also looking for that kind of community.\nIn January, you decide to take a leap of faith and started Saturdays@UBC with V. It started off very small — a weekend gathering of five or so folks regularly talking about what fun thing they learned every week and how their side projects are going. The motley crew grew to be a small group of friends and it quickly became something to look forward to at the end of each week.\nOn a whim, I decided to run a 10km at the Sun Run with the knowledge that race day coincided with my birthday. I made a bet with B that I would have a target time of 55 minutes — the bet being that if I didn’t complete the run in that time, I would get pied in the face. In retrospect, this was honestly very fun to train for. I got to really test the limits of my own body and felt myself get stronger with each run. The actual day of the Sun Run itself was rainy which was kind of ironic and I just barely missed my target time. By all accounts, the actual events of my birthday by themselves weren’t anything to be excited about but living it alongside some of my favourite people means that this is disproportionately the memory I come back to most when I think about the good things that happened this year.\nA few trips here and there:\n\nDisneyland, Scandinavia, and San Diego;\na backpacking trip along the Sunshine Coast Trail with S where we did nothing but hike and read for a week;\na lovely month in Waterloo (where the water is in fact loo but the people are not); and finally,\nacross the stage for graduation.\n\nShortly, I moved to San Francisco to start working full-time. Work was obsessive but not life-consuming — the right kind to feel fulfilled and exhausted after work but still leaves you looking forward to going climbing or playing guitar after work.\nAll things considered, life was really really good.\n\n\n\nFallen Down (Reprise) - Toby Fox\n\nThe end of July marked another visit from relatives. This time, it wasn’t purely just barking and yelling and screaming and cursing but also mismatched love, awkward expressions of care, and attempts to understand.\nThis — to me — was much more difficult to stomach than pure abuse. Unadulterated abuse means that you can consistently shut yourself out to any and all things they say. There is some relief in the helplessness of it all, the pretense of them being the abuser and you the victim.\nIt isn’t as straightforward when there are glimpses of normality and okayness that peek through occasionally. Desperately, we cling to these moments of grace (cut fruit here and there, asking how you are feeling, saying they are sorry) and wonder what could have been.\nIt’s painful because these moments of normality give you a basis on which an overthinking brain can rationalize the rest of their otherwise unacceptable behaviour from.\nThey say things like ‘all of your friends are just there to take advantage of you and don’t truly care about you’. They take everything out of your fridge, slamming it on the table, ready to throw it the moment you say something they don’t like the sound of. They scream, curse, and disown you in ways that no parent should ever act toward their children.\nYet, some part of you finds it convincing and thinks it’s okay because deep down, there is some twisted way to rationalize what they are doing as looking out for you. They tell you they say these things because they are the only ones who love you enough to tell you these cursed truths in your life that no one else has the bravery to. They raised you in fire because they wanted you to be strong.\nFranz-Kafka#^6160dc\n\nWhat I never understood was how you never felt remorse or regret for the things you said or did beyond a cursory ‘sorry’ tossed my way as an act of pity.\nIn fact, there were many times that you chose not to follow through on whatever promised punishment you had devised and considered it as a generous pardon. But the act and the uncertainty again chipped at me more. The way your face got red with fury, spittle flying, you could have convinced me that I was to be hanged.\nAgain, borrowing words from Franz Kafka:\n\nIf he really is hanged, then he is dead and it is all over. But if he has to go through all the preliminaries to being hanged and he learns of his reprieve only when the noose is dangling before his face, he may suffer from it all his life. Besides, from the many occasions on which I had, according to your clearly expressed opinion, deserved a whipping but was let off at the last moment by your grace, I again accumulated only a huge sense of guilt.\n\nThis uncertainty was an eternal sword of Damocles hanging over my head, never allowing me to enjoy the moments of normalcy and calm between fits of anger — truly an unfairness for all parties.\nEach time I asked them to reconsider how they parent, they blew up and accused me of a crucial lack of respect. Of course, I as the child was never able to talk back. They were the ones to set the rules as the head of the household and I was to never challenge it. A child is subordinate to the parent and any violation of this order is fit for punishment.\nFranz-Kafka#^53355b\nThe passive parent also plays the role of the accomplice in the form of emotional betrayal. The act of passively standing by implicitly condones the behaviour, further cementing their worldview as correct. A shrug as if to say “this is just the way things work”. To them, their world was completely inconsistent and I was the one gone mad.\nOn every side, I was in the wrong and I was in your debt. Right and wrong seem to constantly trade places depending on who the doer is.\nThe consequence of this is that it leaves your internal compass of ‘right’ and ‘wrong’ in disarray every time. Each conflict I walk away feeling less sure about what I want and how I feel.\n\nSometimes in a bout of normalcy, you ask me why I appear to treat you so coldly. “Do you treat all your friends like this too? No wonder they don’t like you.”\nYou are probably right in this regard and thank goodness I don’t. There is a coldness there but it comes from a place of self-preservation. It’s because I don’t know when to protect myself; the times I open myself up most to your love are the times I am hurt the most by the successive pain you inflict.\nWhat to do?\nLike the way a gecko detaches its tail when caught, I leave a part of myself behind when I pull away emotionally. I’ve been able to compartmentalize it very well but every time I go home I see that tail I left behind and I wonder what it could have been.\nAttempts at repair require a certain vulnerability that is always taken advantage of. How many times must one burn their hand to learn to not touch the fire? Time and time again, after each successive failure, I pull my hand back wincing in pain in the hopes that for once it will be different.\nAfter all, the most devastating aspect of relationships such as this is that you, as the victim, are made to feel as if you are the aggressor and that you should be the one to apologize and provide recourse over and over and over and over again.\nEach time, the eternal question is one of repair. Do I try to mend it again?\n\nThe trajectory of the year was so good until family stuff and I think I had a lot of sadness about how much headspace that took up from then on. I watched myself get more antsy and nervous in the weeks following. I had (and still have) a harder time focusing on work, projects, friends, and my relationship — things I care about deeply.\nAt this point, I’m not sure how much of it to chalk up to moving to a new country, starting a new job, or the trauma itself though it probably all compounds in some way.\nI know that having a healthy relationship with being obsessive about your work requires a baseline level of psychological safety. I know that as a result of events in July, that baseline safety had been compromised.\nI leaned into work as a major distractor, frequently pulling multiple late nights and at times neglecting my own physical and emotional health. I told myself that I knew if I put enough time, this was something I could excel at and if I excelled at it I could finally feel good about myself again. Work became my life and I had a hard time setting boundaries between it and the rest of my life. This became a vicious cycle and as various parts of my life fell into states of disrepair, I felt like everything in my life was going wrong.\nI write this still feeling similarly about the general trajectory of my life. There are always days of normalcy here and there, but I miss how my life was in the first half of the year.\nI’ve sat with this feeling of not-quite-okayness-but-generally-functional for a few months now but too much of it was waiting around for things to calm down or for myself to ‘feel okay again.’\n\n\nLately, I’ve been asking myself how I want to actually spend my time in the coming new year.\nPart of what made the first half of the year consistently so good was that I had enough of a baseline skeletal structure in my week to provide some self-stabilizing feedback. As of now, my weeks pretty much consist of work and sleep (meals aren’t even predictable anymore).\nI think I actually really do like my hobbies which are more solitary. Pottery, climbing, running, and playing guitar consistently bring me a lot of joy when I actually do them. Pottery specifically has a certain comfort to it. It makes me feel like I have a very specific form of agency over a material thing.\nIn keeping with this, here’s a rough list of things I want to do regularly:\n\nMake 50 pounds of ceramics. Throw 40 cups, 20 bowls, and 20 plates.\nRun a half marathon. Go to the gym each Monday, Wednesday, Friday, starting each workout with a run. Make friends to run with!\nPlay guitar every day for at least 30 minutes. Record a demo video a week to track progress. Try and find a small group to jam/make a band with?\nHost some sort of regular co-working/co-writing/co-learning sessions on Sundays in the city. M, R, and S are all interested in this and I really think it could be a backbone in my schedule I look forward to every week.\n\nI have no broader goals outside of these. I think flourishing will stem from having a solid psychological foundation and that’s really all I’m hoping for in the next year.\nKindly,\nYour present self"},"posts/2024":{"title":"2024: Polarity","links":[],"tags":["personal","sapling"],"content":"使用开源软件，安装知识库。"},"posts/Obsidion帮助文档":{"title":"Obsidion帮助文档","links":[],"tags":["Software"],"content":"Ob的使用技巧"},"posts/aesthetics-and-taste":{"title":"Taste is a guide for what is worthwhile","links":["thoughts/Alexandre-Grothendieck","thoughts/The-Writing-Life","thoughts/Scientific-Freedom","thoughts/Overton-Window","thoughts/Simone-Weil","thoughts/Jestermaxxing","thoughts/epistemology","thoughts/self-confidence","thoughts/How-to-do-Nothing","thoughts/agency","posts/pain-and-great-work"],"tags":["fruit"],"content":"\nJohn Reeve (1958-61, 1966)\nTaste goes by many names: intuition, conviction, self-trust, aesthetics. Roughly, it is the ability to make judgements about aesthetic value; to be able to tell the difference between what is and is not beautiful to you.\nIt turns out that the same ‘good taste’ that applies to food, art, or media can also help find what is meaningful and worthwhile to spend our lives on.\nThis is largely because taste operates in two distinct and almost opposite modes:\n\nType I taste: Exploratory taste that surfaces new forms of beauty you otherwise may not have considered;\nType II taste: Conviction that drives you to sufficiently go deep on what you already find to be beautiful.\n\nThe two both need to coexist to find the right balance between exploring more options and sufficiently going deep on promising ones.\nIf we look to deeply creative pursuits like writing and mathematics, we find many case studies where that exploratory form of taste — specifically the pursuit of beauty — helped to create a deeper sort of attention that revealed a deeper, more fundamental understanding of the underlying medium that gave them an edge.\n\n“It is not so much, it seems to me, a so-called “brain power” that makes the difference between this mathematician and another, or between one piece of work and another of a same mathematician; but rather the quality of finesse, of the greater or lesser delicacy of this openness or sensitivity, from one researcher to another or from one moment to another in the same researcher. The most profound and fruitful work is also that which attests to the most delicate sensitivity in apprehending the hidden beauty of things.”\n— Alexandre Grothendieck, Récoltes et Semailles (translation by Childe)\n\nThough Grothendieck refers to math here, the observation about the importance of “apprehending the hidden beauty of things” generalizes. Annie Dillard relates a tale from a fellow writer who was asked a question by a student:\n\n“Do you think I could be a writer?”\n“Well,” the writer said, “I don’t know… Do you like sentences?”\nThe writer could see the student’s amazement. Sentences? Do I like sentences? I am twenty years old and do I like sentences? If he had liked sentences, of course, he could begin, like a joyful painter I knew. I asked him how he came to be a painter. He said, “I liked the smell of paint”\nIf you’re going to be a gardener, it helps to like the smell of roses. If you’re going to be a writer, it helps to appreciate sentences. That, however, only happens once you’ve spent many afternoons reading.\n— Annie Dillard, The Writing Life\n\nFor any creative pursuit, this ability to notice more deeply is what distinguishes good from great. This specific pursuit of what feels beautiful gives rise to a form of attention that starts to reveal the infinite depth of the world. It surfaces nuances you hadn’t seen before even though nothing about your actual senses has changed. You begin to see wider and deeper.\n\nThis exploratory form of taste also affords a certain lightness and freedom to how we play with and consider nascent ideas.\nWhen I find the craftsmanship of one mug more aesthetic than another, I’m not expected to make it legible in terms of what has already been done before. I know that I really like sharp walls that are slightly tapered outwards. They shouldn’t be too thin or the piece will feel flimsy and fragile but it also shouldn’t be too heavy or it will feel like a heavy rock in your hand. I like more speckled clay bodies that are more earthy and warm in tone paired with slightly cooler glazes.\nThis was developed over many hours of actually throwing and making pottery, not from feeling pressured to compare it to what other people have done. I know that if I did feel it, I would have spent a lot more time learning and reading about various pottery techniques, the names of different forms, and what glazes have been popular on social media rather than actually going and working with clay.\nWe have built up an instinctive habit of looking things up and seeing how other people have done it before trying it for ourselves. But the downside is that this habit primes our brains to value our work in the context of the taste of others rather than of our own. We have outsourced our value systems for what is good and bad (how we may judge aesthetic value) to other people.\nIf we outsource our opinions all the time, we no longer exercise our own taste and lose the capacity to derive our own value systems. Like a muscle, it decays without frequent use and we default to liking and valuing what everyone around us already likes or does. As with anything mediating, the act of relying on the taste of society as a proxy for our own effectively serves as a band-pass filter, dampening the range of what our exploratory taste might consider.\n\nLooking at the history of scientific progress, we see plenty of evidence on how this reliance on the taste of committees and society broadly only serves to inhibit progress. Managed creativity can, at best, produce only what its managers specify. All that remains are the ideas that live in the Overton Window.\nWhen we try to limit what scientists work on via broad-stroke directives and awards for very specific kinds of work that consensus deems best for progress, we can get only what we ask for and nothing more. The limit is then what the research director can imagine and nothing more — this presumes the person doing the selecting has the best overall taste as to what is worthwhile. In the consultancy field, the old adage was that successful consultants merely delivered the advice their customers wanted to hear. Do we really want that of our researchers?\n\n“Nature does not respect consensus. We cannot expect to actually make progress simply because we have agreed among ourselves that progress lies in a particular direction. Unless, of course, that direction is a simple extrapolation from what has gone before, but such objectives usually serve only to consolidate — unless researchers are free to follow interesting observations wherever they might lead.”\n— Donald W. Braben, Scientific Freedom: The Elixir of Civilization\n\nBraben, in that last part, emphasizes the importance of freedom to allow scientists to follow their own research taste. Vannevar Bush: “scientific progress on a broad front results from the free play of free intellects, working on subjects of their own choice, in the manner dictated by their curiosity for exploration of the unknown.” Creativity is the essence of the human spirit, and flowers best when it’s unconstrained.\nCultivating that Type II taste — the deep-seated conviction — helps to avoid unnecessary constraints on our exploratory taste. It helps to create their own possibilities for what is possible, outside what other people believe, relying on their intrinsic, subjective judgement of what feels correct. This is what produces novel and ground breaking work, not research mandates or broad directives.\nThe poster in my room above my bed which I got from a booth at the SF Art Book Fair.\n\nSimilarly, Simone Weil believed that we gain insights not by going in search of them, but instead by being receptive to the true nature of the thing itself. In her teaching, she didn’t limit the way students oriented themselves towards problems, noting that most education systems fixate students to an agenda where they are always searching for a singular right answer to the problem rather than taking the time to really feel and explore.\n\n“The great human error is to reason in place of finding out”\n— Simone Weil, Gravity and Grace\n\nThis lightness in the pursuit of insights that Weil teaches is similar to what C Thi. Nguyen calls intellectual playfulness. When an intellectually playful person evaluates new ideas, they don’t care about plausibility or legibility. They care about the idea itself and the thrilling joy-rides of discovery.\nNguyen argues that this intellectual playfulness actually has some clear epistemic functionality for us. A pure ‘truth-seeker’ has an interest in always getting it right. This attitude constrains the search space, focusing searches on plausible paths to explore and discarding what is implausible or beyond the pale. However, because their assessment of plausibility will always proceed from their standing system of beliefs, even their most rational epistemic attempts can be caught in what Nguyen calls an epistemic trap: belief systems that make us shut down a line of questioning before exploring its validity. These traps constrain our exploratory taste.\nOne specific epistemic trap that has been particularly fatal to exploratory taste is the fear of being wrong.\n\n“One who fears to be wrong is powerless to discover. It is when we are afraid of making mistakes that the mistake inside us becomes immovable like a rock. Because in our fear, we cling to what we have decreed to be “true”, or what has always been presented to us as “true”. If we are moved, not by the fear of seeing an illusory security vanish, but by a thirst for knowing, then error, like suffering or sorrow, will cross us without ever becoming frozen, and the trace of its passage will be a renewed understanding.”\n— Alexandre Grothendieck, Récoltes et Semailles (translation by Childe)\n\nIt because of this fear that we default to settle on what is popular or what has been done before. This, to me, is the clearest example of why the ineffability of taste is actually instrumentally useful. What appears to be the ‘optimal’ or ‘right’ thing to work on according to the taste of others may end up in these epistemic traps.\nBy allowing ourselves to refine our own taste and to pursue what we find intrinsically beautiful, it affords a certain lightness to how one might hold ideas and knowledge. It lets us try ideas on for size without needing to immediately jump to explain it to others. Ultimately (and perhaps paradoxically), this lightness allows us to be better seekers of truth.\n\nA common thread to observe here is that a lot of early revolutionary thinkers didn’t worry themselves with what was ‘optimal’ or deemed right by consensus, but rather what was beautiful and interesting. This incidentally helped them to really lean into their taste as conviction.\nAt times, this led them to repeat work that had already been done (like Einstein reinventing parts of statistical physics or Grothendieck working out his own formal definitions for length). But by the time they did hit the bounds of human knowledge, it no longer mattered to them because they just kept doing what they were already doing: following their own taste for what was beautiful.\nThe societal pressure is to conform to what other people think and do; it is always a psychologically risky endeavor to do something new. Thus, any act of creation necessarily involves strong opinions about how the world should work — strong enough to survive the sharp edges of doubt and the false allure of safety in what is known and what has been done.\n\n“Creation is the act of bringing something nonexistent into reality. You have to have strong opinions about how the world should work. You have to think that things should be a certain way, that you prefer this color over that, this form of walking over another, this sense of beauty over theirs, this way of approaching the world over some other. Taste gives you the capability and the urgency to imagine.”\n— Spencer Chang, creative seeing\n\nUnfortunately, the stereotypical advice of developing the capacity to do novel work plays into the lone genius myth. Pablo Picasso: “Without great solitude, no serious work is possible.” James Baldwin: “Perhaps the primary distinction of the artist is that he must actively cultivate that state which most men, necessarily, must avoid: the state of being alone.”\nIn practice, however, what they are referring to as solitude is not being physically isolated but rather more of a state of mind. They are putting themselves in a state where the opinions of others do not bother them and where they reach a heightened sensitivity for the types of questions worth asking — they’ve developed their taste as a form of conviction1.\nHaving strong taste doesn’t mean that you never look to others for inspiration and ignore what others find beautiful. But rather, strong taste gives you the psychological safety to enable you to look to others for inspiration and help you attend to things you otherwise would not have noticed. Strongly developed taste to bring more things into your periphery, it lets us interface with others as potential sparks of inspiration rather than the prime directive for what to believe in.\n\nSo how might we cultivate this taste? Quoting Voltaire: “In order to have taste, it is not enough to see and to know what is beautiful in a given work. One must feel beauty and be moved by it.”\nThe world reveals itself to those who attend to it.\nEach day is an exercise in developing taste. That’s all life is really — a giant sequence of decisions. It is built over time through repeated applications. It requires intention, focus, and care. It’s a process of peeling back layer after layer, turning over rock after rock. It is the manifestation of self-confidence in what we find beautiful and aesthetic.\nDo things without recipes more often. Trod the unbeaten path more often and take note of how things feel. Do you find yourself noticing something that wasn’t there before? That perhaps after a certain number of hours of obsessing over how to get a good tone on the guitar, you can tell how the temperature of the room affects the sound of the strings? Maybe for the first time, you realize that the songs the birds sing are all a little different, the robin sounds different from the chickadee and so on.\nTake a curious and inherently observatory nature toward the world and let it serve as constant opportunities to test your taste.\nWhen I think about cultivating taste, it appears to me much like how George Saunders approaches curating his own taste in good writing:\n\nThe way I revise is: I read my own text and imagine a little meter in my head, with “P” on one side (“Positive”) and “N” on the other (“Negative”)…\nThis involves making thousands of what I’ve come to think of as “micro-decisions.” These are instantaneous, intuitive – I just prefer this to that… I just have a feeling and react to that feeling, in the form of a cut phrase, or an added word, or an urge to move this whole section, and so on. And then I do that over and over, for months, sometimes years, until that needle stays up in the “P” zone for the whole length of the text…\nWith each choice, even the smallest ones, the story becomes more and more…well, it becomes more [yours], you could say. There’s more of [your] essential nature in it, more of what will distinguish [you] from all of those other writers out there. And gradually, the story starts to become something [you] couldn’t have foreseen when [you] started out – bigger, more complex, smarter, funnier, whatever.\n— George Saunders, First Thohts on Reviision\n\nOver time, just like how we anneal metal, our taste becomes more refined and we are more sure in what feels optimal to us. Crafted well, taste gives us the tools to have more agency over our own lives and how we define meaning.\nWhat greater thing could there be to cultivate?\n\n\nThank you to Marley, Anson, Jasmine, Spencer, and Kelly for reading and giving feedback on early drafts.\nFootnotes\n\n\nThough this has some historical precedence for happening through physical isolation (e.g. Newton, Grothendieck, Pascal, Baldwin), I largely believe the lone genius myth is false. The lone genius is a proxy for being a hard worker but it isn’t the only way of doing great work. ↩\n\n\n"},"posts/agentic-computing":{"title":"Agentic Computing","links":["thoughts/agency","thoughts/cozy-software","thoughts/A-Pattern-Language","thoughts/desire-paths","thoughts/DNS","thoughts/peer-to-peer","thoughts/CORS","thoughts/NAT","thoughts/computer-networking","thoughts/Seeing-like-a-State","thoughts/Postel's-Law","posts/towards-data-neutrality"],"tags":["fruit","Rhizome"],"content":"\nThis post is part of an ongoing letter series with Spencer and some other internet friends about what we think the future of the Internet could look like. Find more letters over on our (we)bsite\n\n\nOne of the most important factors as to what gives objects a felt sense of ‘home’ or ‘coziness’ is the degree to which the object involves you in its own completion. The home is a personal statement. The exact layout they choose to layout posters on the wall; the positioning of the plants; how they choose to let light illuminate the hallways; the colours of the walls: these are all expressions of its residents.\nDoes the built environment enable agency for its residents? To have agency is the ability and freedom to act in their immediate context or environment. Thus, to have agency in an environment — regardless of whether the space is digital or physical — first supposes them to have ownership and ability to assert themselves in the space.\nIn the realm of game philosophy, we call the amount of agency the agential distance: the gap that the game designer explicitly leaves for the player to occupy. This gap is shaped by the rules/constraints of the game.\nThis, to me, explains why places like Minecraft hold such timeless feelings of home-ness. It was the first time the world felt malleable and that I could do something about it. In Minecraft, it was the literal act of ‘modding’ the game and its rules to fit how you and your friends want to enjoy it — the game is your own to define how you want to play it.\nLike Calvinball in Calvin and Hobbes, the only rule of the game is that it can’t be played the same way twice! The whole point of the game is that the players come together to actively reconfigure and rearrange the game.\n\n\nToday, it’s no longer a controversial statement to say that, as citizens of the internet, we have lost our ability to shape the web and make it a home — we live on rented ground.\n\nIn the platform giant’s desperate act to serve everyone, they end up serving no one well.\n\n“People cannot be genuinely comfortable and healthy in a house which is not theirs. All forms of rental — whether from private landlords or public housing agencies — work against the natural processes which allow people to form stable, self-healing communities.” (p.393, Your own home, A Pattern Language)\n\nAnd we also know that these rental areas in the physical world are the first to fall ruin.\n\n“Rental areas are always the first to turn to slums. The mechanism is clear and well known. See, for example, George Sternlieb, The Tenement Landlord (Rutgers University Press, 1966). The landlord tries to keep his maintenance and repair costs as low as possible; the residents have no incentive to maintain and repair the homes — in fact, the opposite — since improvements add to the wealth of the landlord, and even justify higher rent. And so the typical piece of rental property degenerates over the years. The landlords try to build new rental properties which are immune to neglect — gardens are replaced with concrete, carpets are replaced with lineoleum, and wooden surfaces by formica: it is an attempt to make the new units maintenance-free, and to stop the slums by force; but they turn out cold and sterile and again turn into slums because nobody loves them.” (p.394, Your own home, A Pattern Language)\n\nHow might we make it possible for the average layperson to be able to change and adapt software for their own needs?\n\nThe wonderful bit about software (and specifically the web) that feels uniquely special is that you don’t need permission to make things. The internet is the first place people got to build real things without needing to ask someone if they could.\nWe should give people the ability to own technology, to bring it into their own complex life stories. We should involve people in its own completion. The real use cases may be the ones waiting to be discovered. One of my favourite examples is the French bistro.\n\nThe small Parisian restaurants serving home-cooked meals in very modest settings—like the cafe before it, was an invention. Many tales exist of its origin. Some say it was working-class landlords opening their kitchens for extra income. Others say it was the Auvergnats, immigrating to Paris from what is today central-south France, who first worked as rag-pickers, then wood and coal sellers, then metalworkers, who created small working-class restaurants to supplement their income. Either way, it was not planned or engineered, but simply not-disallowed. There were no rules in place to stop this invention.\n(Simon Sarris, Welcome, ghosts, emphasis added)\n\nAgentic software designs for and explicitly allows user-made desire paths and folk-usages of software. People will use software in whatever informal, distributed ways that emerge from real world contexts. Folksonomies are a great example of these informal taxonomies developed by users on social sharing platforms. Tumblr tags, for example, have adapted to become not just a form of tagging or organizing, but also metacommentary, memes, and other comedic content.\n\nAn Yokochō alleyway in Tokyo, Japan. There is an inherent smallness to the design that fosters communication and character. Owners say that the smallness encourages genuine communication between staff and customers as well as among characters, much unlike the homogenization of shopping malls and chain stores (paraphrased from Emergent Tokyo: Designing the Spontaneous City). How might we create Yokochō alleys of the internet?\nAgentic software embodies a Hundertwasser flavour of design where:\n\nThe resident has access to the same tools as the architect.\nEverything is writeable, everything is rewriteable.\nPeople can solve their own problems.\n\nFor this to happen, we need to reduce the burden of building software. Not all tools need to be complex power tools that require university degrees to operate. Progress can mean simplifying tools to enable the layman to shape his immediate environment to his taste. How might we make software not just the tools of the engineering elite but the layperson as well?\n\nLike any society, it is not only architects, builders, or engineers that move us towards this collective consciousness. We need people to bring themselves and assume new identities—perhaps where the role of ‘technologist’ is fluid and all-encompassing. Where ‘technologist’ is everyone and anyone concerned with the role of technology, empowered to use it to shape their experience in our pervasive digital world.\n(Chia, There is an internet that is mine &amp; I would like you to live in it with me)\n\nWe must make it possible for the average layperson to be able to change and adapt software for their own needs; for them to experience creating software not like a professional chef, but a home cook.\n\nFirst, we need to address the double-edged sword that is scale. Scale, of course, can be a good thing. Economies of scale enable us to have cheap hosting, comprehensive web search, and many more luxuries we enjoy on the web today. However, the Silicon Valley mindset of always asking what the billion-dollar version of your idea is and how you can get everyone using it is slowly poisoning how we think about software.\nRealistically, when most of us want to create software, the intention isn’t to release something that the whole world will end up using. Yet, with all the knowledge you end up needing to be able to do it, it seems like everything we release into the world needs to be production ready!\nFor someone to make a web service today, they need to know\n\nHTML, CSS, and JavaScript\nHow to pick and choose a hosting provider to put their service on the web\nBasics of DNS so they can use a custom domain\nChoosing a database, host it, and figure out how to safely talk to their database from their service\nHow to talk to APIs without leaking secrets\n… and many more I’m not mentioning here (especially if they choose to make something peer-to-peer)\n\nI spent a lot of time around university-aged students first learning software engineering and there is a really large gap between how easy it is to get a static website on the web and how difficult it is to add a database to it. This, for most people, is where they decide that software is too difficult and give up.\nIt has become so difficult to learn that it has almost killed software’s viability as a tool for expression. Imagine if, every time you cooked a meal for your friends or family, world-class critics came in to judge and prod at your food. Or if, every time you wanted to write a letter to your partner, the postal service would refuse to send it if it contained even a single grammatical error.\n\nLearning how to store passwords or add OAuth2 to your toy web site is not fun. So much of programming today is busywork, or playing defense against a raging internet. You can do so much more, but the activation energy required to start writing fun collaborative software is so much higher you end up using some half-baked SaaS instead.\nWriting a web service for use by your friends should not be a form of combat, where you spend your days worrying about XSS attacks or buffer overflows. You should be focused on creating something new and wonderful in a place without bad people hounding you.\n(David Crawshaw, Remembering the LAN)\n\nIt wasn’t always like this. In fact, the internet used to be pretty flexible. In the days when we still had plenty of IPv4 addresses to hand out to computers, when CORS and NAT hadn’t made the web peer-to-peer hostile, LANs meant it was easy to learn about computers and experiment with things. I could just run something on my machine and open the port to my computer from my router and anyone in the world could see it.\nUnfortunately, we can’t just naively revert back to this. It is no secret that our modern internet is peer-to-peer hostile. Almost all of the communication we do on the web is fully client-server because that was the easiest way to make things secure and work.\nPart of this comes down to the entrenched nature of how the web is structured. Our browsers and home computers can only speak and request from services, but we’ve lost the ability to listen for others and serve services of our own. Security is a hard problem to solve and there are a lot of malicious people on the web. But going down this path closes many doors for what we could be doing with the internet.\nHow do we make making on the web easy and fun again?\n\nFrom the blogpost introducing CoCo\n\nIt’s true that networks are fundamentally sloppy and all sorts of broken. But broken does not mean unmendable.\nThis latter point is one that Chia talks about in her articulation for a future of the internet. To declare it unmendable or unfixable is to abandon the many people that still need these deeply broken technologies.\nStarting a new system from scratch with a grand vision is not the way to do this either. History has shown us that trying to purge everything and build out a totalizing vision can have terrible implications, regardless of whether they succeeded or not.\nTo reform the web is not to wipe everything and start over tabula rasa, but rather to move through the adjacent possible, figuring out how we can improve the existing condition of those trapped by these systems without uprooting them.\nThe internet is based on Postel’s Law: work the world as it already is, not as you wish it were. In fact, this is how the Internet today evolved. It was bootstrapped on top of existing telephone networks, exapting existing phone hardware to get it off the ground. It didn’t need to deploy expensive new hardware or lay down new cables, it just conformed to existing infrastructure.\nJust as the early internet was built on top of telephone networks, we can build a new set of cozier, smaller networks on top of an internet that is showing its age.\nMaybe we bring back the philosophy of LANs, but rather than networks based around closeness in physical distance bounded by routers, we created networks based around closeness in social and trust space1? What about Communal Computing Networks?\n\nWe could have:\n\nCountless local networks, many overlapping with each other.\nA larger network of networks to allow for cross-network collaboration.\n\nThis is a design pattern we have seen work well in the past with email and Matrix. These platforms often function as networks of networks, allowing communities within them to have control over their own smaller networks but still allowing users on different providers to interact with each other seamlessly.\nWith this infrastructure in place, we can also think about what community owned applications may look like. Perhaps this is infrastructure that will allow us to not necessarily decentralize but rather decenter servers, moving them from the source of truth to a supporting role.\nPerhaps this looks like people having ownership of their own data. They could use servers to help make it available to their peers when they are offline but they are never essential to people accessing their own data.\nYour peers, friends, and colleagues could help replicate, host, and process indexes for data they are also interested in. A big community corkboard of all the things the group may be interested in. In this world, anyone could write applications that pull in data from these indexes, making it easy to experiment and just make things that work on the web.\n\nAs we spend more time on the web, it’s clear what may have worked for a smaller web no longer works today. In today’s web, the powerful become more powerful, the rich become richer. The day-to-day users have no say over the terms of service we are served. We live in a feudal web.\nIt used to be the case that you needed to train to become a scribe to write words for any reason. But just as pens were taken out of the hands of the scribe during the Reformation of Europe, we must take the code out of the hands of software engineers and share it with the masses.\nWriting software shouldn’t take a degree and many years of training, it should be as simple as making a meal at home or writing a letter to a friend. Doing so will lead to a more diverse and resilient internet, with a greater variety of voices and perspectives represented so we may build an internet that works for us.\nLet’s make the web feel agentic again.\n\nHello stranger! If you’re still reading by this point, then you’ve probably been thinking about similar things for a while. I want to extend an open invite to you to lend your thoughts too.\nJoin us as we write about what it would be like to make these fictions become reality because the way these ideas become powerful and revolutionary is to have more people contribute to them.\nThanks again to Anson, Spencer, and Vivian for some really clarrifying feedback.\nFootnotes\n\n\nIn a magic world where IPv6 was adopted by everyone, every computer and device would have a unique address to send and receive things from. It would enable people to host things again, to have their own little home dinner parties instead of always going out to the restaurant. Unfortunately, IPv6 adoption has been really slow and we’re stuck in a world where IPv6 isn’t widespread enough to assume that most users have an IPv6 address as of yet. ↩\n\n\n"},"posts/agi":{"title":"Humanistic AGI","links":["thoughts/GOFAI","thoughts/funding","thoughts/neural-networks","thoughts/NFAI","thoughts/potemkin-village","thoughts/intentionality","thoughts/connectionist-networks","thoughts/representation","thoughts/explainability","thoughts/frame-problem","thoughts/Heidegger","thoughts/context","thoughts/multiple-realization"],"tags":["fruit"],"content":"This blog post is adapted from a term paper I wrote for PHIL250: Minds and Machines at UBC.\n\nIntroduction\nHistorically, development of AI has taken a very specific approach — systems that represent the world through symbols and manipulate those tokens in a systematic way to arrive at a result. This type of AI was coined Good Old-Fashioned AI (GOFAI) by John Haugeland1.\nThis worked well up until around 1984 when the field entered an ‘AI Winter’, a long plateau in progress that was most likely due cynicism in the AI research community that trickled to media and funding bodies, halting research and development2.\nHowever, with the rise of Moore’s Law and the insane amount of compute and data available, a new approach to the development of AI arose — one that focused on statistical methods and connectionist networks like artificial neural networks2. Haugeland1 dubbed this approach to AI design New Fangled AI (NFAI).\nThis paper will examine factors that differentiate GOFAI and NFAI systems, such as their ability to adapt to changes in input, and the explainability of their outputs and internal representations. It will also examine current work in integrating the two approaches to Artificial Intelligence to create an artificial general intelligence.\nGOFAI Systems\nSince the inception of the term GOFAI, the basic idea has remained unchanged: thinking as internal symbol manipulation. Within these GOFAI systems, symbols are representative of aspects of our world. These symbols are manipulated in a systematic and logical matter, performing a series of deterministic steps that results in another sequence of symbols1.\nA very common example of GOFAI systems are expert systems, which are computer systems that emulate the decision making ability of a human expert3. They solve problems via decision-tree reasoning, figuring out whether to perform certain actions based off of if-then rules.\nHowever, just being able to solve a problem shouldn’t be sufficient for intelligence. So what qualifies it? At its core, GOFAI can be considered ‘artificially intelligent’ because of semantic interpretation. If the symbols represent aspects of our world, the result, which is also a symbol sequence, can be translated back into aspects of our world. This is called semantic interpretation, which “seeks to construe a body of symbols so that what they mean (‘say’) turns out to be consistently reasonable and sensible, given the situation”1.\nNFAI Systems\nNFAI, on the other hand, is a diverse and still rapidly evolving set of systems and algorithms. It is more of a grab-bag term, roughly meaning any sort of scientific mind design that is not GOFAI1. Under this umbrella are connectionist networks, which are networks composed of lots of simple units that are interconnected with various strengths. This paper will mostly focus on connectionism as a synecdoche for the greater umbrella of NFAI.\nSome classic examples of connectionist networks include convolutional neural networks (CNNs), which are a form of image classifiers4. These networks operate by applying filters or kernels to an input between layers of the network. Each of those filters have their own set of strengths that will learn and evolve over time to identify certain ‘features’ from the input. Similar to cell assemblies in animal perceptual systems, these filters assemble more complex patterns using smaller and simpler patterns5.\nThese connectionist networks are very inspired by the structure of the brain, with its hierarchical patterns and compositional nature6, rather than the rational manipulation of symbols that is observed in GOFAI.\nThe Potemkin Village Analogy\nWhile it is obvious that GOFAI and NFAI are very different approaches to constructing AI systems, how do they differ in their resilience to failure? An analogy that may be useful in visualizing this is a potemkin village: a fake village that is built to resemble and deceive others into thinking it is real. AI systems attempt to build a sort of ‘potemkin village’ that “works well on naturally occurring data, but is exposed as fake when one visits points in space that do not have high probability”7.\nGOFAI systems are excellent at “processing syntactical patterns like those characteristic of logical formulae, ordinary sentences, and many inferences”1, but are also very narrow-minded and vulnerable when it comes to unexpected variations or oddities in the input given. The potemkin village that a GOFAI system may construct will hold up if only seen from the intended angles, but any slight deviation from an intended or expected input would shatter the illusion immediately.\nNFAI systems, on the other hand, are “adept at finding various sort of similarities among patterns, at recognizing repeated (or almost repeated) patterns and filling in missing parts of incomplete patterns”1. These also happen to be the exact things that GOFAI systems struggle with. The potemkin village that a NFAI system may construct will hold up much more robustly to unexpected patterns or noisy input, but will, at heart, still be a fake village.\nRationality and explainability\nIn GOFAI systems, intentionality — the meaning and semantics behind the tokens — is injected through explicit programming by those who create it. These GOFAI systems are able to process these tokens and make conclusions based off of logic and reason rather than just trial-and-error. Case in point, expert systems. These if-then statements can easily explain decisions by showing which parts evaluated as true or false in its decision making process3.\nConnectionist networks, for the most part, are very hard to explain and are often dubbed black-box models due to the hidden nature of its internal workings. Unlike GOFAI systems, its internal representation model is defined by the state of the entire network rather than that of any single unit — this is commonly referred to as a distributed model of connectionist representation8 and is often claimed to be one of the distinctive features of connectionism.\nModels of representation\nTo put it in sound terminology, note while in the GOFAI system, the tokens are the objects of formal processing, so the system which manipulates the tokens is the actual vehicle of computation. The tokens themselves are also representations of aspects of the world, so they are also vehicles of mental content. In GOFAI systems, tokens are both the vehicle of computation and the vehicle of mental content.\nThis is in contrast with connectionist systems, where computation is performed at the level of simple units (unit activations, backpropagation), meaning the units are the vehicles of computation. However, as these systems use a distributed model of representation, it is not a single unit that represents something, but rather the “network state as a whole thats interpreted as representing”8. Thus, in connectionist systems, the vehicles of computation (units) need to be the vehicles of representation (network state).\nIntegrating GOFAI and NFAI\nGiven that GOFAI and NFAI systems seem so vastly different in their approaches to AI, how might one go about reconciling them?\nOne approach is to combine both into one system. This is used when there’s a rational, known, and algorithmic way to process a subproblem. Systems like AlphaZero, a connectionist based Go playing system, use mixed systems to achieve the level of performance they report. Although at heart, AlphaZero uses a deep neural network to assess new positions, it also uses a Monte Carlo Tree Search (a GOFAI algorithm) to determine its next move based of the assessment of the neural net9.\nAnother, less researched method, are interpretable connectionist systems. As traditional connectionist networks rely on the network state being the vehicle of representation, the complexity, depth, and scale of modern connectionist models means that it is becoming increasingly difficult for humans to interpret the output. The field of explainable AI (XAI) focuses on incentivizing connectionist networks to develop localist representations (i.e. moving away from having the vehicle of representation be at the network level, but at the unit level). Zhang, Wu, and Zhu of UCLA recently showed that it is possible to train a CNN to use ‘interpretable filters’, which encourage networks to group feature detectors into single filters, showing the possibility of moving from distributed representations to more local representations5.\nWhat is AGI?\nWhile intelligence can be understood in many ways, this paper will focus on examining the prospects of emulating or achieving the capacity to understand or learn anything a human can — the hallmark of an artificial general intelligence (AGI).\nMost commentators would agree that current AI systems fall short of implementing general intelligence4. These are narrow AI systems, which are used to accomplish or solve specific tasks like the game of Go or language translation, rather than to attempt to create a system capable of AGI. So, what’s stopping us from making the transition from domain-specific algorithms to domain-general algorithms?\nOne problem that stumped earlier attempts at AGI was the common-sense problem: how do we represent common-sense information that is obvious to most humans in a way that is accessible to AI systems that use natural language? Unsurprisingly, the problem of storing all of this information was solved by the massive explosion in compute and data in the past few decades2. However, the difficult part of this problem, choosing what subset of that huge information bank is relevant in any situation, remains a huge unsolved problem. How do we update our database of knowledge when relationships between symbols change? This is referred to as the frame problem.\nDissolving the frame problem\nDreyfus10 posits that any AI systems which attempt to tackle the frame problem through storing relevant frames are bound to failure. He argues that, “human beings do not simply store common-sense information,” rather they “directly perceive and act upon significance in their environment”. In his view, a more Heideggerian approach to AI will dissolve this problem.\nHeideggerian AI, in its most basic sense, is concerned with the Heideggerian concept of Dasein, which literally means ‘Being-there’11. Through the use of this expression, Heidegger calls to attention the fact that a human cannot exist or be taken into account without existing in context of a world with other things — “to be human is to be fixed, embedded, and immersed in the physical, literal, tangible day to day world”12.\nDreyfus believed that, for any AI system to achieve any sort of general intelligence, it must also exhibit Dasein. Thus, “a successful Heideggerian AI would need a perfect model of the human body – and by implication, that Dasein must be expressed as a human being, organically as well as existentially”10.\nA non-humanistic approach\nHowever, Steed refutes Dreyfus’ overly humanistic interpretation of Heideggerian AI, believing that a AI model only needs to be “embedded and embodied such that what AI experiences is significant for AI in the particular way that AI is,” and thus intelligence would be possible by Heideggerian standards13.\nThe refutation against a purely anthropocentric view of AI brings to light an important concept: the multiple realization argument. Emulating or copying human intelligence isn’t the only way to achieve intelligence that rivals that of humans.\nContemporary AI systems are almost always used as a problem solving tool, a means to tackle uniquely human problems and to convey results that are semantically useful to us. As a result, these approaches are doomed to be constrained by human problems. This is the essence of the bitter lesson of AI. However, if we look outside the anthropocentric view of intelligence, AI systems may not share these human problems with us and “perhaps an authentic, free AI system does not converge to a solution that is interpretable from a human standpoint at all”13.\nAI is already capable of learning, adaptation, and basic Being-in-the-world. Thus, to achieve general intelligence, we should allow AI to contemplate its own problems and existence.\nFootnotes\n\n\nHuageland, John. (1996). What Is Mind Design? Mind Design II, doi:10.7551/mitpress/4626.003.0001. ↩ ↩2 ↩3 ↩4 ↩5 ↩6 ↩7\n\n\nHendler, J. (2008). Avoiding another AI winter. IEEE Intelligent Systems, (2), pp. 2-4. ↩ ↩2 ↩3\n\n\nJackson, Peter (1998). Introduction To Expert Systems (3 ed.). Addison Wesley. p. 2. ISBN 978-0-201-87686-4. ↩ ↩2\n\n\nBuckner, C. (2019). Deep learning: A philosophical introduction. Philosophy Compass, 14(10), e12625. ↩ ↩2\n\n\nZhang, Q., Nian Wu, Y., &amp; Zhu, S. C. (2018). Interpretable convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 8827-8836). ↩ ↩2\n\n\nChurchland, P. (1990). Thinking: An invitation to cognitive science. Vol. 3., pp. 199-228. ↩\n\n\nGoodfellow, I., Shlens, J., &amp; Szegedy, C. (2014) Explaining and harnessing adversarial examples. ArXiv Preprint ArXiv: 1412.6572. ↩\n\n\nCrane, Tim. (2003). The Mechanical Mind. doi:10.4324/9780203426319. ↩ ↩2\n\n\nSilver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., … &amp; Lillicrap, T. (2017). Mastering chess and shogi by self-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815. ↩\n\n\nDreyfus, Hubert L. (2008) Why Heideggerian AI Failed and How Fixing It Would Require Making It More Heideggerian. The Mechanical Mind in History, pp. 331–362., doi:10.7551/mitpress/9780262083775.003.0014. ↩ ↩2\n\n\nSolomon, R. (1972), From Rationalism to Existentialism: The Existentialists and Their Nineteenth Century Backgrounds, Harper &amp; Row, New York. ↩\n\n\nSteiner, G. (1978), Heidegger, The Harvester Press Limited, Sussex ↩\n\n\nSteed, R. (2019). AI is Heideggerian Enough, But Can It Be Authentic? Unpublished manuscript, Carnegie Mellon. ↩ ↩2\n\n\n"},"posts/bft-json-crdt":{"title":"Building a BFT JSON CRDT","links":["thoughts/collaborative-software","thoughts/Order-theory","thoughts/distributed-systems","thoughts/linearizability","thoughts/A-Certain-Tendency-Of-The-Database-Community","thoughts/consistency","thoughts/the-garden-and-the-stream","thoughts/clocks","thoughts/Byzantine-Faults","thoughts/33-percent-Impossibility-Result","thoughts/Byzantine-Broadcast","thoughts/Asymmetric-Key-Cryptography","thoughts/CRDT","thoughts/CALM-Theorem","thoughts/consensus","thoughts/I-Confluence","thoughts/PSL-FLM-Impossibility-Result","thoughts/Public-key-Infrastructure"],"tags":["fruit","technical","Rhizome"],"content":"\nCRDTs are a family of data structures that are designed to be replicated across multiple computers without needing to worry about conflicts when people write data to the same place. If you’ve ever had to deal with a nasty git merge conflict, you know how painful these can be to resolve.\nCRDTs mathematically guarantee that an application can safely update their local state without needing to coordinate with all of its peers. By avoiding the extra coordination overhead, they have very good latency properties and work well in scenarios where real-time collaboration is needed (e.g. text editing, presence, chat, etc.).\nOver the past few years, really cool open source CRDT libraries like Automerge and Yjs have emerged that enable developers to easily add these replicated data types to their own applications. Their support for JSON means that most web-applications can just plug-and-play, enabling collaborative experiences to be created easily.\nWhat normally would have taken weeks or months of engineering to setup infrastructure for a collaborative web experience can be done in a day, bringing us one step closer to a future where the internet feels more default-multiplayer, cozy, and alive with life.\nI learn best through building so I set out to write my own CRDT library from scratch to grok what was going on under the hood. When I was first learning about it, I spent a good few months scratching my head trying to read the papers. A lot of the literature took a long time for me to understand and required me to learn a non-trivial amount about order theory and distributed systems.\nI write this blog post mostly as a note to my past self, distilling a lot of what I’ve learned since into a blog post I wish I had read before going in. I hope you find this useful too.\nAs a brief warning, the target audience of this blog post are people who have worked with a bit distributed systems in the past and is curious about the realm of CRDTs. I try my best to explain any relevant terminology when it comes up but familiarity with the topics helps a lot!\nThis blog post is really long so use the Table of Contents at the top to jump to whatever section interests you the most!\nHow CRDTs differ from traditional databases\nBefore we really dive into CRDT internals, we first need to understand how they are different from databases. When I normally think of shared state, I think of databases. However, the guarantees that databases provide are really different than the ones CRDTs provide.\nTraditional databases focus on a property called linearizability, which guarantees that all operations behave as if executed on a single copy of the data. We call this canonical view the primary site. Every read in a linearizable system, no matter what database node you read from, gives you an up to date value.\n\nThis is great for allowing developers to reason about distributed applications more easily (you just treat your distributed database like a single database). There are no conflicts by definition because there is only one authoritative view on what the right state is.\nHowever it isn’t without downsides either. Achieving this property adds a lot of overhead because both writes and reads need to coordinate (the dashed lines in the diagram above) across all database nodes to ensure consistency. This creates an availability problem: if you can’t reach a majority of your nodes, you can’t process any operations.1\nCRDTs kind of throw all of that out the window and embrace the eventual nature of the real world. In an often cited essay titled A Certain Tendency Of The Database Community, the author argues that trying to provide “single system image” semantics (read: linearizability) is fundamentally flawed and at odds with how systems operate in the physical world. The garden and the stream.\nIn the real world, it takes time for us to learn about things that are happening around us and around the world. It takes time for our mail to send, calls to go through, and light to travel between servers across the world. We can take inspiration from the real world and embrace a design that considers every member in the system as the primary site for the data that it generates.\nThis means we allow peers to actually have different states as long as they eventually converge to a correct result. By relaxing the constraint on needing a globally consistent result, we remove the need to wait for all of our replicas to agree. In more formal distributed systems language, we trade linearizability for a property called strong eventual consistency:\n\nAll updates will eventually reach every node\nEvery node that has received the same updates will have the same state\n\nWhen should we use strong eventual consistency over linearizability?\nIt turns out that this is good enough most of the time. Conventional wisdom in the database world would agree that perfect data consistency is way too expensive in terms of both latency and bandwidth if changes happen frequently. Using eventually consistent approaches generally work better as temporary inconsistencies work out in most cases.\n\nTwo users might simultaneously withdraw money from an account and end up with more money than the account ever held. Would a bank ever want this behavior? In practice, yes. An ATM’s ability to dispense money (availability) outweighs the cost of temporary inconsistency in the event that an ATM is partitioned from the master bank branch’s servers. In the event of overdrawing an account, banks have a well-defined system of external compensating actions: for example, overdraft fees charged to the user.\nfrom Eventual Consistency Today: Limitations, Extensions, and Beyond\n\nWhat actually is a CRDT\nOk so after about 1000 words, you’re probably asking “wtf is a CRDT??“. Maybe we should define that now.\nCRDT stands for conflict-free/commutative/convergent replicated data type. Funnily enough, there is no strong consensus on what the C actually stands for2. Regardless of the name, the tldr; is the same:\n\nYou can write and read from your local copy of the data without needing to coordinate with other peers\nOver time, all nodes converge to the same state by sending each other state modifications/updates they have performed on their local data\n\nBecause of the eventual nature of message delivery, there is no guarantee that the state across all the actors are consistent at any given moment\nIf the CRDT is written properly, a view of a CRDT can only ever be out-of-date, but never incorrect\n\n\nEach operation contains the necessary metadata to figure out a deterministic way to merge any operations that may happen concurrently\n\nI’ll note here that the term ‘conflict-free’ is a little misleading. It’s not that conflict doesn’t ever occur, but rather that CRDTs are always able to determine how to resolve the conflict up front (without user intervention)\n\n\nCRDTs always try to preserve user intent and try not to lose data whenever possible\n\nIf two people insert a character into a string at the same spot, it will try to keep both edits\nNote that these is inherently different from consensus methods. Collaboration involves keeping all edits and merging them. Consensus involves picking one of several proposed values and agreeing on it\n\n\n\nAgain, CRDTs are a family of data structures. There is no one single CRDT. You can make CRDTs that produce single values (registers), lists, maps, graphs, JSON, and many more I haven’t listed here. However, they can’t represent everything. A fundamental limitation here is that there are certain types of data structures (like sets) that cannot be made into CRDTs3. We’ll talk more about these limitations later.\nNow equipped with a 30,000ft overview of CRDTs, let’s dive into how they resolve conflicts.\nOrdering Messages\n\nA small note: this part will be pretty heavy on theory. If that’s not your cup of tea, you can just assume that there exists a way to order operations and skip to the section titled Intuition behind CRDTs\n\n\nThere is a whole branch of math focused on how to order things called order theory. In the case of message ordering, we want to define some way to compare messages such that no two messages are considered chronologically equal (in academic terms, we define a total ordering). If we can do so, we’ve mathematically avoided conflicts. So how do we do that?\nYour first intuition may be to just use clocks. However, it would be naive to just randomly trust two different clocks from two different machines. Clocks can drift out of sync, leap seconds happen, and users can change their system time. Keeping time is famously hard.\nIf we can’t trust actual clocks, what can we do? Well, we can use Lamport timestamps. These track logical time rather than actual wall time, meaning we count number of events that have occurred rather than seconds elapsed.\nThis timestamp is just a simple counter. For the rest of the blog post, we refer to this counter as the sequence number seq.\n\nAll nodes start their counter at 0\nEvery time we perform an operation locally, we increment our counter by one\nEvery time we broadcast a message to our peers, we attach this counter\nEvery time we receive a message, we set our timer to max(self.seq, incoming.seq) + 1\n\nIf a.seq &gt; b.seq then event a must have happened after event b. However, if a.seq == b.seq, we cannot be sure which event came first. This means that Lamport timestamps only give us a partial ordering, which means two identical sequence numbers might not correspond to the same unique event. For example, both nodes could emit an event with seq = 1 even though they are different events.\n\nThankfully, we can actually create a total ordering of events in a distributed system by using some arbitrary (but deterministic) mechanism to break ties. For CRDTs, if we give each node a unique ID, we can actually tie-break on this ID to provide a deterministic way to order concurrent events.\nIn pseudocode, we can create a comparison operation that looks something like the following:\n// We assume this is unique for every node\ntype AuthorID = u8;\n \n// A lamport timestamp\ntype SequenceNumber = u8;\n \n// A CRDT Operation\nstruct Op&lt;T&gt; {\n\tauthor: AuthorID,\n\tseq: SequenceNumber,\n\tcontent: Option&lt;T&gt;\n}\n \n// Compare based off of sequence number\n// If there&#039;s a tie, tie-break on unique author ID\nfn happens_before&lt;T&gt;(op1: Op&lt;T&gt;, op2: Op&lt;T&gt;) -&gt; bool {\n\top1.seq &lt; op2.seq ||\n\t\t(op1.seq == op2.seq &amp;&amp; op1.author &lt; op2.author)\n}\nOrdering solved!\nCausality\nOr so we thought… Let’s think about when it is safe for us to apply an operation that we have received locally.\n\nSay that the largest sequence number we know of is 3. We receive an operation with sequence number 5. We know that we are missing the operation with sequence number 4. Can we still deliver 5?\nIf 5 does not causally depend on 4, then we actually can still safely apply this operation.\nBut we can’t figure out this causality information from just sequence numbers alone. If an event A causes another event B to happen, then a.seq &lt; b.seq. However, we don’t know the converse. That is, if a.seq &lt; b.seq, we cannot say that A caused B to happen.\nIt turns out the fix for this is actually quite easy. As we have a unique way of identifying each operation, we can just send along a list of operations it causally depends on. That way, if we receive a message and we know we’ve received all of its causal dependencies, it should be safe to deliver.\nIf we receive a message where we haven’t received all of its causal dependencies, then we can add it to a queue so that when the message does get delivered, we can apply it afterwards. In the above example, if message 5 marked 4 as a causal dependency, it would wait until 4 was delivered before also applying 5.\nThis means that as long as we declare the right causal dependencies, we can make certain things that don’t seem commutative (like list operations) actually commute.\nIntuition behind CRDTs\nOkay enough about theory, how do we actually go about making a CRDT?\nLet’s survey what we have at our disposal so far (our assumptions):\n\nWe can reliably send messages between peers\nWe have a total ordering on the messages so we don’t get any conflicts\nWe have some way of indicating causal dependencies\n\nThis gives us an ever-growing pile of messages that are deemed ‘safe’ to apply. Here is probably a good time to make a distinction between the data in the CRDT itself and the view of that data.\n\nThe data itself are the operations that are coming in\nThe view is the data structure we compute from it (and what applications end up seeing)\n\nstruct CRDT&lt;T&gt; {\n\tdata: Vec&lt;Op&lt;T&gt;&gt;,\n}\n \nimpl&lt;T&gt; CRDT&lt;T&gt; {\n\t// modify self.data to include op\n\tfn apply(&amp;mut self, op: Op&lt;T&gt;);\n \n\t// traverse self.data to produce the data structure\n\t// we&#039;re actually interested in (a list in this example)\n\tfn view(&amp;self) -&gt; Vec&lt;T&gt;;\n}\n\nSpecifically, we never4 delete any operations from the internal data representation. The best we can do is mark them as deleted using a tombstone. Because a peer could technically reference any past operation as a causal dependency, we need to keep that metadata around.\nOf course, we can’t go about willy-nilly trying to model every data structure this way. We can only have CRDTs of data structures which have invariants that do not depend on knowing the most up to date version of the data structure. From before, CRDTs are always guaranteed to have a correct state, but they may not have the most up-to-date value. This means that receiving any new operations should never break a CRDTs invariants.5\nAn example of something that CRDTs cannot model is an account balance that never decreases below zero. Say that you have $100 in an account. You spend 70 on your laptop and another 40 on your phone at the same time. Without waiting on the other transaction to arrive, there is no way for the CRDT to know whether these are valid! Even though both transactions are valid on their own, when done concurrently, they decrease the value to a negative value. Thus, CRDTs cannot model anything that requires maintaining global invariants.\nList CRDTs (RGA Explained)\nThis seems to lead us to the biggest problem in the room: list CRDTs.\nWhen you think of the API for a list, most operations are normally done by indexing. But isn’t this a global invariant? Don’t we need to know what other operations have been done to the list to figure out what the index of each character is?\nThis is not an easy problem to solve. It may also be a reason as to why the vast majority of CRDT projects focus on lists or text editing (which is essentially a list of characters). Luckily for us, some smart people have figured it out for us so we can look to what they’ve done for inspiration.\nThe key insight here is that instead of using relative addressing (e.g. positional indexing), we can use absolute addressing (e.g. using IDs).\nEvery time we insert an element into the list, we need to know the ID of the character we are inserting after. Then, we just place it somewhere between that element and the element following it.\nSo instead of saying “insert ‘A’ after the character at position 4” we say “insert ‘A’, with ID 5, and insert it after the character with ID 4”. This matches our intuition for how text editing happens anyways. When we imagine inserting text, we are inserting it after something. It wouldn’t make sense to insert the character “a” of “cat” before we insert the character “c”.\nWe can imagine saying that “c” caused “a” which caused “t”. Conveniently, we can encode this causality by making the causal parent of each item the character it was inserted after. Remember causal dependencies? Yeah those.\n\nThis forms a sort of causal tree and this is effectively how RGA, a list CRDT, structures its data internally. Let’s modify our Op to match that:\ntype OpID = (AuthorID, SequenceNumber);\nstruct Op&lt;T&gt; {\n\tid: OpID,\n\torigin: OpID, // causal dependency\n\tauthor: AuthorID,\n\tseq: SequenceNumber,\n\tcontent: Option&lt;T&gt;,\n\tis_deleted: bool // tombstone as we can&#039;t actually remove items\n}\nWhen inserting a character we\n\nFind the origin (causal dependency). If it doesn’t exist, we queue it up for later.\nStarting at this node, all of its siblings were inserted concurrently. We look through the list of siblings until we reach a node that we are greater determined by the happens-before comparison we defined6. Recall that we sort operations first by their sequence number then tie break by the author ID.\n\n\nIn the example above, &#039;s&#039; has &#039;a&#039; as its causal origin. Thus, we look at where to insert it in the list of children of &#039;a&#039;. As &#039;t&#039; and &#039;b&#039; both have smaller sequence numbers, we skip past these. Both &#039;s&#039; and &#039;p&#039; have a sequence number of 5 so we tie-break on the AuthorID. As 1 &lt; 2, we insert &#039;s&#039; between &#039;b&#039; and &#039;p&#039;.\nTo get the actual list this CRDT represents, we perform an in-order traversal over the tree and only keep nodes that are not marked as deleted.\nAdding Byzantine Fault Tolerance for free (almost)\nUp to this point, the CRDTs we’ve covered all work in trusted scenarios. These are scenarios in which you know all of the people participating and you trust them to not screw up the system for everyone else. For example, in a collaborative text document, you may limit collaborators to immediate colleagues who you trust to run the CRDT algorithm correctly and not attempt any funny business.\nHowever, most of the interactions we have online are not in trusted scenarios. Luckily, we have servers to help mediate these interactions, dictate what is and what isn’t possible. Peer-to-peer systems, on the other hand, cannot rely on nodes always behaving the way the designers of the system intended. Here, we would like to be able to guarantee that the system continues functioning correctly even in the face of some nodes crashing, failing, and even acting maliciously.\n\nThe CRDTs we have been exploring so far do not work in these untrusted scenarios. That is, any one node can do something bad and cause state to diverge permanently. Not good.\nTo be more concrete, here are some examples of what a malicious actor can do:\n\nSend malformed updates\nNot forward information from honest nodes (an eclipse attack)\nSend invalid updates\n\nMessages that have duplicate IDs\nSend incorrect sequence numbers (an equivocation attack)\nClaim to be another user\n\n\n\nIdeally, we want to adapt our existing CRDT algorithms to be resistant to these attacks while still allowing honest nodes to continue business as normal. This would allow us to use CRDTs in untrusted environments which opens up the door to lots of cool applications like games, social, and more.\nTo borrow a term from distributed systems, we want to make our CRDTs Byzantine fault-tolerant.7 The ‘Byzantine’ part of the name comes from the Byzantine Generals Problem, a situation where, in order to avoid catastrophic failure of the system, the system’s actors must agree on a concerted strategy, but some of these actors are unreliable and potentially malicious.\nNotation wise, we denote the total number of nodes in the system n. We denote the number of faulty/Byzantine nodes f. Most consensus algorithms claim a tolerance of up to f&lt;3n​, which means they can tolerate up to |33% of the nodes being faulty. However, CRDTs require an even weaker bound. Remember that we are not trying to achieve consensus! All we really need to do is make sure that Byzantine actors can’t interrupt honest nodes from functioning properly.\nIn fact, we can reduce this to a problem called Byzantine Broadcast. Dolev-Strong, back in 1983, showed that it was possible to tolerate up to f&lt;n faulty nodes! This means that, as long as there are honest nodes and they are connected to each other, they can still function properly.8\nKleppmann detailed an approach in his paper Making CRDTs Byzantine Fault Tolerant that works without changing the internals of most CRDTs; it can be fully retrofit on top of it. We can create a ‘BFT adapter’ layer between the transport and application layer that is responsible for filtering out any Byzantine operations.\n\nThis approach has two major components we need to grasp:\n\nHow we ensure Byzantine nodes don’t tamper with messages and pretend to be someone else (hashes + signed message digests)\nHow we ensure messages don’t get blocked from reaching honest nodes (eager reliable causal broadcast with retries)\n\nHashes as IDs and Signed Message Digests\nSo here’s a little trick we can do. Our only requirement for operation IDs is that they uniquely identify a node. We can generate this ID by SHA256 hashing parts of the operation:\n// Set the ID to the hash of its contents\npub fn set_id(&amp;mut self) {\n\tself.id = self.hash_to_id()\n}\n \n// SHA256 computation over an operation\npub fn hash_to_id(&amp;self) -&gt; OpID {\n\tlet fmt_str = format!(\n\t\t&quot;{},{},{},{},{}&quot;,\n\t\tself.origin, self.author, self.seq, self.is_deleted, self.content\n\t);\n\tsha256(fmt_str)\n}\nThen, to check if an operation is valid, we can just hash the contents again and see if it matches the ID. If a Byzantine actor tries to change any of these properties while trying to pass it off as a different ID, there will be a hash mismatch.\nHowever, how can we be sure that a Byzantine actor hasn’t just pretended to be someone else? After all, they could just create an operation, set the author field to someone else, then hash the content so that there is no mismatch between hash and ID.\nLuckily, we can use public key cryptography to help us out here. Each node has a private key they keep to themselves. Earlier, we also mentioned that each node should have a unique identifier. We can actually use the public key of each node as its AuthorID.\nThen, whenever we send a message to another peer, we hash op.id to create a digest, and then sign it with our private key. This way, we can verify that the person who signed the message is the actual author.\n\n// Create a digest to be signed\nfn digest(&amp;self) -&gt; [u8; 32] {\n\t// note that self.dependencies here is *different* from self.origin\n\t// this will allow us to indicate causal dependencies *across* CRDTs\n\t// which we will cover in more detail later\n\tlet fmt_str = format!(&quot;{},{},{}&quot;, self.id(), self.path, self.dependencies);\n\tsha256(fmt_str)\n}\n \n// Sign this digest with the given keypair\nfn sign_digest(&amp;mut self, keypair: &amp;Ed25519KeyPair) {\n\tself.signed_digest = sign(keypair, &amp;self.digest()).sig.to_bytes()\n}\n \n// Ensure digest was actually signed by the author it claims to be signed by\npub fn is_valid_digest&lt;T&gt;(op: Op&lt;T&gt;) -&gt; bool {\n\tlet digest = Ed25519Signature::from_bytes(&amp;self.signed_digest);\n\tlet pubkey = Ed25519PublicKey::from_bytes(&amp;self.author());\n\tmatch (digest, pubkey) {\n\t\t(Ok(digest), Ok(pubkey)) =&gt; pubkey.verify(&amp;self.digest(), &amp;digest).is_ok(),\n\t\t(_, _) =&gt; false,\n\t}\n}\nOk great! We now have a unique way to identify both peers and operations. But there is one more thing here that is a little pesky to deal with: mutability.\nWhen we create references to operations in this internal representation (e.g. we need to declare a causal dependency), we expect that the OpID of an operation stays static throughout its lifetime. However, because we now set OpID to be the hash of the content, updating a node would change its OpID all the time!\nDoes this mean we have to give up hashing the message content for fault tolerance? Fortunately not! There is one tiny hack that allows us to still use this hashing approach.\nInstead of making a copy of the original operation and just changing the content (which would cause the operation to be considered invalid by our hash check!), we can produce an entirely new operation that has the original ID as a causal dependency. These ‘modification’ operations don’t actually get included in the internal representation of the CRDT, rather they modify it directly.\nWe can look to our list CRDT for an example. This delete function produces an entirely valid operation. This also makes sense from a causality perspective, we need to have delivered the original before trying to delete it!\nfn delete&lt;T&gt;(&amp;mut self, id: OpID, keypair: &amp;Ed25519Keypair) -&gt; Op&lt;T&gt; {\n\tlet mut op = Op {\n\t\tid: PLACEHOLDER_ID,\n\t\torigin: id, // the actual operation we are deleting\n\t\tauthor: self.our_id,\n\t\tseq: self.our_seq + 1,\n\t\tis_deleted: true,\n\t\tcontent: None,\n\t\tsigned_digest: PLACEHOLDER_DIGEST,\n\t};\n\top.id = op.hash_to_id();\n\top.signed_digest = op.sign_digest(&amp;keypair)\n\tself.apply(op.clone);\n\top\n}\nWhen we apply it, we treat these modification events differently from insertion events. We look for the origin and just update its deleted field.\nWith that small problem solved, we know that our operations are now tamper resistant!\nEager Reliable Causal Broadcast and Retries\nNow, we just need to make sure that there is a way to get messages between honest nodes such that Byzantine faulty nodes can’t block it.\nThe easiest (and probably most naive) way to do this is through eager reliable broadcast:\n\nEach time a node receives a message with an OpID it has never seen before, it re-broadcasts that message to all its peers it is connected to\nIf we have been missing a causal dependency for a while, occasionally ask our peers if they have it\n\nThis makes sure that as long there is a connected subgraph of honest nodes, they can all still communicate.\n\nHowever, there are a lot of potential optimizations to make here. It may be reliable but we broadcast O(n2) messages for each actual operation. This is really expensive and may flood the network!\nThankfully, we can take inspiration from git to figure out how to do this more efficiently. Kleppmann, again, mentions this approach in Making CRDTs Byzantine Fault Tolerant.\n\nUsing cryptographic hashes of updates has several appealing properties. One is that if two nodes p and q exchange the hashes of their current heads, and find them to be identical, then they can be sure that the set of updates they have observed is also identical, because the hashes of the heads indirectly cover all updates. If the heads of p and q are mismatched, the nodes can run a graph traversal algorithm to determine which parts of the graph they have in common, and send each other those parts of the graph that the other node is lacking.\n\nThis project does not include this more advanced hash graph reconciliation but it is a direction for future work.\nA JSON CRDT\nOk, we’ve seen now how we can create a Byzantine Fault Tolerant list CRDT. How can we make a JSON CRDT out of this?\nNormally with JSON CRDTs, we just have a bunch of nested CRDTs.\n\nValues are LWW registers\nLists are RGA lists\nMaps are lists of key-value pairs\n\nEach nested CRDT also keeps track of its path (e.g. inventory[0].properties.damage) so that when CRDTs produce an event, this information is also included. This ensures that peers know how to route a message to the right CRDT.\nHowever, we have to be careful about how we store this path. We can’t naively just use the index into the list as we saw earlier how this is unstable. A small workaround here is having the OpID be the index.\nAdditionally, in addition to the origin field, we added a dependencies field. The distinction between the two is that the origin field is for same-CRDT causal dependencies whereas the dependencies field allows for cross-CRDT dependencies. This is important if we want, for example, an inventory update to be causally dependent on a LWW register CRDT update.\nThere is one last challenge to account for: JSON has no schema; data types can change! For example,\n\nA sets &quot;a&quot;: [&quot;b&quot;]\nB sets &quot;a&quot;: {&quot;c&quot;: &quot;d&quot;}\n\nHow do we resolve this? The way Automerge and Yjs resolve this is by essentially using a multi-value register: they keep both values and punts the responsibility to the application choose the right answer.\nBut wasn’t the whole point of CRDTs that there is no conflict? With most applications, allowing users to set arbitrary JSON is not actually desirable. We can somewhat mitigate these problems by allowing the application developers to define a fixed schema ahead of time and validating all operations through this9.\nPutting it all together into a crate\nWhat if we took advantage of the type-safety and metaprogramming abilities of a language like Rust to automatically derive these strict-schema BFT CRDTs from programmer-defined data structures?\n#[add_crdt_fields]\n#[derive(Clone, CRDTNode)]\nstruct Player {\n\tinventory: ListCRDT&lt;Item&gt;,\n\tx: LWWRegisterCRDT&lt;f64&gt;,\n\ty: LWWRegisterCRDT&lt;f64&gt;,\n}\n \n#[add_crdt_fields]\n#[derive(Clone, CRDTNode)]\nstruct Item {\n\tname: LWWRegisterCRDT&lt;String&gt;,\n\tsoulbound: LWWRegisterCRDT&lt;bool&gt;,\n}\nAfter 5000 words of writing, I present to you the bft-json-crdt Rust crate. By using the provided CRDT types, programmers can instantly add CRDT functionality to their project.\nThere is a clear boundary between BFT and non-BFT operations as distinguished by data types to make sure that you don’t accidentally apply an operation to the wrong place.\n// initialize a new CRDT with a new keypair\nlet keypair = make_keypair();\nlet mut base = BaseCRDT::&lt;Player&gt;::new(&amp;keypair);\nlet _add_money = base.doc.balance.set(5000.0).sign(&amp;keypair);\nlet _initial_balance = base\n\t.doc\n\t.balance\n\t.set(3000.0)\n\t.sign(&amp;keypair);\n \nlet sword: Value = json!({\n\t&quot;name&quot;: &quot;Sword&quot;,\n\t&quot;soulbound&quot;: true,\n}).into();\n \nlet _new_inventory_item = base\n\t.doc\n\t.inventory\n\t.insert_idx(0, sword)\n\t.sign_with_dependencies(&amp;kp1, vec![&amp;_initial_balance]);\n \n// do something here to send _new_inventory_item to our peers\n// and on a remote peer...\nbase.apply(_new_inventory_item)\nFinally, I want to leave a word of warning. This is not a production ready library by any means. Although I do think this is a very solid proof of concept to demonstrate the potential of BFT CRDTs, it was still first and foremost for educational purposes.\nI do not consider myself to be proficient at Rust so there might be lots of bad code smells/mistakes sprinkled throughout (please do PR if you have any fixes/suggestions)!\nFuture directions for CRDTs\nThe field of CRDTs is still quite young. I really think there is a lot of promising work being done in exploring how CRDTs can be used to enable collaboration on the web.\nJames Addison has been working on creating a real-time 3D engine using CRDTs. Projects like BLOOM work on trying to figure out at compile time what parts of program state require coordination and what parts don’t.\nI’d love to see more exploration of CRDTs applied to games and other real-time things. I think there’s a lot of really cool work to be done with interpolation between operations. Think GGPO or perfect-cursors but for general CRDTs.\nI hope that this blog post is a jumping point for new people interested in the space to really get their hands dirty and see what they can do with this technology. Again, checkout the codebase if you are interested in the internals and feel free to try to tackle anything under the ‘Further Work’ heading in the README!\nAcknowledgements\nIf you are still reading by this point, I want to give you a huge thank you.\nThis was probably the most technically difficult project I’ve ever attempted, let alone finished. I felt like my conviction in my own abilities was tested multiple times and I can say I came out of the other side a better engineer. Thank you to the people who supported me as I struggled and stumbled around while trying to figure this project out.\nThere are a few people I’d like to thank individually. Thank you to Anson for listening to my long and incoherent rambles and celebrating my small wins. Thank you to Scott Sunarto and James Addison for proofreading. Thank you to Nalin Bhardwaj for helping me with my cryptography questions and Martin Kleppmann for his teaching materials and lectures which taught me a significant portion of what I’ve learned about distributed systems and CRDTs.\nFootnotes\n\n\nThere are ways you can mitigate this by ‘predicting’ a successful outcome. However, if the actual write/read fails, we may need to rollback what the user sees which is not ideal from a user experience perspective ↩\n\n\nNB: there is actually two main subtypes of CRDTs. CmRDTs or commutative replicated data types are based off of exchanging messages which contain single operations. CvRDTs or convergent replicated data types send their whole state. They are also sometimes called operation-based and state-based CRDTs respectively. The rest of this blog post assumes CRDT to mean operation-based CRDTs. ↩\n\n\nThe CALM Theorem states that anything that is logically monotonic (read: append-only) can be made into a CRDT. Non-monotonic things may ‘retract’ previous statements. ↩\n\n\nGarbage collection + rebalancing technically requires achieving consensus on nodes in order to do this. “So, as far as I know, we would need a consensus protocol attached to the CRDT in order to get garbage collection / compaction.” (#2) ↩\n\n\nThis is formulated in a more formal manner in I-Confluence ↩\n\n\nNB: performance enthusiasts will be quick to point out how to make this go faster. This is a terribly unbalanced tree. On average, it takes O(n) to find the origin and another O(n) to insert into the tree. There are clear optimizations to be made here. Yjs uses a doubly-linked list for a faster insert. They also use a cursor to track the last ~5-10 visited positions. It makes the assumption that editing patterns are not random (which is true for most applications). Diamond Types and the new Automerge use a range-tree to achieve O(logn) find and O(logn) insert ↩\n\n\nFor those coming from a more traditional distributed systems context, I will clarify that BFT means something different from a consensus context. Traditionally, this means getting some n−f nodes to agree on a particular value. However, because CRDTs focus more on collaboration than consensus, we just want to prevent Byzantine actors from disrupting the functioning of the system. Byzantine actors can still send a bunch of ‘valid’ updates that may be unwanted. Imagine you sent a Google Docs link to a group of people and one of them is mucking around and inserting a bunch of images and removing information from the document. These would all be considered ‘valid’ operations in the sense that a ‘correctly’ functioning node could have done the same thing. ↩\n\n\nSee notes on the PSL-FLM Impossibility Result. We use a PKI assumption to get around this result. ↩\n\n\nAs pointed out by Bartosz Sypytkowski, this assumes that the schema is static and never changes, which may be not the case in many practical scenarios. Additionally, because of the nature of CRDTs, schema updates may be acknowledged by peers in different points in time. This is an ongoing area of research ↩\n\n\n"},"posts/bias-bug":{"title":"On AI's 'Bias Bug'","links":["thoughts/bias","thoughts/data-distributions","thoughts/Goodhart's-Law","thoughts/context","thoughts/fairness","thoughts/black-box","thoughts/explainability","thoughts/trust","thoughts/Design-Justice"],"tags":["fruit"],"content":"This blog post was originally intended to be a TED Talk given at TEDxRedmond but I unfortunately was never accepted. However, I did learn a lot in the process of writing it, and I hope you get some value out of it even its presented through just text.\n\nHey Google\nTake a picture in 5 seconds. Say cheese!\nSee that? This is just one of many examples of where AI is becoming a larger part of what we accept as normal. What seemed far-fetched and straight out of science fiction a few years ago is now becoming a reality.\nToday, tech giants deploy AI to dictate what we see, hear, buy – even feel and think. They control what kind of news we see every morning, suggest places to go, and even drive our cars. In order for us to have a healthy relationship with this technology, we have to adapt as quickly as it is advancing. In doing so, we need to ask ourselves: what kind of decisions AI should be allowed to make?\nSure, most of you in the audience may be comfortable letting an AI decide what kind of music to add to your Spotify playlist, but when an AI needs to decide what kind of jail time a criminal should face we all get a little squeamish.\nIn fact, let me give you a few examples of decisions that AI make every single day and see whether you would be comfortable with an AI making that same decision.\nGoogle AI Blog, 2018\nWould you be comfortable with an AI helping doctors to identify cancerous tumours?\nsurvivalofthebestfit.com\nHow about an AI helping companies decide who to hire based on only your name, age, gender, and resume?\n@teenybiscuit\nThe classic image detection example: are these chihuahuas or muffins?\nIllustration: Simon Landrein\nHow about the trolley problem? Should a self-driving car — given no other option — kill A) the child or B) the elderly person?\nAs you can see, there is a really obvious difference each of those decisions that were proposed. In the tumour example and chihuahua example, you likely weren’t super bothered if an AI were to make that decision. Yet for the hiring example and the self-driving car example, you likely were more uncertain.\nWhat we can see from this is that there is a difference between objective and subjective problems. In the tumour and chihuahua examples, we were mostly comfortable with that decision being made because there is a clear ‘right’ or ‘wrong.’ However, in the case of the hiring example and self-driving car example, the subjectivity makes it difficult. A lot of it has to do with what kind of environment we were raised in and how each of us sees the world. Everyone is born with some kind of bias, favouring certain ways of viewing the world.\nAI can do a lot of really great things such as helping doctors identify tumours or interpret the world for the deaf. But when used improperly, that subjectivity can propel some of the worst biases we have as humans.\nGarbage in, garbage out\nThere’s a timeless saying “garbage in, garbage out” in the field of Computer Science which essentially states that bad data or bad input will produce an output that’s of equal quality. This holds true for almost all the tech we use today, from trading algorithms to search results. If what we put into the system is inherently unclear or flawed, then the output will also give back something that’s ‘wrong’ or doesn’t align with our objectives.\nHowever, this saying “garbage in, garbage out” is most prevalent in the AI which sits at the forefront of this tech revolution. AI, in one form or another, is still created by humans, who are imperfect, make mistakes, and are inherently biased.\nInterestingly, there are two distinct ways that this bias can shine through.\n1. Problem definition\nThe first is in the problem definition. When creating an AI, we need to define an objective for it. That means putting something vague like “create a realistic human-sounding voice” or “help me translate this speech to French” into definitive, and certain terms and mathematical concepts. How do we do that? Because we don’t have algorithms that do this step for us, this is usually done by a team of machine learning engineers. They are responsible for deciding how to represent our ‘objective’ in terms of penalties and rewards. This also means that how the team of engineers decide to represent the problem is a product of their biases.\nTake Amazon for example. In 2014, Amazon decided to create a recruitment engine that was able to look at a job applicant and rate them from a one-star rating to a five-star rating. However, by 2015, Amazon realized that their software was not evaluating candidates for positions related to tech in a gender-neutral way1. Although unintentionally, Amazon’s engineers included a gender field. The algorithm, after sifting through 10 years worth of resumes, began to favour men and penalize women. After deeper inspection, this was most likely an unfortunate reflection of the male-dominated tech industry. The lesson is clear. The algorithm served to reflect this bias that was observed in the past.\n2. Lack of Data Diversity\nThe second, less obvious way bias can poison AI is with data diversity — or rather the lack of it. I think the best way to explain this is through a metaphor. Imagine the AI as a small child. It likes to learn from its environment. If this child were to be raised in a racist family, it will almost undoubtedly hold similar views in the future. This is a very similar case for AI. It learns from the environment and data it’s given.\nOne case of this is the very first iteration of Google Photo’s image classification feature back in 2015. This feature claimed to be able to identify people, places, and things with high accuracy. Twitter user @jackyalcine found that the algorithm identified people with darker skin as gorillas2. Google quickly was able to work and manually ‘patch’ the issue, but the actual issue was much deeper – and it had to do with the data used. In this case, the dataset that Google used to train their algorithm had an over proportional amount of middle-aged Caucasian people under the category of ‘people.’ This meant that while the recognition accuracy was really high for that select group of people, the accuracy for people of colour was significantly worse.\nFrom both cases, we can see that an over-focus on results and accuracy can cause these companies to ignore these biases. When the deadline is too tight or the manager sets an expectation for a “10% increase in accuracy,” there is a very strong incentive to ignore the ‘edge cases’ or things that happen very rarely.\nToo often our society is focused on the raw accuracy that we forget that the same accuracy metric is something that we set for ourselves – created arbitrarily by humans which have bias (see: Goodhart’s Law).\nUnfortunately, this results in things like the Google Photos and Amazon Hiring cases.\nFairness in AI\nTruth is, data lacks context. While the trends in the data may show that in the past there have been more men in the women in the workforce, the majority of the population can agree that we are moving away from that more traditional view into more of an equal playing ground. Unfortunately, these models that we create don’t have a deeper understanding of these changes and as a result, produces naïve predictions that we believe are wrong or ‘garbage.’ But is it really? Is it really only a ‘bad’ result because of what we define as fair or right?\nProPublica, 2016\nWe can take a look at the COMPAS system which is a piece of software used by U.S. courts to assess the likelihood of a criminal to reoffend. ProPublica, a non-profit newsroom, did an investigation back in 20163 and claimed that COMPAS was biased against those of African descent — citing that it overestimated the false positive rate of reoffending for those of African descent by almost twice as high as those for Caucasians. ProPublica reasoned that a fair algorithm would not have such a big difference.\nSo, is COMPAS fair?\nWell, there’s no concrete answer. The algorithm never had any access to any contextual information about the neighbourhoods or the actual situation for each of the offenses. Was the area more heavily policed because it was a predominantly black neighbourhood? Were the officers themselves biased in making arrests?\nEven more interestingly, a study done at Dartmouth4 showed that random volunteers, when given the same information as the COMPAS algorithm, achieved a nearly identical accuracy of identifying the rate of recidivism.\nThis is interesting. This means that either COMPAS is accurate or holds the exact same biases as we do as a society. Unfortunately, this is a problem that I don’t think we can solve, so we don’t have a solid definition as to what makes an algorithm fair. But what is clear, is that there is bias in play here, whether that be through the police, the companies, the actual algorithm, or society itself. This bias is what causes that “subjectivity” and “garbage in.” This is what is preventing us from making ‘fairer’ AI and applying AI to more tasks.\nThe inevitability of bias\nThe point is, unless we work to prevent, catch, and deter bias, it will inevitably occur. One of the biggest problems in the field of AI is that so many of the models exist in a black box, meaning that its inner workings are only known by a select few. This makes it near impossible to identify and train out bias. Machine intelligence will become almost integral to our lives, becoming less visible in the process, and AI’s bias bug will get harder to beat. Our time to act is now.\nWhat we can do\n\nSo? What can we, as the next generation, do to help?\n\nFirst, we need to build a better understanding of how the AI systems we are building work. Through this understanding, we can better trust and, as a result, effectively manage the emerging generation of AI. So, don’t be afraid to learn that programming language you heard about. Read up on how that cool translation algorithm works.\nSecondly, we need to diversify. Diversify not only in the sense of having better representation in datasets, but in tech. Next time you pitch a new product, or create a new project, ask yourself this:\n\n“How many people have you considered before you make that decision?”\n\nLet’s design for all. Consider people of different ethnic groups, sexualities, income, just to name a few. By having more representation in these teams, AI can cater to more than just that select group of people who are western, educated, and rich. Instead, by bringing in a fresh perspective on the problems that these teams are trying to tackle, they can create innovation that benefits a whole range of communities.\nWith increased diversity and representation, the Google Photo misidentification problem never would have happened. Together, we can help to build a future where diversity is no longer an issue in both machine learning models and tech, but society too. Where we don’t just use AI mindlessly but understand it and use it in such a way that it helps to empower humanity. Where we can work towards a future where we can begin to trust the more subjective decisions that an AI can make.\nTogether, we can make that future a reality tomorrow. Thanks for coming to my TED talk (unironically this time).\nFootnotes\n\n\nNews coverage on Amazon Hiring case ↩\n\n\nNews coverage on the Google Photos case ↩\n\n\nProPublica Analysis ↩\n\n\nDartmouth Study on COMPAS accuracy ↩\n\n\n"},"posts/bm/博客管理":{"title":"博客管理","links":[],"tags":["博客管理"],"content":""},"posts/casual-magic":{"title":"Casual Magic","links":[],"tags":["fruit","writing"],"content":"A small collection of poetry about magic in the everyday\n\nchild of the light\nPhotography literally means &#039;writing with light&#039;.\nFrank Lloyd Wright understood this\nhanging light like pictures on a wall,\nknowing it to be as important a material as brick and wood.\n\nThere&#039;s a certain beauty to the shimmering\nof blades of light as it cuts through the dust of a family home;\nthe way it twinkles as it touches the drop of rain and,\nat just the right angle, blossoms and blooms into all the colours of the world;\nhow it paints the mountains and valleys a smattering of different colours,\na different palette every morning;\nenthralling people by the way the rays filter through the trees\nand bounce off the waves.\n\nEratosthenes used light to figure out\nthe size of this little ball of earth we stand on.\nI think often about how each ray of light\ntravelled 8 minutes and 20 seconds,\nburned its way through the atmosphere,\nbounced off countless people, houses, trees,\njust to be stopped by our eyes,\nunbashed and perfectly content at its destination.\n\nLight enchants at every scale it is perceived.\nThe way it dances and shimmers, you would think it had a life of its own.\n\nI am a child of the light,\nstriving to live each day with the beauty of the cozy light of a sunset,\nthe confidence of a single photon traveling 100 million miles to its destination.\n\n&#039;Striving&#039; is the keyword.\nOn some days, my eyes get moist,\nacquiring the same glistening as water droplets falling from the sky.\nIt is on these days that the light is especially beautiful,\ntaking on an almost ephemeral and magical feeling.\n\nI borrow strength from the light:\na gentleness of the touch,\na twinkling of the soul,\na lightness of the self.\n\nI see the light as beautiful\nmaybe the light will see me as beautiful too.\n\nsun(set/rise)\nThe shape of the sun is a contour, carved by its intensity, angle and number of\npeople who are awake to see it. In the evening, the sun sets the sky ablaze\nwith the vibrant shades of a ripe peach; a final spectacle.\nSunset: a time of day when a significant portion of all\npeople on Earth all glance at the same thing and\nadmire its beauty. A small act of cohesiveness\nand shared beauty in a world that\ndesperately looks for things to\nthings to glue it\nback together.\n\nAt night,\na million\nstars pierce\nthe darkness.\nA stillness,\nilluminated\nby none other\nthan a\nreflection of\nthe sun.\n\nIt is not by chance\nthat the first piece of art\nmade by a human in space is of \nthe orbital sunrise. A promise that \nthe day will start anew, a sign that tomorrow exists, \nthat things dormant can wake once again, showing us that\nsomething as immovable as the heavens and the cosmos can still change.\nAs the rays peek over the crests of mountains and waves, there is a warmth that wakes the world again.\n\nstargazing\n    From   far   away,\n    it   looks   like   we   are   huddled   close.\n    A   community   of   stars,\n    clustered   and   cozy.\n\n\nYet   the   distance   between   us\nis   so   vast   that\ntravelling   at   the   speed   limit\nof   the   universe\nwould   still   take   billions   of   years\nto   traverse.\n\n\n\tPeople   used   to   tell   stories\n\tmillennia   ago\n\tabout   how   stars   are   just\n\tholes   in   the   blankets   of   the   sky,\n\tthere   so   the   light   of   day\n\tcan   peek   through   at   night.\n\n\nI   wish   we   could   pull   the   blankets   closer,   to   keep   us   warm.\nAll    we    can    do    is    longingly    gaze,\nwatching     us     each     shift     red,\nhoping      that      one      day      our      dust      will      meet.\n\ncasual magic\nAt 1:00, the sun streamed at just the right angle through the patterned glass,\nsplitting the uniform beam of light into a hidden spectrum of fractals,\nbathing the room in a shower of brilliant colours.\n\nThe dripping of the water from the faucet was always erratic,\nnever settling on any sort of discernible pattern.\nDrip drip       drip     drip drip         drip    drip            drip     ,\nIt was always 8 drips of the faucet before each perennial was well watered.\n\nThe clouds seemed to agree that today would be a great day to read at sunset.\nA friend once commented how they found books so weird,\nyou just stared at marked slices of a tree for hours, vividly hallucinating.\nI joked how the beach matched the cover of my book; they laughed,\nour bodies bathed in the mellow yellow that seemed to melt the world around us\nuntil it was just us, the ocean, and the setting sun.\n\nThere was a cozy spot to put my head,\njust below the PULL FOR STOP cord,\nbetween the metal frames of the bus windows,\nenough space for me to have an earbud in.\nI would spend the 40 minutes bus ride shuffling for new songs,\nsinging under my breath,\nfeeling and voicing the shape of each word,\nwatching the clouds slowly losing their tinges of colour as night fell."},"posts/cm/搜藏管理":{"title":"搜藏管理","links":[],"tags":["搜藏管理"],"content":"\nGithub搜藏的一些学习资源\n\n\n贵校课程资料收集 https://lib-pku.github.io/\n"},"posts/collaborative-thinking":{"title":"Collaborative Thinking","links":["thoughts/collective-intelligence","thoughts/academia","thoughts/language","thoughts/zero-sum","posts/digital-gardening"],"tags":["fruit"],"content":"\nHumans rarely think for themselves. Rather, we think in groups. Just as it takes a tribe to raise a child, it also takes a tribe to invent a tool, solve a conflict, or cure a disease. No individual knows everything it takes to build a cathedral, an atom bomb, or an aircraft.\n\nIt is no secret that the key to the rise of Homo Sapiens and the anthropocene wasn’t due to the rationality of any individual human, but rather our collective unparalleled ability to think and share knowledge in large groups. This concept has arisen in the form of specialization of labour.\nAs a society, we consistently rely on the knowledge of others to live our own comfortable lives. I may not know how to grow my very own russet potatoes, but a farmer in Alberta might. Similarly, a plumber in Massachusetts may not know how to build their own website, but I might. Through being able to supplement each other’s knowledge of the world, our collective intelligence is much greater.\nLeft: Collective knowledge of the group. Right: My subset of knowledge\nI like to think about collective knowledge as a big blob. Each individual contributes a unique subset of knowledge. We get the entirety of human knowledge by taking the union of the knowledge of all the individuals in the group. Of course, as these groups of individuals grow ever larger due to globalization, each individual may choose to specialize in a narrower subset of knowledge as generations go on to reduce redundancy. Why know how to build your own car when you can buy it yourself? The adoption of globally accepted currencies has made this easier than ever.\nKnowledge illusion\nHumans also have this ‘knowledge illusion’ where we think we know a lot, even though individually we know very little. We treat the knowledge of the human collective as if it were our own, even subconsciously.\nA crude illustration of a jacket zipper\nAn example Harari used was the sweater zipper. If I were to ask you if you knew how a zipper works, the vast majority of you would exclaim “yes, of course!” Yet, if asked to describe in detail every single step, most would fail to do so. Even with something that seems so basic and intuitive seems to elude an explicit explanation. We have begun to stand on the shoulders of giants yet refuse to acknowledge their presence.\nAlthough we may have increased the overall area of our collective knowledge, the surface area of each individual has also shrunken, turning from balanced and broad to narrow and unwieldy.\nRebalancing our blob of knowledge\nAnother interesting property about the knowledge blobs of individuals is that they are magnetic. I mean this in the sense that individuals that have one blob of knowledge tend to attract and be attracted to individuals with similar orientations and shapes in their blobs of knowledge, much like how magnetic dipoles align in a magnet. As humans, we tend to want to minimize our cognitive dissonance and surrounding ourselves with like-minded individuals is the easiest way to do that.\nIndividuals in these clusters experience a sort of echo chamber effect whereby the magnitude of their knowledge is amplified through the mutual alignment of their knowledge. However, this also poses a unique challenge where movement only happens in one direction and there is little to no room to deviate from that direction and try something new; something incredibly dangerous for innovation. This, in a sense, is turning collaborative thinking into groupthink.\n\nPeople afraid of losing their truth tend to be more violent than people who are used to looking at the world from several different viewpoints.\n\nBlue: Previous ‘specialized’ knowledge. Purple: broad foundational knowledge\nWhat we can do to counteract this extreme alignment is to build a broader foundation of knowledge. When you come across an individual with differing views, hopefully you will at least have the base fundamental knowledge to understand their perspective.\nLearning communities\nThis, at least in part, is why I’ve recently become more certain of wanting to go to academia in the future. I used to have tunnel-vision in thinking that all I wanted to do in the future was to just work in industry CS. Recently, I’ve started to realize that CS not a single discipline, but rather it’s a tool that can help solve uniquely human problems, and these human problems are inherently multidisciplinary.\nI’ve started to read and learn more about the world around me outside of my little bubble of CS-related topics and it’s been eye-opening to see issues I read about in my philosophy class come up in a linguistics lecture which in turn comes up in a book I’m reading. I’ve found that the best way for me to cement my learning and understanding is through discussion with people, rather than just sitting and ruminating on my own — a very different pace than the typical code-Stackoverflow-copy-repeat self-learning cycle that most programmers (including myself) are familiar with.\nThe important part of learning communities like colleges is not necessarily the alignment in what you’re studying, but rather in the shared mindset of discussion, learning, and understanding. To broaden my foundation of knowledge is to read more about the opinions and findings of others and to critically discuss these among peers who may have different views. If we want our specialized knowledge to be applicable in a wide range of situations, we need a broad foundational base that can support that.\nWiggle room\n\nIf you want to go deeply into any subject, you need a lot of time, and in particular, you need the privilege of wasting time. You need to experiment with unproductive paths, explore dead ends, make space for doubts and boredom, and allow little seeds of insight to slowly grow and blossom\n\nFor me, learning is very close to a zero sum game with my time. I can only expand the area of my knowledge blob so fast. If I want to broaden my foundational knowledge base, I need to reign in the amount of time spent growing purely technical strengths and to stop saying ‘yes’ to any and every opportunity that comes up.\nSlowly but surely, I’m learning to value my own time and to set it aside to just absorb more about the world and to extend the reaches of my knowledge just a little bit further. To dilly-dally among the Wikipedia rabbit holes, cultivate my digital garden, and faff among the ridiculously long list of side projects I planned to start. I’m experimenting with what I previously thought were deadends and little seeds of insight are starting to grow. Maybe I’ll find something interesting to share among the construction of this broader foundation.\nAcknowledgements\nA big thank you to Anson for always being a sounding board for fresh dough (half-baked ideas would be too generous of a description for these). Thanks to Anne and Joice for also giving feedback on rough drafts :))"},"posts/context-collapse":{"title":"Framing and Context Collapse","links":["thoughts/introductions","thoughts/identity","thoughts/social-contracts","thoughts/Asch-conformity-experiments","thoughts/communities","thoughts/Vanilla-Ice-Cream-effect","thoughts/ephemereal-content","thoughts/feedback-loops","thoughts/Dark-Forest-Theory-of-the-Internet","thoughts/Internet","thoughts/bandwidth"],"tags":["fruit"],"content":"\nThe content I throw on Twitter is not something I would readily post on LinkedIn. I also wouldn’t post LinkedIn-style business content to my Facebook timeline. To an extent, my social presence across these platforms is incongruent. The core tenet is that different apps curate different types of communities and thus social contexts. As a result, we try to separate these contexts in the best way we know how to: by acting differently, and that’s ok.\nIt is not ‘inconsistent’ for us to want to frame ourselves differently on Twitter than on Facebook, and in fact, a lot of key aspects of human behaviour point to why we default to doing so.\nThe problem comes when these spaces start bleeding into each other without our control, and the boundaries between the different audiences on each of them start amalgamating. The overlap in target audience between these spaces then becomes very large and all of these different social contexts ‘collapse’ and it becomes increasingly difficult to maintain these separate personalities.\nWhat’s (not) in the frame\nIn this vlogbrothers video from March, John Green talks about the metaphorical frame in which people present themselves. We tailor and adjust our behaviour depending on the environment we’re in, or the audience we find ourselves in front of. However, in doing so, we inadvertently portray an incomplete picture – we frame only the parts we want them to see (see also: introductions).\n\n“I mean of course the central trick of the social internet, is that whenever you make something, you choose what’s inside the frame, but as viewers the rest of us can’t help but believe what’s inside the frame, because it’s literally all we can see”\n\nAn unfortunate side effect of this is that it starts setting expectations for the audience as to how we ‘normally’ behave. This ‘audience’ of people can be friends, followers, or strangers. They are a group of people that pay attention to what you have to say because of your thoughts, opinions, and general persona. So what happens when we change?\nIf we’re expected to keep growing as people, these thoughts and opinions will obviously change over time. So why does it feel so weird, for both the audience and ourselves, to deviate from these set identities that we’ve become used to portraying in certain scenarios?\nContext separation\n\nIn order to answer that question, let us make a brief foray into the realm of psychology. Why do humans tend to act differently in different environments? I think three key aspects of behaviour play a role here:\nPeople are unique\nNo one individual has the exact same interests, goals, values, and identity as you. People find spaces where they can authentically express all the dimensions of themselves. If a situation only exercises one axis of our self-identity, we will actively seek out places to exercise other axes.\nBehaviour is sticky\nIt is easier to travel along a path well-travelled than one covered in weeds and bush — if you do something once, it becomes easier to do it again. Similarly, doing something with a certain context once means it becomes easier to repeat that same behaviour in that context again. If I use Twitter as my social media of choice to post memes, I’m likely to continue using it like so.\nIn some ways, this is Newton’s First Law but applied to behaviour: if you do something one way, you are likely to keep doing things in that way unless encouraged to do otherwise.\nIndividuals tend to conform to group norms\nIn 1951, Solomon Asch conducted an experiment to investigate the extent of which social pressures can affect a person’s behaviour. In this experiment, he asked, “Which line is the longest out of the three given?“. In an individual setting, 99% of people answered correctly. However, when put in a room of actors instructed to answer incorrectly, nearly 75% of all participants gave at least one incorrect answer throughout 12 trials.\nPeople conform for two main reasons:\n\nThey want to fit in with the group (normative influence)\nThey believe the group is better informed than they are (informational influence)\n\nHistorically, having different behaviours in different contexts may have been evolutionarily beneficial, helping us to better collaborate and form connections with each other without causing undue conflict.\nEven as children, when we get hurt, we look to authority figures around us to see how to react. If our mother comes to us expressing worry and concern, we cry. If everyone around us continues to play as if everything is normal, we will brush aside the injury and continue to play with the group.\nOur ability to ‘frame’ ourselves different depending on the social situation likely stemmed from this behaviour too.\nWhy context collapse is bad\n\nWhen experiencing context collapse, talking to your audience beyond a surface level becomes very difficult. Smaller groups, by nature, have more commonalities to relate to, whether that be interests, music taste, place of study, or lived experiences. These groups provide the chance for individuals to have intimate conversations and exercise different facets of themselves that they otherwise wouldn’t get the chance to. A queer person may be out to a close group of friends but not to their family; a budding writer can have a safe place to explore potential ideas without alienating the rest of their friend group.\nWhen we experience context collapse, these social groups and audiences become massive, and the amount of overlap in interests or values of the collective group is miniscule. How are you supposed to have any sort of meaningful connection when the only thing you share in common on the platform is that you ‘know’ each other?\nCentrifuging contexts\nI’ve noticed three main strategies individuals, companies, and platforms have adopted in trying to tackle context collapse.\n\n\nLowest Common Denominator (Vanilla Ice Cream effect). Only making posts that anybody will be able to understand and relate to, staying away from controversial or overly personal topics. This approach places an emphasis on quantity of impressions over quality of impressions.\n\n\nRealtime. Platforms and individuals are leaning more towards ephemereal content, like stories or streaming. The ‘live’ aspect of it means that the audience is encouraged to interact immediately, creating a tighter feedback loop with the audience.\n\n\nFragmentation. Purposefully having conversations in places where your audience is smaller, like group chats, direct messages in the cozy web. In these cases, the specific context and audience is well-defined. More users are also turning to ‘finstas’ which are accounts focused for a closed group of friends and family rather than the entire public internet.\n\n\nI don’t think there is a single ‘right’ approach, and in part that’s why so many types of social networks exist today: they each have their own merits that incentive different approaches to communication. But one part I do want to emphasize is the tradeoff of depth and breadth in these approaches.\nSocial bandwidth\nI think in part, this is why nobody has been successful in creating a single social network to replace all of the existing apps. As humans, we want the ability to closely connect with people, and context collapses makes that exceedingly difficult. We have limited social bandwidth — there’s only so much attention we can give to everyone. By increasing the breadth of our audience, we reduce the depth at which we can communicate with each individual. As app developers rush to combine all of these platforms into one, maybe we should step back and think whether this is really necessary.\nOn these larger social media apps where I’m ‘connected’ to hundreds of people, I don’t feel like I can have very insightful conversations or thoughts. Those conversations almost exclusively happen in communities like nwPlus and Reboot where there is enough alignment of values and interests that we can scrape past the surface level chit chat and get to really exploring ideas and knowing people.\nBecause we’re so actively a part of all of these different contexts, we are forced to be excessively general on these platforms to try to cater to everyone. We should have platforms where it’s ok to engage with different elements of who you are. We have different groups of friends to cater to different aspects of who we are, so why should social media platforms be any different?\nDifferent social contexts call for different social behaviours, and that’s ok.\n*Special thanks to Anson, Ivan, Rishi, Joss, Jasmine, Kat, and Anh for helping read over and edit this piece!"},"posts/digital-gardening":{"title":"Digital Gardening","links":["posts/networked-thought","thoughts/the-garden-and-the-stream","thoughts/writing","posts/2020","posts/collaborative-thinking","thoughts/exploit-explore","thoughts/building-in-public"],"tags":["fruit"],"content":"\nAs of 2021, this process is no longer accurate. I’ve embraced a much more networked form of note taking which I’ve written about here and here.\n\nA gardener tenderly shovels and removes unwanted weeds and pours water to nourish the plants he wants to grow. He takes care to make sure the soil isn’t too wet and that each plant has ample space to grow. When the time comes, he harvests each fruit and vegetable and makes sure to plant seeds so that he can repeat the process anew in a few weeks time.\nA garden is a metaphor for a lot of things: growth, persistence, and the constant battle against entropy.\nWhen I talk about digital gardening, I don’t mean digital gardening in the Stardew Valley or FarmVille sense. I mean gardening as in the tending and growth of my own ideas and projects on my own little plot of the world wide web — namely through my writing, notes, and projects. After reading Joel Hook’s blog post on his own digital garden, I’ve been thinking and reflecting on my own processes for managing my own digital garden (which you are on right now).\nThe garden plot\nAccording to GitHub, I first created this blog in late August of 2020. At the time, it was more of a novelty thing. I wanted to get off of Medium and onto my own platform where I had more fine-grained control over how I could present my work and how people discover it.\nI spent a few hours building out my own Hugo theme, making it as frictionless as possible to write produce new content. I threw up a few of my old Medium blog posts just to see how it would look, and I was happy with it. However, even with the novelty of the blog, I had nothing to write about.\nThe plot was there; I just didn’t have anything to plant in it.\n\nPlanting the seeds\nIf you have no seeds in your garden, the only things that will grow are weeds. I could barely remember the last time I read a book on my own time. Needless to say, garbage in, garbage out.\nOver the summer, I began to read again. Technical write-ups, fiction novels, traversing into self-help, and memoirs. I started to read more about the state of the world and critically discuss these with family and friends. Reading helped me colour in the lines as to why we need to build in the first place. I started to realize that the problems we try so hard to solve with technology are not actually tech problems, but inherently human ones.\nI’ve started to write more about some these ideas (like within this blog post!), at first to help me organize my own thoughts, but eventually segued into an excuse for me to talk to people about interesting ideas and get their perspective. It’s started a sort of chain reaction, with observations from a book leading to a conversation with a friend to a blog post ad infinitum. I’ve been able to slowly build up my base of knowledge so that I can contribute meaningfully to conversations.\nThese are the seeds I plant in my garden, but these seeds will stay seeds unless watered.\n\nWatering the plants\nI keep a bunch of random thoughts and ideas in my new tab page. Whenever I come across something interesting in a conversation or book or article, I file it away here. Slowly, categories have appeared as blog posts, articles, and papers coalesce and self-organize.\nThis input of information — the water — is what allows the ideas to grow. Just as it’s not enough to just water a plant once and forget about it, I’ve found that in-taking information inconsistently is about as good as not doing it at all. To be an effective watering can, I need to be intentional and consistent with my watering.\nMy process of collecting random scraps of information first sprung up from coming across interesting blog posts on Reddit. I’d always just read a cool post, follow a few hyperlinks, nod to myself and say “huh”, and maybe forward it to a friend or two. While it may have been entertaining to read, I got no real value out of it. Now that I’ve started cultivating this garden of ideas, I have a reason to be more intentional in how I sift through the information and be more mindful of what I’m actually taking away from each piece. Writing, specifically on this blog, has helped me to go back to more mature ideas and condense and refine them into something presentable and legible to others — quite literally picking the fruit of my labour.\n\nWe are all constantly bombarded with information, a lot of it is really good information too, but the challenge is absorbing it and applying it to the context of our lives and careers. — Joel Hooks\n\nNow that I have all of these new-found ideas and tidbits of information, what do I do with them? Keep growing them forever? At some point on the exploit explore tradeoff, this knowledge should be applied to something in order to manifest it into something useful. The problem is, which ideas get priority of my time and effort?\n\nPulling weeds\nNo garden has unlimited space or nutrition to go around. Maybe some have a bigger garden than most, but that doesn’t mean you can grow whatever you want in it without abandon. Some will need more space than others and others you simply just cannot grow in the same garden plot.\nSimilarly, no one has unlimited time and energy they can put into projects and learning. I, unfortunately, have yet to fully learn this lesson. After having an empty plot for so long, having a little greenery show up has inspired me into a planting frenzy, trying to cram as much into the garden as possible.\nI’m starting to step back and reflect on my current commitments and deciding to either to step down from things that I’m less passionate about to make more time and room to double down on the things I’m truly passionate about.\n\nEnd\nNone of what I do is perfect. Looking back at writing from a year ago, reading code from a few months ago even, makes me cringe a little. Like any garden, this one evolves and grows over time; there is no ‘end-state’ that I’m trying to get the garden to. This little garden is just a little place for me to experiment, to push out things that I’m working on, and to provide snapshots of all the in-progress things going on in my life.\nThrough tending to this garden in public, I hope to show my success, failures, and everything in between and offer it as an open garden to learn from for anyone who stumbles upon it in the future. If just one person is inspired by it, learns from a mistake I made, or builds off of my work, then I would consider this garden a success.\nMaybe you’ll find this as an incentive to start your own."},"posts/digital-identity":{"title":"On Digital Identity","links":["thoughts/identity","thoughts/quantization","thoughts/access-control","thoughts/Self-sovereign-Identity-(SSI)","thoughts/Verifiable-Credential","thoughts/UCAN","thoughts/DID","thoughts/Asymmetric-Key-Cryptography","thoughts/petname","thoughts/peer-to-peer","thoughts/local-first-software"],"tags":["fruit","rhizome"],"content":"What is identity? Who defines it? Who controls it? What is its relationship to software?\nSoftware developers and computer scientists have been tackling identity for almost half a century now, trying to model identity in ways that are understandable to machines. Different models seek to emulate different aspects of the identity.\nBut for engineers building these digital identities, the primary focus is on legibility: the process of simplifying, labelling, and modelling.\nLegibility on its own is not a bad thing. It’s how Google assembles droves of information on the web so we can search through it easily. It’s how we have transparency into the progress of publicly funded projects and initiatives by our governments.\nBut this process of legibility becomes dangerous when it forcefully shapes users. When legibility becomes the lens you view the world through, relationships are front-run by a deluge of data rather than formed more organically between individuals1. This systemized legibility of the world is an interpretive and transformational force that changes how we perceive others2.\nIn the process of being made legible, nuance is excluded. Legibility means that ‘only what matters’ and can be quantified is kept; all else is discarded. This is especially dangerous when that legibility happens without the choice of the users.\nIllegible natural forest vs legible “scientific” forest (James C. Scott in Gordon Brander)3\nForced legibility may look like a set of failing grades on a report card without an accompanying note explaining how you missed finals week because you needed to grieve for the death of a loved one. Forced legibility may look like a conviction charge without the context behind how the officer was racially motivated. Forced legibility may look like having to choose between identifying as a man or a woman on the national census when neither describes you well, erasing their lived experience. When legibility is forced upon people, it only serves to widen the gulf that already exists in society and disproportionately impacts marginalized groups1. This legibility is beneficial for companies and governments seeking to better model user-data, but it doesn’t serve humans who seek to govern their own identities and control who accesses their information.\nIf we want to flip access control back to the users, we need to consider other representations of identity. Clearly, it doesn’t make sense to try to make every part of our digital identities legible. Can we develop alternate systems that provide similarly rich models of identity that also allow people to be illegible? Or at least self-selectively legible?\nThis essay seeks to explore alternate abstractions for identity to better resolve the identity needs of all relevant stakeholders, not just centralized providers. We can categorize digital identity models based on the primary representation of identity along with the locus of control (managed versus self-sovereign).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentity as…ManagedSelf-sovereignAttributesVideo game character, e-shopping account, TikTokVerifiable Claims, NFTsCapabilitiesIAM (Identity and Access Management)OAuth Tokens, UCANsRelationshipsMessenger, Whatsapp???\nFig 1: Different models of digital identity.\nWhat are ways we can lean towards self-sovereign models of identity? How do we give users freedom to choose how legible they are online?\nIdentity as Attributes\n\nOur digital representations consist of:\n\nWhat we bought last time we visited Amazon;\nAll the files you’ve uploaded to Google Drive;\nGoogle Analytics data points about what pages we visited and when;\nWhat type of videos we watched on TikTok over the past week.\n\nMost data models default to this assumption of identity as objects with attributes. Almost all programming languages model things, including people, this way. ISO/IEC 24760-1, the only formal international standard for identity, sees identity as a set of attributes to be managed4. Databases are similar, either being documents with attributes or rows in a table. Relations are encoded as references to other documents, not as things in and of themselves.\nWhen we model identity as a collection of attributes, all aspects of identity are, by default, quantifiable and measurable.\nHaving individuals completely own their identities (e.g. Self-sovereign Identity (SSI) and Verifiable Credentials (VCs)) gives agency to people to control these representations but the main function of these identities is still to make the bearer legible.\nIdentity as Capabilities\n\nWhen I think about what digital identity ultimately feels useful for, it is to gesture at capabilities rather than attributes. My identity can also be represented by how I act and what I have permission to do. Gordon Brander suggests similarly: “digital identity should not be about who you are, but what you are authorized to do.” 5\n\n“[A model of identity as capabilities] considers it to be dynamic, multiple, informational, temporary, contextual. Whereas the [model of identity] as attributes] attach actions and interactions to actors, the [capability model] recognises that identity is co-emergent with actions and interactions in contexts.” (AKASHA and Kernel6)\n\nFrom a software perspective, this isn’t a new representation of digital identity either. UCAN serves to be a promising way to delegate permissions and actions through a decentralized identity. JWTs have been granting permissions to users for a decade. New forms of token-based access using NFTs are being experimented with.\n\nEverything that a user is allowed to do is captured directly in a key or token, and can be sent to anyone that knows how to interpret this format. (ucan.xyz)\n\nThere is no ‘identity’ to be managed but rather a set of capabilities to be possessed. Signable messages using public-key cryptography means that we can prove the same person you issued the access token to is now requesting access without revealing who it is. As there is no global registry of who has what permissions, this is by default illegible unless a user wants to manually publish their key to make it known.\nThis feels promising. A token that grants access isn’t making legible any information that doesn’t need to be, it just grants access to whoever has it. It grants a basic level of illegibility to those who prefer to keep real-world identities and digital ones separate.\nYet, I think there is still room for improvement here. Identities based off of tokens and keys aren’t human-meaningful. In gaining the option for illegibility, we’ve lost any resemblance of a human identity. I want to know that I’m talking to my friend Kevin and not just a key like 0x8ff6b283368b5f149f1de2005d763243 or a phone number like 323-594-1604.\nIdentity as Relationships\n\n\n“Unfortunately, due to their peculiar nature, humans are unable to memorize large numbers of keys, and use them as names for a multitude of objects.”7\n\nYou are already most likely familiar with a system for ‘memorizing’ these large keys already. All of our phones have a personal address book that we use to map meaningless phone numbers to human-meaningful names. HCI researchers call systems like these petname systems.\n\nFor example, if you meet someone named Becky who plays trombone, you could name her “Becky Trombone” and someone else could name her “Becky 101B.” This personal relationship is more recognizable to each individual than a single, self-described user profile named “Becky Smith.” Instead of a single global contact list (like Facebook), we want many personal contact lists (like phonebooks). (Backchannel, Ink &amp; Switch)\n\nIn this way, the petname doesn’t just represent the person you are referring to, but also the relationship between the two of you. The real identity is neither the phone number nor the petname. Rather, the real identity is the intersection of all of the relationships they have with others. Just as we have many ‘alt accounts’ that exist to approximate the many facets of our being, each identity in this model is specific to each relationship. Even if data gets leaked, it is extremely difficult to trace back to the original author because there is no ‘global’ identity to trace it back to — all of it is contextual.\nThe key thing in a relational notion of identity is that the relation – the ‘join’ between identities – is an entity in and of itself. It can be described, and it has a history which can be built on top of. This is a relation that is private by default (only between parties involved) and unique (no other relation like this exists).\nPerhaps the new atom of identity is not a single entity but a set of relationships: a group chat.\nIdentity may be a difficult thing to model, but it is worth thinking about deeply. Our models for identity will impact how many future generations of internet users are categorized and made legible. It makes sense to ensure it best serves the people it represents.\nAs peer-to-peer, local-first, and doorless apps slowly make a resurgence, I hope that agency is the value at the forefront of new applications and protocols, enabling users to choose which parts of themselves to make legible.\nThank you to Anson Yu, B Cavello, Cent Hosten, Saffron Huang, Shrey Jain for reading earlier drafts and providing clarifying feedback.\nFootnotes\n\n\nSource: Is “acceptably non-dystopian” self-sovereign identity even possible?, Molly White ↩ ↩2\n\n\nSource: To Live in Their Utopia, Ali Alkhatib ↩\n\n\n“Scott details a pattern of disaster that repeatedly manifests around legibility. His opening example is from the late-18th century discipline of “scientific forestry”. A natural forest is illegible. A tangle of plants. This is inconvenient from the standpoint of harvesting lumber. How do you quantify yield? Can you even make a meaningful map of this mess? Much easier to clear the forest and plant a legible “scientific” forest. Uniform rows of trees that produce good lumber. Now we can count the trees, make a map, track sustainable yield. What’s missing from our map? Everything else. The forest has been made legible to lumber production. In the process, the entire ecological web of trees, shrubs, birds, bugs, moss, soil microbiota are stripped away. They didn’t fit into our map. By the second generation of planting, there is a noticeable decline in forest health. Within one century: Waldsterben, forest death, ecological collapse.” Quote from Soulbinding Like a State, Gordon Brander ↩\n\n\nSource: ISO Standard ↩\n\n\nSource: Five Mental Models of Identity, presented in Rebooting the Web of Trust VII by Joe Andrieu, Nathan George, Andrew Hughes, Christophe MacIntosh, and Antoine Rondelet ↩\n\n\nSource: Human identity: the number one challenge in computer science, AKASHA Foundation and Kernel ↩\n\n\nSource: PetName Markup Language by Mark S. Miller et. al ↩\n\n\n"},"posts/em/考试管理":{"title":"考试管理","links":[],"tags":["考试管理"],"content":"\n考试管理\n\n"},"posts/hackathons":{"title":"Hacking the Hackathon","links":["thoughts/Design-Justice","thoughts/From-Counterculture-to-Cyberculture","thoughts/Hackers","thoughts/incentives","thoughts/move-fast-and-break-things","thoughts/play","thoughts/sleep","thoughts/pain","thoughts/independent-research","thoughts/Mindstorms","thoughts/money","thoughts/funding"],"tags":["fruit"],"content":"The ‘Involution’ of Hackathons\nA late night breakthrough on our PennApps XX Project\n\n“Quick, quick, Caden! Wake up — the drone is flying!”\n\nHalf of our team had been sleeping for the past hour and a bit. Acceptable, considering the time was now 2:42 am. The drone was whirring midair in our little room, nearly getting snagged on our laptops, chargers, and free event swag. Beside us, a blackboard wall displayed a rough chalk diagram of our technical architecture. We had roughly 20 hours left in the hackathon and now that the drone worked, our biggest roadblock was out of the way — it was time to build! We grabbed a plate of cold noodles from the meal line and ran past the crowd of people doing cup stacking competitions, karaoke, and Smash tournaments.\nThere is something truly special about being able to manifest an idea into reality, that’s what pulled me into hackathons in the first place. Anytime I went to a hackathon not knowing anyone, I left with a new group of friends and mentors and a handful of new skills. I’ve learned a sizeable amount of my technical knowledge through workshops and hackathon projects. These events inspired me to be intellectually curious. To this day, they’re the main reason why I still to this day spend hours tinkering on projects and exploring new ideas.\nIn comparison, something feels different about the vast majority of hackathons held today. Recent hackathons may have all the bells and whistles of earlier events, featuring elaborate meal plans and free bubble tea, but it feels performative at best. When asking most people about why they want to go to a hackathon, most people will mention something about the prizes, the recruiting, or the free t-shirts with company logos plastered over them. There is an unavoidable corporate air to the events.\nHowever, this transition goes much further back than just the past few years. The origin of the more ‘modern,’ corporate hackathons arose out of corporate spaces like Facebook, which coopted design spaces like hackathons to further their hiring pipelines or to maintain their outward appearance being ‘cool’ to work for.1\nThis corporate co-optation of hacker culture at hackathons has been on the back of my mind ever since reading the chapter on design sites in Design Justice by Sasha Costanza-Chock. As someone who first got their footing in computer science through hackathons, it pains me to see that this is the rep that hackathons have slowly gotten over time, moving from safe spaces for idea exploration to increasingly corporate, competitive events where hackers spin up apps to test company products in exchange for the slim chance of winning prizes and recognition.\nHow did we get here?\nHacker Culture\nThe hacker subculture formed mainly out of the collaborative (and often competitive) DIY ethic of the counterculture of the 1960s. This was the generation of the ’Hackers,’ those interested in figuring things out as they go and invented for the pure ecstasy of building and learning new things. As Steven Levy defines it, ‘hacks’ were projects undertaken by these hackers not to fulfill any sort of end goal other than to take pleasure from working on it.2\nThe congregation of hackers eventually led to the creation of alternative design and hackerspaces like hackathons. These very first hackathons were gatherings of excited groups of people ready to build something cool over the weekend.\nThese spaces were described as originally being “third spaces” outside of the influence of the state and the capitalist market.3 Yet, without intentional intervention, it is difficult for these design spaces to even uphold their claim as spaces for intellectual exploration for all, as they become dominated mostly by the privileged to expend free time and income, and overrun by corporations clamouring to sponsor hackathons in order to get as much cheap testing on their products as possible, shoving discount code after free credits at each hacker.\nThese design sites used to be valorized as places of learning, making, and building. Why then, have they become increasingly corporate places of extraction of free labour?\nIncentive Structure of Hackathons\nStacks of hackathon stickers ready for hackers’ laptops\nIt takes money to run events. Providing an adequate venue, food, internet, and power are not free resources. Yet, with the exception of a few corporate hackathons, the majority of hackathons remain free for attendees. To accomplish this, most hackathon organizers decide to try to acquire monetary sponsorships, food partnerships, and in-kind sponsorships in order to offset costs.\nAs a result a viscious cycle of incentives forms:\n\nSponsors try to maximize the benefits they get for their money (e.g. hosting workshops, hiring booths, keynote speakers, company branding)\nWith more money, hackathon organizers increase the size and scope of the event, leading to more hackers\nHackers see sponsors as increasingly quintessential to the hackathon experience, being a key reason why hackers attend the events (sponsor prizes, free merch, hiring opportunities, etc.)\nRepeat\n\nThis leads to some key downstream effects on hackathon culture.\nPrize Incentives\n\n“People are now prioritizing their projects for the dollar value of prizes and the clout of awards. There is a trend where more and more attendees no longer come in to learn and make memories. Teams are now arriving to hackathons fully formed, with an idea in hand, and a checklist of the prizes they want to win.” — Jonathon Xu\n\nIncreasingly so, hackathons have placed more emphasis on prizes rather than building. They prominently display the total value of prizes available to entice hackers to attend, tout the available prizes again at opening ceremonies, and repeatedly blast announcements on them during hacking hours. Even DevPost, the ‘homepage’ for hackathons, organizes hackathons by how much there is to reap. Then again, who wouldn’t want a free iPad and tens of thousands in cash prizes and online subscriptions?\nDevPost Top Hackathon categories by prize pool\nUnfortunately, this incentive structure attracts people to hackathons for the wrong reasons. Individuals are motivated to tick boxes on a judging rubric rather than to learn and build new skills on a solid foundation; people who are there to learn are pushed to try and compete for prizes instead. Beginners, who may have wanted to just learn how to build a website, are instead cajoled into unsustainably trying to learn how to build something completely out of their skill range, becoming frustrated and losing sleep in the process. This is not to say that pushing hackers outside of their comfort range is a bad thing, but there is a limit to how much useful knowledge is retained after patching together APIs and blindly copying tutorials.\nThese incentive structures push hackers to tailor projects specifically for prizes to see just how many sponsor prizes they can shotgun for. In most of these cases, hackers don’t dare build outside of their comfort zone, instead choosing to work with technology they are already familiar with or project ideas they know have been successful in the past (don’t say you haven’t seen a gamifying volunteering or smart garbage bin hack before).\nShort Term Optimization\n\n“A one day hack for homelessness takes away from the complexity of social justice issues… you can’t just come up with an app that solve the world’s problems” — Design Justice3\n\nMost hackathon projects are unsustainable and are unlikely to be used or continued to be worked on outside of the hackathon. A lot of this practice arises out of the Silicon Valley saviorism problem and ”move fast and break things” attitude. 4\nThe problem with this approach is that it becomes incredibly reductionist. Hacks rarely build on existing knowledge and work in the field and often ignore important context about the issues they so readily reduce to a single web app. Ever seen an app claiming to solve the fentanyl crisis or healthcare for the elderly? People think hackathons can do things that they usually can’t, such as solve global problems, create new products overnight, or ‘level the playing field’ of innovation through meritocracy.3\nHackathons, as they stand today, seem to optimize for short term excitement, and not so much for long term benefit. But this short-term burst of new products and ideas is exactly what corporations need to fuel their endless hunger for products, testing, and new talent.\nCorporate Cooptation\nCupstacking, a commonplace hackathon activity\nCompanies realized that the attendees of these design spaces would readily give up their free time to build potentially marketable products. If hackers so readily built things in their own time, why couldn’t we co opt these for the company? Hackers provided a source of interesting ideas that could be milked, and, as a result, the creative outputs of these hackerspaces were “suddenly highly acclaimed, applied, and copy-pasted into capitalist developing laboratories.”5\nThe free market has warped the hacker ethos and hackerspaces into something almost unrecognizable from what it was supposed to stand for. This set of values which may have worked well for small startups or individuals just doesn’t scale well for an entire industry. Sharon Zukin and Max Papadantonakis in their work Hackathons as Co-optation Ritual describe three quasi-Orwellian principles that describe this new hacker culture: Work is Play, Exhaustion is Effervescent, and Precarity is Opportunity.1\nWork is Play\n\n“Forget about work-life balance. It’s all about work-life integration. Why else would the office have on-site acupuncture, nap pods, and free dinner after 7 pm?” — Arielle Pardes, WIRED\n\nThe initial hacker ethos of wanting to ‘innovate’ was evident in how vehemently startups disowned the 9-to-5 cubicle life. Walk into any tech company office building and you’d barely be able to tell if it was supposed to be a corporate office or adult playground with all the ping pong tables and colourful decor. The expectation was for employees to treat coworkers as family, office as home, and work as play.\nBut this playful front is not exactly as it seems. A 2017 study by Sage Business Researcher found that employees who work in offices with these benefits tend to stay in the office longer after work, pushing individuals to spend more and more of their lives in the office. Corporations frame events like hackathons as fun events to attend in an effort to appeal to the hacker ethos and to maintain their public image of ‘coolness’. Disguised under the nerf gun fights, ping pong tables, and free food is a more sinister intent to treat work as play. Is it wrong to love what you do and treat it as play? No, but corporations shouldn’t conflate the hacker ethos with a willingness to ‘play’ at work.\nExhaustion is Effervescent\n\n“There are way easier places to work, but nobody ever changed the world on 40 hours a week” — Elon Musk\n\n100 hour work weeks and ‘hustle culture’ are becoming increasingly normalized by tech moguls like Elon Musk. Being constantly tired is somewhat of a status icon as individuals boast about how little sleep they got.\nIt has been disturbingly normalized and even celebrated that exhaustion is a sign of strength. Among a global sleep-loss pandemic, events like hackathons which push for attendees to stay up for lengths of 24 to 36 hours are particularly worrisome.\nI am viscerally reminded of sharing a hard gym floor with dozens of other sleep-deprived hackers, taking shifts to sleep so some people can stay up to work on the hack. Despite the timeboxed nature of hackathons, there is no reason purposefully staying up all night should be quintessential to the hackathon experience.\nPhysical and mental health should not be “instrumentalized in service of being useful to a startup mission, or even a life philosophy.”4 These events set the precedence of what the next generation of engineers, builders, and designers consider as ‘normal’ for the industry. Why are we saying that it’s okay to prioritize ‘success’ above personal wellbeing?\nPrecarity is Opportunity\nThe startup and hustle culture heavily idolizes those who live precarious lifestyles. High risk, high reward. The industry has normalized high turnover rates and job-hopping, with companies like Google and Amazon reporting median tenures of around a year, compared to the national average of 4.1 years.6\nToday, the majority of tech employment consists of internships, contract work, short tenures at medium to large companies, and precarious work at startups. Because of the incredulous demand for tech jobs, candidates have considerably more leverage than employers do in offer negotiations, with individuals often using competing offers between companies to renege better signing bonuses and compensation.\nWhile not precarious in the traditional labour sense, the employment itself is unstable and temporary. The concept of a ‘life-long job’ just doesn’t exist in this industry. As a result, individuals are constantly asked to market themselves for continually shifting jobs.\nSimilarly, these coopted design spaces reshape precarious and unpaid work from exploitative to opportunity. Writing code and building apps for free becomes something to be clamoured and competed over. In other words, “institutions use the allure of hackathons, with sponsors, prizes, snacks, and potential for career advancement, to get people to work for free.”7\nReclaiming Design Spaces\nLiminal spaces: The morning after nwHacks 2020\nMaybe this is the death of the hackathon as we know it, and that might not necessarily be a bad thing. Maybe we can throw away the focus on prizes, winning, and short-term projects and replace it with something better.\nSustainable Learning\n\nThe word hackathon is the portmanteau of hacking marathon. Why then, do we treat it like a sprint?\n\nThe ‘finish a complete project in two days’ mindset of hackathons rarely transfers well into the real world. Realistically, most projects are complex in scope and attempting to reduce complex problems down to a one-weekend hack omits a lot of context and nuance that is often important.\nWork done at hackathons should be toward long-term sustainable processes instead of short term precarious work. During the past year, there has been a noticeable uptick in design spaces intended to facilitate longer term work like coliving houses, incubators, and fellowships. Hacker houses like Edyfi and School 2.0 give individuals the option to work on existing projects or scale new ones in a supportive community. A few organizations like the Cal Hacks Foundation are pioneering initiatives in this space with programs like the Cal Hacks Fellowship (a semester-long idea accelerator that invites teams to build beyond their side-projects) and Hack Month (which is a month-long build-a-thon focused on recentering the fun of building). MLH is also working on non-hackathon initiatives like the MLH Fellowship and the Local Hack Day workshops that refocus learning and maintenance rather than just blind creation. I’ve even noticed more individuals starting larger-scale personal projects and independent research that happen on the order of weeks to months rather than single days.\nEncouraging the growth of more sustainable skill building enables individuals to take into account a more holistic view of problems. Hopefully, this leads to building longer term, larger scope, and bigger impact projects and skills.\nPlaces of play\nA majority of communities focused around building (e.g YCombinator or OnDeck) are ultimately not places of play – they have external outcomes they’d like to achieve.\nWe should create dedicated spaces for exploration and learning without needing to justify it via some specific outcome. This is not to say that spaces with those goals in mind are bad, but giving individuals the option to have a space for unfettered exploration can give them the freedom to explore ideas that may not have clear monetary value in the short term. Communities like Hack Club and Reboot do this particularly well. There is no central ‘goal’ of trying to launch a product or anything, but rather it’s a group of individuals that are intellectually curious and want to learn and build cool things.\nThe goal is to provide the infrastructure so that everyone can play, not just those privileged enough to throw spare income and time at it. Hackathon organizers shouldn’t assume that everyone is able and willing to stay up the entire event and barely set aside enough time for meals. It is important to consider food, bio breaks, accessible bathrooms that are friendly to all body types and genders, comfortable spaces to nap or relax, and decent lighting, etc.3\nMoving Forward\nA project demo at nwHacks 2020\nIf well-organized, hackathons can provide a fertile ground for pathways to employment as well as being a place of exploration. Those identities don’t need to be mutually exclusive. My argument here is that hackathons have recently placed too much emphasis on the pathways to employment; the main focus of design spaces like hackathons should be on hackers, not sponsors. Even for lower-income students, the ‘employment’ opportunities are usually short-term and precarious. Of course, sponsorship and money does have a role in making these hackathons possible, but not to the scale we’ve come to expect at these events.\nThe Future of Hackathons\nWhat might the future of the hackathon look like, if not what it is now?\n\nEmphasis on sustainable learning. Realistically, hackers will not learn if all they did was copy tutorials off the internet. Most learning happens through sharing between individuals and teams. Hopefully, we can bake this into the hackathon structure by creating cohorts of hackers for parallel play and refocus the hackathon as a means to learn and explore for the sake of learning and exploring rather than to hit the checkboxes on a rubric.\nRealistic Scoping. Too many of today’s hackathons boast that they focus on creating ‘hacks for social good.’ The reality of the situation is that these complex and nuanced societal problems cannot be solved overnight with a simple web app. Either we stop advertising hackathons for good on the timescale of a single weekend, or we increase the timescale of the hackathon from a weekend to months or even years.\nDeprioritization of prizes and winning. Instead, we should provide a space for hackers to play and explore ideas. Obviously in an ideal world, both can happen at the same time, but the rubric-based approach that most hackathons take make these almost mutually exclusive. For now, let’s incentivize participation, completion, and novelty over prizes, competition, and winning.\nHacker-focused spending. Money spent on often unsustainable swag could be much better spent in favour of more emphasis on higher quality venues, food, and accommodations. In doing so, we can work on raising enough money to run a successful event for the hackers rather than a sponsor’s notion of a successful event. At the end of the day, money is power. If you have external funding coming in, the expectation is that they have partial influence over the event. Let’s make sure that we draw the line where appropriate with sponsors so that the focus remains on the hacker.\n\nA Promising Pilot\nThe sudden switch to virtual events due to COVID-19 may actually have been the catalyst to the potential future of hackathons. Virtual hackathons this year have shown how successfully we can run events on low budgets that are often 1/10th of our regular running budget. This means we were able to shave off swag, venue, transportation, and food costs to just a tiny fraction of what they used to be. This taste of what hacker-centric hackathons could be like was incredibly exciting to see, almost a glimpse into what the future of hackathons could look like.\nAfter HackCamp 2020\nThis year, I was responsible for leading logistics at our beginner-focused hackathon called HackCamp. To recenter hackathons around learning and to rebuild a healthier hacker culture, we decided to restructure our event into a virtual conference weekend. Here are a few things we changed:\n\nWe separated learning and workshops from building and hacking and turned it into a two-day hackathon. This allowed hackers to attend workshops and learn without fear of missing out on precious hacking time and to have scheduled sleeping time between the Learn and Build days.\nWe replaced our ‘top’ hackathon project awards with a $25 donation to charity for every project submitted. In the past, we’ve had feedback from hackers saying how this gave them the confidence to finish a project rather than to fail and give up working on the ‘perfect’ or ‘winning’ project.\nAll events were live streamed. This meant that hackers could do the event on their own time and rewatch important events like opening ceremonies while still providing a ‘live’ viewing experience for those who are available.\nWe changed judging to be focused on feedback rather than on evaluation. To do so, we replaced exposition judges (who were normally volunteers or company sponsors) with hackers and did peer-based judging. With this approach, each team averaged 7 pieces of feedback.\n\nAlthough this is by no means a full overhaul of the traditional hackathon, we think this is a great pilot into what a more explorative hackathon could look like. We were able to reach over 500+ attendees, 3.2k+ live stream viewers, and over $1200 in donations to charities, and the feedback we got was absolutely stellar.\nTakeaways\nHaving more of the tools to articulate and locate exactly why hackathons have felt increasingly corporate is the first step to reinstate hackathons as third spaces not as places of competition and exploitation, but as places of play and exploration. Hackathons have so much potential to be safe havens for people to find like-minded people away from school assignments or startup grinds or corporate products.\nLet’s hack the hackathon. This time, with hackers first, not companies.\nSpecial thanks to Anson, Joice, Ivan, Jasmine, and Jess for reading and helping edit earlier drafts :)\nFootnotes\n\n\nHackathons as Co-optation Ritual: Socializing Workers and Institutionalizing Innovation in the “New” Economy ↩ ↩2\n\n\nFrom Counterculture to Cyberculture ↩\n\n\nDesign Justice: Community-Led Practices To Build the Worlds We Need ↩ ↩2 ↩3 ↩4\n\n\nBeyond Instrumentalization by Jamie Wang ↩ ↩2\n\n\nPost-it Note City ↩\n\n\nForbes on High Turnover Rate in Tech ↩\n\n\nWired on Hackathons and Exploitation ↩\n\n\n"},"posts/index":{"title":"所有文章","links":[],"tags":[],"content":""},"posts/km/知识管理":{"title":"知识管理","links":[],"tags":["知识管理"],"content":"\n知识管理\n\n\n\n知识管理工具有哪些？\n\nroamresearch\n公开的个人笔记\n\n\n知识管理操作\n\nPDCA\n\n\n使用知识管理的方法论\n\n目前使用的是 LogSeq 做渐进式阅读\n\n使用LogSeq的好处\n\n\n\n\n"},"posts/me-myselves-and-i":{"title":"Me, my selves, and I","links":["thoughts/trust","thoughts/time","thoughts/self-confidence","thoughts/The-Writing-Life"],"tags":["fruit","personal","writing"],"content":"An exploration on commitment, trust, and growth.\nThe Persistence of Memory by Salvador Dalí\n\nThe sound of the waves are nice, huh?\nYeah.\nThe ebb and the flow of the waves, the toppling of sand sculptures of children. Of so much chaos in the world, the constancy of the ocean and its rises and falls gives me comfort.\nYou sound quite melancholic. Is everything alright?\n… I’ve been thinking about the concept of commitment and trust. And maybe a bit about belonging too.\nCan I lend an ear or a shoulder? We can just sit in the sun in quiet if you’d like.\nThank you.\n[And they sat there for a while. The sun inched downwards, setting the sky ablaze.]\nWhat do you think it means to commit to an identity?\nHmm. I think there is an element of trust — that the ground below them will hold, that tomorrow will come again, and that the ocean will continue to rise and fall. Trust is an unquestioning attitude, an absence of deliberation over reliability. To commit is to believe it as a truth about oneself.\nDo you think identities can change with time?\nYes. I don’t think truth is absolute — I think truth is a function of identity and identity itself is a function of time. Maybe, at this slice in time, certain axioms hold true about the universe. A few hours, years, millennia might pass and those truths might be different. Our different selves can choose between different systems of meaning and sense as our basic axioms.\nI think my sense of self is undergoing a shift. It feels very liquid right now. One supposedly is supposed to ‘live in the present’ to enjoy the tea1, but I can’t help but feel… scattered. I feel like I am trying to live too many lives at once and I feel the very fabric of my being being tugged at the seams.\nI think empathy is a means of time dilation. For one to slow down or speed up their rate of life to match another. Of course, to constantly time travel to bridge the worlds of slow and fast is incredibly tiring. You must be very worn.\nI want to be whole again. I want to be able to pour my everything in one life and to live that life well.\nI understand. What is stopping you?\nWell. What does it mean to responsible to a commitment that a past self has made? I promised [x] to some people very dear to me. At the time, it very much felt like the right thing to promise. I think my selves have changed though. The me that used to really enjoy [x] doesn’t get that same spark, that same excitement as it used to. I found [y] pretty recently, and it gives me so much life to put time aside to work on it. Yet, I feel guilty. I made a commitment. A promise. It feels selfish to go back on that. I want to be someone who is reliable — someone people can depend on and trust unquestioningly.\nAh yes, the desire to be a constant in the lives of others, yet to be fluid and growing in the life of your own. It is an unfortunate truth in life that we rarely get to choose both.\nIs is possible? With all the time flowing through my hands, I am afraid that I am reaching for too much and end up not catching any at all.\nWell, let’s see. What do you like so much about [y]?\nI think recently, I’ve been able to better articulate the questions I want to spend my life answering. I think I’ve grown as a person and [y] aligns much more with what I want in the future, both in terms of myself and and the people I surround myself with. [x] feels like the trappings of the past, the remnants of a self I’m not sure I still consider my own. Yet, I am scared to let go of [x].\nI think you are still muddled. I think what you lack is self-belief. The people in [y] inspire you, they amaze you. They have a ceiling of self-belief so high that it raises yours just through osmosis. These people are flawed too. They come from histories of [x], yet they still believe in themselves and what they want. Surround yourself with people in [y], for they are free in ways you’re not2. They are playing an infinite game in the hopes of waking you from the finite one you play in.\nWhat shall I do?\n[Both held their palms up to sunset.]\nEven if it is not your ideal life, you can always choose it. No matter what your life is, choosing it changes everything. I think you need to work on transitioning out of doing [x]. The self that committed to doing [x] has moved on. The self that is here and present needs to deal with the consequences of the past, but is not a slave to it. How we spend our days is, of course, how we spend our lives3. What we do with this hour and that one is what we are doing. What can you do to make your days meaningful? I think [y].\n[The two sat in silence, their silhouettes indistinguishable in the fading sunlight.]\nThank you for your company, your space, and your time. I want to take the necessary time to ease out of [x] and to do it with care and tenderness.\nYou and me, we are not so different. One of us might be a few chapters ahead, but you’ll be back at this beach someday. I’ll be going now. See you soon.\nYes, I hope so.\n[And the waves continued to crash down. One got up to leave, another came to sit down.]\nThe sound of the waves are nice, huh?\nYeah.\nFootnotes\n\n\nThich Nhat Hanh’s Tea Meditation ↩\n\n\nPatricia Mou, Rabbit Holes ↩\n\n\nAnnie Dillard, The Writing Life ↩\n\n\n"},"posts/networked-thought":{"title":"Networked Thought","links":["thoughts/search","thoughts/Internet","thoughts/organizing-system","thoughts/desire-paths","thoughts/interoperability","thoughts/A-City-is-not-a-Tree","thoughts/rhizomatic-vs-arborescent","thoughts/the-garden-and-the-stream","thoughts/writing","thoughts/knowledge-distillation","thoughts/idea-list","thoughts/urban-planning","thoughts/tools-for-thought","thoughts/building-in-public","thoughts/epistemology"],"tags":["fruit"],"content":"\n“Gardens … lie between farmland and wilderness … The garden is farmland that delights the senses, designed for delight rather than commodity.” — Bernstein\n\nWe live in an information age. The amount of data we produce far outweighs what we consume, so much so that it has extended far beyond our ability to make meaningful use of it. Even our modern day search systems seem to be falling apart under the stress of today’s overwhelming flow of data with the quality of our search engines degrading from all of the SEO hacks, paid advertiser content, and clickbait headlines.\nOver the past year, I’ve slowly found processes that have worked well in creating a little curated corner of the Internet, rich with wonderfully curious people exploring exciting ideas.\nWhat is digital gardening?\nA digital garden is not a file cabinet, nor is it fully an index. A digital garden is less so a well-kempt plot for farming and more a mess of entangled growth. It is a network of interconnected ideas and thoughts, clustered by how they are associated with each other.\nThis is not because I don’t like order, but because I think a dash of chaos and entropy is good for new ideas. They help connect two separate ideas that you normally would not have associated with each other, and to imagine the ‘what if’ more frequently.\n\nBut there is also a philosophical basis for this which to me is quite practical. A pretty good master analogy is that it’s an attempt to make a big stew pot out of my brain – there is tremendous value in allowing all of these notes and ideas and observations to stew and ferment in there. For me, the real power comes when ideas intermingle and I’m able to discover connections that I truly could never have dreamed of under normal conditions.\nRobin Sloanon Rhizomatic notetaking\n\nMy goal with a digital garden is not purely as an organizing system and information store (though it works nicely for that). I want my digital garden to be a playground for new ways ideas can connect together. As a result, existing formal organizing systems like Zettelkasten or the hierarchical folder structures of Notion don’t work well for me. There is way too much upfront friction that by the time I’ve thought about how to organize my thought into folders categories, I’ve lost it.\nMany try to organize their lives through note taking. There is a classic “top down” vs “bottom up” design tension projected onto how people take notes. Some people trust their ability to predict the future, they want top-down, they want to pave the paths in the garden. Others (normally those that have tried and failed) don’t trust their own ability to predict the future, they want to make it possible for the cows to roam safely, then pave the desire paths after they form.\nThis is the problem with the file cabinet: it focuses on efficiency of access and interoperability rather than generativity and creativity. Thinking is not linear, nor is it hierarchical. In fact, not many things are linear or hierarchical at all. Then why is it that most tools and thinking strategies assume a nice chronological or hierarchical order for my thought processes? The ideal tool for thought for me would embrace the messiness of my mind, and organically help insights emerge from chaos instead of forcing an artificial order. A rhizomatic, not arboresecent, form of note taking.\n\nThe garden is the web as topology. Every walk through the garden creates new paths, new meanings, and when we add things to the garden we add them in a way that allows many future, unpredicted relationships.\n(The Garden and the Stream)\n\nHow I garden\n\nDigital gardens focus not on being a definite source of truth, but rather a source which is constantly evolving as your own knowledge grows and changes\n\nMy first blog made me scared of posting. I was scared of putting things into the public because I was anxious about all the different ways people could perceive it, both in the present and in the future. What if I posted something that people thought was stupid? Maybe somebody would see my current work in the future and look down on me for how naive my thinking was. But honestly, the more I wrote and just put things out there, the less that line of thinking made sense.\nI, by and large, write for myself. Writing, for me, is a form of knowledge distillation. It helps to clarify my thinking and condense my knowledge so I can easily articulate it to others. If done well, I have a shareable representation of my thoughts that I can send out into the world and people can respond. Even for my most half-baked thoughts, this helps me create a feedback cycle to strengthen and fully flesh out that idea.\nDigital gardening is not just passive knowledge collection. It’s a form of expression and sharing. The goal should be to tap into your network’s collective intelligence to create constructive feedback loops, not to post content that ‘gains clout’ or make you look smart.\n\n“[One] who works with the door open gets all kinds of interruptions, but [they] also occasionally gets clues as to what the world is and what might be important.” — Richard Hamming\n\nHere are some learnings over the past year of digital gardening:\n\nLink by concept rather than by exact match. I will often explicitly have a See also: (some concept) link somewhere if I feel two subjects are closely associated. Linking new knowledge to existing knowledge makes it easy to remember. This has helped me find really cool connections on numerous occasions.\nName notes to be as simple as possible. I prefer using verbs or nouns to make it easier to link concepts and thoughts.\nGood search matters a lot. When I search, I usually don’t know the exact name of the thing I’m looking for, otherwise, why would I be searching for it in the first place? I use search as an entry-point into a single node, then recall by associativity rather than by indexing. But having a good entry-point can make or break my flow into finding what I’m looking for.\n\nThis is still a process I’m refining to this day. Likely, my learnings will have grown by the next time around when I rewrite this again.\nKnowledge Flow\nI don’t read everything I come across. I am at a point where I am constantly bombarded with new and interesting things from my Twitter Feed, Curius, friends, Slack groups, Discord Servers, Telegram DMs, and so much more.\nDespite my manic page parking, I really only get time to read about 30% of content sent my way. Even among those 30%, I probably only have meaningful thoughts and connections with about half of those.\nI’ve started developing a process to better manage my process for information intake.\nSeeds\nI am a person who needs a very low friction way to dump new ideas and things to look at in the future.\nI built TabSpace to be a ‘scratch space’ in my new tab page where I have my running list of todos, temporary thoughts, and things to read in the next little bit. It’s the Apple Notes for my laptop.\nI tend to generally bookmark things for later then revisit them when I have time. For projects, writing, and all sorts of reading. Even when reading books, I don’t like to take complex notes right away will only bookmark or highlight phrases. I will eventually come back to the bookmarks a second time to generate insights and actual thoughts. It feels like this weeds out unnecessary noise and provides a natural chance for spaced repetition.\nThese are the seeds that form the basis of ideas and thoughts.\nSaplings\nSaplings are single nodes or thoughts. When linking notes, I generally do not silo notes into categories. Sometimes, the presence of specific ‘folders’ or ‘topics’ prevents us from making surprising connections between otherwise related topics (for example, urban planning and data structures).\nFruits\nOf course, a knowledge index isn’t much use if it doesn’t inform future thinking and output. Fruits are what I like to call derivative or ‘new’ pieces of content.\nIt’s the act of creating ‘newer’ work from saplings, mostly longer form essays, projects, etc. At this stage, thoughts and ideas have matured enough to be able to share and collaborate. Right now, traditional ’tools for thought’ are not great for this aspect, lacking the ability to publish, edit, and share notes with others.\nStart your own\nI’ve found having my own digital garden has been immensely helpful. It’s created a playground for me to experiment with writing, have an excuse to read, learn, and share with others, and be less scared of putting this out into the public.\nThrough tending to this garden in public, I hope to show my success, failures, and everything in between and offer it as an open garden to learn from for anyone who stumbles upon it in the future. If just one person is inspired by it, learns from a mistake I made, or builds off of my work, then I would consider this garden a success.\nMaybe you’ll find this as an incentive to start your own.\n\nI’d like digital garden to be like a bonsai tree. Carefully growing, trimming, pruning, artfully shaping a beautiful tree of resources and ideas\n\n\n‘Everyone tends his or her own little epistemological’ garden, growing ideas from seed and sharing them with anyone who comes by.\nImage Source"},"posts/new-words":{"title":"A New Glossary of Words","links":["thoughts/terminology","thoughts/consensus","thoughts/language","thoughts/hermeneutical-injustice","thoughts/decentralization","thoughts/plurality","thoughts/context","thoughts/Microworld","thoughts/verisimilitude","thoughts/philosophy-of-science","thoughts/Plato's-Ship-of-State","thoughts/right-to-be-forgotten","thoughts/forgetting","thoughts/Hackers","thoughts/truth"],"tags":["fruit"],"content":"This is an expansion of thoughts on terminology and why we need new words. The following is a dialogue on the original consensus problem: agreeing on what we mean. You can view the full artifact here.\n\nCharacters\n\nW: a wordsmith, a poet, a pluralist\nP: a pedantic\n\nP: W, have you ever wondered we need so many words?\nW: I can’t say I have. The whole point of language is to have shared mental models to enable us to communicate complex ideas more easily. We have complex ideas so having the right materials to express those thoughts feels important.\nP: Well, don’t we have enough words to express all we want to express already? There are innumerable ways to combine the words we already do have.\nW: There is a form of epistemic injustice known as hermeneutical injustice where one has no labels or common terminology to describe or explain experiences to others. Clearly, there are not enough words to express the human experience.\nP: Okay, well I can admit that having more words can be helpful. But who is to say what each word means? Who is to say that the word ‘apple’ even refers to the fruit I just ate, or that ‘metaverse’ is even a real word?\nW: Language is logically decentralized, is it not? There is no governing body that determines what a word means. The meaning of a word lies in its use1, and that is decided by those who speak the language.\nP: That is not necessarily true though. The language of the law sets out centralized definitions for words so that definitions cannot be swayed or morphed to fit the needs of its wielders. Similarly, at the start of a mathematical proof, one should always define the axioms or givens to be agreed upon before attempting to use them in any capacity.\nW: There seems to be a flaw in your argument. Yes, I agree that a consistent set of definitions is required for any sort of productive knowledge sharing. But you fail to account for plurality. Context modifies meaning.\nP: Plurality as in a multitude of definitions?\nW: Precisely.\nP: Well that is almost certainly problematic as well, is it not? Too many competing definitions cannot be a good thing. We have dictionaries for a reason. How would anyone new to learning English be able to grasp the nuances of all of the meaning and history behind each term? It would get overwhelming incredibly quickly.\nW: I concede that you have a good point. However, we can take a leaf out of Karl Popper’s 2 book: the lie-to-children. We can create simple glossaries and terminological definitions as some abstraction for the larger, more nuanced concepts of the real world.\nP: Wait… lying is categorically bad is it not?\nW: Hold on and let me finish my thought. According to Pratchett3, “a lie-to-children is a statement that is false, but which nevertheless leads the child’s mind towards a more accurate explanation.” I think what I’m trying to get at is this concept of verisimilitude: that there is no binary true or false, some propositions are more true than others, especially in context.\nP: Okay, I accept that taking a toy-model approach to new terminology makes sense. But I still don’t understand why we couldn’t just have a single definition for each word in this model.\nW: My concern with a ‘standard’ library of definitions is that interpretations of concepts are not allowed to adapt to usage over time. How does a dictionary or thesaurus adapt to the historical meaning of racial slurs, for example? Those who hold the ability to rewrite the past are the ones with power. Again, this is why having centrally controlled language is a bad idea. This was quite clear in Orwell’s work.\nOf course, this is not to say that definitions are not important. In fact, quite the opposite. Definitions hold immense power in shaping how we talk and think about the world writ large. What I am saying is that language needs the ability to evolve on its own. Individuals and groups should have the agency to ‘reclaim’ harmful and outdated definitions.\nP: Isn’t this exactly why we need centralization? This is like Plato’s Ship of State4. Any large vessel by their very nature needs to be steered firmly. Those aboard must yield to their captain’s commands; no reasonable person believes that a ship can be run democratically.\nW: Would sailors want to obey a captain who takes them somewhere they don’t want to be? No. I imagine language like multiple small ships, each one with its own crew (crew of course, meaning an agreed meaning for a set of definitions). Let me reiterate without the ship metaphor: language requires localized consensus for it to function.\nP: Ah, now I see that you don’t mean complete decentralization, that makes a lot more sense. I can agree on this. I am, however, curious about how local agreement should propagate to become widely accepted. After all, almost all English speakers can agree and what an ‘apple’ refers to, yet nobody seems to have a good definition for relatively newer terminology like the ‘metaverse’.\nW: This is an interesting question to think about. Not only do we need to think about spatial locality, but also temporal locality as well. I’d like to think about this in terms of metallurgical annealing if you’d let me.\nP: As in the process of heating metal to make it more malleable?\nW: Yes, exactly! Let us imagine definitions of words as metal nodules. After a nodule is heated to a high temperature, it is workable. Similarly, when new terminology is coined, it has a period where it is malleable and adaptive and achieving local consensus is relatively easy.\nOf course, over the course of the annealing process, meaning can drift. Centralizing then, is a form of metal hardening and shaping. It anchors meaning and prevents it from being easily modified. Just as one should not have metals be load bearing until they’ve hardened and been shaped for their specific use case, semantically fluid terms should not be epistemologically load bearing.\nWe imagine new words as small nodules. The size correlates with the number of individuals that agree with the definition. Smaller nodules reach the right temperature for annealing more easily. As the definition becomes more widely accepted, it gains mass and becomes harder to work.\nP: Sorry, I don’t follow this metaphor very well. Could you rephrase?\nW: I hope you excuse my thinking out loud, but this metaphor has given me the clarity to better express this idea in terms of regular language.\nEarly on, achieving local consensus on a definition is rather easy; there are only a handful of people who then know about the term, let alone use it regularly. As the term grows more popular, it becomes increasingly difficult to manoeuvre and adapt in such a way that the meaning of the term cannot shift very much without causing fracturing.\nP: Ah that makes a lot more sense. Hmm. This covers the initial semantic definitions for a word but is there any way to change the meaning of a definition after it’s been concretized?\nW: This seems quite difficult for terminology already ingrained within society. Especially as meaning is not dictated by some central organization, any old definitions\nneed to be collectively forgotten. As we concluded earlier, this is incredibly difficult for widely used terms.\nAs we enter an age of digital permanency, we should normalize the right to be forgotten for terminology. It should be normal for terminology to be forgotten — for it to slip through the hands of time like sand.\nP: This ignores a lot of history, does it not? “By changing what we were, you change what we are and what we are going to be.”5 This is a form of erasure through terminology change. By ’forgetting’ terminology, you deny its existence.\nW: I may have phrased my words poorly, that was not my intention. Maybe abandoned is a better word? I want to create dictionaries and glossaries that keep terminological history. A sort of ‘append-only’ record of how terminology has split, died, and evolved over time.\nP: What do you mean by append only?\nW: The only way to overwrite an existing definition is to either hard-fork it within a subcommunity or to create entirely new words. Hard-forking involves an agreement from a subcommunity to use an alternative definition for specific terms.\nP: Well, what words need to start anew? Clearly there are words that could benefit from redefinition.\nW: The ‘metaverse’ for one could use a lot of redefinition. Meta has put a lot of effort into claiming and defining this term for their own benefit — a closed, profit-driven, and attention-farming dystopia. A few groups have cropped up around reclaiming some of this terminology, including one forking John Perry Barlow’s 1996 Declaration for the Independence of Cyberspace as a reaction to Facebook’s recent rebranding as Meta.\nLikewise, many groups across the world have tried to redefine and hard-fork the definition of ’hackers’ for quite some time. Hacking, as known by the general public, generally refers to gaining unauthorized access to technology. Yet within this local hacker subcommunity, a hacker is widely defined as one who builds and creates for the sake of creating.\nP: I am increasingly convinced by your argument against centralized definitions. I think, as a society, we need to think more critically about language and terminology and how they carry power. We want to enable evolution and creation of new terminology to enable others to have the language to speak of their lived experiences and complex ideas.\nW: Yes! In fact, I think we need a new approach to building glossaries and dictionaries. At the core of it all, we are hoping to better solve the original consensus problem: agreeing on what each other mean. To abolish the existing centralized dictionaries and glossaries without suggesting an alternative would be in bad faith.\nP: What sort of alternative are you proposing?\nW: A collectively curated glossary of sorts. One which involves a rich history and context of terms, pluralist in nature, and always ongoing. If meaning is a negotiation, then the history of that negotiation needs to be a crucial part of achieving consensus on the meaning.\nLet us create a new glossary of terminology. May it be a source of truth rather than the source of truth.\nFootnotes\n\n\nPhilosophical Investigations, Ludwig Wittgenstein ↩\n\n\nThe Logic of Scientific Discovery, Karl Popper ↩\n\n\nThe Science of Discworld, Terry Pratchett ↩\n\n\nRepublic, Plato ↩\n\n\nMartín Becerra on Natalia Denegri ↩\n\n\n"},"posts/nm/笔记管理":{"title":"笔记管理","links":[],"tags":["笔记管理"],"content":"\n笔记管理相关的软件\n\n卡片笔记软件\n\n\n\n\n参考链接\n\n[1]: OB中文帮助文档  https://publish.obsidian.md/chinesehelp/"},"posts/nothing-stops":{"title":"Nothing stops","links":["thoughts/quotes","thoughts/pain","thoughts/time","thoughts/friendship","thoughts/Nozick's-Experience-Machine"],"tags":["fruit","writing"],"content":"It was so heavy-handed and so stupid, hitting the nail on the head so hard that it rang like a bell. Nothing stops. — Helena Fitzgerald on Substack\n\nNothing stops. None of it. The good times, nor the bad. There is no “until this is over”, or “when I’m done this.” There is no “when I get less busy” or “after this term.” Nothing stops.\nThere is one Annie Dillard quote I hold central to my ‘consciousness cannon’,\n\nHow we spend our days is, of course, how we spend our lives.\n\nWhat we do with this hour and that one is what we are doing. What can you do to make your hours, days, and life meaningful? Certainly not toiling away for the next few years on things that pain you and question your sanity. Pain is not the unit of effort that matters1. Time is.\nTake the time to find yourself. Take the time to smell the proverbial flowers. Take a break to sharpen your saw for it will not delay you from cutting more wood.\nThere is no permanence to this universe. If we could stop time, we would. Bill Waterson once said that ‘If people could put rainbows in zoos, they’d do it’. Everything we build is a sand castle waiting to be washed away at high tide.\nWe build and craft the most beautiful sandcastles we can, knowing they will be washed away. We get others to build with us, suspending the knowledge that this will all eventually disappear.\nI think that’s what gives life its beauty — its scarcity. If one could live forever and do everything they could ever want, why wouldn’t they choose it2? We value the time of others because we know that is finite. Life has meaning because it is finite.\nLove then, of the world, is knowingly choosing the losing side. What is love but the constant battle against entropy that drives everything apart and strips it of its salience? What greater project is there, in an unbearable time, in a perpetual future, where nothing stops? In a society that never stops, isn’t loving the ultimate form of protest? To be able to be whole in your existence, to share your time on this little piece of rock drifting through space together?\nCompanionship is valuable because it affords the opportunity to feel ‘seen’ by another. We can only, according to Nathaniel Braden, view ourselves conceptually but we need others to view ourselves perceptually. Other consciousnesses function like a mirror — being seen in this way is recognition of personhood. The feeling of being seen is psychological visibility. Love then, is witness through it all.\nWitness is deep attention3. To witness is not to tether or to pop their balloon, but to hold their strings carefully. adrienne maree brown described relationships like a spiderweb—diaphanous yet strong, thick yet porous. “A web allows things to fall through, like a sieve,” she said. “Some things are not meant to be caught.” David Whyte believed that the ultimate touchstone of a relationship is not improvement, neither of the other nor of the self.\n\n“The ultimate touchstone is witness, the privilege of having been seen by someone, and the equal privilege of being granted the sight of the essence of another, to have walked with them, and to have believed in them, and sometimes, just to have accompanied them, for however brief a span, on a journey impossible to accomplish alone” — David Whyte\n\nNothing stops isn’t a statement about how nothing matters. Rather, it is a call to reroute time on what does matter. As Patti Smith said when discussing William Blake and her creative influences, “Who are the people, ideas, and books that magnify your spirit? Find them, hold on to them, and visit them often.”4\nIn the lifespan of the universe, how lucky are we to share our lives together?\nFootnotes\n\n\nI’ve seen a lot of people take interest in this article by Mark Manson which argues that “what pain you want in your life” is a much more defining question than what we want in life. I agree that happiness requires struggle. But struggle for yourself and what you want. Struggle because it is meaning-laden, not because society demands it and optimizes for the appearance of sufferance. I like this anecdote from alkjash (more thoughts here): “If it hurts, you’re probably doing it wrong… If your wrists ache on the bench press, you’re probably using bad form and/or too much weight. If your feet ache from running, you might need sneakers with better arch support. If you’re consistently sore for days after exercising, you should learn to stretch properly and check your nutrition.” ↩\n\n\nNozick’s Experience Machine ↩\n\n\nI loved Jasmine’s piece on attending to the other and I consider it a critical part of my consciousness cannon ↩\n\n\nPatti Smith on Time, Transformation, and How the Radiance of Love Redeems the Rupture of Loss ↩\n\n\n"},"posts/open-source-and-politics":{"title":"Open Source and Politics","links":["thoughts/Do-Artifacts-Have-Politics","thoughts/Collingridge-dilemma","thoughts/catch-22","thoughts/ethics","posts/towards-data-neutrality","thoughts/network-effect","thoughts/credible-exit","thoughts/DID","thoughts/The-ones-who-walk-away-from-Omelas","thoughts/move-fast-and-break-things","thoughts/progress","thoughts/To-Live-in-their-Utopia"],"tags":["fruit"],"content":"Coraline Ada Ehmke is an acclaimed speaker, writer, engineer, and activist with over 25 years of experience in software and almost 20 years in open source. She works to promote diversity, equity, and justice in open source communities and the tech industry as a whole. She created the Contributor Covenant, the very first code of conduct for open source communities, as well as the Hippocratic License, which legally prohibits an open source project from being used for human rights violations.\nJacky Zhao is studying computer science and philosophy at the University of British Columbia. He thinks a lot about how we can better incentivize public goods funding, support better interactions with computers, and be more responsible stewards of technology. He has built and maintains many widely used open source projects like Quartz, which enables users to host their digital gardens online for free, and cursor-chat, which is a library to add Figma-like cursor chat to websites.\nThis conversation has been edited for clarity and length.\nHow did we get here?\nJacky: I’m curious about how you first got involved with the tech justice movement and open source.\nCoraline: I was a software engineer for 26 years. When I started my gender transition in 2013, I started experiencing first-hand some of the pervasive problems in open source and the tech industry that I had only been aware of intellectually before. It woke something up in me. It was around that time that I started becoming more interested in the issues of justice and equity and technology.\nI remember back in the early 2010s, when tech conferences started becoming popular, there was a big fight to get tech conferences to have codes of conduct. It’s something that seems so normal and natural today, but it was actually a very, very difficult fight.\n2014 was also when I wrote version 1.0 of Contributor Covenant, which was the first code of conduct for open source communities. I feel like over the years — eight years now — that Contributor Covenant has been around, we’ve seen codes of conduct become more normalized in open source communities.\nToday, it’s hard to count the number of adoptions. It’s in the ten thousands. And it’s kind of wild! I was talking to a friend last year who said, “Coraline, there’s an entire generation of engineers who have never worked in an open source project that did not have a code of conduct.”\nOne of the things I worry about though, with that normalization, is that we don’t recognize our history. People in tech have very, very, very short memories, which is part of why we keep reinventing the same stuff over and over.\nJ: Yeah. In my free time, I do a lot of open source projects and hack on a lot of little things in general. I initially started posting a lot of my projects on GitHub more for backup and archival purposes and never really expected any real usage, so I never really thought about being “a maintainer,” or whatever that meant.\nIt wasn’t until my very first project started getting real usage that I realized there never was any real introduction to being an open source maintainer, or what it means to be a good maintainer. There’s no “How to be a Good Maintainer 101” course. It wasn’t until I started getting contributors that were like “Hey, you actually don’t have a license or a code of conduct in your repository, have you considered adding one?” Then I realized, wait, I actually don’t know much about — as you said — how we got here. What is the history of all these codes of conducts? I think that definitely kicked off a personal learning journey for me to figure out the history of a lot of this as well.\nC: Yes. So that takes us to 2018. An activist group called Mijente was in the midst of their No Tech for ICE campaign. One of the things that they were doing was posting the names of companies that had contracts, either with Customs and Border Protection or with ICE directly. One of the companies that got called out was Chef, which plays a large role in infrastructure, server deployments, and was very necessary for a lot of the large-scale internet operations.\nSeth Vargo, who had previously been a developer at Chef and had worked on open source tooling for them, saw that Chef was called out as one of these companies profiting from human rights abuses at the border. In an act of conscience, he pulled all of his open source code that was part of the Chef ecosystem and made a statement about why. But within two hours, both GitHub and RubyGems forcefully restored the code he took down because it was affecting the many companies around the world who depended on those libraries.\nThe open source establishment, or what I call “open source traditionalists,” saw this and said “No, no, you can’t do that because open source is neutral.” I saw that as an epic moral failure on the part of the establishment. So I wrote version 1.0 of the Hippocratic License, which was not intended to be a viable license, but rather a lightning rod for broader discussion around the neutrality of open source tech in general.\nJ: That’s really interesting. At school, I study computer science and philosophy, so I spend a lot of time thinking about how technology impacts the people that use it. And I think one of the foundational pieces that I read that really shaped my thinking around this was “Do Artifacts Have Politics?” I think that paper was really influential in terms of making me think, “Wait, actually, this technology that we spend so long claiming to be neutral actually has political implications as well.” And I think a lot of people working in tech spend a lot of time trying to deny the fact that it does.\nC: A couple of years back, I gave a talk called “The Rising Ethical Storm in Open Source.” I actually traced that illusion, or, you know, honestly, that lie, that computer technology in particular is neutral. I traced it all the way back to the 1950s with Edmund Berkeley, who was one of the cofounders of the Association for Computing Machinery, and he served on the committee called the Social Responsibility of Computer Professionals.\nTheir findings were that yes, technologists are absolutely responsible for how and what is developed. The how, what, why and its impact. And the fledgling computer science industry at the time rejected that.\nJ:  I watched that talk that you mentioned. One of your quotes in that talk that really resonated with me was: “I believe that as technologists, we have a moral imperative to prevent our work from being used to harm others. Responsibility is about impact and not intent.” That definitely feels relevant.\nOne model I use often to think about tech is to model it as a multiplicative tool instead of something that’s purely additive1. Multiplicative in the sense that it will only exacerbate the existing discrepancies in distributions of power, right? Some people will obviously be way better off and then there’s some that will be disproportionately harmed by it.\nI feel like some other people hold a very strong belief that technology is purely additive in that it will just truly raise the ground bar of everyone who uses it. Yeah, I don’t know. I feel like that’s always been missing from how people think about technology, that there’s always a hidden tradeoff or downside to whatever technology that you’re building with.\nI wonder whether the developers of these technologies should be responsible for expecting how their tools will be used in whatever way down the line, right? If you build an open source project, it’s very hard to tell what type of people will actually end up using your project. And so, at what point do developers have to start thinking about these tradeoffs? For example, who will my end users be and what is acceptable use and what’s not?\nEven from a developer standpoint, it obviously helps if you’re educated in these issues in order to make these calculations. But even as someone who is educated about these things, how do you weigh the potential upsides and the potential downsides.\nI came across the concept of the Collingridge Dilemma a while ago, which I think captures the double bind of technology quite well. In essence, it says that any efforts to influence or control further development of technology kind of faces a double bind, where you come across two irreconcilable problems. One is an information problem. You can’t really predict what impact your technology will have until it is extensively developed and widely used. But then, two, you also run into a power problem where by the time you’ve already extensively developed and put it into wide use, changes become too difficult because the technology has already become so entrenched in society. And at any point, it  is incredibly difficult to even begin to evaluate that type of impact. So, what is even the ideal place to start thinking about this impact?\nC: Continually. You have to do it continually. You have to do it after deployment, you have to do it after it’s widespread. You have to do it continuously.\nIn academia, if you’re a sociologist or an anthropologist or in any of the social sciences, you have to go before an institutional review board when you’re planning a research or development project. One of the requirements for launching any such activity is having an effective plan for not only preventing harm, but mitigation plans when someone actually is harmed. We don’t see that same principle being applied to hard science, we don’t see it applied to engineering. Why not?\nBut to your point, it is very difficult to predict. One of the instruments that we’re developing at the Organization for Ethical Source is something we’re calling a priority of constituencies which comes from one of the W3 specs for HTML. So there’s this one sentence in the spec that was developed that said, in case of conflict, we “consider users over authors over implementors over specifiers over theoretical purity.”\nWhen you draw a line like that, what you’re explicitly saying is that even if this makes it inconvenient for adopters, even if it makes it inconvenient for developers, even if it makes it inconvenient for end users, we have to make that decision based on the most vulnerable and work upward. It may be uncomfortable, but if the potential is there to reduce harm, to mitigate harm, or to have a plan for what to do when harm occurs, that cuts through a lot of the ethical gray areas.\nJ: I wanted to poke from the opposing side a little bit. I think there’s a non-negligible number of people who argue that by increasing consideration for ethics, even in the medical industry where I think a lot of this regulation is important, they say that the regulations are too tight to enable innovation at a speed that is continually beneficial to progress by imposing all of these restrictions on what you can do2. It limits people from trying new things and innovating and developing new technologies that potentially could have far greater upsides than we could have predicted.\nC: Every technology for the entire duration of human history has been modulated by understanding that it doesn’t exist in a vacuum, that it exists in an increasingly complicated sociotechnical space. There’s no telling ourselves that there’s neutrality. Doing so ignores the actual mechanisms of human community, human society, human civilization as a whole. So if that stifles innovation, if that means a given technology is five years late, isn’t it worth being careful? Isn’t it worth being safe?\nRegulations are imperfect. But they are a way of codifying constraints or guardrails. And, you know, maybe it’s okay to slow down a little bit, right? If we’re gonna reduce harm, maybe it’s okay to slow down a bit.\nWhat can we do?\nJ: One take I have been seeing a lot is that top-down regulation is explicitly bad and we should “decentralize.” And I think one interesting aspect that I spend a lot of time thinking about is the value of decentralization when it comes to technology. A lot of these new technologies like blockchain treat decentralization as an end rather than as a means.\nBut there’s a lot of use cases that are very much building decentralized applications for the wrong reasons. I don’t think decentralization is objectively a good thing [on its own], but rather something that can return agency to users. A lot of my research work this summer has been figuring out how to apply, for example, the net neutrality debate to data. The net neutrality debate was very much about separating content from providers; similarly, how do we separate data from applications?\nA lot of our modern centralized providers are incredibly successful and have such large network effects. It is so hard to migrate from them because they have these huge data moats where it’s impossible to just move from Facebook, for example, to some other provider that claims to be better. So a lot of it has been asking questions of how we reclaim agency for people to choose how to use their data, how they want to store their data.\nI think the convincing case for decentralization is in terms of enabling agency for people to choose what types of frameworks they want, rather than having to be locked into these providers.\nC: Twenty some years later, we’re never going to have an internet that’s 100 percent open source. And hopefully, we’ll never have an internet that’s 100 percent closed or proprietary either. What we have to recognize is the reality of where we are — that we need more than data portability. We need data autonomy, and we need permeability between closed and open systems. And I believe permeability as opposed to mobility is an important aspect.\nJ: How would you differentiate permeability and mobility?\nC: Mobility means you can export all of your tweets. Okay, well, what are you going to do with that? Can you import them into Mastodon? No. So when you withdraw your data, it becomes valueless. It’s not in a form that you can ever reuse. So is that really ownership of your data? No.\nJ: Yeah, one common theme I am noticing in a lot of retrospectives of older peer-to-peer projects that have been alive for a while but haven’t really garnered any major usage is that they’ve thought about all these ways to create new shiny platforms that claim to be better or give more agency to users, but no one has really thought about how to off-ramp easily from existing systems to get people onto these new platforms.\nC: I think the way we guarantee that kind of permeability between closed and open systems is through standards.\nBut the problem is that representation in these standards bodies are primarily private corporations, and they are trying to influence things. Logically, they will try to veto things that will probably impact their business models. Amnesty International actually wrote a paper in 2018 where they flat-out said that for platforms like Google and Facebook, their entire business models are predicated on human rights abuses and on harvesting data and surveillance capitalism. So we can’t expect those companies to do the right thing for the right reason. And those are the companies that have not only the economic power, but also power in the standards bodies and governing bodies. I think that reclaiming standards bodies is a way of having meaningful consequences for willful violations of standards. I think that’s critical.\nJ: I agree. At some point, being able to download the source code isn’t enough. Governance and accountability is critically important too.\nWhat do you think the first step to reclaiming standards bodies even looks like? I read the Decentralized Identifiers (DID) specification a while ago and I remember that of all of the W3C members, only three members had formal objections about the proposal. They were Google, Apple, and Mozilla. When you propose new standards that inevitably will undermine the business model of these large companies, it feels incredibly difficult to get these pushed through.\nAlso, a lot of these standards and processes are illegible to people new to this space. I read a lot of these, like Internet Engineering Task Force (IETF) proposals, and they’re ridiculously long at times, often almost 100 pages. It feels like some of them require decades of experience working in the space to even begin to have a voice. It’s incredibly difficult for the average person to participate and make meaningful decisions.\nSo if you want to create a widely accepted standard, is there any way to do that without dismantling these original systems with these large corporations sitting on the standards boards?\nC: I don’t know the answer to that. I don’t know how we do it. But I think we need to prioritize it. I think we need to figure it out.\nThere’s a growing trend: More people are asking the question of whether tech companies should pay for the tech that open source contributors are freely giving them today.\nI think we have a lot of power. As participants and members in good standing in the open source community, we should find ways to hold these large players accountable, either by threatening their primacy through the development of alternatives, but also figuring out how can we pressure them to make governance as intentional, as equitable, as diverse as we’ve done within our developer communities.\nIt’s sort of a radiating effect, right? We’ve normalized codes of conduct. Now let’s normalize representative and equitable governance of open source projects. Let’s go a step beyond that and talk about standards and enforceable standards. And then beyond that, of course, we have the legal aspects.\nI think of an InfoSec metaphor. You have a server on the internet. It is not secure, as it is impossible to secure any resource on the internet 100 percent. But what we do is we add layers of protection, layers of privacy, layers of security to make it not impossible to breach a system, but make it so involved and so expensive, that it’s no longer worth someone’s while.\nAnd I think that’s an approach that we can take with the development of ethical and equitable technology as well. If we’ve made it difficult for Google to sweep issues like accessibility standards under the rug, then we’re incentivizing them to do the right thing. Because if they don’t, they will lose status, they will lose their ability to draw employees. We as developers do have the ability to exercise some moral authority and we have the ability to decree meaningful consequences for the corporations.\nWhere do we go from here?\nJ: How do you think we can best bring about these changes to build more ethical and equitable technology? The logical path forward seems to be either finding ways to empower a more diverse set of people to help build these standards and technologies or regulating how open source is used by these large companies through licenses and the law.\nC: To be clear, ethical source is not about licensing. Ethical source is not about the Hippocratic License. It is not solely about legal instruments for trying to protect the vulnerable, marginalized, and under-represented. It’s bigger than that. It’s about codes of conduct. It’s about governance. It’s about social contracts and rights.\nJ: It’s about building those layers, right?\nC: Exactly, that layered approach. I think it represents a movement to change our mindset to really ask ourselves some of these difficult questions, to look at who’s making the decisions around what technologies are developed. Are we comfortable leaving these decisions in the hands of those players?\nIt’s not a fire-and-forget thing either. It’s not a problem that we solve at the beginning and we’re good forever. These systems that can cause harm and perpetuate systemic inequities — they’re not static either. Just like how consent isn’t an event, it’s a process, harm reduction is also not an event, it’s a process.\nAnd I think we have to start normalizing those processes [of harm reduction at all of these different layers], if we want any chance at all of allowing the internet to be the incredible force for good that it has the potential to be3.\nAnd, as you said, part of that is being interdisciplinary, transdisciplinary, and multidisciplinary in our thinking. Bringing together the people who have different areas of expertise, whether it be technological or social in nature, because, you know, a lot of these problems that we’re facing have happened before. They’re not new problems.\nJ: I think a big part of this is moving beyond the individualistic perception of open source as a lone hacker in the basement and more towards curators and crafters of a community around this project that you’re building.\nBased on personal experience, I think building a visible community around synchronous interaction with the actual users makes such a big difference when it comes to maintenance. As a maintainer or creator of a software library, most people are like, “I just want to make new features and do whatever works best for me.”\nBut there are so many edge cases and small bugs that don’t work for a lot of users of your library. For example, users from Saudi Arabia said that they would really appreciate right-to-left support using Quartz. How do we enable people from all over the world who weren’t necessarily the users that you initially had in mind to still be able to use the software and tools you make?4\nI think by curating a community that is open and accepting of more types of people and getting them to suggest and contribute.\nC: Absolutely. And I think a lot of that comes down to ingroup-outgroup biases. Just look at it from the perspective of how much open source technology is simply tooling for people just like us.\nJ: Yeah. And I think this is a great call for more diversity in the space as well. A while ago, a friend and I noticed that the people who build developer tooling and tools for other developers do that because that’s the only problem they’ve really known their entire lives! And by bringing people who’ve had problems and experiences in other fields, then you start getting useful applications of technology in those areas. So this is a call for all types of people to contribute to technology, to contribute to open source in hopes of a more diverse future.\nC: And we do that not through consultation, but meaningful empowerment. Getting people who are not like us in positions of power, by yielding power and distributing agency.\nFootnotes\n\n\nThis model emerged after reading ”The Ones Who Walk Away From Omelas” by Ursula Le Guin for the second time. The story presents a classic utilitarian problem: is it morally justifiable to inflict suffering on one person in the service of others’ happiness (and a potential utopia)? Is it then morally just to develop technology to benefit others knowing that it will exacerbate the suffering of marginalized groups? Is progress to one person necessarily progress to the collective? ↩\n\n\nDo we care more about technological progress or social progress? Historically, Silicon Valley has valued ‘moving fast and breaking things’, but progress implies direction. What is progress towards? Who decides that? The relatively new field of Progress Studies attempts to critically take apart and answer this question, including looking at the potential drawbacks and mitigating risks of progress. ↩\n\n\nSystems of feedback and regulation are incredibly important if we want to prevent absurd and tragic events from happening on the internet. As Ali Akkhatib states in his work To Live in Their Utopia: “Absurdity follows when algorithmic systems deny the people they mistreat the status to lodge complaints, let alone the power to repair, resist, or escape the world that these systems create.” ↩\n\n\nMany treat algorithmic systems as ‘mathematically pure’ objects, taking only pure inputs and producing pure outputs. To these engineers, human lives are treated as ‘externalities’ that spoil that purity. Impacts on humans should be first an foremost. To quote Runar Bjarnason post on the future of software, “Why does a computer even exist? The reality is that computers exist solely for the purpose of executing programs. The machine is not a metaphysical primary. Reality has primacy, a program is a description, an abstraction, a proof of some hypothesis about an aspect of reality, and the computer exists to deduce the implications of that fact for the pursuit of human values.” ↩\n\n\n"},"posts/paid-open-source":{"title":"A case for funding Open Source","links":["thoughts/internet-computing","thoughts/git","thoughts/money","thoughts/Internet","thoughts/maintenance","thoughts/public-goods","thoughts/interdependence","thoughts/incentives","thoughts/attention-economy"],"tags":["fruit"],"content":"An exploration of processes in open source, the value it provides, and how money fits into the picture.\nWorking in Public: The Making and Maintenance of Open Source Software\nHow it’s made\n\n“Open source developers were frequently characterized as ‘hobby’ developers, because the assumption was that only companies could make ‘real’ software.”1\n\nAs it stands, there are two primary schools of thought about how open source software is created.\n\nFirm-based production involves companies, organizations, governments, or any institution with centralized resources. Their driving thesis is that only companies make software because, from a coordination standpoint, centralized firms are the most efficient way to manage resources. Most development done this way is motivated extrinsically by means of monetary compensation.\nCommons-based production is a more vague concept that involves a distributed group of developers that work on a resource that is used, owned, and governed by its own community - free of employer affiliations. Most development done this way is motivated intrinsically, people do work because they want to do it.\n\nTraditionally, software has been seen as a product of firms. Open source developers were often treated as hobbyists and the projects they made trivialized as toys. The assumption was that only companies could make ‘real’ software. However, the rise of Internet computing and collaboration tools like Git have decreased the barrier to entry enough that producing software through a commons is now feasible and very much alive. The success of projects like Apache, Linux, and FreeBSD proved just how successful a commons-based method of production could be.\nSurprisingly, this may also help to explain why some developers view open source and money as completely separate. If the commons-based method of production is rooted in intrinsic motivation, then money, an extrinsic motivator, will be seen as opposite to core ideals that open source stands for.\nCreation vs Maintenance\n\n“Creation is an intrinsic motivator, maintenance usually requires extrinsic motivation”\n@balupton, isaacs/github #167\n\nWhen an artist finishes a painting, or a runner finishes a marathon, that usually signifies the end of said responsibility. There is no such finish line for an open source project, even after pushing out an initial product.\nCreating a project is fun. It’s a wild exploration into a new idea, a frivolous journey to create something useful or to learn something new. As cloud platforms continue to eat the world, the costs of distributing and sharing a project are almost completely nullified.\nJust a few clicks and a few taps of your keyboard and your project is readily available to any of the 4.66 billion people around the world with Internet access. This adrenaline rush of finally releasing the labour of your work onto the world is the moment developers are constantly chasing. For most developers, the process of creation and distribution is intrinsically motivated; it’s an enjoyable process.\nMaintenance is less so. This is akin to a writer that’s been asked to edit and revise the same book day in and day out, long after they’ve reaped the initial financial and reputational rewards from its creation. Even when the creator wants to leave the project to work on something else, they can’t. They’re tightly shackled by the fact that hundreds of thousands of other organizations, companies, and tools rely on their code to keep their operations running. Bringing on additional developers may not help either, as they still require onboarding, code reviews, and general guidance.\nCode may be nearly free to create and distribute, but maintenance is still expensive.\nTypes of code\nCode as an artifact\nThere are two main ways we can look at code. The first of which is static code. Code that, on its own, does nothing but exists as an archive. Others can copy and download the code without incurring any additional costs to the author. For the maintainers, it should make no difference in regards to cost whether 10 or 10,000 people use it.\nThis type of code is a pool resource or public good, it is\n\nNon-rivalrous. My ability to copy the code doesn’t affect your ability to copy it. (This isn’t exactly true due to some marginal costs but I’ll discuss this later)\nNon-excludable. If someone has a copy of the code, it is very difficult to prevent them from sharing that code with others.\n\nAny code that is in this state is easy to share, copy, and distribute. This is the type of code that lives dormant on Github, on StackOverflow answers, and in GitHub’s Arctic Vault2. However, the main purpose of consuming code is not to simply read and study it, but to actually use it and to let it interact with other code.\nIn doing so, we bring it to life.\nCode as an organism\n\n“Open source code derives its value not from its static qualities but from its living ones.”1\n\nAs soon as you hit CTRL-V on that snippet of code, as soon as that static code is inserted into your own, that code comes to life. Immediately, it might surface ridiculous amounts of red squigglies, break other code, or force you to rewrite your previous code just to make it work. But when code transitions from a resting static state to an active living state, it starts to also incur a set of hidden costs.\nLike a living organism in a symbiotic relationship, there is a mutual interdependence between it and others in the software ‘ecosystem’ in order to survive. As a result, this ecosystem requires constant upkeep to ensure that components don’t fall out of balance: dependency bumps, documentation updates, and infrastructure changes.\nFree as in speech, not as in beer\n‘Free’ software doesn’t refer to its price. In fact, ‘free’ software is often extremely expensive. As Richard Stallman first described free software, it’s “free as in speech, not free as in beer.” The point Stallman was trying to make was that ‘free’ refers to what one could do with the software, rather than the price tag.\nLatent cost of software\nIn reality, code in its alive state is more like a free puppy. In the beginning, it’s a great and wonderful thing! Super fun and super cute. As it grows and gets older, you realize “geez, it actually takes a lot of my own time to take care of this thing.” Unlike a piece of inanimate furniture, bringing a living creature into one’s home comes with bringing in a new set of responsibilities too.\nMarginal costs are costs increase on a per-user basis. I mentioned earlier that these costs mean that software is actually rivalrous, meaning that at some point, the project won’t be able to support the n+1th user. Some of this cost comes from physical infrastructure like code hosting and infrastructure. However, the majority of the cost comes from user support. Say you have a billion users and only 0.1% of them require support. If it takes you roughly 10 minutes to resolve each issue, you would still need 20,833 people working 8-hour shifts a day just to be able to keep up with the support volume. Maintainers are constantly wrestling with keeping their issue volume low and questions answered. Eventually, it just becomes a hindrance preventing them from working on the core product.\nTemporal costs are those which build up and compound over time. Most of it comes from technical debt, choices that are easier today at the expense of time and money in the future. This is the eternal battle against entropy: the inevitable decay of systems over time. When code changes, all the supporting knowledge that surrounds it must be updated too. Documentation, tutorials, programming books, videos, and more slowly become obsolete.\nPaying off these latent costs is seldom intrinsically motivated. When people talk about how fun making new projects is or contributing to open source, it’s never referring to writing documentation or refactoring code. This isn’t the ‘fun’ part of writing software. This is the nasty upkeep that goes into maintaining a building from the 1850s that’s had new rooms, plumbing, and electric wiring frankenstein-ed into it over the years.\nFunding Open Source\nI first started on BentoML3 as a casual contributor last summer, submitting a few decently sized PRs. It was almost all intrinsically motivated; I found issues that I enjoyed working on and that I knew I would learn lots from. Satisfied with my experience, I decided to join the team as a paid contractor expecting to just continue the type of work I was doing in the summer. As issue after issue piled on, I slowly started to realize just how much extra work being a maintainer meant and why it was a paid position. Making proposals, triaging issues, adding tests, and writing documentation took up the majority of my time. While I recognized it was important work, it was not work I was intrinsically motivated to do. Thus, to motivate people like me to get that work done, an extrinsic motivator — in this case, money — needed to be applied.\nHow do we best incentivize maintainers to work tasks stripped of the very excitement and promise of creation that initially drew them to the project in the first place? There is a jarring disconnect between work that is needed versus work that is intrinsically motivated. This is where I believe open source funding should play a role.\nFunding projects\nOne possibility is to fund projects directly. This route builds a brand around the project. The status of the project then transcends any single person’s contributions and becomes a tangible entity that has the brand recognition and reputation that comes with becoming an independent entity.\nProjects also tend to attract corporate/government funding much better than individuals can since companies are more comfortable paying for a product (code) than for a one off contract (talent). As part of the transaction, companies are typically promised service availability, influence in decision making, or technical support. However, this tradeoff also means that projects lose a bit of their freedom. Not only do maintainers have to worry about the future of the project, they need to make sure that the agreements laid out between the project and the companies are met too.\nFunding individuals\nA more individualistic model would provide greater flexibility and avoids the centralized governance issues that are so antithetical to what open source stands for. Maintainers then don’t need to deal with figuring out who should get paid how much as each maintainer is responsible for securing their own funding (if even needed). Unlike a company, what each maintainer looks to get from contributing to a project may look completely different. One may be looking to build technical skill, another to gain reputation in the community. From a governance perspective, funding individual developers is also better aligned with the distributed nature of open source projects too.\nAs the world’s media moves towards empowering independent creators through platforms like Twitch, YouTube, and TikTok, funding individuals is becoming an increasingly viable option. The way most of these creators make money is through patronage rather than donations. Although often conflated with donations, patronage is an interest and commitment to following a creator’s future work based on their current reputation rather than a one-time tip for their current work.\nWhen you are funding an individual, you are paying for the regular delivery of well-defined value.1 There are three key parts to this:\n\nPaying. This is an ongoing commitment to the production of content, not a one-off payment for one piece of content that catches the eye.\nRegular Delivery. It isn’t random discovery, rather the content is delivered directly to the user via email or application.\nWell-defined value. It is clear what the money is going towards and what value it is helping provide.\n\nFunding individuals means trusting not just in the projects they are currently working on, but also that they will continue to deliver future value too. Rather than being tied down to one project, creators then have the creative freedom to apply what they learned and create more groundbreaking initiatives.\nMixing money and open source\n\nQ: Won’t financial rewards adversely affect developer’s incentives to contribute?\nA: Yes, but it depends on who you’re funding.\n\nAttention is the main currency of production. Attention is what you divert when you choose to focus on prioritizing a feature request over adding support for a library that you promised to add a few months back. Attention is what limits you from doing everything at once.\nAttention, then, is a common pool resource. It is non-excludable (anyone can bid for their attention) and rivalrous (limited attention). But, by charging for access to maintainer’s attention, it then becomes a private good: excludable and rivalrous. The belief is that, by making attention excludable, the quality of contributions will increase as contributors and users compete for the attention of producers.\nThere are two main camps on funding open source projects. The first camp believes that we should pay all maintainers to keep their projects alive. The other camp believes that paying project maintainers will destroy the entire ideology that open source is based upon.\nI stand somewhere in between the two camps. I’m a big proponent of “if it ain’t broke, don’t fix it.” There are a lot of competing motivations already, both intrinsic and extrinsic, that powers open source today. We shouldn’t touch the parts that are currently working. Rather, we should focus on funding places that are absent from existing motivation like software maintenance and documentation\nConclusion\nThere is an abundance of open source projects and casual contributors as it stands today. With the ever lowering barrier to entry, these will only become more abundant. Casual contributors already incur a marginal cost on maintainers, their contributions need a maintainer to review whether it’s okay to merge or not. Adding more extrinsic motivation will just exacerbate this existing problem. This is the same reason why initiatives like Hacktoberfest, which promises to give a free t-shirt to anyone who makes a few pull requests, are counterproductive to open source.\nWhat isn’t significantly increasing are the number of maintainers responsible for maintaining the existing projects we have today. We should be funding maintainers who already have the contextual knowledge to be able to effectively tackle issues and guide the project. Funding will be an important extrinsic motivator to make sure that the difficult work that needs to be done, gets done.\nThis is not, by any means, an all-encompassing post about funding in open source. If that’s what you’re looking for, then Eghbal’s book is a great starting point. What this post does do, however, is raise important points about the processes behind open source and prod at why exactly we need to fund these seemingly ‘free’ processes. In order for open source to continue on its growth trajectory, these are questions we need to put more collective effort into thinking about.\nFootnotes\n\n\nWorking in Public: The Making and Maintenance of Open Source Software by Nadia Eghbal ↩ ↩2 ↩3\n\n\nGitHub’s Arctic Vault ↩\n\n\nBentoML ↩\n\n\n"},"posts/play":{"title":"Play to Win: A Post-Work Society","links":["thoughts/play","posts/hackathons","thoughts/paratelic-action","thoughts/How-to-do-Nothing","thoughts/The-Grasshopper,-Games,-Life-and-Utopia","thoughts/attention-economy","thoughts/selfish","thoughts/games","thoughts/fiction","thoughts/decentralization","posts/new-words","thoughts/plurality"],"tags":["fruit"],"content":"Extension of my thoughts about play and discussions from our Interact Circle on Hackathon Culture and Play.\nDefining play\nI’ve spent so much time thinking about how to ‘bring back play’, yet not a lot of thinking about what play is. The more I think about it, the more nebulous it feels to define. I know examples of it when I see it — the building of blanket forts during sleepovers, the joy of beating Minecraft for the first time with your friends — but putting it into words feels difficult.\nThe most intuitive definition is one that comes from Bill Watterson: play is anything that you do on your own volition or agency just for its own sake. It is an intrinsically motivated, paratelic activity.\n\nPlay is the act of enjoying the process, the means to the end. To have as much fun as possible along the way. Quoting Kernel, to “turn life into a canvas, rather than a graph with checkpoints”.\nYet this definition still seems inadequate. What separates work from play? Are they mutually exclusive? What about those who cannot afford time out of their day to do anything but work? Those who play games professionally?\nHere, I turn to Jenny Odell’s How to Do Nothing and Bernard Herbert Suit’s The Grasshopper: Games, Life, and Utopia to further explore this definition.\nHow to Do Nothing\nOdell’s book How to Do Nothing focuses on resisting the current attention economy. Doing ‘nothing’ within a productivity-obsessed environment can, in fact, help to restore communities, both locally and beyond.\n‘Nothing’, in this case, refers not to actually doing nothing but rather to not participating in the attention economy and hustle culture. ‘Nothing’ means doing nothing productive.\nRecently, I have been grappling with the question of whether self-care is selfish. My inner self wants to be able to do nothing: just do projects I find fun, hang out with my friends, and go outside. However, I know that long term, working hard at doing well at my job and in school will do me more good, career-wise.\n\nHow can I justify setting aside time to do nothing when I have so many people asking for my time and pulling me in so many directions? In the attention economy, attention is scarce.\nPlay allows us to create environments where saying ‘no’ is okay. By definition, nobody forces you to play. Play is what allows us to create local spaces of abundance.\n\n“Caring for myself is not self-indulgence, it is self preservation, and that is an act of political warfare.”\n\nPlay is the catalyst that will enable the post-attention-economy society. But what do we do when we get there?\nThe Grasshopper\nOf all the things you could do in utopia, why would one play games?\nThe Grasshopper was a book unlike any I’ve ever read before. A talking grasshopper and his insect disciples convinced me, through Socratic dialogue nonetheless, that “refusing to work and insisting upon devoting himself exclusively to play” is a perfectly acceptable thing to embody.\nWhereas Odell focused on how to get to a post-work society, Suit’s approach to play and games focuses on what to do in a post-work society. A utopia of doing absolutely nothing feels dreadfully boring. What is there left to do? To play games, obviously!\nWhen I say ‘game’, I don’t just mean chess, basketball, or a video game. I mean a more holistic one that includes climbing mountains, the pursuit of knowledge, and the creation of art itself.\nGames, as Suits defines it, are “goal-directed activities in which inefficient means are intentionally chosen.” In games, one purposefully ignores the more efficient method. To play a game is to do the crossword puzzle without looking up the answers, even though looking it up would be the most efficient way of finishing the crossword. The added rules and restrictions are what make possible the act of playing outside of the endless pursuit of efficiency.\nPlaying a game is the voluntary attempt to overcome self-imposed obstacles.\nA post-work society\nIn fact, I posit that play is not only necessary but the only thing that can exist in a post-work utopia. Let us explore this argument through a Socratic dialogue.\n\nCharacters\n\nG: the player of games.\nS: the disillusioned worker, a skeptic.\n\nG: I would rather die than work another day, toiling away to produce goods and services in an attempt to sate the infinitely hungry needs of society.\nS: You talk as though there are only two possibilities: either a life devoted exclusively to play or a life devoted exclusively to work. How will you feed yourself? Put a roof over your head?\nG: The only argument against the life I wish to lead is death. One dies if they do not work. Yet, death is also inevitable. To continue to work is to bring about the death of my character, my curiosity about the world, my will to exist. To continue to work would effectively cause my death. If I die regardless, I might as well live a life I fully enjoy, if not short. What is life for, if not to enjoy it and to seek pleasure out of things you find valuable?\nS: This seems awfully hedonistic of you. What about personal fulfillment? Don’t you have responsibilities to the rest of society? If everybody lived like you, society itself would collapse.\nG: I am not saying everyone should lead the life I claim to want. I simply claim that this life is the one that would lead me to be the most fulfilled. However, is it wrong to dream of a future where a life like mine could be the norm? To create shared fiction we can rally around and build towards? Having dreamers who believe in and drive this vision forward are a necessary step to manifest it into reality.\nS: I guess there is no harm to dreaming a little. I’m curious what this future of yours could look like.\nG: Wonderful. Let us imagine an utopic future where all work has been automated by machines activated purely through thought, requiring no labour to maintain its running cogs or to provide its goods and services to society. This is a utopia of abundance. Anything you could wish for, you can have: food, luxury, knowledge, happiness. In this utopia, one does not need to work to sustain themselves. I argue that there is no work to be done here. In fact, the only rational activity is to play.\nS: Forgive my interruption, but how do you define ‘play’ here? Isn’t it at odds with your definition of ‘work’?\nG: I suppose I should clarify some terminology. Let us first define something as instrumental if it serves as the means to an end (i.e. as a way to accomplish a goal). Work then, is explicitly defined as labour which is instrumentally valuable.\nPlay, on the other hand, is defined as labour which is non-instrumental. It should be intrinsically valuable and self-motivated. Games, then, are the reversal of means and ends. When playing, the means that traditionally entail an end become the ends themselves.\nLet us take mountain climbing as an example. Say person A considers reaching the top as the end goal. For A, climbing is just a means of reaching the top. If a helicopter came by to offer them a lift to the top, they would happily take it.\nSay person B climbs mountains for the thrill of climbing itself. B actually doesn’t really care if they reach the top each time, it is just a means for them to climb more mountains. Even if a helicopter offered B a ride to the top, they would happily decline and continue their trek up the side of the mountain.\nPerson A clearly considers mountain climbing work whereas B considers mountain climbing play.\nS: Ah, I see. This clarifies my understanding of how you define ‘work’ and ‘play’, but I still have a counterpoint. What about those who enjoy their work? The scientist, who after a great effort, has a major breakthrough on a problem they’ve been stumped on for ages. Far from rejoicing in the discovery, they are eagerly searching for their next challenge to be engaged once more. The carpenter who builds houses because she likes how they look and the feeling of it coming together. Can these individuals not exist in your utopia?\nG: Hmm. I would posit that both of the mentioned individuals are actually playing games. The resolution appears to be the fact that activities which one views as instrumentally valuable (work) can, for another, be intrinsically valuable (play). The human experience is subjective, there is no universal standard for whether society considers something work or play. An environment of play is created when one self-imposes rules to prevent them from the most efficient way of achieving their goal.\nTo the carpenter who enjoys building for its own sake, that otherwise instrumental activity has intrinsic value as well. The same could be true of anyone who really enjoys their work, whatever that work might be. This is the definition of game playing.\nS: This does make a lot of sense. So in this utopia that you propose, I could theoretically just do what I find intrinsically valuable — play? And I suppose the rest of my needs would be met by the machines?\nG: Correct.\nS: I am excited to create this utopia of ours. Let us dream together then.\n\nA re-worked definition\nObviously, this is an exaggeration of the argument I’m hoping to make — we clearly don’t live in this caricature of a post-instrumental society. However, we do live in is a society that is malleable to change.\nLanguage is one of the only logically decentralized aspects of humanity. It is also one that has considerable effects on how we think about ourselves and the world.\nWe started off this journey by asserting that our current accepted definition of play was inadequate. In its place, we offer a plurality of alternatives:\n\nPlay is an intrinsically motivated activity.\nPlay is labour which is non-instrumental.\nPlay is the act of enjoying the means to the end.\nPlay is what allows us to create local spaces of abundance.\n\nRe-defining play gives us power in the form of shared fiction which we can build towards and manifest into reality.\nHere’s to building the future we can play in."},"posts/pm/计划管理":{"title":"搜藏管理","links":[],"tags":["搜藏管理"],"content":""},"posts/primacy-of-consciousness":{"title":"On Consciousness","links":["thoughts/philosophy-of-science","thoughts/mind-body-problem","thoughts/Hard-problem-of-consciousness","thoughts/metaphysics","thoughts/Neural-Correlates-of-Consciousness-(NCC)","thoughts/Materialism","thoughts/map-as-territory","thoughts/Descartes'-Meditations","thoughts/Panpsychism","thoughts/Primacy-of-Consciousness","thoughts/truth","thoughts/Consciousness-is-not-Information","thoughts/emergent-behaviour","thoughts/idealism"],"tags":["fruit","PHIL451A"],"content":"“Bridging the explanatory gap from the other side” @midjourney\n\nAn exploration into the philosophy of science.\n\nWhat is consciousness even?\nFor many, it is the ability to be human. To feel the warm of sunlight on our skin, to see the redness of sunset, to taste the crunch of a sweet apple, to love and be loved. Distilled, it is the ability to have subjective human experience.\nPhilosophers in particular define consciousness as experience. Something is conscious if there is an experience to be like that thing. But there is a deeper question that philosophers have been digging at for the past few millennia: why should all of this experience feel like anything? How do we get from the physical act of feeling things in our environment to the mental experience of being human?\nThis is the mind-body problem and the fundamental question of the hard problem of consciousness.\nEver since the turbulent events of the early 20th century, logical positivism had flowered. Logical positivism is a belief that scientific knowledge is the only kind of factual knowledge, discounting and pushing away the realm of metaphysics to the sidelines. The question of consciousness was (and in many ways, still is) taboo to many philosophers and scientists alike, perceived as too “new-age-y” or metaphysical to be considered a worthwhile pursuit. Philosophers were encourage to “earn their keep” by providing actual contributions to science rather than musing about what it means to be human.\nAs metaphysical questions about consciousness faded slowly, behaviourism found its footing through psychology. Behaviourists believed that the mind could be entirely understood purely by reducing it to its environmental inputs and behavioural outputs.\nTo give them credit, many scientists found success through this approach. Scientists figured out how to correlate brain states with solving problems and even feelings of pain and pleasure. They figured out how activations in the V1 area of the cortex react to certain colours like the redness of an apple. Further physiological work discovered the existence of the V4-V8 areas which had an even stronger correlation with shapes, motion, and lighting conditions. The moonshot goal was to build up to a full neural correlate of consciousness or NCC — some way of explaining what physical brain states correlate to mental ones.\nYet, at the end of the day, all of this work only proved correlation. Behaviourist approaches were no closer to answering the causal question of how physical brain states produce mental ones. Any theory that attempted to explain consciousness in terms of the physical is forced to take no less than a leap of faith from the objectively physical to the subjectively mental. This leap is across what is known as the explanatory gap.\n\n“The mind-body problem is not just a local problem having to do with the relation between mind, brain, and behaviour in living animal organisms … it invades our understanding of the entire cosmos and its history.” (Thomas Nagel, Mind and Cosmos)\n\nThis isn’t just a problem for philosophy or neuroscience but rather our entire understanding of the physical world — this is the blind spot of science.\n“Vector illustration of hands grasping at the explanatory gap” @midjourney\nScientific materialists argue that science and the scientific method enables us to get “outside of experience” and grasp the world in and of itself. Yet, subjective experience is present at every step1. When we look to send people to the cosmos, we do so by formulating theories and models about how we think they work. All of this depends on the subjective experience. We look at the results of our complex telescopes and formulate theories based off of what we have learned and have observed in the world. We pull scientific models from our experiments and observations but again, these are models and idealisations, not actual instantiations of things in the world.\n\n“In principle, it is absurd to think that we can explain consciousness by reducing it to certain objects of science, since these objects are abstract relational structures extracted from the life-world of lived experience” (Husserl, The Crisis of the European Sciences and Transcendental Phenomenology)2\n\nGottfried Leibniz, Immanuel Kant, Arthur Schopenhauer, and Bertrand Russel were all strong believers that a fully physical account of the world actually offers no explanation of the intrinsic nature3 of the things within it.4 The ideal gas law tells us how pressure, volume, amount, and temperature of a gas are all related to each other, but tells us nothing about what each of those things in and of themselves are. Chemistry tells us that Carbon has an atomic number of six. At first glance, this seems be an intrinsic property. But probe deeper at what an atomic number truly represents and all it is the number of protons it has. Protons themselves are not “real” things. They are a convenient model of how this group of abstractions we call quarks behaves together depending on their relations. Mass is a property that determines how an object will obey the relation m=aF​. Again, it is abstractions all the way down. Purely physical descriptions tell us not what matter is but what what it does.\nPhysics, by name, is supposed to be a mathematical theory of the physical. Yet mathematics by nature is purely relational; numbers are quantifiers on abstract objects, formulas describe precise relations between variables. But intuitively, there must also be an intrinsic nature to these objects as well. What is an atom in and of itself? This question is not answered by a relational account of the world.\nIt is tempting to say at this point that perhaps a relational view all there is to reality. After all, this is realistically all that is useful to the functioning of society. It has enabled us to program silicon, photograph the depths of the universe, and predict weather across the world.\nYet intuitively, a world held up purely through relations does not make sense. As Hedda Hassel Mørch pointed out in her critique of physics for ignoring consciousness, “for there to be a relation there must be two things being related.” 5 Otherwise, the relationship is empty — “a show that goes on without performers, or a castle constructed out of thin air.” Mørch argues that all physical relations should be made real by some substance that itself is not purely relational or else there would be no difference between mere mathematical abstraction and the concrete universe.\nClearly, if we wish to poke beyond this veil of pure abstractions, our current explanations of our reality will not do: intrinsic natures simply cannot be captured through a purely physical approach. Materialism as given so far does not seem to stand. Taking its physical description as the totality is like confusing the map as the territory. It may be fine if you just need the map to navigate the world, but to open one’s eyes to the real world, we must dig deeper.\nWe thus try to bridge the explanatory gap from the other side. Perhaps consciousness is fundamental to reality, not the other way around.\n“The life-world contains the universe” illustrated in the style of Studio Ghibli @midjourney\nUp until this point, I have painted a picture of why the intrinsic nature of consciousness cannot be fully explained by the physical. Now, I push to make a stronger claim that consciousness is primary — namely existentially, transcendentally, and epistemologically. For something to be primary is for it to be the first and foremost, a prior for all else that comes after it.\nIt seems almost self-evident that consciousness is existentially primary — it is through the subjective human experience that the universe is disclosed to us. Arthur Eddington argued that the one thing we know concretely about consciousness is that it has an intrinsic nature.6 René Descartes famously said “cogito, ergo sum”: I think, therefore I am.7 It is the foundation upon which Descartes builds upon his certainty in his knowledge about the world. In all the ways we can be mistaken about reality, consciousness is not one of them — it is a reality that we apprehend directly and without inference. Thus, consciousness is existentially primary.\nConsciousness is also transcendentally primary. Kant defined transcendental primacy as all knowledge which is “occupied not so much with objects as with the mode of our knowledge of objects in so far as this mode of knowledge is to be possible a priori.” 8 In more colloquial language, the transcendental primacy of consciousness refers to how consciousness is not another object of knowledge, but that by which any object can become knowable.\nEdmund Husserl, in his 1936 work The Crisis of European Sciences and Transcendental Phenomenology, defines a concept called the life-world. Roughly defined, it refers to the world as it is collectively experienced. Husserl likened this model of consciousness to our visual horizon: it is not really an object, but a rather a process of uncovering or displaying potentialities.2\nIt is in this way then that the horizonal sense of consciousness is not something that can be had, but rather something we live. Quoting Bertrand Russel, “we know nothing about the intrinsic quality of physical events except when these are mental events that we directly experience.”9 As such, consciousness is transcendentally primary.\nMerleau-Ponty on the world and consciousness\nConsciousness is additionally epistemically primary — it is the source and destination of all knowledge. In creating models, we set aside aspects of experience on which we have doubts about (e.g. our senses, emotions, etc.) and extract idealised and abstract models (e.g. mathematics, physics, logic). Even the most abstract physical relation or mathematical formulas describe some “real” thing we are trying to model or express a relation between. These models ideals and models are only as useful insofar as we can implement these abstractions as things we can use to measure, predict, and control phenomena within our lived experience. In this way, consciousness is epistemically primary.\nI will pause here to clarify that I am not claiming consciousness to be ontologically primary. I am not making any sort of panpsychist claim that consciousness exists as a fundamental aspect of reality where everything has a small amount of consciousness.10 Neither am I claiming that consciousness exists inherently in the natural world as a fundamental aspect of reality.11\nRather, I am positing a form of neutral monism that sits somewhere between physicalism and idealism. Monism, in its simplest form, suggests that there is only one kind of underlying reality. A neutral stance on this does not side with either matter or mind, instead a potential 3rd substance. Russel explained this form of reality as having “a single underlying nature that is neither mental nor physical but capable of being expressed in these two different ways.”12 Much like the interiors and exterior of any object, Russel’s account of the mental and physical imply and necessitate each other as reflection of a single nature.\nIn Husserl’s horizon metaphor, the horizon is not possible without a world to be observed but the world also cannot be perceived without a perceiver. Similarly, the horizon of consciousness is not possible without the physical but the the physical cannot be perceived without the mental. It is absurd to try to reduce one completely to another.\nFrancisco Varela’s notion of “mutually generative constraints,”13 points toward a possibility where both physicalism and idealism work together towards reciprocal enrichment:\n\nPhenomenological reports may help to pick out and ascribe meaning to previously unnoticed neural configurations\nNeurological findings may become an incentive for re-categorization and further development in phenomenological research\n\nThis neutrally monistic view of consciousness does not “solve” the hard problem. Rather, the problem never even arises because the physical world is no longer the standard for being, and objectivity is no longer the ultimate standard of being.\nIt is in this neutrally monistic view that one can acknowledge consciousness as primary without necessarily needing to discount our existing objective knowledge about the world.\nInstead of absorbing or reducing contents of experience into the relational network of objective science or vice versa, we could strive towards embedding these experiences within a broader network of a potentially new amplified science of structures which we may not know of yet.\nAdapted version of my PHIL451A final paper.\n“A beautiful observatory in the park of Mars” in the style of James Paik and Ross Tran @midjourney\nFootnotes\n\n\nThe upshot is that there is no simple way to remove our experience as scientists from the characterization of the physical world. In Popperian fashion, scientific knowledge then is a self-correcting narrative made from the world and our experience of it evolving together. Popper personally rejected logical positivism as well. He believe that there are statements that have varying statements of ‘truth’ or verisimilitude relative to our conscious experience of the world. ↩\n\n\nHusserl, E., 1936, The Crisis of the European Sciences and Transcendental Phenomenology ↩ ↩2\n\n\nIn philosophy, an intrinsic property is a property that an object has in and of itself, whereas extrinsic properties are properties than depend on that object’s relation with other things. ↩\n\n\nKant, Immanuel, 1787, Critique of Pure Reason. Leibniz, Gottfried Wilhelm, 1686, Discours de métaphysique (Discourse on Metaphysics), G, IV. Schopenhauer, Arthur, 1818, Die Welt als Wille und Vorstellung. Russel, Bertrand, 1959, My Philosophical Development ↩\n\n\nIs Matter Conscious? in Nautilus ↩\n\n\nEddington, A., 1928, The Nature of the Physical World, ↩\n\n\nDescartes, R., 1641, Meditations on first philosophy, Meditation II ↩\n\n\nKant, I., 1787, Critique of Pure Reason A295/B352 ↩\n\n\nRussel, B., 1959, My Philosophical Development ↩\n\n\nIn fact, I think there are quite a few problems in panpsychist theories, namely how theories like Giulio Tononi’s IIT define information and how they fail to adequately resolve the Combination Problem. The typical panpsychist response would be to agree that consciousness could never emerge from exclusively physical processes and that consciousness exists as a fundamental aspect of reality (often referred to as the Intrinsic Nature Argument). One of the original arguments for panpsychism posited by Eddington, Russell, Strawson relies on quite a few problem assumptions (namely, that “relational properties are determined by intrinsic properties” and “own inner awareness reveals that phenomenality is an intrinsic property”). A more modern form of this argument crops up in IIT and has been widely lauded as influential in finally providing a testable hypothesis of consciousness. However, Tononi defines information in very atypical fashion and refuses to address potential edge cases in which integrated information of seeming non-conscious objects (e.g. a large number of connected logic gates which do not compute anything meaningful) is unreasonably high (and potentially infinite). ↩\n\n\nThe position of ontological idealism explores this further, positing it is fundamentally human consciousness that gives rise to the physical world. This is not a new idea, having being explored in both Eastern thought (through Pratyabhijna self-awareness) and Western thought (platonic idealism). ↩\n\n\nRussel, B., 1919, “On Propositions: What They Are and How They Mean”, Proceedings of the Aristotelian Society, Supplementary Volume 2: 1–43. pp. 283–321. ↩\n\n\nVarela, F. J. (1996). Neurophenomenology: A methodological remedy for the hard problem. Journal of consciousness studies, 3(4), 330-349. ↩\n\n\n"},"posts/rm/阅读管理":{"title":"阅读管理","links":[],"tags":["阅读管理"],"content":"\n阅读管理\n\n"},"posts/the-fools-who-dream":{"title":"Here's to the fools who dream","links":["posts/2021","thoughts/Rhizome-Proposal","posts/towards-data-neutrality","thoughts/distributed-systems","thoughts/peer-to-peer","thoughts/The-Writing-Life","thoughts/utopia","thoughts/Archipelago","thoughts/academia","thoughts/tribe-flourishing","thoughts/Jestermaxxing"],"tags":["fruit","rhizome"],"content":"\nAlternatively: A hitchhiker’s guide to independent research\n\nHere’s to the ones who dream\nFoolish as they may seem\nShe told me\n“A bit of madness is key\nTo give us new colors to see\nWho knows where it will lead us?\nAnd that’s why they need us”\nLa La Land - “Audition (the fools who dream)”\n\nA year ago, I wrote myself a letter. In it, I told myself that if I ever found an idea I had high conviction in, I would drop everything to give it a fair shot. I hoped that I would be ambitious in my dreams and to embody a quiet confidence in my own abilities and interests.\nRhizome turned out to be project that would let me do all of that. I spent this past summer doing independent research focused on how we can enable data neutrality on a web dominated by data moats. I had no academic research experience, barely knew anything about distributed systems, and didn’t personally know anyone else doing independent research. At the time, I had no idea this was even called independent research.\nI’m not sure what about it drew me in exactly. The project started out with a smattering of thoughts around peer-to-peer networking and what I found frustrating about it. Yet, the project took up latent headspace, simmering away quietly only to surface mid-scrub in the shower or on a walk to the bus stop.\nI knew that ‘good’ research practice usually involved writing a detailed research statement or proposal to present exactly what one would work on. I also knew that I had no clue how I would manifest the stirring pot of concepts into something legible. I spent the months leading up to summer bumping around in the dark, trying to phrase and form what exactly about this interested me so much. In my head it felt clear, but each time I tried to force those ideas through my fingers and onto the page they seemed to flit away and vanish, refusing to be expressed in any precise form.\nAt the time, this felt like useless floundering. After all, what kind of researcher doesn’t even know what they’re researching? Later on, I would find that this floundering was sense-making hard at work. Researching to figure out what you are researching… is still research. There is a certain amount of looking around and orienting yourself you need to do before you know what direction to head. I spent hours cautiously broaching the idea with friends over late night transit rides home. In most of those conversations, my friends nodded blankly, happy to see me obviously deeply invested into an idea but not really understanding what I was blabbing on about.\nIn these early days, I was often dejected. I felt incredibly foolish for pursuing something I had absolutely no reason to be so deeply invested in, and I felt foolish that I couldn’t explain why it felt so compelling to me. I felt terrible about my inability to get any grant money to sustainably work on this project. I felt a deep need to prove myself. Maybe it was to prove to myself that I wasn’t wasting my time. Maybe it was to prove to others around me that I was doing something just as valid as their internships, startups, or academic research.\nOver the span of a month period, I had 3 separate emotional breakdowns\n\nOn May 11th: “I can’t help but sometimes feel like I’m wasting my time — there are so many smart people working on the same problem, what makes me feel like I can be the one to make a meaningful contribution to it?”\nOn May 16th: “I’m often spending 12+ hour days writing grants and I just feel so behind. And I don’t get why!!!! I’ve been looking forward to this summer for so long.”\nOn May 27th: “Once again had a breakdown :)) Constantly feel like I’m not doing enough and that time is slipping between my fingertips…”\n\nWhen I started with the project, I had a healthy dose of naivete, a belief that anyone could make a change and make their project work if they tried hard enough. But now, I started to believe that hard things are hard for a reason; I should leave hard problems to people who are actually skilled and have spent decades of their lives working on these problems. Who was I to think that I, an undergrad student who had still yet to take a distributed system course, would be able to contribute anything meaningful to this decades-old field?\nIn mid-conversation with someone who’s opinion I cared deeply about, I realized that I strongly needed to figure out how to untie my self-worth from my project.\nMidsummer, on a whim, I picked up Annie Dillard’s The Writing Life from a local bookstore and read it cover to cover. In it, she mentions that the greatest teacher of writing is the blank page.\n\nWho will teach me to write? a reader wanted to know.\nThe page, the page, that eternal blankness, the blankness of eternity which you cover slowly, affirming time’s scrawl as a right and your daring as necessity; the page, which you cover woodenly, ruining it, but asserting your freedom and power to act, acknowledging that you ruin everything you touch but touching it nevertheless, because acting is better than being here in mere opacity; the page, which you cover slowly with the crabbed thread of your gut; the page in the purity of its possibilities; the page of your death, against which you pit such flawed excellences as you can muster with all your life’s strength: that page will teach you to write.\n\nI think it was this lone paragraph that was the turning point where I started to believe that this applied to my work too.\nTo do independent research is to learn how to confront the infinite possibility of a blank canvas. To not be intimidated by the possibility of making a fool of yourself, but to embrace the courage to even put brush to canvas, pen to paper. To assert your freedom and power to act.\nThe midwit would think it truly silly to try and change something as entrenched in our society as the very computing fabric we tap into everyday. But those who do not submit to this status quo recognize there is merit to trying regardless. Octavia E. Butler gestures to this tension between optimism and pessimism, and the possibility of actually breaking through to something new:\n\nThere is nothing new\nunder the sun,\nbut there are new suns.\n— Octavia E. Butler, Trickster\n\nThe less beaten path is often less beaten for a reason. But if it feels right to take it — pushing aside the debris and brush and walk the trail because something, a bird or perhaps a ray of light, caught your eye — then take it.\nThis realization dawned on me slowly for me over the span of about a month. I wasn’t pursuing it because society deemed my research useful or that I knew it would eventually make me heaps of money or make me incredibly famous. At the end of the day, it was enough that this research is something that I wanted to spend time pursuing.\n\n\nPeople will impose their limiting beliefs onto your world because it’s what governs theirs. They will tell you to stay in something you want to leave, will tell you to keep pushing toward something that feels wrong, feels misaligned. You always have a choice. You can yield to expectation, pessimism, set structures, or [you can take the other path].\n— Nicole, internal confidence\n\nI didn’t need to prove to anyone that what I painted on this metaphorical canvas was ‘worth’ the space that it took up. Instead of claiming to be some sort of expert who had all the requisite skills and knowledge, I began to see myself as an explorer, excited to share what I found with others. I let more people in to see what I was working on because I was excited to share what I found rather than afraid of what they thought of what I knew.\nIn early June, I had my first call with people who were as equally excited about the idea as I was. I saw seasoned veterans who have worked decades on related problems ask careful questions and gave my ideas serious consideration. These people, who I had thought would likely shun or ridicule my pursuit, turned instead to close collaborators and thought partners. This was refreshing. I was no longer pursuing it alone but also sharing this vision with others, seeing their eyes light up at the potential or future of it.\nI found it invaluable to surround myself with people like this. I lived with others who were also incredibly intellectually curious about the world. They would ask “tell me more” instead of offering blank stares of ambivalence. Their close consideration and generous imagination enabled my work to truly blossom.\n\nWhen people ask about where I am working this summer, I often laugh and reply “I’m ‘funemployed’! But what I’m really doing is independent research.”\nNine times out of ten, the follow up question they ask is “With who?” Usually, I laugh again and say that “that’s the fun part about independent research, I get to do it on my own!”\n\nThese days, if you say you work in research, most people assume you work in academia. But it’s sort of odd that we assume you need someone’s permission to do research. There’s no reason that universities need to be the gatekeepers of exploring and developing new ideas.\nNadia Asparouhova, The independent researcher\n\nWhat people often find confusing is that independent research doesn’t mean I do it by myself. Rather, it refers to how my work is not tied to any particular institution. I think the whole career advice of doing internships at big companies and academic research being the two main options is incredibly flawed. Neither leaves much space for individuals to have time or space to figure out what they actually want to do with their life. Neither leaves space for individual sense-making.\nEvery few months, I’ll get an email or two along the lines of: “Help, I’m stuck in Leetcode hell, how do I escape and do other things?” I love these emails because I know these are people who have started that introspection process, an internal questioning of “what do I actually care about and why?” and are looking for containers and institutions for their work.\nSooner or later, that line of questioning leads to a question of hypothetical utopia. In a world where you don’t need to work to stay alive, what would you do? What gives you excitement and joy? Completely ignoring what other people tell you is useful or good, what do you find intrinsically beautiful and good to do in the world?\nTo most, the answer as to what they want is clear but the difficult question is how. Having dreams and working to make them a reality is a privilege in this world. I truly think it’s one of the greatest gifts of life but unfortunately not a gift very many get to have.\nHow might we create spaces for abundance so that more people have this sort of privilege? Édouard Glissant’s Archipelago gives a glimpse into a “future [which] lies not with the great powers, but with the little islands, lands, and cities.” Not all research needs to happen within the monolithic institutions in academia or profit-hungry companies. Perhaps the next generation of innovation and discoveries that advance society will be made through small squads banding together to build things, live together, and create something more intricate, comprehensive, and wonderful than any one individual could have achieved on their own.\n\nBenjamin Franklin had the Junto Club, Tolkien and C.S. Lewis had The Inklings, Jobs and Wozniak had Homebrew. The Bloomsbury Group was integral to the success of Virginia Woolf, Clive Bell, and John Maynard Keynes, while MIT’s Model Railroad Club spawned much of modern hacker culture.\n— James Mulholland, Small Group\n\n\nTo those who don’t take this path — parents included — what I’m doing seems a little foolish. But that’s not necessarily a bad thing. The fool is often characterized as naive, a beggar, a hedonist. But the fool is also many other things that more people should be.\nThe fool is the jester, daring to challenge what other people wouldn’t. The fool is the blazer of new trails, happily taking the less beaten path because he does not know better. The fool is the one who does things because they think it is worth doing, not for coin or status.\n\nI leave this as an invitation to you. Ask yourself: “what do you really want?” Be honest with yourself. Get your hopes up a little bit; let yourself be a little foolish.\n\nI know I’m naive; I’m a dreamer. But maybe we should get our hopes up for things sometimes. Maybe we shouldn’t dampen how we feel to avoid the possibility of disappointment. Maybe we should hope for and demand everything—ask for the world, the stars, and the ever-expanding universe for ourselves. Maybe naïveté is how we keep imagining in vivid colors, connecting with rich feelings, dreaming of diverse characters.\n— Spencer Chang, quarter life commitment\n\n\nAcknowledgements\nIt is with the generosity of my sponsors of GitHub, close friends, and you, kind reader, that I’m able to continue to do this type of work.\nThank you to Anson Yu, Spencer Chang, Sebastien Zany, Jamie Wang, Raymond Zhong, Vincent Huang, Justin Glibert, Morgan Gallant, Ryan Johnson, David Zhou, Aadil Ali, JZ, Nishant Medicharla, Anh Pham, Farzaa Majeed, Amir Bolous, Aaron Pham, Rishi Kothari, Jasmine Sun, and Athena Leong for your continued support. This independent research wouldn’t be possible without all of you."},"posts/towards-data-neutrality":{"title":"Towards Data Neutrality","links":["thoughts/network-effect","thoughts/decentralization","thoughts/interoperability","thoughts/privacy","thoughts/agency","thoughts/local-first-software","thoughts/Unix","thoughts/neutrality","thoughts/blockchain","thoughts/incentives","thoughts/Byzantine-Faults","thoughts/distributed-web","thoughts/collaborative-software","thoughts/Moderation","thoughts/Rhizome-Proposal","thoughts/Rhizome-Research-Log","thoughts/skyhooks","thoughts/independent-research"],"tags":["fruit","rhizome"],"content":"\nThis essay was originally published in Reboot.\n\nDISCLAIMER: To borrow words from Robin Sloan: While it is okay to share this link, I want to underscore that I am sending it specifically to you with the hope that you will really think about it! At such a primordial stage, a proposal like this doesn’t need diffuse, drive-by attention. It needs, instead, close consideration and generous imagination.\nThe competitive advantage of the vast majority of today’s centralized platforms are in their data moats and network effects. Services like Facebook, Twitter, and Reddit conceptually aren’t difficult to replicate — in fact, your average computer science graduate could probably recreate the functionality of these apps without much difficulty. Rather, the major reason why these platforms remain so dominant is because of their data and users: Facebook has all of our childhood friends, Twitter is the go-to place for unhinged humour and political discourse, and Reddit has millions of niche micro-communities found nowhere else on the internet.\nThese platforms, especially aggregators, are incentivized to resist decentralization and interoperability. After all, ‘data is the new oil’. These services almost entirely depend on making sure that only they have access to that valuable data. Interoperability, on the other hand, means you no longer have a data moat, or a privileged hub position in the network.\n\nAs a result, apps have become inseparable from data. They are extractive, asking for ever-increasing access to our personal lives. We willingly sign over the ability to control our data, blindly scrolling miles and miles of Terms of Service Agreements because we know that at the end of the day, we have no power to change what they want from us. You can’t choose what parts you like; you either use the platform and sign all of your rights to them, or don’t use it at all. Privacy and security in this world mostly means “which company do you trust with your safety?” The answer often is the one with the largest walls and deepest moats.\n\nClearly, this leaves much to be desired. We spend so much time online that it is worthwhile to explore better ways of existing online.\nAn Ideal World\nDecentralization is not the solution for everything but it has value in empowering people to act decisively within their social contexts1. Decentralizing the Web means that people gain the ability to store their data wherever they want while still getting the services they need. Decentralization is about agency: we get choice about where we store our data, who we give access to which parts of that data, which services we want on top of it, and how we pay for those.\nIn an ideal world, instead of being forced to accept package deals we cannot customize, we get modular interoperable local-first software2 which we can stack to a global scale. Apps and platforms in this model follow the Unix philosophy: expect the output of every program to become the input to another, as yet unknown, program. Like the Lego “dot” that is the universal connector between all Lego pieces, there exists a universal API that freely enables all software of this model to freely interoperate. With a universal API, each composition between each tool increases the total possible compositions and workflows by n∗(n−1), all without developers needing to write the transformations between each one.\nIn an ideal world, there is data-neutrality. Much like how the Net Neutrality debate strives to maintain the separation of the content and connectivity markets, data neutrality strives to maintain the separation of data and application markets. Our current market is competitive based on data ownership when it could be competitive based on service quality instead. If we conceive a decentralized approach as a way to enable data and platform neutrality, application platforms and data providers can mix and match, much like how you can browse the many websites of the web on any Internet provider.\nIn an ideal world, we focus on local-first software that works independently of large platforms — at the end of the day platforms should be used to support efficiency of collaboration at scale, not to gate users from moving their data for the sake of retention.\nPeer-to-peer\nPeer-to-peer technology has existed for a while and in theory, gets quite close to realizing this ideal world on its own. Federated open source software means anyone can run their own local instances and customize them to their liking. Organizations like the IETF and W3C work on standardizing open data formats to act as universal formats to store and convert between. Yet, most platforms do the minimum they need as required by law to maintain interoperability and data-neutrality.\nSo why does it fall short?\n1. Running your own infrastructure\n\nEven nerds do not want to run their own servers at this point. Even organizations building software full time do not want to run their own servers at this point. If there’s one thing I hope we’ve learned about the world, it’s that people do not want to run their own servers.\nMoxie in My first impressions of Web33\n\nRunning your own infrastructure and servers is hard. Maintenance and upkeep of your software is hard. There is a reason that companies which offered to do that for others were so successful.\nIf one company figured out a good way to do x, it was incentivized to offer doing x as a service (hence the explosion of SAAS startups) instead of making it easy for competitors to do the same. Overtime, companies specialized at getting really good at doing x and thus became known as the go-to people for that thing. This centralization-over-time of this knowledge leads to the monopolies that we see today.\nCentralizing this knowledge in open standards and public, forkable code rather than data moats and proprietary technology is a great start but it isn’t enough if the general public doesn’t know how to use it. Just as you wouldn’t expect the average home owner to setup their internet connection, we shouldn’t expect the average person to run their own infrastructure.\nIt should be easy for people to create competing yet interoperable platform providers and it should be easy for people to switch between platform providers as one can switch between internet providers today.\n2. Data availability and durability\nThe vast majority of peer-to-peer applications have yet to solve the data availability problem. In short, all connections are ephemeral — there is no persistent state. Imagine if everybody you shared a Google Doc with had to be online at the same time everytime you wanted to edit it, or if all 3 billion users of Facebook all had to have the app open to even use it. Imagine if you had your Twitter account deleted every time you closed your browser window.\nThis means that asynchronous collaboration isn’t possible in most peer-to-peer apps. Platforms usually get around this by storing the state of a user on one of their many servers who make it available on your behalf but peer-to-peer apps do not have this luxury — most people do not have a device that is “always-on” like a server is.\n3. Existing network effects\nMigrating data off of existing platforms is extremely difficult as this is something large platforms are disincentivized from supporting. Even if there are ‘export’ tools on platforms, they are the worst they can be while still meeting GDPR Requirements. New platforms almost never have ‘import’ tools because each platform has their own data format and that format changes unpredictably. This creates a form of n-to-n problem where every app needs to know what the APIs of another app are to even begin to interoperate.\n\n“But usually you don’t want a dead snapshot; you want to “use this data elsewhere”—which requires repeatedly exporting &amp; reconciling.”\nAndy Matuschak on Twitter\n\nThis means that, even if an alternative platform offers a better service, switching is often impossible.\nThe important question is: can applications on top of decentralized data behave the same way as centralized apps? Can we still aggregate information into feeds and present a cohesive user experience even if all of our friends’ data is stored in different places?\nWhy not use blockchains?\nI admit that it is true that blockchain actually solves most of these problems. Blockchain approaches have great approaches to solving both identity and availability through a combination of wallet addresses and token incentive mechanisms. Yet, they solve it in a way that leaves much to be desired.\nBlockchain causes a whole new set of problems that makes it quite cumbersome to build on top of it. Some of the core problems that I have personally seen include:\n\nLack of ability to store large files on-chain in a cost-effective manner\nMassively reduced speed and efficiency (the global Ethereum computer operates at roughly the speed of a Raspberry Pi)\nHigh latency for transactions and finality (not to mention transaction + gas fees but I am assuming these will be negligible at some point down the line)\n\nAll of these make it incredibly unfeasible for data-intensive or real-time applications (e.g. file sharing, games, collaborative text editing) without aggressive application of blockchain scaling ideas. Of course, there are certain applications that benefit from the unique properties that blockchains possess (namely strong guarantees about consistency and message ordering among the presence of byzantine actors) that make it worthwhile for certain applications like cryptocurrencies, but for most applications these tradeoffs make it hard for end users to adopt.\nBlockchain is suitable for a very small subset of use-cases. Is there a more general purpose technology that still addresses these main problems?\nThe personal cloud\nRhizome aims to be a data-persistence and identity layer for the distributed web. The goal of Rhizome is to enable data-neutrality by separating data from applications.\nIt is made up of two layers\n\nRoot: a personal data pod that you own. Think iCloud or Dropbox but you have agency over how much storage you want, who has access to it, and what you want to do with it.\nTrunk: a framework for easily developing cohesive peer-to-peer applications on top of data from Root\n\nAs a whole, it forms the basis for a new model of the internet where first and foremost, people own their own data. This enables entirely new dimensions of computation and collaboration on the web.\n\nSingle purpose apps backed by general-purpose data4. Apps in this new model are now just views on top of data rather than a tight coupling of data and logic. If two apps are views on the same data, any change to the underlying data will instantly update both apps.\nApplications ask for access rather than store their own data. Instead of maintaining a separate log-in for each app, you give apps permission to read or write specific parts of your data.\n\nAdditionally, this means that existing platforms to relieve themselves of the impossible burden of being the steward of moderating what every person on their platform is doing.\n\n\nLocal-first means interaction times are measured in microseconds not seconds, resulting in more responsive-feeling applications and no loading spinners.\nTwo users can collaborate by simply ‘inviting’ another to temporarily synchronize a subset of their data. Developers no longer have to worry about building out separate infrastructure for live editing or collaboration.\nAs there are separate markets for data and applications, it creates competition based on service quality rather than on data ownership.\n\nWith Rhizome, we get the convenience of a single centralized platform without the lack of agency that comes with it.\nYou can find more technical details in the proposal, and rough notes from my day-to-day in my research log.\nI encourage you to imagine with me what a world like this could look like. I miss when we would dream of worlds to come, filled with exciting possibilities and hopeful futures. Think of this project as a skyhook: a dream about a future so that we may build towards it.\n\nWe live in capitalism, its power seems inescapable – but then, so did the divine right of kings. Any human power can be resisted and changed by human beings. Resistance and change often begin in art.\n(Ursula K. Le Guin)\n\nQ &amp; A\nThis proposal has seen a lot of evolution. How does this current version differ from your initial proposal? What shaped your ideas?\nThe early proposal contained very lofty goals for the future but I don’t think it critically thought about how to get there or why people would want a future like it. It had very scattered technical ideas and lacked conviction in the vision.\nI found it really odd that this type of technology has existed for a while but there were no widespread examples of usage. I spent about a month and a bit looking at retrospectives of peer-to-peer protocols and applications to get a good foundation for what’s been done and what hasn’t been done.\nThis literature review era led me to revise the proposal to go a lot more in-depth about the why of the research, placing heavier emphasis on adoption and why the average internet user should care. The internal metric I had was whether my Mom, who doesn’t work in technology and mostly uses the internet to look at articles, would be able to understand why what I was working on was important.\nWhat’s something that surprised you in your work so far this summer?\nPeople say independent research is a lonely journey. I disagree. I don’t think being independent means you need to do it alone. Rather, it means without being attached to any particular institution or label.\nI’ve been very surprised at just the number of people who have been thought partners, collaborators, and supporters throughout the whole process.\nRhizome is very much in early stages right now from a research perspective. Let’s say the research phase fully succeeds — what do you think would come next? What would it take for this to shape how individuals interact with the internet?\nI honestly don’t think the technical part or the research will be the hard part of this project! Getting people to build on top of it and to imagine possible futures using this technology is the hard part.\nBuilding distributed systems and peer-to-peer tech shouldn’t require a PhD to do. I think a large part of the next step if research is successful is to build demo apps with real users to show what’s possible with it.\nI want your average computer science student to be able to build apps for their friends and family using this. I want people to actively play with this new framework, this new mental model of computing, and then to build the apps they wish they could have themselves.\nFootnotes\n\n\nDivya Siddarth, Danielle Allen, E. Glen Weyl, The Web3 Decentralization Debate is Focused on the Wrong Question in Wired Magazine (Source) ↩\n\n\nMartin Kleppmann, Adam Wiggins, Peter van Hardenberg, Mark McGranaghan, Local-first software in Ink &amp; Switch (Source) ↩\n\n\nMoxie Marlinspike, My first impression of web3 (Source) ↩\n\n\nSpencer Chang on Twitter ↩\n\n\n"},"posts/wm/AUTOSAR":{"title":"汽车电子","links":[],"tags":["AUTOSAR"],"content":""},"posts/wm/工作管理":{"title":"工作管理","links":["posts/wm/汽车电子"],"tags":["工作管理"],"content":"\n工作管理\n\n\n\n\n汽车电子\n"},"posts/wm/汽车电子":{"title":"汽车电子","links":["posts/wm/AUTOSAR"],"tags":["汽车电子"],"content":"\n软件架构\n\n\nAUTOSAR\n"},"posts/wm/系统设计":{"title":"工作管理","links":[],"tags":["工作管理"],"content":"\n系统设计\n\n\n\n需求收集\n系统架构\n数据设计\n"},"tags/PHIL240A":{"title":"PHIL240A","links":[],"tags":[],"content":""},"tags/PHIL451A":{"title":"PHIL451A","links":[],"tags":[],"content":""},"tags/Software":{"title":"Software","links":[],"tags":[],"content":""},"tags/evergreen":{"title":"Evergreen","links":[],"tags":[],"content":""},"tags/fruit":{"title":"Fruit","links":[],"tags":[],"content":""},"tags/pattern":{"title":"Pattern","links":[],"tags":[],"content":""},"tags/personal":{"title":"Personal","links":[],"tags":[],"content":""},"tags/rhizome":{"title":"Rhizome","links":[],"tags":[],"content":""},"tags/sapling":{"title":"Sapling","links":[],"tags":[],"content":""},"tags/seed":{"title":"Seed","links":[],"tags":[],"content":""},"tags/technical":{"title":"Technical","links":[],"tags":[],"content":""},"tags/writing":{"title":"Writing","links":[],"tags":[],"content":""},"thoughts/33-percent-Impossibility-Result":{"title":"33% Impossibility Result","links":["thoughts/system-model","thoughts/safety","thoughts/liveness","thoughts/Public-key-Infrastructure","thoughts/PSL-FLM-Impossibility-Result","thoughts/Byzantine-Faults"],"tags":["seed"],"content":"In general (assuming a partially-synchronous system model), a protocol can achieve safety all the time and additionally liveness in synchronous conditions if and only if n≥3f+1 (equivalently, f&lt;3n​)\nThis result holds despite the assumption of PKI (unlike the PSL-FLM result), so this bound must be driven by the possibility of unbounded message delays.\nIntuition:\n\nWe can’t wait indefinitely for all nodes to respond (one valid strategy for Byzantine nodes is to never respond, even after GST) so realistically we can only wait to hear from n−f nodes before deciding on the next possible action\nBut we can’t say for certain the f nodes are actually Byzantine (they could be honest nodes that are congested), thus of the n−f nodes, more than half should be honest, f&lt;21​(n−f) or equivalently f&lt;3n​\n"},"thoughts/50-pounds-of-pots":{"title":"50 pounds of pots","links":[],"tags":["seed","pattern"],"content":"Quantity is the journey to quality\nThe ceramics teacher announced on opening day that he was dividing the class into two groups. All those on the left side of the studio, he said, would be graded solely on the quantity of work they produced, all those on the right solely on its quality. His procedure was simple: on the final day of class he would bring in his bathroom scales and weigh the work of the “quantity” group: fifty pound of pots rated an “A”, forty pounds a “B”, and so on. Those being graded on “quality”, however, needed to produce only one pot — albeit a perfect one — to get an “A”.\nWell, came grading time and a curious fact emerged: the works of highest quality were all produced by the group being graded for quantity. It seems that while the “quantity” group was busily churning out piles of work — and learning from their mistakes — the “quality” group had sat theorizing about perfection, and in the end had little more to show for their efforts than grandiose theories and a pile of dead clay.\nFrom Art &amp; Fear by David Bayles, Ted Orland"},"thoughts/A-Certain-Tendency-Of-The-Database-Community":{"title":"A Certain Tendency Of The Database Community","links":["thoughts/distributed-systems","thoughts/linearizability","thoughts/consistency","thoughts/plurality","thoughts/Network-Theory","thoughts/peer-to-peer","thoughts/local-first-software"],"tags":["sapling"],"content":"Source\nDistributed Systems that provide “single system image” semantics (read: linearizability) is fundamentally flawed and at odds with how systems operate in the physical world\nThere is no authoritative copy\nA lot of engineers attempt to treat distributed state as a single system image where there is a single primary site (main source of truth)\nBut in reality, the world is eventually consistent and pluriversal\n\nMembers of the same system exchange information by “interacting”, or sending messages containing information to each other. These messages between members of the system can be arbitrarily dropped and delayed, just like in traditional, unreliable, asynchronous networks.\n\nThe web was able to scale to the scale it did because there is no authoritative copy of the web.\nIn fact, truly global applications can only achieve some sense of responsiveness through having multiple primary sites. This is because at a global scale, having only one primary site just doesn’t work:\n\nSupporting any non-trivial application at global scale requires a lot of compute which often can’t be colocated on the same machine for performance reasons\nHaving a single primary site means that every read or write needs to be done through it. As a result, latency for users that are on the other side of the world may have latencies that make the application frustrating or unusable.\n\nTake Facebook for example. It has a single user profile for each user that is active in the system. It would be extremely impractical to have to hear information directly from the primary site every time you needed information. Each of these profiles is replicated across several of their data centers for performance. Distributed databases are optimizations which makes for extremely efficient distribution of information (scale-free rather than random)\nEach node is primary\nWhat is another model of databases or state machine replication we can come up with? What about one which places each node as its own primary site?\nIn this model, each member of the system has some partially-replicated knowledge and some knowledge that they are the primary site for. This information is exchanged between members of the system and merged with each member’s local information: this provides both fault-tolerance, and lower latency in servicing requests for information from peers.\nAs we continue to increase the number of globally connected devices, we must embrace a design that considers every single member in the system as the primary site for the data that it is generates. It is completely impractical that we can look at a single, or a small number, of globally distributed data centers as the primary site for all global information that we desire to perform computations with.\nCan we build abstractions that allow devices to communicate peer-to-peer, acknowledging the true primary site for a particular piece of information and scale to the amount of information that exists, not only between all computers in a planetary-scale distributed system, but all entities in the universe?\nLimits of the speed of light\nCoordination in the universe is limited by how much you can observe. That means a globally consistent anything will be limited by the speed of light\nThis is problematic for global (and potentially inter-planetary) communication. The great thing about local-first software is that you don’t need to know anything about the other side of the world when doing stuff locally :)\nHarmony\nWerner Herzog in Burden of Dreams (1982)\n\nThere is no harmony in the universe. We have to get acquainted to this idea that there is no real harmony as we have conceived it.\n"},"thoughts/A-City-is-not-a-Computer":{"title":"A City is not a Computer","links":["thoughts/Internet","thoughts/urban-planning","thoughts/move-fast-and-break-things","thoughts/Collingridge-dilemma","thoughts/generational-learning","thoughts/Theory-of-Niche-Construction","thoughts/quantization","thoughts/computability","thoughts/Technosolutionism","thoughts/traditional-knowledge","thoughts/creation-vs-maintenance","thoughts/instrumentalism","thoughts/potemkin-village","thoughts/Design-Justice","thoughts/agency","thoughts/GDPR","posts/towards-data-neutrality","thoughts/privacy","thoughts/governance","thoughts/Seeing-like-a-State","thoughts/A-City-is-not-a-Tree"],"tags":["seed"],"content":"Reboot event with Shannon Mattern on her new book A City Is Not A Computer. Main summary and introduction on the Substack here.\nArticle Notes\nSource: A City is not a Computer\nSidewalk Labs\nDoctoroff, the founder of Sidewalk Labs: “What would a city look like if you started from scratch in the internet era — if you built a city ‘from the internet up?‘”\nSidewalk aims to be the ‘fourth revolution’ of urban infrastructure where the first 3 were\n\nThe Steam Engine\nElectricity Grid\nAutomobile\n\nConstant theme of Doctoroff is that of constant move fast and break things-style of innovation. Their solution to the Collingridge dilemma is to constantly develop and “version” much like a software product in an agile process.\nCities as information centres\nNot only for transmitting information within a generation (breadth-wise) but also between generations (depth-wise): generational learning and Theory of Niche Construction!\n\n“By means of its storage facilities (buildings, vaults, archives, monuments, tablets, books), the city became capable of transmitting a complex culture from generation to generation, for it marshaled together not only the physical means but the human agents needed to pass on and enlarge this heritage.”\n\n“What are the non-textual, un-recordable forms of cultural memory? These questions are especially relevant for marginalized populations, indigenous cultures, and developing nations.”\nEspecially related to quantization, how does always labeling and quantizing our data affect these forms of information? These forms cannot be reduced to ‘information’ nor can they be ‘processed’ easily within our digital systems. “Yet they are vital urban intelligences that live within bodies, minds, and communities.”\nEvent Notes\n\nSo much of how we interact with cities is through a computer; how can we reclaim cities for the people within them as people instead of just as data? What have cities lost in the transition to a data governed model\n\nHow we define computation matters a lot. It feels like something is definitely something is lost in this ‘generalization’ of people as data\nTheres a lot of ‘messiness’ in history (e.g. why people were evicted, etc.) especially in embodied knowledge. Let’s think about what can’t be digitized and be put on a dashboard\n\n\nWhat are potential solutions to this overly quantized technosolutionist approach? You mentioned indigenous knowledge and traditional knowledge as having potential\n\nWe can start to think more epistemologically broadly (how do we know what we know, are things inherently quantifiable or not)?\nHow can we provide public alternatives to privatized social systems (e.g. public interest Google)?\nMaintenance and Care\n\nFocus on maintenance instead of just plain innovation\nTech fetishizes creating new things, is there value beyond instrumentalism?\n\n\nWhat is the ‘dashboard’ in the proto-city?\n\nSource: Cybernetic Revolutionaries: Technology and Politics in Allendes Chiles (by Medina)\nFeels like a lot of abstraction away from actual problems\nPotemkin village control room! It only gives a semblance/facade of control rather than actual control/usefulness\n\n\n\n\nReboot has an ethos of techno-optimism, how do we reconcile this with humanists who vehemently reject technology as a whole\n\nA lot of blanket rejection of tech and demonisation of the algorithm. However, this misses a lot of nuance! We have a lot of good algorithms, (e.g. predicting weather, modelling climate change, etc.)\nRather than just bringing humanists in, starting the collaboration on a more neutral ground and make it as participatory as possible\n\n\nWhat can city governments do to help the ‘exclusion’ of previously ‘unseen’ or minority groups?\n\nDesign Justice approach, specifically acknowledge this bias and try to compensate (e.g. Data for Black Lives). And equity rather than equality approach.\nWe may not always be able to prevent ‘data harvesting’, and it sometimes might not all be bad! How do we fairly represent everyone?\nHow do we have agency over our own data and how we are presented within algorithms? Data sovereignty (e.g. GDPR? see also: data neutrality)\n\n\nWould smart cities be any different if they prioritized the citizens and participation at the get-go rather than as an after-thought\n\nMultiple definitions of smart cities (corporate extractivist approaches, civic approaches, improving democratic processes, open data, etc.)\nHow do we manage data and data privacy?\n\n\nGround-up emergence vs Top-down governance for cities\n\nHistorically a lot of data has been centralized regardless (e.g. libraries, town halls, etc., more in Seeing like a State)\nSimilar discussion to a an article thinking about hierarchies within cities: A City is not a Tree\n\n\n"},"thoughts/A-City-is-not-a-Tree":{"title":"A City is not a Tree","links":["thoughts/semilattice","thoughts/urban-planning"],"tags":["seed"],"content":"A City is not a Tree\nWhy graphs instead of trees\nBoth the tree and the semilattice are ways of thinking about how a large collection of many small systems goes to make up a large and complex system. More generally, they are both names for structures of sets.\n\nFor the human mind, the tree is the easiest vehicle for complex thoughts. But the city is not, cannot and must not be a tree. The city is a receptacle for life. If the receptacle severs the overlap of the strands of life within it, because it is a tree, it will be like a bowl full of razor blades on edge, ready to cut up whatever is entrusted to it. In such a receptacle life will be cut to pieces. If we make cities which are trees, they will cut our life within to pieces.\n\nUrban Planning\nColumbia, Maryland: Neighbourhoods, in clusters of five, form ‘villages’. Transportation joins the villages into a new town. The organization is a tree.\nHilberseimer’s book The Nature of Cities: He describes the fact that certain Roman towns had their origin as military camps. The symbol is apt, for, of course, the organization of the army was designed precisely in order to create discipline and rigidity.\nSee more in the note on Urban planning"},"thoughts/A-Pattern-Language":{"title":"A Pattern Language","links":["thoughts/urban-planning","thoughts/composable","tags/pattern","thoughts/information-scales","thoughts/quantization","thoughts/Tools-for-Conviviality","thoughts/maintenance","thoughts/privacy","thoughts/Buddhist-Economics","thoughts/iconic-space","thoughts/convex"],"tags":["seed","book","pattern"],"content":"\n“every individual in such a society will have a unique language, shared in part, but which as a totality is unique to the mind of the person who has it. In this sense, in a healthy society there will be as many pattern languages as there are people—even though these languages are shared and similar.”\n\nContains hundreds of patterns aimed at encapsulating a community-centric, bottom-up philosophy of urban planning, architecture, and collective livability\nPattern\n[a] pattern describes a problem which occurs over and over again in our environment, and then describes the core of the solution to that problem, in such a way that you can use this solution a million times over, without ever doing it the same way twice\nPattern languages are composable, structured without being prescriptive, and for the purpose of practical design. Each pattern is connected to certain “larger” patterns which come above it in the language; and to certain “smaller” patterns which come below it in the language. The pattern helps to complete those larger patterns which are “above” it, and is itself completed by those smaller patterns which are “below” it.”\nSee a list of my own personal patterns\nNotes\nIndividuality\n\n“Although a person may have a different mixture of attributes from his neighbour, he is not truly different, until he has a strong center, until his uniqueness is integrated and forceful.” (p.45, Mosaic of Subcultures)\nSmall social communities\n\n“It seems clear, then, that variety, character, and finding your own self, are closely interwoven. In a society where a man can find his own self, there will be ample variety of character, and character will be strong. In a society where people have trouble finding their own selves, people will seem homogenous , there will be less variety, and character will be weak” (p.46, Mosaic of Subcultures)\n“If, every day you do something, you meet someone with a slightly different background, and each of these peoples’ response to what you do is different even when your actions are the same, the situation becomes more and more confusing. The possibility that you can become secure and strong in yourself, certain of what you are, and certain of what you are doing, goes down radically. Faced constantly with an unpredictable social world, people no longer generate the strength to draw on themselves; they draw more and more on the approval of others; they look to see whether people are smiling when they say something, and if they are, they go on saying it, and if not, they shut up. In a world like that, it is very hard for anyone to establish any sort of inner strength.” (p.46, Mosaic of Subcultures)\n“A person will only be able to able find his own self, and therefore to develop a strong character, if he is in a situation where he receives support for his idiosyncrasies from the people and values which surround him” (p.48, Mosaic of Subcultures)\n“In nature, the differentiation of a species in to subspecies is largely due to the process of geographic speciation, the genetic changes which take place during a period of spatial isolation… members of the same species develop distinguishable traits when separated from other members of the species…” (p.78, Subculture Boundary)\n\n\nHierarchies of details (see also: information scales): “Our own bodies and the natural surroundings in which we evolved contain a continuous hierarchy of details, ranging all the way from the molecular fine structure to gross features like arms and legs and trunks and branches. We know from results in cognitive psychology that any one step in this hierarchy can be no more than 1:5, 1:7, or 1:10 if we are to perceive it as a natural hierarchy. We cannot understand a hierarchy in which there is a jump in scale of 1:20 or more.” (p.1114, Half-inch trim)\n“People are different sizes; they sit in different ways. And yet there is a tendency in modern times to make all chairs alike” (p.1158, Different Chairs)\n\nCommunity size\n\n“Paul Goodman has proposed a rule of thumb, based on cities like Athens in their prime, that no citizen be more than two friends away from the highest member of the local unit. Assume that everyone knows about 12 people in his local community. Using this notion and Goodman’s rule, we can see that an optimum size for a political community would be about 123 or 1728 households of 5500 persons.” (p.72, Community of 7000)\nRegression to the mean in large communities\n\nMargaret Mead: “There is a tendency to reduce all values to simple scales of dollars, school grades, or some other simple quantifiable measure, whereby the extreme incommensurables of many different sets of cultural values can be easily, though superficially, reconciled” (p.47, Mosaic of Subcultures)\n\n\nBernard Bass has conducted an experiment relating group size to participation (p.713, Small Meeting Rooms): \n\nMaking a place feel like home\n\nTraffic (p.83, Identifiable Neighbourhood)\n\nLight traffic (2000 vehicles/day, 200 vehicles/peak hour, 15-20mph)\n\n“I feel it’s home. There are warm people on this street. I don’t feel alone.”\n“Definitely a friendly street.”\n“I feel my home extends to the whole block”\n\n\nModerate traffic (6000 vehicles/day, 550 vehicles/peak hour, 25mph)\n\n“You see the neighbours but they aren’t close friends”\n“Don’t feel there is any community any more, but people say hello”\n\n\nHeavy traffic (16,000 vehicles/day, 1900 vehicles/peak hour, 35-40mph)\n\n“It’s not a friendly street — no one offers help.”\n“People are afraid to go into the street because of the traffic”\n“It is impersonal and public”\n\n\n\n\nNo high-rises\n\n“High-rise living takes people away from the ground, and away from the casual, everyday society that occurs in the sidewalks and streets and on the gardens and porches. It leaves them alone in their apartments. The decision to go out for some public life becomes formal and awkward; and unless there is some specific talks which brings people out in the world, the tendency is to stay home, alone.” (p. 116, Four-story limit)\n\n\n“A building will also seem alien unless it gives to its users a direct and intuitive sense of its structure — how it is put together. Buildings where the structure is hidden leave yet another gap in people’s understanding of the environment around them. We know this is important to children and suspect it must be important to adults too.” (p.944, Structure Follows Social Spaces)\n\nTransit\n\n\n“The graph is based on 127 observations in the Berkeley City Hall. People were asked to define all the trips they had to make regularly during the work week, to state their frequency, and then to state whether they considered the trip to be a nuisance.” (p.409, Office connections)\n\nPublic Gathering Spaces\n\n“People with a shared way of life gather together to rub shoulders and confirm their community” (p.169, Promenade)\n“Men seek corner beer shops, where they spend hours talking and drinking; teenagers, especially boys, choose special corners too, where they hang around, waiting for their friends. Old people like a special spot to go to, where they can expect to find others; small children need sand lots, mud, plants, and water to play with in the open; young mothers who go to watch their children often use the children’s play as an opportunity to meet and talk with other mothers. Because of the diverse and causal nature of these activities, they require a space which has a subtle balance of being defined and yet not too defined, so that any activity which is natural to the neighbourhood at any given time can develop freely and yet has something to start from” (p.349, Public Outdoor Room)\n\nGateways and Gradients\n\n“If access is restricted, this means, by definition, that those few points where access is possible, will come to have special importance.” (p.89, Neighborhood Boundary)\n“Buildings, and especially houses, with a graceful transition between the street and the inside, are more tranquil than those which open directly off the street” (p.549, Entrance Transition)\n“In one exhibit people had to cross a huge, deep-pile, bright orange carpet on the way in. In this case, though the exhibit was no better than other exhibits, people stayed. The authors concluded that people were, in general, under the influence of their own ‘street and crowd behaviour,’ and that while under this influence could not relax enough to make contact with the exhibits. But the bright carpet presented t hem with such a strong contract as they walked in, that it broke the effect of their outside behaviour, in effect ‘wiped them clean,’ with the result that they could then get absorbed in the exhibit” (p.550, Entrance Transition)\n“Lay out the spaces of a building so that they create a sequence which begins with the entrance and the most public parts of the building, then leads into the slightly more private areas, and finally to the most private domains” (p.613, Intimacy Gradient)\n\nConviviality\nSee also: Tools for Conviviality\n\nEducation\n\n“People of all walks of life come forth, and offer a class in the things they know and love: professions and workgroups offer apprenticeships in their offices and workshops, old people offer to teach whatever their life work and interest has been, specialists offer tutoring in their special subjects. Living and learning are the same” (pp. 101-102, Network of Learning)\n“The original universities in the middle ages were simple collections of teachers who attracted students because they had something to offer. They were marketplaces of ideas, located all over the town, where people could shop around for the kinds of ideas and learning which made sense to them. By contrast, the isolated and over-administered university of today kills the variety and intensity of the different ideas at the university and also limits the student’s opportunity to shop for ideas… the social and physical environment must provide a setting which encourages rather than discourages individuality and freedom of thought… the environment must provide a setting which encourages the student to see for himself which ideas make sense — a setting which gives him the maximum opportunity and exposure to a great variety of ideas, so that he can make up his mind for himself.” (p.232, University as a Marketplace)\n\n\nHealthcare\n\n“Hospitals put the emphasis on sickness. They are enormously expensive; they are inconvenient because they are too centralized; and they tend to create sickness, rather than cure it, because doctors get paid when people are sick.” (p.252, Health Center)\n“The Peckham Center was a club, run by two doctors, focused on a swimming pool, a dance floor, and a cafe. In addition, there were doctors’ offices, and it was understood that families — never individuals — would receive periodic check-ups as part of their activities around the swimming and dancing. Under these conditions, people used the center regularly, during the day and at night. The question of their health became fused with the ordinary life of the community, and this set the stage for a most extraordinary kind of health care” (p.254, Health Center)\n\n\n\nMaintenance and ownership\n\n“Processes of maintenance and repair hinge on the fact of user ownership. In other words, the places where people are user-owners are kept up nicely; the places where they are not, tend to run down. When people have their own homes … they extend themselves to make it personal and personal and comfortable.” (p.257, Housing in Between)\n“People cannot be genuinely comfortable and healthy in a house which is not theirs. All forms of rental — whether from private landlords or public housing agencies — work against the natural processes which allow people to form stable, self-healing communities.” (p.393, Your own home)\n“Rental areas are always the first to turn to slums. The mechanism is clear and well known. See, for example, George Sternlieb, The Tenement Landlord (Rutgers University Press, 1966). The landlord tries to keep his maintenance and repair costs as low as possible; the residents have no incentive to maintain and repair the homes — in fact, the opposite — since improvements add to the wealth of the landlord, and even justify higher rent. And so the typical piece of rental property degenerates over the years. The landlords try to build new rental properties which are immune to neglect — gardens are replaced with concrete, carpets are replaced with lineoleum, and wooden surfaces by formica: it is an attempt to make the new units maintenance-free, and to stop the slums by force; but they turn out cold and sterile and again turn into slums because nobody loves them.” (p.394, Your own home)\n\n“Though renting in general has a devastating impact on the environment, our experience has been that face-to-face rental, with the owners occupying the main structure, is the one kind of rental relationship that is reasonably healthy. The landlord is actually there, so he is directly concerned with the well-being of the life around him and with the environment” (p.721, Rooms to Rent)\n\n\n“We must treat every new act of building as an opportunity to mend some rent in the existing cloth; each act of building gives us the chance to make one of the ugliest and least healthy parts of the environment more healthy.” (p.510, Site Repair)\n“It is desirable to build a building in such a way that it starts out loose and flimsy while final adaptations in plan are made, and then gets stiffened gradually during the process of construction, so that each additional act of construction makes the structure sounder.” (p.963, Gradual Stiffening)\n\nThe family and coliving\n\n“It seems very likely that the nuclear family is not a viable social form. It is too small. Each person in a nuclear family is too tightly linked to other members of the family; any one relationship which goes sour, even for a few hours, becomes critical; people cannot simply turn away toward uncles, aunts, grandchildren, cousins, brothers.” (p.377, The Family)\n“It is our experience that groups have not taken this need for privacy seriously enough. It has been shrugged off, as something to overcome. But it is a deep and basic need; and if the setting odes not let each person and each small household regulate itself on this dimension, it is sure to cause trouble.” (p.379, The Family)\n“In a small household shared by two, the most important problem which arises is the possibility that each may have too little opportunity for solitude or privacy… It is true that each partner is trying to maintain an individuality, and not be submerged in the identity of the other, or the identity of the ‘couple’. Each partner needs space to nourish this need.” (p.386, House for a Couple)\n“There is a critical point beyond which closer contact with another person will no longer lead to an increase in empathy. (A) Up to a certain point, intimate interaction with others increases the capacity to empathize with them. But when others are too constantly present, the organism appears to develop a protective resistance to responding to them… (B) Families who provide time and space for privacy, and who teach children the utility and satisfaction of withdrawing for private reveries, will show higher average empathic capacity than those who do not.” (p.669, A Room of One’s Own)\n\nWork\nSee also: Buddhist Economics\n\n“The two main elements of job dissatisfaction as the diminishing independence of workers, and the increasing simplification, fragmentation, and isolation of tasks — both of which are rampant in modern industrial and office work alike.” (p.401, Self-governing Workshops and Offices)\n“First of all, what do we mean by ‘settled work’? It is the work which unites all the threads of a person’s life into one activity: the activity becomes a complete and wholehearted extension of the person behind it. It is a kind of work that on cannot come to overnight; but only by gradual development. And it is a kind of work that is so thoroughly a part of one’s way of life that it most naturally occurs within or very near the home: when it is free to develop, the workplace and the home gradually fuse and become one thing.” (p.734, Settled Work) see also: iconic space\n\nPhysical Spaces\n\nSun and light\n\n“People use open space if it is sunny, and do no use it if it isn’t… Always place buildings to the north of the outdoor spaces that go with them, and keep the outdoor spaces to the south. Never leave a deep band of shade between the building and the sunny part of the outdoors” (p.516, South Facing Outdoors)\n“Place the most important rooms along the south edge of the building, and spread the building out along the east-west axis.” (p.617, Indoor Sunlight)\n“People are by nature phototropic — they move toward light, and, when stationary, they orient themselves toward the light. As a result the much loved and much used places in buildings, where the most things happen, are places like window seats, verandas, fireside corners, trellised arbors; all of them defined by non-uniformities in light, and all of them allowing the people who are in them to orient themselves toward the light.” (p.645, Tapestry of Light and Dark)\n“Rooms lit on two sides, with natural light, create less glare around people and objects; this lets us see things more intricately; and most important, it allows us to read in detail the minute expressions that flash across people’s faces, the motion of their hands … and thereby understand, more clearly, the meaning they are after. The light on two sides allows people to understand each other.” (p.748, Light on Two Sides of Every Room)\n“The primary function of windows is not to provide light but to provide a link to the outside and, furthermore that this link is most meaningful when it contains a view of the ground and the horizon.” (p.1051, Low Sill)\n\n\nInstead of designing buildings that create negative, shapeless space, we should create buildings that create positive, shapeful space. “Another way of defining the difference between ‘positive’ and ‘negative’ outdoor spaces is by their degree of enclosure and their degree of convexity” (p.519, Positive Outdoor Space). Convex spaces are positive and non-convex spaces are negative\nThe fire: “It leads to a very special kind of attention which has nothing in common with the attention involved in watching or observing. Very rarely is it utilized for any other kind of contemplation. When near the fire, one must be seated; one must rest without sleeping; one must engage in reverie on a specific object…” (p.839, The Fire)\nSound\n\n“Sound is an important cue in the perception of distance between people (voice, footstep, rustle, and so on), this means that the ceiling height will alter the apparent distance between people. Under a high ceiling people seem further apart than they actually are.” (p.878, Ceiling Height Variety)\n\n\n"},"thoughts/A-Pragmatic-Precautionary-Principle":{"title":"A Pragmatic Precautionary Principle","links":["thoughts/Precautionary-Principle","thoughts/game-theory","thoughts/meaning-laden","thoughts/Pascal's-Wager","thoughts/probability"],"tags":["fruit","PHIL321A"],"content":"\nFinal paper for PHIL321\n\nThe Precautionary Principle has had many different interpretations across game theory; there is no single accepted formulation. In general, the Precautionary Principle suggests that we should take appropriate measures to prevent potential harm, even if the likelihood or severity of that harm is uncertain. However, part of the reason why there has been such heavy debate around Precautionary Principle is because many terms are already meaning-laden: it is not always clear what constitutes appropriate precautionary measures, and different stakeholders may have different ideas about what is appropriate in a given situation.\nIf the principle is to be useful in policy-making, we must make it more concrete. The real meat of the matter comes down to how we characterize what constitutes a relevant thread and what threshold of certainty is necessary to enact action.\nNeil A. Manson (2002) in Formulating the Precautionary Principle1, attempts to derive a more robust definition. In this paper, I will build on Manson’s interpretation of the Precautionary Principle and argue that we should regard the weak formulation of the Precautionary Principle as unproblematic. In fact, I claim that controversy and refutations of the Precautionary Principle occur when there are issues of ill-defined terms or the formulation contradicts itself rather than a flaw with the core principle itself. That is, there are constructions of the Precautionary Principle that are incoherent, but this does not mean that the Precautionary Principle as a whole is incoherent.\nIn the first part of the paper, Manson formalizes the Precautionary Principle into a general three-part structure shared between all versions: For a given e-activity that may have a given e-effect on the environment, the precautionary principle is supposed to indicate an e-remedy. Here, the terms e-activity, e-effect, and e-remedy refer to abstract elements of various Precautionary Principles (e.g. burning fossil fuels is an e-activity, climate change is an e-effect, and enacting a carbon tax is an e-remedy).\nAdditionally, Manson defines conditions on each of these terms. The damage condition specifies the characteristics of an e-effect in virtue of which precautionary measures should be considered. The knowledge condition specifies the status of knowledge regarding the causal connections between the e-activity and the e-effect. The remedy condition specifies the e-remedy that decision makers should take in response to the e-activity. Formally,“if the e-activity meets the damage condition and if the link between the e-activity and the e-effect meets the knowledge condition, then decision makers ought to enact the specified e-remedy” (Manson 2002, p. 265). This formulation, I argue, is unproblematic. We can boil it down to a simple logical entailment of the form A∧B→C where A is the damage condition, B is knowledge condition, and C is the remedy condition. The soundness of this statement heavily depends on what A, B, and C are, but there is nothing inherently wrong with it in and of itself.\nIn the second part of the paper, he distinguishes a stronger formulation of the Precautionary Principle (which he dubs the Catastrophe Principle). The Catastrophe Principle is an instantiation of the Precautionary Principle where the damage condition is that the e-effect is catastrophic and the knowledge condition is that there is a possibility that the e-activity leads to the e-effect. That is, if there is even a mere possibility that something potentially catastrophic were to happen as a result of the activity, then we should unconditionally ban it.\nSpecifically, Manson argues that the stronger Catastrophe Principle is self-defeating but leaves the earlier formulation of the Precautionary Principle intact. Strong versions of the Precautionary Principle, if applied consistently, lead to paradoxical outcomes.\nThe careful reader will notice the similarities between the Catastrophe Principle and Pascal’s Wager. As there is dominance of the action of believing in God over not believing in God, any rational decision maker must believe in God, no matter how low the probability of ‘God does not exist’ is.\nSimilarly, if we draw a decision table for the Catastrophe Principle, we see they follow a similar structure. Any rational decision maker should always enact the e-remedy given that there is a nonzero possibility that the e-activity leads to the e-effect.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKnowledge Condition is metKnowledge Condition is not metEnact e-remedyFiniteFiniteDo nothing−∞Finite\nOf course, Pascal’s Wager has been subjected to a number of philosophical criticisms. Manson specifically focuses on the “many gods” objection as a way to dismantle both Pascal’s Wager and the Catastrophe Principle.\nAlthough many have pointed out flaws to the many-gods objection, it still brings to light the following general point regarding the catastrophe principle: “even if an e-effect is catastrophic, that fact cannot rationally compel us to impose an e-remedy unless we also know that the e-remedy itself does not lead to catastrophic results”1 (Manson 2002, pp. 272-3).\nA proponent of the Catastrophe Principle may then try to strengthen the principle by adding an extra clause stating that it should only be applied if imposing the given e-remedy will not cause another catastrophic e-effect. However, we quickly see that even with this clause, the Catastrophe Principle is still incoherent. Earlier, when we formulated both Pascal’s Wager and the Precautionary Principle, we assumed that any rational decision maker would assign non-zero probabilities to any imaginable outcome (whether that be a belief in God or the knowledge condition being satisfied). Thus, we also cannot ever rule out the fact that a given e-remedy will not cause another catastrophic effect. The e-remedy could bring about an outcome which also leads to human extinction and we couldn’t rule it out!\nThe upshot here is that even well-intentioned safety measures can lead to damaging consequences if we use the Catastrophe Principle. Note specifically, that this isn’t a fatal blow for the weak formulation of the Precautionary Principle but rather just the Catastrophe Principle.\nManson concludes by saying that a formulation of the Precautionary Principle is coherent, if and only if, it meets a list of 5 core requirements. Of the ones that are applicable to this paper, Manson clarifies that the component concepts used in the conditions should be clearly defined and that the formulation must not be self-refuting. In the case of the Catastrophe Principle, the formulation was self-refuting, so it is not coherent.\nStephen M. Gardiner, in his 2006 work A Core Precautionary Principle2 shares Manson’s central concern, stating that because of how low the epistemic standards for the application of the catastrophe principle are, there are always way to construct these mere possibilities in a way that not only recommends the action but prohibit it as well. Thus, it is incoherent.\nGardiner defines further criteria to narrow what should be considered a rational Precautionary Principle, noting that the real action involves identifying the relevant circumstances under which the Precautionary Principle is operative (Gardiner 2006, p. 38).\nHow do we distinguish between reasonable outcomes (ones we should consider) and those outcomes which are merely imaginable (ones we should not)?\nOpponents may attempt to argue that the precautionary principle is committed to counting any imaginable outcome as possible, no matter how unreasonable it may seem. John Harsanyi makes an illustration of how foolish it may seem to consider unreasonable outcomes (Harsanyi in Gardiner 2006):\n\nIf you took the [strong formulation of the precautionary] principle seriously then you could not ever cross the street (after all, you might be hit by a car); you could never drive over a bridge (after all, it might collapse); you could never get married (after all, it might end in a disaster), etc. If anybody really acted this way he would soon end up in a mental institution.\n\nI argue that this argument depends heavily on the definition of ‘possible.’ Gardiner proposes that we add another circumstance so that the range of outcomes considered are in some appropriate sense “realistic,” so that, for example, only credible threats are considered and cases like these do not arise.\nThis begs the question, what makes a circumstance realistic? This is a question Gardiner explicitly leaves out of his argumentation and assumes that such a criterion exists and does not take a position on what it would be.\nI suspect that there is a human limit to the size of a probability we find meaningful. Just like how the size of certain numbers are incomprehensible to humans, some probabilities are so unlikely that they are nearly meaningless.\nEspecially as we want this to serve as a framework to make practical decisions, we should discard probabilities that people implicitly discard. A good lower bound on if a probability event could feasibly happen is if it could have reasonably occurred within the timespan of the entire universe. This is effectively strengthening the knowledge condition by placing a lower bound on the conceivable range of probabilities we consider. Anything below this bound is considered negligible and assigned a probability of 0. As a result, this makes much more robust the conditions under which it is coherent to apply the Precautionary Principle.\nIn conclusion, I have illustrated how the Precautionary Principle itself, if carefully constructed, can come to coherent and rational decisions. By resolving issues with ill-defined terms or self-contradiction in the principle’s formulation, we can create a more robust Precautionary Principle that is more suitable to pragmatic use cases like in environment decision making.\nFootnotes\n\n\nManson, Neil A. 2002. “Formulating the Precautionary Principle.” Environmental Ethics 24, 263-274. ↩ ↩2\n\n\nGardiner, Stephen M. 2006. “A Core Precautionary Principle.” The Journal of Political Philosophy 14, no. 1: 33-60. ↩\n\n\n"},"thoughts/A-Tale-for-the-Time-Being":{"title":"A Tale for the Time Being","links":["thoughts/Tomorrow,-and-Tomorrow,-and-Tomorrow","thoughts/time"],"tags":["sapling","book"],"content":"The amount of depth to the story being slowly unravelled over time is astounding; it keeps me on edge and always wanting to read more. Very similar in pacing/writing to Tomorrow, and Tomorrow, and Tomorrow\nOn Time\nI think this book has really got me thinking about time and time differences. Not just in absolute offsets in terms of like time zones, but how different people experience life at differing speeds depending on their context, environment, etc.\nTo me, it feels like empathy is really just emotional time dilation and temporarily moving at their pace of live if only for a brief moment. The Māori word for autism is takiwatanga: “in his/her own time and space” There is no ‘cognitive’ slowness, they are rather living at their own pace."},"thoughts/AI-alignment":{"title":"AI Alignment","links":["posts/agi","thoughts/social-contracts","thoughts/quantization"],"tags":["seed"],"content":"How do we get AI systems to align with real human social contracts and values? Or, in more mathematical terms, how do we make legible our soft and squishy human values into hard mathematical formula and policy?\nMostly sourced from OpenAI’s approach to alignment\n\nRLHF: Summarization from human feedback was really the first convincing proof-of-concept that RLHF works on language models and that you can optimize goals that are fuzzy and somewhat ambiguous.\n\nHow do we optimize for goals that are not easily quantizable?\n\n\nInstructGPT demonstrated that there is a real “alignment overhang” in language models that wasn’t very hard to access. The amount of human feedback needed to achieve an astounding 100x improvement was pretty moderate and achievable: ~50,000 comparisons, and ~300,000 episodes of training. That number is so small that we could actually have humans hand-label every training episode\nUsing models to augment rather than replace. Helping humans find 50% more flaws that they would have unassisted with a model that isn’t superhuman on a task that isn’t hard for humans is a surprisingly strong result, showing that our model can basically already add a lot of value for feedback assistance.\n"},"thoughts/ARP":{"title":"Address Resolution Protocol (ARP)","links":["thoughts/Network-Layer","thoughts/Link-Layer","thoughts/IP-Address","thoughts/MAC"],"tags":["seed","CPSC317"],"content":"Purpose: links the Network Layer (IP address) with the link layer (MAC address)\nCase: A wants to send a datagram to B, but A doesn’t know B’s MAC address\n\nA broadcasts an ARP query packet with an IP Address: “who has IP address 130.207.160.47?”\nB receives ARP request with that IP address on the LAN will respond with appropriate MAC address.\nGenerates an ARP Table maps IP to MAC\n\nThis is soft state, information that goes away unless refreshed. Each entry has a time limit\n\n\n\nGeneral Notes\n\nUseful because frames use MAC addresses for addressing\nARP is stateless, doesn’t remember whether it sent a request (always reads response)\nNot authenticated, anyone can ARP\nEasily spoofed\n"},"thoughts/Abilene-Paradox":{"title":"Abilene Paradox","links":["thoughts/Vanilla-Ice-Cream-effect","thoughts/Arrow's-Impossibility-Theorem"],"tags":["seed"],"content":"A group of people collectively decide on a course of action that is counter to the preferences of many or all of the individuals in the group\nSee also: Vanilla Ice Cream effect, Pareto condition in Arrow’s Impossibility Theorem"},"thoughts/Abstract-Machines":{"title":"Abstract Machines","links":["thoughts/Lambda-Calculus"],"tags":["seed"],"content":"An abstract machine is a theoretical model that allows for a detailed and precise analysis of how a computer system functions.\nAbstract machines are\n\n“machines” because they allow step-by-step execution of programs;\n“abstract” because they ignore many aspects of actual (hardware) machines.\n\nA typical abstract machine consists of a definition in terms of input, output, and the set of allowable operations used to turn the former into the latter.\nOne such example is the Turing machine.\nSee also: Lambda Calculus\nClassifications\n\nDeterministic abstract machines\n\nFor some given allowable operation and some input, it always produces the same output.\n\n\nNon-deterministic abstract machines\n\nNon-deterministic algorithms are helpful for obtaining approximate answers when deriving a precise solution using a deterministic approach is difficult or costly.\n\n\n"},"thoughts/ActivityPub":{"title":"ActivityPub","links":["thoughts/federation"],"tags":["seed"],"content":"From W3C Editor’s Draft\n\nThe ActivityPub protocol is a decentralized social networking protocol.\n\nIn ActivityPub, a user is represented by ”actors” via the user’s accounts on servers. User’s accounts on different servers correspond to different actors.\nUsed to be called OStatus which was the basis for Mastadon\nBased on ActivityStreams, a social data syntax.\nEach actor has:\n\nAn inbox: How they get messages from the world\nAn outbox: How they send messages to others\n\nHere’s how sending and reading messages work\n\n\nYou can POST to someone’s inbox to send them a message (server-to-server / federation only… this is federation!)\nYou can GET from your inbox to read your latest messages (client-to-server; this is like reading your social network stream)\nYou can POST to your outbox to send messages to the world (client-to-server)\nYou can GET from someone’s outbox to see what messages they’ve posted (or at least the ones you’re authorized to see). (client-to-server and/or server-to-server)\n\nMessages made by clients get posted to their own server’s outbox and the server then posts that to the receiver’s inbox."},"thoughts/Alexandre-Grothendieck":{"title":"Alexandre Grothendieck","links":["thoughts/Jestermaxxing","thoughts/meditation"],"tags":["seed","book"],"content":"Reflection and testimony on a past as a mathematician\nRécoltes et Semailles — Harvests and Sowing (1986). Notes from source. An internal race of two horses — of mathematics and of meditation. The former which is “not personal to me, they are meant to be communicated” whereas the latter “knowledge that arises from the work of meditation is “solitary” knowledge, knowledge that cannot be shared[partagée], let alone “communicated”“.\n\n“The little child discovers the world as he breathes - the ebb and flow of his breath make him welcome the world in its delicate being, and makes him project himself into the world that also welcomes him. The adult can also discover, in those rare moments when he has forgotten his fears and his knowledge, when he looks at things or himself with eyes wide open, eager to know, new eyes - the eyes of a child.” \n“When I am curious about something, mathematical or otherwise, I question[interroge] it. I question it, without caring if my question is perhaps stupid or if it will appear so, without it being carefully weighed. Often the question takes the form of an assertion - an assertion which, in truth, is a knocking probe. I will then believe more or believe less in the assertion, which depends of course on where I stand in the comprehension of the things I’m looking at. Often, especially at the beginning of a research, the assertion is completely false - but this still had to be done to convince yourself. Often, it suffices to write it down for it to become obvious that it is false, whereas before writing it down there was a vagueness[flou], like an uneasiness[malaise], instead of obviousness.”\n“One who fears to be wrong is powerless to discover. It is when we are afraid of making mistakes that the mistake inside us becomes immovable like a rock. Because in our fear, we cling to what we have decreed to be “true”, or what has always been presented to us as “true”. If we are moved, not by the fear of seeing an illusory security vanish, but by a thirst for knowing, then error, like suffering or sorrow, will cross us without ever becoming frozen, and the trace of its passage will be a renewed understanding.”\n“This fundamental inertia of the mind, suffocated by its ‘knowledge’, is certainly not specific to mathematicians. I am straying somewhat away from my subject: the prohibition imposed upon the mathematical dream, and through it, anything that does not present itself under the usual appearance of a finished product, ready to be taken in. The little I have learned about the other natural sciences is enough to make me realise that a similarly rigorous prohibition would have condemned them to sterility, or to progressing like a tortoise, a bit like in the Middle Ages when there was no question of cadging[écornifler] the letters of the Holy Scriptures. But I am also well aware that the deep source of discovery, as well as the process of discovery in all its essential aspects, is the same in mathematics as in any other region or thing of the universe that our body and mind can know. To banish the dream is to banish the source - to condemn it to an occult existence.”\n\nSee also: epistemic play\n\n\n“Much more than I did, he had retained a sense of the simple and essential things - the sun, the rain, the earth, the wind, the song, the friendship…”\n“My vocation is to learn, to know this world through myself, and to know myself through this world. If my life can bring any benefit to myself or others, it is to the extent that I am true to this vocation, that I am true to myself.”\n“My principal guide in my work was the constant search for a perfect coherence, a complete harmony that I divined behind the turbulent surface of things, and which I patiently strove to uncover, without ever being tired of it. It was a heightened sense of “beauty”, surely, that was my flair and my only compass.”\n\n“It is not so much, it seems to me, a so-called “brain power” that makes the difference between this mathematician and another, or between one piece of work and another of a same mathematician; but rather the quality of finesse, of the greater or lesser delicacy of this openness or sensitivity, from one researcher to another or from one moment to another in the same researcher. The most profound and fruitful work is also that which attests to the most delicate sensitivity in apprehending the hidden beauty of things.”\n\n\n“Once the work is done, the “result” appears obvious, and can be formulated in a few words. But if someone perceptive had said these words to me before or during the work, it would probably not have helped me at all. If the work took so long, it is because the resistance was strong, and deep.”\n“In maths, the “obvious” things are also the ones that sooner or later someone has to come across. They are not “inventions” that you can or cannot make. They are things that have always been there, that everyone walks by without paying attention, even if it means taking a long diversions around them, or tripping over them every time. After a year or a millennium, inevitably, someone finally pays attention to the thing, digs around it, unearths it, looks at it from all sides, cleans it, and finally gives it a name.”\n“To put it another way: meditation is a solitary adventure. It is solitary in nature. And not only is the work of meditation a solitary work - I think this is true of any work of discovery, even when it is part of a collective work. But the knowledge that arises from the work of meditation is “solitary” knowledge, knowledge that cannot be shared[partagée], let alone “communicated”; or if it can be shared, it is only in rare moments. It is a work, a knowledge that goes against the grain of the most inveterate consensus, consensus which concerns[inquiètent] each and everyone. This knowledge is certainly expressed simply, in simple and clear words. When I express it to myself, I learn in the process of expressing, because the expression itself is part of a work, carried by an intense interest. But these same simple and clear words are powerless to communicate meaning to others when they come up against the closed doors of indifference or fear.”\n“The most immediate meaning of this work has been that of a dialogue with myself, therefore of a meditation”\n"},"thoughts/Algorithms-of-Oppression":{"title":"Algorithms of Oppression","links":["thoughts/search","thoughts/double-consciousness","thoughts/Extended-Mind-Hypothesis","thoughts/library","thoughts/small-technology"],"tags":["seed"],"content":"Chapter 1\nFocuses on moving the blame away from ‘problematic users and data’ and towards search architecture itself.\n\nIf the majority rules in search engine results, then how might those who are in the minority ever be able to influence or control the way they are represented in a search engine?\n\nThese search results influence the values that surround what is being searched for. This means that minority groups often have their own values and identities influenced by the majority: double-consciousness\nWhy have we become so reliant on search? Is it a part of our Extended Mind Hypothesis?\n\nSearch is a symbiotic process that both informs and is informed in part by users.\n\nAdditionally, decreases in funding for public information institutions such as libraries and educational institutions and shifts of responsibility to individuals and the private sector have reframed the ways that the public conceives of what can and should be in the public domain.\nPerhaps to reduce the raw amounts of power search aggregators have, we should focus on funding and empowering small software like libraries that act as boutique search engines."},"thoughts/Antimatter":{"title":"Antimatter","links":["thoughts/CRDT","thoughts/Operational-Transform","thoughts/system-model","thoughts/Yjs"],"tags":["seed"],"content":"Source\nA CRDT + OT text editing algorithm with history pruning (read: GC). Permissionless system model.\nComponents\n\nAcknowledgements: require all peers to have acknowledged up to a certain point, then we can bloop\nBlooping: collapsing history. This is kind of like span-merging which Yjs implements\n\nWhen we get acknowledgement from all peers up to a certain point, we can then ‘bloop’/flatten all history that is not used in producing the current state\n\n\n\nFissures: keeps track of disconnections and network partitions\n\nDuring a fissure/disconnect, all events concurrent with that disconnect are marked to prevent from being blooped\n\n\n"},"thoughts/Application-Layer":{"title":"Application Layer","links":["thoughts/Transport-Layer","thoughts/human-computer-interaction","thoughts/encryption"],"tags":["seed","CPSC317"],"content":"Layer 1, the layer above the Transport Layer\nApplication/Presentation/Session layer\n\nUnit: Data\nResponsibilities: human-computer interaction layer, where applications can access the network services (includes encryption, connection, port, and session management)\n"},"thoughts/Archipelago":{"title":"Archipelago","links":["thoughts/contact-language","posts/digital-gardening","posts/networked-thought","thoughts/agency","thoughts/utopia","thoughts/fiction"],"tags":["seed","book"],"content":"by Édouard Glissant\nA beautiful dialogue between Glissand and the curator Hans Ulrish Obrist.\n\nI still believe that the future lies not with the great powers, but with the little islands, lands, and cities.\n\nGlobality versus Globalization\n\nGlobality does not homogenize culture. It produces a difference from which new things can emerge. (22)\nGlobalization standardizes and dilutes. It reduces communities to a single model, attacking them from the top down, diminishing them.\n\nArchipelagos\n\n“a world of many worlds” as quoted by the Chiapas\n\nthey desire a creole — a mixed Mexico\n\n\nCreolization is the means by which several distinct cultures or their elements, come into contact in a particular place in the world (contact languages enable this)\nDistinction between multiethnic (where many cultures exist but are distinct) and creole (where many cultures mix and form new ones)\n\nDigital gardening and networked thought as tending to free isles? The reader is a free agent. This freedom produces chance, it produces the unexpected.\nOn utopias\n\nUtopia is what is missing to us in the world — and thus it is never complete\n\n\nIf we imagine utopia as a finished work, then we’re continuing the old debates, we’re continuing the old science, and we’re continuing the old demands.\n\nWhy we need to continue to write new shared fiction\n\n\nUtopias cannot have norms. When we have norms, we banish to hell anything that does not fall within the rule of that utopia.\nTo reach utopia, we must accept that our world changes radically and perpetually, and that it changes with us and in us. We should reject stasis and and immutable.\n\n\nUtopia is a feeling: an ability to sense that all is entangled\n"},"thoughts/Arcosanti":{"title":"Arcosanti","links":["thoughts/urban-planning","thoughts/From-Counterculture-to-Cyberculture","thoughts/morphology"],"tags":["seed"],"content":"A provocative urban planning experiment of a compact city designed not around roads and cars but around people. It was intended to be a Western caravansary, a traditional inn with central courtyard for travelers in the desert regions of Asian or North Africa that often served as a stopover for ideas.\nIt was designed and established by Paolo Soleri, around same time as the counter-cultural movements of “back to the land”.\nSoleri strongly believed in the concept of arcology: the mixture of architecture and local ecologies. He strongly believed that dispersal was antagonistic to life and that density is the only morphology that can give us a lively existence.\n"},"thoughts/Arrow's-Impossibility-Theorem":{"title":"Arrow's Impossibility Theorem","links":["thoughts/Social-Contract-Theory","thoughts/utility","thoughts/Pareto-optimality","thoughts/access-control"],"tags":["seed"],"content":"Related: social choice\nAssume there is more than one individual, and there are at least three distinct social states. Then there is no SWF that meets the following four conditions:\n\nNon-dictatorial: no individual is decisive\nOrdering: must produce social preference orderings which are complete, asymmetric and transitive (see also: Interval Scales)\nPareto condition: If every voter prefers alternative X over alternative Y, then the group prefers X over Y.\nIndependence of Irrelevant Alternatives (IIA): If every voter’s preference between X and Y remains unchanged, then the group’s preference between X and Y will also remain unchanged\n\nUnstated: Arrow also requires the unrestricted domain assumption (U)\nHow can we get around this?\n\nSen says that we should give up liberalism (the Pareto condition). He argues that liberalism + Pareto leads to a contradiction in ordering axioms\nNozick’s solution is to give up the unrestricted domain assumption (U). Liberalism excludes certain kinds of states (‘private’ alternatives) from social scrutiny in advance.\n\nSimilarities to Group Membership\nRough thoughts on how we might prove decentralized access control to be impossible.\nSuppose we encode access in terms of some function Ai,j​ where Ai,j​ is true if subject i considers j to be in the group and false otherwise.\nA social state S for some collection of individuals G is i∈G such that Aj,i​∀j∈G.\nWe suppose there is some function that takes in individual preference orderings (what it thinks the group membership currently looks like) and produces a group preference ordering (what the true group membership is). This is normally called the SWF.\nSome properties of said SWF:\n\nNon-dictatorial: we don’t want a single admin who has power to dictate who is in the group. If this were to happen, we couldn’t remove the admin if they were compromised\nIntention-preserving: If every voter prefers alternative X over alternative Y, then the group prefers X over Y.\n\nStrong requirement is needed. Consider a weaker version: If the majority of voters prefers alternative X over alternative Y, then the group prefers X over Y\nThis may not work when we have a Sybil actor that can add new accounts to overwhelm the majority\n\n\n…anything else I’m missing?\n\nIf we can show that these properties are equivalent to the Arrow Axioms, then there may be no way to come to a singular group where all members agree."},"thoughts/Arweave":{"title":"Arweave","links":["thoughts/blockchain","thoughts/ethereum","thoughts/BitTorrent","thoughts/fault-tolerance","thoughts/decentralization","thoughts/Moderation","thoughts/DNS"],"tags":["seed"],"content":"Blogpost and Yellowpaper\nArweave uses a blockchain-like structure called the blockweave. It is capable of reaching 5000 transactions per second (compared to 15 on Ethereum)\nWildfire is the Arweave’s self-organising network topology system. Wildfire ensures that miners are selfishly incentivised to store and share data as quickly as possible with other miners in the network, in order to build a positive reputation. While more complex under the hood, Wildfire can be summarised as: ‘if you share with me, I will share with you’. (similar to how incentives on BitTorrent work). As nodes in blockweave networks require fast access to data in order to mine efficiently, they are selfishly-incentivised to give data to other members of the network promptly and continuously, autonomously improving the sharing to lightning-fast speeds.\nThe blockweave solves two fundamental problems currently associated with public decentralised blockchains:\n\nOn-chain storage constraints; and\nUnsustainable consensus mechanisms\n\nIn order for an information store to be truly permanent, it must be both fault tolerant and decentralized. Blockchain technology has much obvious promise in the area of resilient, decentralized information preservation, as a key feature of the technology is that all data inside the blockchain is immutable, and cannot be altered once it is stored. However, traditionally, such technology severely lacks scalability which clearly limits its utility for storing significant quantities of data.\nOf especially great importance is users ability to reliably maintain access to all permaweb applications and websites themselves, not simply the content they display, forever.\nRequiring proof of access (PoA) incentivises storage as miners need access to random blocks from the blockweave’s history in order to mine new blocks and receive mining rewards.\nUnlike traditional blockchain systems, Arweave does not have a typical notion of full and light clients – merely clients that downloaded more or less of the blockweave. With Arweave, full synchronisation is not a risk or an obligation, but an optional upgrade path for which miners receive higher rewards.\nFrom the user’s perspective, there are two types of transactions in the network: data transactions and value transactions. A user can initiate a data transaction to store data in a block.\nMost of the transaction fee is contributed towards a storage endowment, which is distributed to the wallets of miners over time. From our current position, at an optimistic 30% annual data density growth rate, it will take 434 years to reach the maximum theoretical limit, at 20% – 697 years, at 10% AGR – 1,329 years.\nContent policies\n\nGiven that the miners collectively maintain the Arweave network, a mechanism is required to allow them to express their opinions on what content should and should not be hosted in the system.\nNodes express preferences about content through content policies. Content policies can be arbitrary computation performed upon transactions that classify them as acceptable or not acceptable to the local node. In the reference Arweave implementation, content policies are supported in the form of substring matches as well as hashes of the data stored in the transaction.\n\nThis is one approach to decentralization moderation\n\n\nTwo complementary incentives at play here\n\nAn incentive not to over-zealously reject too many transactions, as this would lead to a decline in mining rewards\nAn incentive not to accept transactions that the majority of the network is likely to reject, as this will result in mining candidate blocks that the rest of the network will ignore\n\n\n\nArchitectures\nClient-server\nTraditional web or native applications have a client-server architecture. This model is still possible with the Arweave, as a web server can act as a front-end for data stored on the network’s permanent ledger.\nIn this centralised Arweave-app model, these services can maintain a pool of AR tokens in order to pay for data storage requests on behalf of the client.\nServerless\nDecentralised applications reside directly on and operate directly from the blockweave itself, and can be accessed by a typical web browser.\nServerless applications hosted on the Arweave network allow users to pay directly for their interactions with the network. This frees the developer from having to subsidise the cost of user interactions themselves\nDNS\nThe owner of a domain can run a permanently-available, decentralized web application just by storing a transaction on the Arweave network and registering DNS records via the usual external service providers\nYou need\n\nA DNS CNAME record pointing to an Arweave gateway: www CNAME arweave-gateway.net\nA DNS TXT record linking the domain with a specific transaction ID: arweavetx TXT kTv4OkVtmc0NAsqIcnHfudKjykJeQ83qXXrxf8hrh0S\n"},"thoughts/Asch-conformity-experiments":{"title":"Asch conformity experiments","links":["thoughts/communities"],"tags":["seed"],"content":"In 1951, Solomon Asch conducted an experiment to investigate the extent of which social pressures can affect a person’s behaviour. In this experiment, he asked, “Which line is the longest out of the three given?“. In an individual setting, 99% of people answered correctly. However, when put in a room of actors instructed to answer incorrectly, nearly 75% of all participants gave at least one incorrect answer throughout 12 trials.\nPeople conform for two main reasons:\n\nThey want to fit in with the group (normative influence)\nThey believe the group is better informed than they are (informational influence)\n"},"thoughts/Asymmetric-Key-Cryptography":{"title":"Public-key Cryptography","links":["thoughts/RSA","thoughts/Elliptic-curve-Cryptography-(ECC)"],"tags":["seed","CPSC317"],"content":"Asymmetric cryptography involves a pair of keys, one for encrypting (public) another for decrypting (private). One is private (K−), the other is public (K+). The key property is that one key cannot be obtained from the other in reasonable computation time\nCommon forms of Asymmetric Cryptography are RSA and ECC\nTwo use cases\n\nSender encrypts with public key\n\nOnly private key can decrypt it\nUsed for confidentiality\n\n\nOwner encrypts with private key\n\nAnyone can decrypt as public key is public\nUsed for authentication/proof of ownership\n\n\n"},"thoughts/Atlas-of-AI":{"title":"Atlas of AI","links":["thoughts/Do-Artifacts-Have-Politics","thoughts/seeing","thoughts/move-fast-and-break-things","thoughts/quantization","thoughts/observer-expectancy-effect","thoughts/accountability","posts/bias-bug"],"tags":["seed","book"],"content":"\n“To understand how AI is fundamentally political, we need to go beyond neural nets and statistical pattern recognition to instead ask what is being optimized, and for whom, and who gets to decide.”\n\n“As author and engineer Ellen Ullman puts it, this belief that the mind is like a computer, and vice versa, has infected decades of thinking in the computer and cognitive sciences,’ creating a kind of original sin for the field. It is the idealogy of Cartesian dualism in artificial intelligence: where AI is narrowly understood as disembodied intelligence, removed from any relation to the material world.”\n“An atlas is an unusual type of book. It is a collection of disparate parts, with maps that very in resolution from a satellite view of the planet to a zoomed-in detail of an archipelago. When you open an atlas, you may be seeking specific information about a particular place — or perhaps you are wandering, following your curiosity, and finding unexpected pathways and new perspectives.”\nAI is an attempt to be the atlas — the dominant way of seeing the world. We should instead try to construct “humble geography” that acknowledges one’s specific perspectives rather than claiming objectivity or mastery.\nEarth\n“Commerce follows the flag, but the flag follows the pick.”\n“Those who profit from mining do so only because the costs must be sustained by others, those living and those not yet born. It is easy to put a price on precious metals, but what is the exact value of a wilderness, a clean stream, breathable air, the health of local communities?”\n“It was the ’move fast and break things’ of a different time”\n“The mines were located far from the city they enriched, and this remoteness allowed city dwellers to remain ignorant of what was happening to the mountains, rivers, and laborers that fed their fortunes.”\nLabour\nMegamachine: illustration of how all systems, no matter how immense, consist of the work of many individual human actors.\nHumans are increasingly treated like robots or the connective tissue between robots.\nData\nThere is a new pervasive belief that everything is data and is there for the taking.\n“The meaning or care that might be given to the image of an individual person, or the context behind a scene, is presumed to be erased at the moment it becomes part of an aggregate mass that will drive a broader system.”\n“Even the largest troves of data cannot escape the fundamental slippages that occur when an infinitely complex world is simplified and slices into categories.” (see also: quantization)\n“Data systems allowed scientists during wartime to operate at a psychological distance from the people ‘who would be maimed and killed by the weapons systems that would result from the ideas they communicated’.”\nRecognition Systems\n“This is the danger of affect recognition tools. As we’ve seen, they take us back to the phrenological past, where spurious claims were made, allowed to stand, and deployed to support existing systems of power.” (again: observer-expectancy effect)\nAccountability\nAndrew Ferguson: “We are moving to a state where prosecutors and police are going to say ‘the algorithm told me to do it, so I did, I had no idea what I was doing’.”\nAI and algorithms create a false objectivity, “it’s just math, math can’t be biased, right?” — where’s the accountability? (see also: bias bug)\nThe historian of technology Alex Campolo calls this enchanted determinism: “AI systems are seen as enchanted, beyond the known world, yet deterministic in that they discover patterns that can be applied with predictive certainty to everyday life.”"},"thoughts/Autoencoders":{"title":"Autoencoders","links":["thoughts/latent-factor-model","thoughts/generative-models"],"tags":["seed","CPSC340"],"content":"Autoencoders are neural networks with same input and output. They are latent-factor models\nArchitecture:\n\nIncludes a bottleneck layer: with dimension k smaller than input d.\nFirst layers “encode” the input into bottleneck.\nLast layers “decode” the bottleneck into a (hopefully valid) input\n\nCan be used as a generative model!\n\n\n\nApplications\n\nSuperresolution\nNoise removal\nCompression\n\nRelationship to principal component analysis (PCA):\n\nWith squared error and linear network (no non-linear h), equivalent to PCA.\nSize of bottleneck layer gives number of latent factors k in PCA.\n"},"thoughts/Bentoism":{"title":"Bentoism","links":["thoughts/Community-of-Fate"],"tags":["seed"],"content":"My long term Bento\nA way of planning with a wider view of interests than just what we want right now, like our future selves, the people we care about, and the future of our children.\nSee also: Community of Fate"},"thoughts/Bias-in-Search":{"title":"Bias in Search","links":["thoughts/bias","thoughts/LLMs","thoughts/transparency","thoughts/double-consciousness","thoughts/potemkin-village","thoughts/information-retrieval","thoughts/explainability","thoughts/accountability","thoughts/black-box","thoughts/Algorithms-of-Oppression","thoughts/To-Live-in-their-Utopia"],"tags":["fruit"],"content":"Term paper for INFO303\n\nOne of the critiques of search engines is a lack of transparency and potential for bias in their algorithms.  This issue has become even more critical now that Artificial Intelligence is a core search technology.  Review and critically assess these concerns in light of the growing body of research that seeks to address Algorithmic Bias in search.  What methods are available or proposed to address this issue?\n\nPart I: a detailed description of the topic or issue\nThe scale of the web is huge, spanning many billions of web pages with a wide range of modalities (Teevan &amp; Dumais in Ruthven &amp; Kelly, 2011, p. 189). It is obvious that search is important for people to find what they need amidst the sprawling web we call the internet. Search engines connect its users with information resources that meet their information needs (Teevan &amp; Dumais in Ruthven &amp; Kelly, 2011, p. 192).\nSearch engines, then, are like gateways to the information on the web, allowing easy and universal access to online information to all of its users, and defining what is relevant, knowable, and authoritative. On the other hand, these arbiters of digital information can also create embedded biases that cause knowledge disparity and allow for ill-informed decision making for information-seekers (Gao &amp; Shah, 2021, p. 2643; Friedman &amp; Nissenbaum, 1996).\nThis paper seeks to critically examine some critique leveraged at these search engines around lack of transparency and potential for bias in their indexing, search, and ranking algorithms and conducts a brief survey over different methods of mitigating or addressing these issues.\nFirst, let us cover some basic terminology around what lack of transparency and bias mean in the context of this paper.\nIn conjunction with the rise of large language models for understanding queries, search has become very difficult to ‘explain’. The problem is no longer understanding a transparent algorithm (like PageRank), but rather finding out exactly why the search engine result gave the result it did. Current search engines lack any indication of how they use your personal data to personalize the results, how the language model understands your query, or what determines search ranking. Diakopoulos and Koliska refer to this phenomena as a lack of algorithmic transparency. Transparency itself is the disclosure of information about algorithms to enable monitoring, checking, criticism, or intervention by interested parties (Diakopoulos &amp; Koliska, 2017).\nAdditionally, search engines are not the fully neutral, mathematically absolute systems they sometimes claim to be. Instead, they have embedded biases that inherently favour some values over others, for example favouring some types of sites over others in query results. Here, we use Friedman and Nissenbaum’s definition of bias from their influential work Bias in computer systems: “[bias refers] to computer systems that systematically and unfairly discriminate against certain individuals or groups of individuals in favour of others… A system discriminates unfairly if it denies an opportunity or a good or if it assigns an undesirable outcome to an individual or group of individuals on grounds that are unreasonable or inappropriate” (1996, p. 332). For example, the orderings of results themselves can create exposure bias due to their considerable impacts on relevance and click-through rates. It is clear that biases that are woven in search systems are becoming increasing threats to information seeking and sense-making processes (Friedman &amp; Nissenbaum, 1996).\nPart 2: an analysis of the associated societal benefits/harms and ethical issues\nHowever, the impact of these search systems go much deeper than making it hard to find what you are looking for. If these search engines are gatekeepers, then they also influence the values that surround what is being searched for. This means that minority groups often have their own values and identities influenced by the majority: a form of double-consciousness that Safiya Umoja Noble describes in her seminal work Algorithms of Oppression (Noble, 2018). If the majority rules in search engine results, then how might those who are in the minority ever be able to influence or control the way they are represented in a search engine?\nFurthermore, Ali Alkhatib’s work paints an even grimmer possible future where algorithmic systems live in a sort of potemkin village or pseudo-reality of their own construction that they see the world as (Alkhatib, 2021). Large search engine companies like Google have massive stores of personal data in the attempts of modelling and predicting who we are and what we will do next (Pariser in Alkhatib, 2021). In doing so, these algorithmic systems produce a simplified yet inaccurate view of the world. However, the problem is not in creating these models, but from projecting the models onto the world to try and create change. These systems become more actively dangerous when they go from “making sense of the world” to “making the world make sense” (Alkhatib, 2021).\nOf course, this is exactly what these search engines are doing. They attempt to use these models to alter what information they feed to us, which in turn shapes our information seeking behaviour and what we know. Pariser (2011) points out that “[search personalization] serves up a kind of invisible autopropaganda, indoctrinating us with our own ideas, amplifying our desire for things that are familiar and leaving us oblivious to the dangers lurking in the dark territory of the unknown”.\nThe problem is exacerbated when these ‘abridged maps’ of the world start diverging from reality and people can’t override the delusions baked into those imaginations. Echoing Noble’s earlier work, this creates vicious cycles of unfairness where users frequently only click on the top results and a ranking or recommendation algorithm that takes user feedback into account will continue putting those same items at the top, reinforcing what the algorithm believes to be true about the world. As the power dynamics unfold and play out, the system drifts further and further away from reality (Gao, Ge, &amp; Shah, 2021). This is why data and power monopolies are so dangerous. These bureaucracies that have no power to self-correct (or allow themselves to be corrected) leads to a world where people cannot freely walk away or reject the bureaucracy’s nonsense model of the world (Alkhatib, 2021). If there is no way for the users to speak up and to demand change, then it becomes an Orwellian nightmare where these search engines become sources that dictate truth.\nThe large scale usage of these search engines have now changed their positions in society from aggregators of information to arbiters and oracles of truth. Historically, search algorithms relied on perhaps complex algorithms but all they did was rank literally whats relevant and ask the user to determine what info they actually need with respect to their query. Now, more search engines are leaning into trying to tell you the ‘right’ answer in an effort to reduce the number of clicks it takes for the average user to find the answer they are looking for. Yet none of these results are actually objective. These engines freely provide “a sorting of the wheat from the chaff, and answer our most profound and most trivial questions” and in doing so, become an object of faith (Noble, 2018). Cathy O’Neil likens search engines to objects of faith: “Like gods, these mathematical models were opaque, their workings invisible to all but the highest priests in their domain: mathematicians and computer scientists” (2016). Many searchers view the results of these searches as objective truth.\nThis misconception of search engines as sources of truth leads to epistemologically irrational behaviour where searchers take the answer of these algorithms without critically examining them or performing their own further research.\nPart 3: a discussion of how the issue can or should be addressed to minimize negative impacts and increase benefits.\nWith such a significant problem with one of the most critical pieces of our modern day information seeking, many have attempted to try to mitigate some of the negative impacts of these search systems. The following section briefly surveys a variety of different methods for addressing these problems.\nFairness-aware Algorithms\nOne such approach are fairness-aware algorithms which aim to integrate metrics of diversity and fairness along with more traditional information retrieval metrics like relevance and novelty. We look to Gao, Ge, and Shah’s 2021 work on FAIR: Fairness-aware information retrieval evaluation as a case study.\nThey define fairness as the subjective moderation of the ratio between different groups (p. 1). This approach to fairness comes from a more social perspective which attempts to moderate the exposure of information so that “different information and resources get fair chances to receive users’ attention” (p. 2). They argue that search currently does a poor job of distributing attention across a diverse set of results and that we should look to fairness-aware algorithms where “different items should receive equal exposure, or exposure proportional to their utilities or impacts” (p. 3).\nTo do so, they propose to optimize directly for an integrated metric — arguing that fairness and utility are not necessarily orthogonal and optimizing for one does not necessarily lead to a decline in quality in another.\nOf course, this is not without limitations either. Fairness-aware algorithms like FAIR seem promising but have yet to see more mainstream adoption from large search companies. Additionally, FAIR is designed with a focus on group fairness rather than individual fairness (p. 11), which makes it much more difficult to adapt to the standard of personalized search we see today.\nAlgorithmic Transparency\nAnother method is to vie for more explainable search systems. Explainable recommendation and search, as defined by Zhang, Zhang, and Zhang, are “models or methods that not only generate high-quality recommendation or search results, but also intuitive explanations of the results for users or system designers, which can help to improve the system transparency, persuasiveness, trustworthiness, and effectiveness, etc.” (p. 1411).\nWork done by Rader, Cotter, and Cho show potential for including explanations in search rankings and feed as ways of improving algorithmic transparency about how the search engine result arrived at its answer (2018), citing what factors contributed to why certain results appear to users.\nHowever, this method is not without critique either. Some mention that revealing the ranking algorithm would lead to catastrophe, given the adversarial stance between search and spammers and SEOs. Granados and Gupta agree, citing that while transparency can be seen as beneficial to engendering trust, “seeing the inner workings of a government, business, or newsroom can result in negative implications such as undermining competitive advantages or creating costs without concomitant gains” (Granados &amp; Gupta cited in Diakopoulos &amp; Koliska, 2017).\nDiakopoulos and Koliska noted that participants in their study recognized this issue but still found it did not detract from the necessity of transparent systems, citing that “people will game the system no matter what, and that by disclosing information publicly it would level the playing field” (2017). Leveling the playing field, in this context likely refers to healthy competitive advantage that prevents the same monopolies of power that Alkhatib’s work mentioned.\nAlgorithmic Accountability\nAnother potential approach is to better hold search engine companies accountable for the results of search rather than making the algorithms themselves explicitly transparent.\nKrafft, Reber, Krafft, Coutrier, and Zweig define the concept of algorithmic accountability, which focuses on the behavior of the algorithm or the algorithmic system in question which has to be justified and explained by the person or company who puts it in use (Krafft &amp; Reber, et al. 2021).\nThis approach is more of a ‘black-box’ approach which doesn’t require a transparent understanding of how the algorithmic system works for it to be held accountable, only for the effects to be known. Krafft et. al. specifically note this advantage over approaches that focus on transparency, citing that with accountability approaches, the workings of the algorithms themselves usually “remain undisclosed or obfuscated by design as they constitute trade secrets whose disclosure would allow gaming the system” (p. 2).\nThe ACM similarly outlines 7 principles for algorithmic transparency and accountability (ACM, 2017): awareness, access and redress, accountability, explanation, data provenance, auditability, and validation and testing. The main focus that the ACM is trying to drive is that groups affected by these algorithmic systems should be able to 1) be aware of biases + risks, 2) have enough context and data to have informed feedback, and 3) have a way to give feedback and hold the systems and their creators accountable.\nHaving human and legal entities take responsibility for the actions of their algorithms helps to prevent cases where people blame algorithms and data for being biased in order to shift the responsibility off of themselves. This form of human-in-the-loop computer system helps to prevent the algorithmic model of the world from being too detached from the real one.\nPersonal Search Engines\nPerhaps a more radical approach that is less explored in the academic space is one that advocates instead of extreme personalization (and thus bias) in search.\nEric Goldman in fact strongly advocates for this approach, suggesting that perhaps problems involving objectivity in these search engines that claim to be arbiters of truth to be solved by increasing subjectivity through the form of personalization in these results (Goldman, 2008). If each search is tailored to the individual, it is not espousing on truth to anyone, it will rather be a suggestion than an objective truth.\nGoldman goes on further to say that increasingly personalized search algorithms will “reduce the effect of search engine bias because there will likely be multiple ‘top’ search results of a particular search term instead of a single winner [and] personalized algorithms will eliminate many of the current concerns about search engine bias” (p. 130).\nSari Azout advocates for a world in which search engines are not universal, but rather boutique and personal (Azout, 2021). The big thing here is that universal search sites like Google use the same interface to search everything, relying on natural language to decipher user intentions. Vertical search players like Yelp/Zillow use domain specific knowledge to take away some of the guessing that universal search needs to go through by encoding it through structured search formats appropriate to the medium.\nIn this model of search, search engines are not oracles but rather another opinion for you to consider you in your information-seeking journey.\nConclusion\nIt does not feel like there is a single ‘silver bullet’ solution to bias and lack of transparency in the search space. If it was that easy, I’m sure it would have been developed and implemented already. This is clearly a nuanced space with a lot of interplaying factors that make it hard to find any one ‘objectively good’ solution to these problems.\nLike Krafft and Reber mention in their paper analyzing challenges in black box analyses, “search engines, like most other algorithmic systems embedded in a complex socio-technical system, are not a stable research subject” (Krafft &amp; Reber, et al. 2021). The search experience is constantly changing due to companies running A/B tests to optimize their advertisement conversion numbers and user engagement, world events that affect what is popular and searched for, previous search behaviour that affects the engine’s model of you and the world (Krafft &amp; Reber, et al. 2021). These are just a few of the many problems search engine researchers come across when trying to be exact in their science of dissecting and probing these algorithmic systems.\nThese systems are so deeply entrenched within our society that there is no way to ‘isolate’ changes in any one part of the system. It is incredibly hard to prod and fix these issues when they are constantly changing and adapting, but that doesn’t mean there hasn’t been good attempts in the space to address it. At the end of the day, web search is integral to how we conduct increasingly online portions of our lives. The least we can do is to make sure that it works equitably and well for everyone who needs to use it.\nCitations\n\nRuthven, I., &amp; Kelly, D. (Eds.). (2011). Interactive information seeking, behaviour and retrieval. Facet publishing.\nGao, R., &amp; Shah, C. (2021, July). Addressing bias and fairness in search systems. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (pp. 2643-2646).\nFriedman, B., &amp; Nissenbaum, H. (1996). Bias in computer systems. ACM Transactions on Information Systems (TOIS), 14(3), 330-347.\nDiakopoulos, N., &amp; Koliska, M. (2017). Algorithmic transparency in the news media. Digital journalism, 5(7), 809-828.\nRader, E., Cotter, K., &amp; Cho, J. (2018, April). Explanations as mechanisms for supporting algorithmic transparency. In Proceedings of the 2018 CHI conference on human factors in computing systems (pp. 1-13).\nNoble, S. U. (2018). Algorithms of Oppression. In Algorithms of Oppression. New York University Press.\nAlkhatib, A. (2021, May). To Live in their Utopia: Why algorithmic systems create absurd outcomes. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (pp. 1-9).\nPariser, E. (2011). The filter bubble: How the new personalized web is changing what we read and how we think. Penguin.\nGao, R., Ge, Y., &amp; Shah, C. (2021). FAIR: Fairness-Aware Information Retrieval Evaluation. arXiv preprint arXiv:2106.08527.\nO’neil, C. (2016). Weapons of math destruction: How big data increases inequality and threatens democracy. Broadway Books.\nZhang, Y., Zhang, Y., &amp; Zhang, M. (2018, June). SIGIR 2018 workshop on explainable recommendation and search (EARS 2018). In The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval (pp. 1411-1413).\nKrafft, T. D., Reber, M., Krafft, R., Coutrier, A., &amp; Zweig, K. A. (2021, April). Crucial challenges in large-scale black box analyses. In International Workshop on Algorithmic Bias in Search and Recommendation (pp. 143-155). Springer, Cham.\nCouncil, ACM US Public Policy (2017). Statement on algorithmic transparency and accountability. Commun. ACM.\nGoldman, E. (2008). Search engine bias and the demise of search engine utopianism. In Web Search (pp. 121-133). Springer, Berlin, Heidelberg.\nAzout, S. (2021, October 18). Re-organizing the world’s information: Why we need more boutique… Re-Organizing the World’s Information: Why we need more Boutique… - Sari Azout. Retrieved April 12, 2022, from https://sariazout.mirror.xyz/7gSSTJ96SEyvXeljymglO3zN4H6DCgVnrNZq8_2NX1A\n"},"thoughts/BitTorrent":{"title":"BitTorrent","links":["thoughts/Protocol","thoughts/TCP"],"tags":["seed"],"content":"Suppose some N (could be 1) machines have one file and another M (could be very large) machines want the file.\nThis could be very slow! The speed is limited by the throughput possible by those N machines. Peer to peer could speed this up!\n\nN hosts are called ‘seeds’\nHosts are called peers\nAs soon as one of the M peers has a portion of the file it can share it with other peers\n\nThe file is broken into many pieces\n\nGenerally fixed size except for the last one\nProtected by a cryptographic hash (allows reliable detection of corruption)\n\nA summary (torrent file) gives the necessary start up information\n\nHow many pieces there are\nThe hash of each piece\nSomewhere to start looking for peers\n\nBasic operation\n\nFinding other peers\n\nSeeds or trackers to start with\nPeer exchange: each peer ‘gossips’ about the peers they know about\nGroup of peers for one file is a “swarm”\nEach peer talks to some subset of the “swarm” at any time\n\n\nFinding pieces\n\nEach peer shares the identity of the pieces they own with its peers\nA peer asks a peer who has the file to share it\n\n\n\nStrategy\n\nA peer asks for the ‘rarest’ piece which increases the overall ‘health’ of the file\nPrioritize ones that are sending the most data to it (preferred peers) — tit-for-tat\n\nDiscourages ‘selfish’ behaviour where peers accept pieces but don’t share many pieces\nOpportunistic unchoking\n\nRandomly choose a peer sometimes (prevents problems at the start where there aren’t many peers)\n\n\n\n\nUnpopular/rare files can be hard to find seeds for\n\nIt is an open protocol, most implementations use TCP"},"thoughts/Bluesky":{"title":"Bluesky","links":["thoughts/DNS","thoughts/CID","thoughts/credible-exit","thoughts/HTTP","thoughts/search","thoughts/DID","thoughts/UCAN"],"tags":["seed"],"content":"ATProtocol\nSource\nIdentity\nUsers are identified by domain names in AT Protocol. ATProto uses a form of domain verification similar to _dnslink which involves adding a TXT record through your DNS registrar that looks like\n_atproto.jzhao.xyz TXT &quot;did=did:plc:2jpflw6cyk27tp34il2tud7o&quot;\n\nData storage\nUser data is exchanged in signed data repositories. These are stored in personal data servers (PDS).\nA “Data Repository” is a collection of data published by a single user. Repositories are self-authenticating data structures, meaning each update is signed and can be verified by anyone. These repositories are collections of records which include posts, comments, likes, follows, media blobs, etc.\nEvery node is an IPLD object which is referenced by a CID hash.\nAdditionally, ATP’s goal is to ensure that a user can migrate their account to a new PDS without the server’s involvement, providing credible exit.\nFederation\nATP syncs the repositories in a federated networking model. Federation was chosen to ensure the network is convenient to use and reliably available. Commands are sent between servers using HTTP + XRPC\nScaling\nATP distinguishes between “small-world” vs “big-world” networking. Small-world networking encompasses inter-personal activity while big-world networking aggregates activity outside of the user’s personal interactions.\n\nSmall-world: delivery of events targeted at specific users such as mentions, replies, and DMs, and sync of datasets according to follow graphs.\nBig-world: large-scale metrics (likes, reposts, followers), content discovery (algorithms), and search\n\n\nModeration\n\nDecentralizing components of existing social networks is about creating a balance that gives users the right to speech, and services the right to provide or deny reach.\n\nOur model is that speech (data, networking) and reach (crawling, aggregation, algorithm) should be two separate layers, built to work with each other. The “speech” layer should remain neutral, distributing authority and designed to ensure everyone has a voice. The “reach” layer lives on top, built for flexibility and designed to scale.\nThere’s no one company that can decide what gets published; instead there is a marketplace of companies deciding what to carry to their audiences.\n\n\nThe base layer of ATP (Personal Data Repositories and Federated Networking) creates a common space for speech where everyone is free to participate, analogous to the Web where anyone can put up a website. The Indexing services then enable reach by aggregating content from the network, analogous to a search engine.\n\nDID Consortium\nSource of truth for DIDs on ADX, operated by multiple different operators (organizations) who share ownership of service. They all operate a shared append-only log. Client send transactions to operators. Auditors can monitor the append-only log to ensure the consortium is operating as it should.\nKey management\nWe believe users should be given the options to use both custodial and non-custodial solutions. Key management is (at this stage) difficult for average consumers and so a custodial solution should be made available, but for professionals and security-conscious users a non-custodial option should also be supported.\nThe key manager has the following responsibilities:\n\nStore root private keys\nPublish updates to the users’ DID Documents\nCreate delegated keypairs through UCAN issuance\nHandle recovery flows in the event of key loss\n"},"thoughts/Bluetooth":{"title":"Bluetooth","links":[],"tags":["seed"],"content":"\nClasses:\n\nCBCentralManager &amp; CBCentralManagerDelegate\n\nAre responsible to check that Bluetooth is ON and then to scan, discover, connect, and manage peripherals.\n\n\nCBPeripheral &amp; CBPeripheralDelegate\n\nRepresents physical BLE devices as they were discovered by CBCentralManager. They are identified by UUID (universally unique identifier) which contains one or more services.\nGenerally, peripherals public data to central delegates\n\n\nCBService\n\nRepresents service physical BLE device, and provide data associated behaviors and characteristics given BLE-device.\n\n\nCBCharacteristics\n\nRepresent the data of the device’s service and contains a single value. Here is where we can read, write, and subscribe to the data from the device (ex: battery level, temperature, LED light).\n\n\n"},"thoughts/Braid-HTTP":{"title":"Braid HTTP","links":["thoughts/HTTP","thoughts/interoperability","thoughts/CRDT","thoughts/peer-to-peer","thoughts/local-first-software"],"tags":["seed"],"content":"Braid’s goal is to extend HTTP from a state transfer protocol to a state sync protocol, in order to do away with custom sync protocols and make state across the web more interoperable.\nBraid puts the power of operational transforms and CRDTs on the web, improving network performance and enabling natively p2p, collaboratively-editable, local-first web applications.\nIt turns out that HTTP is very close to being a HTSP, we just need to add 5 headers to requests and responses as well as a new status code 209 Subscription.\nFrom the IETF Internet Draft"},"thoughts/Brains-in-a-Vat":{"title":"Brains in a Vat","links":["thoughts/virtual-worlds","thoughts/semantics"],"tags":["seed"],"content":"It outlines a scenario in which a person’s brain is removed from the body and suspended in a vat of life-sustaining liquid, and connect its neurons by wires to a supercomputer that would provide it with electrical impulses identical to those a brain normally receives\nThen, the computer would then be simulating the world and the appropriate responses to the brain’s own output and the “disembodied” brain would continue to have perfectly normal conscious experiences, such as those of a person with an embodied brain, without these being related to objects or events in the real world.\nRelated to virtual worlds and the Matrix\nCould we say/think/believe that we are brains in a vat? Putnam states that this argument is false as it is self-refuting\n\nAssume we are brains in a vat\nIf we are brains in a vat, then “brain” does not refer to brain, and “vat” does not refer to vat (via CC)\nIf “brain in a vat” does not refer to brains in a vat, then “we are brains in a vat” is false\nThus, if we are brains in a vat, then the sentence “We are brains in a vat” is false (1,2,3)\n\nAre the things that BIVs refer to the same things that we as real people refer to? That is, if the relation is not grounded in the same thing, are the semantics still the same"},"thoughts/Brentano's-Thesis":{"title":"Brentano's Thesis","links":["thoughts/intentionality"],"tags":["seed"],"content":"Intentionality is the mark of the mental. His thesis posits two points:\n\nAll mental phenomena exhibit intentionality\n\nRephrased: mentality is sufficient for intentionality\nWhat about pain, feelings, moods? These can be accounted for. Physical pain can have a place, the felt location can cause a plausible difference in intentionality (what the mental state is directed at). Depression may seem to be general, but can be characterized as overall “world suck” but the world is still a subject\n\n\nOnly mental phenomena exhibit intentionality\n\nRephrased: mentality is necessary for intentionality\nAre minds the only things that have intentionality? To prove this wrong, we need to find something with intentionality that doesn’t have a mind\nWhat about words / pictures / maps? These only exhibit derived intentionality and are interpreted rather than having intrinsic intentionality\n\n\n"},"thoughts/Buddhism":{"title":"Buddhism","links":["thoughts/emptiness","thoughts/Buddhist-Economics"],"tags":["seed","PHIL240A"],"content":"\nCease to do evil; try to do good\n\nFour Noble Truths\n\nDukkha (Suffering) is an innate characteristic of existence\nSamudaya (Origin/Cause) of suffering is tanhā (craving) and fundamental ignorance. We believe we are a separate, independent existence\nNirodha (Cessation) of suffering comes from letting go of this tanhā\nMagga (The path) leading to nirodha\n\nNoble Eightfold Path\n\nRight Understanding\nRight Thought\nRight Speech\nRight Action\nRight Livelihood (see: Buddhist Economics)\nRight Effort\nRight Mindfulness\nRight Concentration.\n"},"thoughts/Buddhist-Economics":{"title":"Buddhist Economics","links":["thoughts/Buddhism","thoughts/economics","thoughts/Tools-for-Conviviality","thoughts/paratelic-action","thoughts/instrumentalism"],"tags":["sapling"],"content":"\nThe Buddhist sees the essence of civilisation not in a multiplication of wants but in the purification of human character\n\nSee also: Buddhism, economics, Tools for Conviviality\nSource\nOn Labour\nThe Buddhist point of view takes the function of work to be at least threefold:\n\nto give man a chance to utilise and develop his faculties;\nto enable him to overcome his ego-centredness by joining with other people in a common task;\nand to bring forth the goods and services needed for a becoming existence\n\nTo organise work in such a manner that it becomes meaningless, boring, stultifying, or nerve-racking for the worker would be little short of criminal; it would indicate a greater concern with goods than with people, an evil lack of compassion and a soul-destroying degree of attachment to the most primitive side of this worldly existence.\nEqually, to strive for leisure as an alternative to work would be considered a complete misunderstanding of one of the basic truths of human existence, namely that work and leisure are complementary parts of the same living process and cannot be separated without destroying the joy of work and the bliss of leisure.\nThere are therefore two types of mechanisation which must be clearly distinguished: one that enhances a man’s skill and power and one that turns the work of man over to a mechanical slave, leaving man in a position of having to serve the slave.\n\n“The craftsman himself can always, if allowed to, draw the delicate distinction between the machine and the tool. The carpet loom is a tool, a contrivance for holding warp threads at a stretch for the pile to be woven round them by the craftsmen’s fingers; but the power loom is a machine, and its significance as a destroyer of culture lies in the fact that it does the essentially human part of the work.”\n(Ananda Coomaraswamy)\n\nIf a man has no chance of obtaining work he is in a desperate position, not simply because he lacks an income but because he lacks this nourishing and enlivening factor of disciplined work which nothing can replace.\nIn this framing of economics, the pursuit of work as an integral part of character is in fact a paratelic pursuit.\nOn well-being\nThe modern economist is used to measuring the “standard of living” by the amount of annual consumption, assuming all the time that a man who consumes more is “better off” than a man who consumes less.\nA Buddhist economist would consider this approach excessively irrational: since consumption is merely a means to human well-being, the aim should be to obtain the maximum of well-being with the minimum of consumption.\nThe latter matches my own personal aesthetics for beauty and form much better.\nOn natural resources\nJust as a modern European economist would not consider it a great achievement if all European art treasures were sold to America at attractive prices, so the Buddhist economist would insist that a population basing its economic life on non-renewable fuels is living parasitically, on capital instead of income.\nWaste Your Time, Your Life May Depend On It\nThe Convivial Society: Vol. 4, No. 8\n\nWhat precisely are we saving time to do?\n\nWithin the order that generates the tyranny of tiny tasks, the one which privileges efficiency and tempts us with the promise of time-saved for the sake of some nebulous higher purpose, a human being is valuable only to the degree that they become sites of automated consumption and on-demand productivity.\nThis external order also fosters a corresponding mode of being within us. We come to understand our own experience according to the logic of techno-economic order. We presume that our worth is bound up with our productivity. We enter into an adversarial relationship with time. We develop a distaste for rest. We forget how to play. Our relationships are instrumentalized. The world becomes to us, in Hartmut Rosa’s memorable phrase, nothing more than a series of points of aggressions, “all matters to be settled, attended to, mastered, completed, resolved, gotten out of the way.”\nCare is ultimately what transforms the quality of our involvement and engagement with the world so that we pass from “getting things done” to living."},"thoughts/Byzantine-Agreement":{"title":"Byzantine Agreement","links":["thoughts/Byzantine-Broadcast"],"tags":["seed"],"content":"All non-Byzantine nodes need to agree on a single common value. There is no distinguished sender.\nDiffers from Byzantine Broadcast"},"thoughts/Byzantine-Broadcast":{"title":"Byzantine Broadcast","links":["thoughts/fault-tolerance","thoughts/system-model","thoughts/message-broadcast","thoughts/Raft-Consensus-Algorithm","thoughts/digital-signatures","thoughts/State-Machine-Replication-(SMR)","thoughts/PSL-FLM-Impossibility-Result","thoughts/Public-key-Infrastructure"],"tags":["seed"],"content":"In Byzantine broadcast (BB) or Byzantine reliable broadcast (BRB), there is a designated sender that sends its input value to all parties, and all non-faulty parties must deliver the same value.\nAssumptions:\n\nHonest users all output a message if the leader is honest (termination)\nHonest users never output different messages (consistency)\n\nOne way to do it is using a single leader, but what happens if the leader crashes/becomes unavailable? We can just manually failover: human operator chooses a new leader and reconfigures each node to use new leader, but this is non-ideal.\nNormally, we solve BB using consensus algorithms to solve this. Some common consensus algorithms include (all assume partially synchronous, crash-recovery system model):\n\nPaxos: single-value consensus\nMulti-Paxos: generalization to total order broadcast\nRaft, Viewstamped Replication, Zab: total order broadcast by default\n\nBlockchain consensus models are slightly different as they assume partially synchronous Byzantine system model.\nFor all approaches below, we assume\n\nPublic Key Infrastructure exists (i.e. nodes know the mapping of public key to entity)\nInternet exists (i.e. there is a reliable way to send message between nodes)\n\nWe denote f as the number of Byzantine nodes\nNaive Approach\nReliant on a synchronous system model\n\nt=0: sender sends a signed value v∗ to all other nodes\nt=1: nodes echo msg from sender to all other nodes, signed again (cross-checking)\nt=2: each node i chooses output vi​ by majority vote (at most one vote from sender, at most n−2 from other peers). Break ties consistently (e.g. lexicographically)\n\nSolves BB for f≤1, n≥4. Doesn’t hold for Byzantine sender, only Byzantine peers.\nDolev-Strong (1983)\nTrying to generalize the naive approach for potentially Byzantine senders by utilizing signature chains\nIn essence, node i is only convinced of value v at time t if it receives a message that\n\nreferences the value v\nis signed first by the sender\nis also signed by ≥t−1 other distinct nodes (none of which are node i)\n\nThe principle is to only accept a value in the last round if its contents can certify that all parties have received this value. This leads to a very powerful idea in the synchronous model: The validity of a message is a function also of the time it is received\nAt the end of f rounds of cross-checking (one round per possible Byzantine node), if node i is convinced of exactly one value, that is the correct value. Otherwise, output the. default value (e.g. an empty list of txs)\nSolves BB for f&lt;n, but really only useful at f&lt;2n​ for state machine replication. If f&lt;2n​ then we can take majority vote to arrive at consistent state (not the case if f≥2n​). This bypasses the PSL-FLM Impossibility Result because we assume PKI exists"},"thoughts/Byzantine-Faults":{"title":"Byzantine Faults","links":["thoughts/consensus","thoughts/fault-tolerance","thoughts/Byzantine-Broadcast","thoughts/PBFT"],"tags":["seed"],"content":"Sources: Byzantine Faults on Wikipedia and Paper on the Byzantine Generals Problem\nA Byzantine fault is any fault presenting different symptoms to different observers.\nA Byzantine failure is the loss of a system service due to a Byzantine fault in systems that require consensus between nodes.\nByzantine fault tolerance (BFT) is the property of a system that is able to resist the class of failures derived from the Byzantine Generals’ Problem. This means that a BFT system is able to continue operating even if some of the nodes fail or act maliciously.\nSee also: Byzantine Broadcast, PBFT"},"thoughts/CALM-Theorem":{"title":"CALM Theorem","links":["thoughts/consistency","thoughts/consensus","thoughts/CRON-Theorem","thoughts/I-Confluence","thoughts/semilattice","thoughts/Universal-Scaling-Law","thoughts/Datalog"],"tags":["seed"],"content":"\nConsistency As Logical Monotonicity\n\nLogically monotonic distributed code is eventually consistent without any need for consensus protocols (distributed locks, two-phase commit, etc.)\nSee also: CRON Theorem, I-Confluence\nLogically monotonic state is something that can be represented using a join semi-lattice.\nBasically, avoid coordination where possible. It’s the dominant term in the Universal Scaling Law.\nMonotonicity and Datalog\nMonotonic properties arise from things in the form of a ∃ question (the presence of one positive example gives us an answer in the affirmative and additional positive examples don’t change that fact). Non-monotonic properties arise from things in the form of a ∀ question (can only answer a question like that once we’ve looked at every example). Example of the later also include !∃, the negation property.\nWhat we’ve learned from these examples is that negation and universal quantification mess with monotonicity. Thus, we can also express CALM in terms of a programming language like Datalog:\n\nA program has an eventually consistent, coordination-free execution strategy if and only if it is expressible in (monotonic) Datalog.\n\nIn fact, this is what Bloom is based off-of. Bloom is underpinned by a programming language called Dedalus which is a Datalog variant that cleanly captures what we see as the salient semantic issues for parallel and distributed computing"},"thoughts/CAP-Theorem":{"title":"CAP Theorem","links":["thoughts/consistency"],"tags":["seed"],"content":"CAP theorem states that when designing and deploying applications in distributed environments, you can only optimize for 2 out of the following 3 properties:\n\nConsistency: a system operates fully or not at all, all nodes agree (the system’s behaviour is indistinguishable from a centralized system)\nAvailability: system is always able to answer a request\nPartition Tolerance: if data is distributed and some nodes fail, the whole system can continue to function\n\nOne way to illustrate this is to imagine a set of clusters trying to agree on a value. There is a network partition between two groups of nodes in the cluster called A and B. They all initially have a value x = 0. A client ever issues a command x = 1 to a node i in A and sometime in the future, a client issues another command return x.\n\nIf node i ever returns 1, this violates consistency as B cannot have heard of the update from A.\nIf it only ever answers 0, this violates availability as x = 1 was never appropriately set.\n\nACID vs BASE\n\nACID stands for atomicity, consistency, isolation, durability\n\nPrioritizes C and A\nImmediate consistency limits scale-out performance\n\n\nBASE stands for basically available, soft state, eventual consistency\n\nPrioritizes A and P\nScale-out performance is greatly enhanced\nFine when nature of the data can tolerate some imprecision in query results\n\n\n"},"thoughts/CID":{"title":"CID","links":["thoughts/content-addressed-storage","thoughts/hash-function"],"tags":["seed"],"content":"Summarized from Github Specification\n\nSelf-describing content-addressed identifiers for distributed systems\n\nBasically a hash with some metadata. CID is a self-describing format for referencing content, it is a form of content addressed storage.\nFormat: &lt;cidv1&gt; ::= &lt;multibase-prefix&gt;&lt;multicodec-cidv1&gt;&lt;multicodec-content-type&gt;&lt;multihash-content-address&gt;\nWhere\n\n&lt;multibase-prefix&gt; is a multibase code (1 or 2 bytes), to ease encoding CIDs into various bases. NOTE: Binary (not text-based) protocols and formats may omit the multibase prefix when the encoding is unambiguous.\n&lt;multicodec-cidv1&gt; is a multicodec representing the version of CID, here for upgradability purposes.\n&lt;multicodec-content-type&gt; is a multicodec code representing the content type or format of the data being addressed.\n&lt;multihash-content-address&gt; is a multihash value, representing the cryptographic hash of the content being addressed. Multihash enables CIDs to use many different cryptographic hash function, for upgradability and protocol agility purposes.\n\nIPVM\nBrooklyn Zelenka from Fission Codes on IPVM\nCID-based computation also means that we can use memoization to inform us if an operation has been run before so we can optimize our efforts and copy the CIDs of those outputs into our work, saving time and compute power.\nAcyclicality\nGraphs that use CIDs for references are acyclical! Hashing a cycle would mean that you need to know the CID of the contents without actually traversing its contents.\nThis is impossible! Consider a cycle A -&gt; B -&gt; C -&gt; A. To figure out the CID of A, we need to know the CID of B. To know the CID of B you need to know the CID of C. C’s CID needs to know the CID of A and we are back where we started.\nSide note: I guess this could be done by brute forcing a hash collision but this is so statistically improbable we might as we well consider it impossible"},"thoughts/CORS":{"title":"CORS","links":["thoughts/HTTP"],"tags":["seed"],"content":"Cross-Origin Resource Sharing (CORS) is an HTTP-header based mechanism that allows a server to indicate any origins (domain, scheme, or port) other than its own from which a browser should permit loading resources.\nSecurity model\nFor security reasons, browsers generally follow the same-origin policy. This means that a web application using those APIs can only request resources from the same origin the application was loaded from unless the response from other origins includes the right CORS headers.\nThis is best illustrated with an example:\nSay you visit https://example.com via a GET request. This initial request defines the origin of the request.\n\nThe document requests other resources from the same-origin like an image at https://example.com/image.png or a stylesheet at https://example.com/style.css.\n\nThese would be allowed as they are from the same origin.\nTwo URLs have the same origin if the protocol, port (if specified), and host are the same for both.\n\n\nThe document requests other resources from other origins (e.g. https://another-page.org/page.html).\n\nThis is subject to CORS as this request is considered cross-origin.\nThen, the browser checks to see if the request is considered a simple request. A simple request must meet all the following conditions:\n\nBe a GET, HEAD or POST request\nCan only contain allowed headers, mainly: Connection, User-Agent, Accept, Accept-Language, Content-Language, Content-Type, Range\nIf Content-Type is set, it must be one of application/x-www-form-urlencoded, multipart/form-data, or text/plain\n…and a few other more obscure requirements\n\n\nIf the request is a simple request, there is no preflight check. And the request is sent as normal.\n\nYour browser will automatically set the Origin header on the outgoing request. This cannot be spoofed/changed by any client-side code.\nThe cross-origin server must respond with a Access-Control-Allow-Origin header.\n\nThe browser then checks to see if the value of the header matches the origin.\n\nAccess-Control-Allow-Origin: * means that the resource can be accessed by any origin.\nAccess-Control-Allow-Origin: https://example.com means that the resource can only be accessed when https://example.com is the origin.\n\n\nIf the origin doesn’t match, the browser will raise a CORS failure. CORS failures result in errors but for security reasons, specifics about the error are not available to JavaScript.\nIf server doesn’t send back Access-Control-Allow-Origin, then the browser will refrain from providing the response to the caller and raise a CORS failure.\n\n\n\n\nIf the request isn’t a simple request, it must first send a preflight request to the server to see if the actual request is safe to send using the OPTIONS HTTP verb.\n\nThis OPTIONS request contains a bunch of metadata about the original request (that hasn’t been sent yet) like what the HTTP method it uses, other headers it includes, etc.\nThen, the server will respond with a few access control headers:\n\nAccess-Control-Allow-Origin\nAccess-Control-Allow-Methods\nAccess-Control-Allow-Headers\nAccess-Control-Max-Age: gives the value in seconds for how long the response to the preflight request can be cached without sending another preflight request\n\n\nThe browser then looks at the preflight response from the server and then chooses whether to send the actual request or not. Again, any failure here results in a CORS failure that is opaque to client-side Javascript.\n\n\n\n\n\n\n\n                  \n                  Info\n                  \n                \n“What stops some actor from just not setting the ORIGIN header correctly? Wouldn’t that invalidate CORS?”\nCORS is a browser-level concept. Browsers are in control of setting the Origin header, and users can’t override this value. So you won’t see the Origin header spoofed from a browser. You can manually craft an HTTP request that bypasses these restrictions but keep in mind that CORS is not security.\n\nAbolish the same-origin policy\nIn a Web context, the user must be able to safely load any arbitrary URL, to safely click on any arbitrary link. The way in which this is achieved is that the runtime places strict limits on what a Web page can do. This puts stringent limits on the Web’s ability to allow people to combine two services together, which in turn limits the Web’s usefulness and prevents it from evolving an application architecture that is better than native apps."},"thoughts/CRDT-Implementations":{"title":"CRDT Implementations","links":["thoughts/CRDT"],"tags":["seed"],"content":"All examples below are written in pseudocode that happens to carry a lot of syntax from Typescript. Syntax liberties are taken where intention is clear\nSpec\nOp-based\nSee operation-based CRDTs for more properties\n// the initial value of the data type (on each replicate)\ntype State = {\n\t...\n}\n \nclass OpCRDT&lt;State&gt; {\n\tstate: State\n \n\t// any function that computes a view of the payload and has no side effects\n\t// can return a value\n\t@query\n\tfunction query(...args: any[]): any {\n\t\tif (invariant) {\n\t\t\t// do something\n\t\t\treturn\n\t\t}\n\t}\n \n\t// any global function that take in arguments and has two phases\n\t@update\n\tfunction update(...args: any[]): Closure {\n\t\tif (local_invariant) {\n\t\t\t// phase 1: may compute results to be prepared as arguments for the second phase\n\t\t\t// includes precondition checks, etc.\n \n\t\t\t// phase 2: returns a closure to be run on all nodes, including this one\n\t\t\treturn (...) =&gt; {\n\t\t\t\tif (downstream_invariant) {\n\t\t\t\t\t...\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\nState-based\nSee state-based CRDTs for more properties\n// the initial value of the data type (on each replicate)\ntype State = {\n\t...\n}\n \nclass StateCRDT&lt;State&gt; {\n\tstate: State\n \n\t// any function that computes a view of the payload and has no side effects\n\t// can return a value\n\t@query\n\tfunction query(...args: any[]): any {\n\t\tif (invariant) {\n\t\t\t// do something\n\t\t\treturn\n\t\t}\n\t}\n \n\t// any function that when evaluated, has side-effects on the payload\n\t@update\n\tfunction update(...args: any[]) {\n\t\tif (local_invariant) {\n\t\t\t// do something\n\t\t}\n\t}\n \n\t// a function that compares two states in the semilattice (see: order theory)\n\t@compare\n\tfunction cmp(a: State, b: State): boolean {\n\t\t// is a &lt;= b in the semilattice?\n\t}\n \n\t// a function that performs a least-upper-bound merge on two states\n\t@merge\n\tfunction merge(a: State, b: State): State {\n\t\t// least-upper-bound merge on a and b at any replica\n\t}\n}\nCounters\nOp-based\nThis implementation is trivially correct as both addition and subtraction commute\ntype State = {\n\ti: number\n}\n \nclass OpCRDT&lt;State&gt; {\n\t@query\n\tvalue(): number {\n\t\treturn this.i\n\t}\n \n\t@update\n\tfunction increment(): Closure {\n\t\treturn (node) =&gt; node.i := node.i + 1\n\t}\n \n\t@update\n\tfunction decrement(): Closure {\n\t\treturn (node) =&gt; node.i := node.i - 1\n\t}\n}\nState-based\nInspired by vector clocks. Merge takes max of each entry so this forms a monotonic semilattice. We need two vectors as just operating on a single vector as max wouldn’t even work if we included decrement.\nFor example, say you have two states [1, 0, 1] and [1, 1, 1]. You would never tell if the first one happens after the second (second node subtraction) or if the second one happens after the first (second node addition).\ntype State = {\n\tplus: number[n];\n\tminus: number[n];\n}\n \nclass StateCRDT&lt;State&gt; {\n\t@query\n\tfunction value(): int {\n\t\treturn sum(this.plus) - sum(this.minus)\n\t}\n \n\t@query\n\tfunction increment() {\n\t\tconst id = this.id()\n\t\tthis.plus[id] = this.plus[id] + 1\n\t}\n \n\t@update\n\tfunction decrement() {\n\t\tconst id = this.id()\n\t\tthis.minus[id] = this.minus[id] + 1\n\t}\n \n\t@compare\n\tfunction cmp(x: State, y: State): boolean {\n\t\treturn zip(x.plus, y.plus).every((x_i, y_i) =&gt; x_i &lt;= y_i) &amp;&amp;\n\t\t\t   zip(x.minus, y.minus).every((x_i, y_i) =&gt; x_i &lt;= y_i)\n\t}\n \n\t@merge\n\tfunction merge(x: State, y: State): State {\n\t\treturn Payload {\n\t\t\tplus: zip(x.plus, y.plus).map((x_i, y_i) =&gt; max(x_i, y_i))\n\t\t\tminus: zip(x.minus, y.minus).map((x_i, y_i) =&gt; max(x_i, y_i))\n\t\t}\n\t}\n}\nLast-writer-wins Registers\nA register is a memory cell storing a single value.\nOp-based\nX is an arbitrary type\ntype State&lt;X&gt; = {\n\tval: X;\n\tt: number;\n}\n \nclass OpCRDT&lt;State&gt; {\n\t@query\n\tfunction value(): X {\n\t\treturn this.val\n\t}\n \n\t@update\n\tfunction assign(x: X): Closure {\n\t\tconst t_now = now()\n\t\treturn (node) =&gt; {\n\t\t\tif node.t &lt; t_now {\n\t\t\t\tnode.val = x\n\t\t\t\tnode.t = t_now\n\t\t\t}\n\t\t}\n\t}\n}\nState-based\nTimestamp is monotonic increasing so compare created a valid monotonic semilattice.\ntype State&lt;X&gt; = {\n\tx: X;\n\tt: number;\n}\n \nclass StateCRDT&lt;State&gt; {\n\t@query\n\tfunction value(): X {\n\t\treturn this.val\n\t}\n \n\t@update\n\tfunction assign(x: X) {\n\t\tthis.t = now()\n\t\tthis.x := x\n\t}\n \n\t@compare\n\tfunction cmp&lt;X&gt;(x: State&lt;X&gt;, y: State&lt;X&gt;): boolean {\n\t\treturn x.t &lt;= y.t\n\t}\n \n\t@merge\n\tfunction merge&lt;X&gt;(x: State&lt;X&gt;, y:State&lt;X&gt;): State&lt;X&gt; {\n\t\t// return most recent write by logical clock\n\t\treturn cmp(x, y) ? y : x\n\t}\n}\nSets\nA foundational data structure that form the basis of containers, maps, and graphs.\nNaively adding and removing from a set does not commute so we can only approximate the properties of a set.\nMost implementations below differ by how they handle concurrent add(e)∥remove(e)\nFor example:\n\nGrow-only set (G-Set) avoids remove altogether\n2-Phase set (2P-Set) is a variant where both add and remove are valid operations but an element cannot be re-added once removed\nUnique set (U-Set) is an extension of 2-Phase set where we additionally assume elements are unique. Additional requirement that causal dependencies are respected (op-based CRDTs are sufficient to ensure this)\nAdd-wins set (OR-Set/AW-Set) supports both adding and removing elements. Add has precedence when an add and remove happen concurrently.\n\nState-based 2P-Set\nThe compare function (checking to see if x comes before y in the semilattice) here is quite tricky and not immediately obvious why it is correct.\n\nIf x.set is a subset of y.set, then x must have come before y because nothing is ever removed from set\nIf x.set is the same set as y.set, then x can only have come before y if x.removed is a subset of y.subset\nIf x.set is not a subset of y.set then x cannot have come before y\n\ntype State&lt;X&gt; = {\n\tset: Set&lt;X&gt;;\n\tremoved: Set&lt;X&gt;;\n}\n \nclass StateCRDT&lt;State&gt; {\n\t@query\n\tfunction has(x: X): bool {\n\t\treturn this.set.has(x) &amp;&amp; !this.removed.has(x)\n\t}\n \n\t@update\n\tfunction add(x: X) {\n\t\tset.add(x)\n\t}\n \n\t@update\n\tfunction remove(x: X) {\n\t\tif has(x) {\n\t\t\tremoved.add(x)\n\t\t}\n\t}\n \n\t@compare\n\tfunction cmp&lt;X&gt;(x: State&lt;X&gt;, y: State&lt;X&gt;): boolean {\n\t\treturn x.set.is_subset_of(y.set) ||\n\t\t\t   x.removed.is_subset_if(y.removed)\n\t}\n \n\t@merge\n\tfunction merge&lt;X&gt;(x: State&lt;X&gt;, y:State&lt;X&gt;): State&lt;X&gt; {\n\t\t// return most recent write by logical clock\n\t\treturn Payload {\n\t\t\tset: union(x.set, y.set)\n\t\t\tremoved: union(x.removed, y.removed)\n\t\t}\n\t}\n}\nOp-based U-Set\nAgain, this op-based implementation assumes causal ordering in message delivering\ntype State&lt;X&gt; = {\n\tset: Set&lt;X&gt;;\n}\n \nclass OpCRDT&lt;State&gt; {\n\t@query\n\tfunction has(x: X): boolean {\n\t\treturn this.set.has(x)\n\t}\n \n\t@update\n\tfunction add(x: X): Closure {\n\t\treturn this.set.add(x)\n\t}\n \n\t@update\n\tfunction remove(x: X): Closure {\n\t\tif this.has(x) {\n\t\t\t// due to causal ordering assumption, add(x) must have been delivered already\n\t\t\treturn (node) =&gt; node.set.remove(x)\n\t\t}\n\t}\n}\n \nOp-based AW-Set\nIntuition here is to generate a unique ID for each element added. Multiple adds will add multiple values and delete will delete all elements with the same value.\nConcurrent adds commute as each add is unique. If a concurrent add and remove happen, it also commutes as add has precedence.\ntype State&lt;X&gt; = {\n\t// track element and uuid\n\tset: Set&lt;(X, number)&gt;;\n}\n \nclass OpCRDT&lt;State&gt; {\n\t@query\n\tfunction has(x: X): boolean {\n\t\treturn this.set.values.any((val, id) =&gt; val === x)\n\t}\n \n\t@update\n\tfunction add(x: X): Closure {\n\t\tconst id = uuid()\n\t\treturn (node) =&gt; node.set.add((x, id))\n\t}\n \n\t@update\n\tfunction remove(x: X): Closure {\n\t\tif this.has(x) {\n\t\t\tconst vals_to_delete = this.set.values.filter((val, id) =&gt; x === val)\n\t\t\treturn (node) =&gt; node.set.remove(vals_to_delete)\n\t\t}\n\t}\n}\nSequences\nA sequence for text editing (or just sequence hereafter) is a totally-ordered set of elements, each composed of a unique identifier and an atom.\nFor the rest of this section, assume the following definitions\nconst __LEFT: any = (&quot;START&quot;, -1)\nconst __RIGHT: any = (&quot;END&quot;, 0)\n \n// e.g., a character, a string, an XML tag, or an embedded graphic\ntype Atom = any\n \n// Timestamps are unique, positive, and increase consistently with causality\ntype T = number\ntype Vertex = (Atom, T)\nReplicated Growable Array (RGA)\nAutomerge the library uses this!\nRepresented as a 2P-Set of vertices in a linked list.\nEssentially,\n\nBuild the tree, connecting each item to its parent\nWhen an item has multiple children, sort them by sequence number then by their ID.\nThe resulting list (or text document) can be made by flattening the tree with a depth-first traversal.\n\ntype State = {\n\t// 2P-Set of vertices\n\tv_added: Set&lt;Vertex&gt; = [__LEFT, __RIGHT];\n\tv_rmved: Set&lt;Vertex&gt; = [];\n \n\t// G-Set of edges\n\tedges: Set&lt;(Vertex, Vertex)&gt; = [(__LEFT, RIGHT)];\n}\n \nclass OpCRDT {\n\t@query\n\tfunction lookup(v: Vertex): boolean {\n\t\treturn this.v_added.has(v) &amp;&amp; !this.v_rmved.has(v)\n\t}\n \n\t// is u before v in the sequence?\n\t@query\n\tfunction before(u: Vertex, v: Vertex): boolean {\n\t\tif this.lookup(u) &amp;&amp; this.lookup(v) {\n\t\t\t// see if there is a valid path from u to v using dfs\n\t\t\tconst stack = [u]\n\t\t\twhile stack.length &gt; 0 {\n\t\t\t\tconst cur = stack.pop()\n\t\t\t\tif cur === v {\n\t\t\t\t\treturn true\n\t\t\t\t}\n \n\t\t\t\tconst outgoing_vertices = this\n\t\t\t\t\t.edges\n\t\t\t\t\t.filter((_u, _v) =&gt; u === _u)\n\t\t\t\t\t.map((_, _v) =&gt; v)\n \n\t\t\t\tstack.push(...outgoing_vertices)\n\t\t\t}\n\t\t\treturn false\n\t\t}\n\t}\n \n\t@query\n\tfunction successor(u: Vertex): Vertex {\n\t\tif this.lookup(u) {\n\t\t\treturn this.edges.find((_u, _v) =&gt; u === _u)[1]\n\t\t}\n\t}\n \n\t@update\n\tfunction addRight(v: Vertex, x: Atom) {\n\t\t// ensure valid insert\n\t\tif v !== __RIGHT &amp;&amp; this.v_added.sub(this.v_rmved).has(v) {\n\t\t\tconst t = now()\n\t\t\tconst w = (x, t)\n \n\t\t\treturn (node) =&gt; {\n\t\t\t\t// find right place to insert node\n\t\t\t\tif node.v_added.has(v) {\n\t\t\t\t\tconst l = v\n\t\t\t\t\tconst r = node.successor(v)\n\t\t\t\t\twhile true {\n\t\t\t\t\t\tconst _v, _t = r\n\t\t\t\t\t\tif t &lt; _t {\n\t\t\t\t\t\t\t// move forward one step\n\t\t\t\t\t\t\tl = r\n\t\t\t\t\t\t\tr = node.successor(r)\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t// right spot!\n\t\t\t\t\t\t\t// remove old edge\n\t\t\t\t\t\t\tthis.edges.remove((l, r))\n\t\t\t\t\t\t\t// add new ones\n\t\t\t\t\t\t\tthis.edges.add((l, w))\n\t\t\t\t\t\t\tthis.edges.add((w, r))\n\t\t\t\t\t\t\treturn\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n \n\t@update\n\tfunction remove(v: Vertex) {\n\t\tif this.lookup(v) {\n\t\t\treturn (node) =&gt; v_rmved.add(v)\n\t\t}\n\t}\n}\nContinuous Sequence using real numbers\nWe need to translate indices into unique immutable positions (what the user intuitively means when they say ‘insert here’).\nThis assumption of relative order of elements remains constant over time is called the strong list specification.\nPerformance depends critically on the implementation of identifiers. One possible implementation is to use a dense identifier space like R where a unique identifier can always be allocated between any two identifiers.\nIndices are based off of what % of the text they get inserted at. 0.0 is the index of the start sequence, 1.0 is the index of the end sequence (this is similar to what Treedoc does).\n0.0       1.0\nNUL       NUL\n\nInserting a single character would be halfway between 0.0 and 1.0 so it would have an index of 0.5.\n0.0   0.5   1.0\nNUL   &#039;B&#039;   NUL\n\nInserting to the left of ‘B’ would be between 0.0 and 0.5 so 0.25.\n0.0  0.25  0.5   1.0\nNUL   &#039;A&#039;  &#039;B&#039;   NUL\n\nWe represent the continuum using a tree. The first element is allocated at the root. Thereafter, it is always possible to create a new leaf e between any two nodes n and m.\nWe do this by representing the tree using a U-Set\ntype Element = (Atom, number)\n \ntype State = {\n\tset: Set&lt;Element&gt; = [];\n}\n \nclass OpCRDT {\n\t@query\n\tfunction lookup(u: Element): boolean {\n\t\treturn this.set.has(u)\n\t}\n \n\t@query\n\tfunction before(u: Element, v: Element): boolean {\n\t\tif this.lookup(u) &amp;&amp; this.lookup(v) {\n\t\t\tconst _, u_id = u\n\t\t\tconst _, v_id = v\n\t\t\treturn u_id &lt; v_id\n\t\t}\n\t}\n \n\t@update\n\tfunction addBetween(u: Element, x: Atom, v: Element) {\n\t\tif this.before(u, v) {\n\t\t\tconst _, u_id = u\n\t\t\tconst _, v_id = v\n\t\t\tconst new_el = (x, (u_id + v_id) / 2)\n\t\t\treturn (node) =&gt; {\n\t\t\t\tnode.set.add(new_el)\n\t\t\t}\n\t\t}\n\t}\n \n\t@update\n\tfunction remove(u: Element) {\n\t\tif this.lookup(u) {\n\t\t\treturn (node) =&gt; node.set.remove(u)\n\t\t}\n\t}\n}\nGraphs\nGenerally, graphs are difficult to maintain due to the property that CRDTs cannot compute and maintain global invariants like structure.\nHowever, some stronger forms of acyclicity are implied by local properties, for instance\na monotonic DAG, in which an edge may be added only if it oriented in the same direction\nas an existing path. Vertices and edges can be stores as sets.\nSee reference implementations in this paper"},"thoughts/CRDT":{"title":"Conflict-free Replicated Data Type (CRDT)","links":["thoughts/causality","thoughts/consistency","thoughts/message-broadcast","thoughts/CRDT-Implementations","thoughts/Byzantine-Faults","thoughts/Merkle-DAG","thoughts/Order-theory","posts/bft-json-crdt","thoughts/consensus","thoughts/Antimatter"],"tags":["sapling"],"content":"Provides causal consistency as well as strong eventual consistency: over time, all actors converge to same state without data loss but there is no guarantee of exact same state across actors at any given moment (not ACID).\n\nNote: In general, maintaining global invariants (e.g. shapes such as a tree or a DAG), cannot be done by a CRDT. Global invariant cannot be determined locally; maintaining it requires synchronisation.\n\nCRDTs should always strive to preserve user intent.\nTwo main families of CRDTs are operation-based and state-based CRDTs. They have their trade offs\n\nOperation-based\n\ngenerally smaller messages\nrequires causally-ordered delivery for messages\ncan be more complex because it requires reasoning about history\n\n\nState-based\n\ncan tolerate message loss/duplication\nrequires best-effort broadcast delivery for messages\n\n\n\nSee example implementations here: CRDT Implementations\nI prefer CRDTs over OT whenever possible because it is just so much easier to grok for the average engineer. The framework tells you clearly what you’d need to do to make async editing actually work (make the update operation commutative), why that’s so difficult (delete operations lose state) and how to make your life much easier (retain delete state and do some form of GC after the fact).\nOperation-based\n\nSometimes also called commutative replicated data types (CmRDT)\n\nReplication requires one of the following assumptions:\n\nall concurrent operations to commute given causal ordering (most common)\nall operations to commute given no ordering\nall operations to commute and be idempotent if message duplication can occur\n\nHistory is kept through the notion of a causal history C\n\nInitially, C(xi​)=∅\nAfter executing the 2nd (downstream) phase of operation f, C(f(xi​))=C(xi​)∪{f}\n\nState-based\n\nSometimes also called convergent replicated data types (CvRDT)\n\nCan broadcast the values of the state using best-effort broadcast and then merging using a defined merge operator ⊔.\nThe merge operator ⊔ must be:\n\nCommutative: s1​⊔s2​=s2​⊔s1​\nAssociative: (s1​⊔s2​)⊔s3​=s1​⊔(s2​⊔s3​)\nIdempotent: s1​⊔s1​=s1​\n\nHistory is kept through the notion of a causal history C\n\nInitially, C(xi​)=∅\nAfter an update operation f, C(f(xi​))=C(xi​)∪{f}\nAfter merging states xi​, xj​, C(merge(xi​,xj​))=C(xi​)∪C(xj​)\nThe happens-before relation ≤ is then defined as f→g⟺C(f)⊂C(g)\n\nDelta-based (hybrid)\nDelta-based CRDTs propagate delta-mutators that encode the changes that have been made to a replica since the last communication.\nFor efficiency, CRDT implementations can ‘hold on’ to outbound events and compact/compress the log by rewriting operations (e.g. turning two add(1) operations into a single add(2) operation)\nStrategies for Designing CRDTs\nA CRDT can be specified by relying on:\n\nthe full history of updates executed;\nthe happens-before relation among updates; and\nan arbitration relation among updates (when necessary)\n\nA query can be specified as a function that uses this information and the value of parameters to compute the result (i.e. goes from the state to a value).\nSecure CRDTs\n\nWhat does encryption in CRDTs look like? homomorphic encryption for merge operations for example\nhttps://martin.kleppmann.com/papers/snapdoc-pets19.pdf\nhttp://www.complang.tuwien.ac.at/kps2015/proceedings/KPS_2015_submission_25.pdf\n\nFault Tolerance\nHow can we make CRDTs Byzantine fault-tolerant?\nKleppmann shows that is possible to guarantee the standard CRDT consistency properties even in systems in which arbitrarily many nodes are Byzantine.\nCRDTs can become BFT by ensuring eventual delivery and convergence even in the presence of Byzantine nodes.\nThe main construct here is constructing a hash graph (aka a Merkle-DAG): The graph is essentially the Hasse diagram of the partial order representing the causality relation among the updates. The ID of an operation is the hash of the update containing that operation. A ‘head’ is just an operation which is not a dependency of another operation.\n\nThis hash graph helps to ensure eventual consistency as two nodes p and q can exchange the hashes of their currents heads and if they are identical, they can ensure the set of updates they have observed is also identical.\nIf the heads of p and q are mismatched, the nodes can run a graph traversal algorithm to determine which parts of the graph they have in common, and send each other those parts of the graph that the other node is lacking.\n\nSee: bft-json-crdt\nUndo\nApproach inspired by xi-editor. Source\nThis means that the easy way to implement history, which is to simply roll back to a previous state, does not work. The state that is created by undoing your change, if other people’s changes have come in after it, is a new one, not seen before.\nTo be able to implement this, we can define changes (steps) in such a way that they can be inverted, producing a new step that represents the change that cancels out the original step.\nEach editing operation is assigned an “undo group.” Several edits may be in the same group. For example, if the user types &quot;, then a smart-quote plugin may revise that to ‘“’. If the smart-quote revision is assigned the same undo group (because it is a consequence of the same user action), then a single undo would zorch both edits. Each undo group gets a distributed counter, and the group is considered to be undone when the counter is odd-valued.\nPerformance\nStorage + State Compaction\nPractical experience with CRDTs shows that they tend to become inefficient over time,\nas tombstones accumulate and internal data structures become unbalanced. The compacted portion of the CRDT must retain enough metadata to allow future operations to reference it on an atomic level and order themselves correctly. From the outside, a compacted CRDT must continue to behave exactly the same as a non-compacted CRDT.\nHowever, GC + rebalancing technically requires achieving consensus on nodes in order to do this.\n\nSo, as far as I know, we would need a consensus protocol attached to the CRDT in order to get garbage collection / compaction. (#2)\n\nOne potential way of overcoming this is to have a small, stable subset of replicas called the core which achieve consensus amongst each other. The other replicas asynchronously reconcile their state with core replicas.\nSee also: Antimatter\nExploiting good connectivity for stronger consistency\nUpgrading network assumption from asynchronous to partially synchronous enables us to potentially define weak operations which only eventually need to be linearized.\nUnsolved Problems\n\nConcurrent move + edit in sequences is unsolved\n\nAlmost all implementations cause duplication\nFugue\n\n\n\nReadings\n\nA comprehensive study of CRDTs\nConflict-free Replicated Data Types: An Overview\n"},"thoughts/CRON-Theorem":{"title":"CRON Theorem","links":["thoughts/causality","thoughts/CALM-Theorem"],"tags":["seed"],"content":"\nCausality Required Only for Non-monotonicity (CRON).\n\nEventual consistency can be guaranteed in any program by protecting non-monotonic statements (“points of order”) with consensus protocols\nRephrased: Program semantics require causal message ordering if and only if the messages participate in non-monotonic derivations (e.g. set removal)\nSee also: CALM Theorem"},"thoughts/Casper-FFG":{"title":"Casper FFG","links":["thoughts/consensus","thoughts/Tendermint"],"tags":["seed"],"content":"\nPartial consensus mechanism as an overlay on top of proposal mechanisms to finalize blocks (selecting a unique chain that represents the canonical history of the chain)\n\nIt enables:\n\nan accountability mechanism so that Byzantine validators can be penalized.\nsupport for a dynamic set of validator nodes\nadditional defences against long range revision attacks\n\nFrom a foundational/technical perspective, Casper is essentially chained Tendermint\nSource Paper"},"thoughts/Category-Theory":{"title":"Category Theory","links":["thoughts/functional-programming","thoughts/Homotopy-Theory"],"tags":["seed"],"content":"A lot of this is sourced from Category Theory for Programming\nThe power of category theory arises from abstraction: by boiling down constructions to their essence, analogous situations can be formally identified using category theory.\nIn essence, a simple collection which can be thought of as a graph. Three components\n\nA collection of objects (nodes)\nA collection of morphisms (edges).\n\nIf f is a morphism with source C and target B, we write f:C→B\n\n\nA notion of composition of morphisms.\n\nIf we have g:A→B and f:B→C, they can be composed resulting in a morphism f∘g:A→C\nComposition of morphisms needs to be associative. Typically applied right to left\n\n\n\nCategories\nA category C consists of\n\nA collection of objects, denoted by C0​\nFor any given X,Y∈C0​, a collection of morphisms from X to Y denoted by homC​(X,Y) or C(X,Y) or even X→Y\nAn identity morphism for each object X∈C0​ such that IdX​∈homC​(X,X)\nA binary infix composition operator ∘ that takes hom(Y,Z)→hom(Y,Z)→hom(X,Z) (this can be thought of as X→Y∧Y→Z⟹X→Z)\n\nSee also: functional programming, Homotopy Theory"},"thoughts/Chesterton's-Fence":{"title":"Chesterton's Fence","links":["thoughts/terminology","thoughts/context","posts/new-words","thoughts/Lindy-effect"],"tags":["seed","pattern"],"content":"\n“Don’t ever take a fence down until you truly know the reason why it was put up” — G.K. Chesterton\n\nThe principle that reforms should not be made until the reasoning behind the existing state of affairs is understood.\nThe same reason why we should look at terminology and why words with historical context came to be (e.g. new words)\n\n“Before proposing new words, examine the beliefs and practices that we—and “they”—know perfectly well how to speak about.” Source: On Language Games by Jon Baskin\n\nSimilarly: Lindy effect"},"thoughts/Chinese-room-argument":{"title":"Searle's Chinese room argument","links":["thoughts/language","thoughts/intentionality"],"tags":["seed"],"content":"Source: Stanford Encyclopedia of Philosophy\nMainly a refutation against the Turing test as a means for measuring intelligence. The narrow conclusion of the argument is that programming a digital computer may make it appear to understand language but could not produce real understanding.\n“Searle imagines himself alone in a room following a computer program for responding to Chinese characters slipped under the door. Searle understands nothing of Chinese, and yet, by following the program for manipulating symbols and numerals just as a computer does, he sends appropriate strings of Chinese characters back out under the door, and this leads those outside to mistakenly suppose there is a Chinese speaker in the room.”\nWe often attribute “understanding” and other cognitive predicates by metaphor and analogy to things that can’t be intentioned like cars and adding machines — we make these attributions as we extend our own intentionality onto them (derived intentionality)"},"thoughts/CodeMirror":{"title":"CodeMirror and ProseMirror","links":["thoughts/Operational-Transform"],"tags":["seed"],"content":"Source\nAn extensible and fast rope-based code/prose editor component for the web.\nFunctional Core, Imperative Shell\nTo resolve this contradiction, the library’s state representation is strictly functional—the document and state data structures are immutable, and operations on them are pure functions, whereas the view component and command interface wrap these in an imperative interface\nIt also means that directly changing a state value, or writing extensions like additional state fields in an imperative way will not do what you’d hope (and probably just break things).\nDocument Changes\nDocument changes are themselves values, describing precisely which ranges of the old document are being replaced by which bits of new text. This allows extensions to track precisely what happens to the document, allowing things like an undo history and collaborative editing to be implemented outside the library core\nWhen creating a change set, all changes are described in terms of the original document—they conceptually all happen at once. If you really need to combine lists of changes where later changes refer to the document created by earlier ones, you can use the change set compose method\nOffsets\nCodeMirror uses plain numbers to address positions in the document. These represent character counts—more precisely, they count UTF16 code units (so astral characters count as two units)\nLine breaks always count as a single unit.\nIt is sometimes necessary to figure out where a position in a start document ends up in a changed document. For this purpose, the library provides a position mapping feature, which, given a transaction (or just a change set) and a start position, can give you the corresponding new position.\nThis is similar to position mapping in OT\nSelections\nOverlapping ranges are automatically merged, and ranges are sorted, so that a selection’s ranges property always holds a sorted, non-overlapping array of ranges\nCRDTs\nSource\ndmonad (creator of Yjs) on a ProseMirror plugin for Yjs:\n\nMapping a CRDT to a ProseMirror document is not always possible, because ProseMirror has a schema that the document needs to comply to. E.g. given a blockquote that must have at least one child. If client1 deletes the first child, and client2 concurrently deletes the second child, you may end up with a blockquote without children (well, that depends on the CRDT). In this case I simply delete the node that does not comply with the schema anymore.\n\nEnforcing that a CRDT complies to a schema might be impossible without an inordinate amount of bookkeeping. But something you could try is to create a view on the data that complies to the schema. For example, if a blockquote does not have any children, ignore it.\n“For doing offline work (where you keep editing when not connected) or for a branching type of work flow, where you do a bunch of work and then merge it with whatever other people have done in the meantime, the model I described here is useless (as is OT).”"},"thoughts/Collingridge-dilemma":{"title":"Collingridge Dilemma","links":["thoughts/catch-22","thoughts/Pacing-Problem"],"tags":["seed","pattern"],"content":"Efforts to influence or control the further development of technology face a double-bind problem\n\nAn information problem: impacts cannot be easily predicted until the technology is extensively developed and widely used.\nA power problem: control or change is difficult when the technology has become entrenched.\n\nSee also: Pacing Problem"},"thoughts/Community-of-Fate":{"title":"Community of Fate","links":["thoughts/communities","thoughts/Bentoism","thoughts/Mutual-Aid","thoughts/fiction"],"tags":["seed"],"content":"Margaret Levi in Noema Magazine\n\nThe keystone is generating an “expanded community of fate.” For societies to survive and thrive, a significant proportion of their members must engage in reciprocal altruism\n\n“Our goal is a form of farsighted reciprocal altruism in which members are willing to make costly sacrifices on behalf of those with whom they believe their fates, and their descendants’ fates, are entwined but who may never be able to directly reciprocate.” — reminds me a lot of Bentoism, specifically the ‘future us’ square\nA lot of very similar values to Unions and mutual aid, e.g. “An injury to one is an injury to all.”\nHeresthetics: using rhetoric, storytelling and strategy to redefine the situation in a way that changes the choices for a significant proportion of the populace (see also: fiction)"},"thoughts/Consciousness-is-not-Information":{"title":"Consciousness is not Information","links":["thoughts/Stream-of-Consciousness","thoughts/Panpsychism","thoughts/Goodhart's-Law"],"tags":["fruit","PHIL451A"],"content":"\nPaper #2 for PHIL 451A\nPrompt 1: Is consciousness essentially a kind of information?\n\nWhen we examine theories of consciousness, we find that we can divide the majority of theories into two major categories: process theories and vehicle theories1. I further borrow terminology from Velmans2 to describe these as behaviourist and cognitivist approaches to consciousness respectively, and argue that consciousness under cognitivist approaches runs into quite a few glaring holes.\nLet us first begin by defining the two major categories of theories of consciousness.\n\nProcess theories assume that consciousness depends on the functional or relational properties of representational vehicles3, namely on the types of computations the vehicles engage in. Process theories are also referred to as cognitivist theorists — these theories can, without scientific loss, be translated into talk about information processing2.\nVehicle theories assume that consciousness is determined by intrinsic properties of representational vehicles3. These theories are also referred to as behaviourist theorists — these theories can, without scientific loss, be translated into talk about behaviour2.\n\nTo ask whether consciousness is essentially a kind of information is to ask whether the cognitivist theories of consciousness should be considered true. I disagree with the cognitive theories of consciousness as they fail to adequately address several critical questions. To concretize my argument in real theories, I look to Chalmer’s process theory4 and Tononi’s Information Integration Theory5.\nBoth of these are deeply rooted in cognitivist theories of consciousness. For example, in IIT, consciousness of the system refers to the information generated above and beyond the information generated from the separate parts of the system5. Chalmer’s process theory posits that information states can be realized physically and that these information states themselves are conscious4.\nYet, neither theory completely accounts for:\n\nDefining information in a manner at odds with how information is regularly defined\nHow a serial stream of consciousness can arise from a parallel distributed network\nInformation carrying in obviously non-conscious objects\nBrute optimization of its mathematical definition\n\nWe expand on each of these in turn.\nChalmers’ process theory starts by defining information in the world as having two aspects, physicality and phenomenality. Information then, is both a physical thing and has phenomenal intentionality (or what is is like to be information). In doing so, Chalmers defines information as having the property of being conscious. However, this is quite different from colloquially accepted and typical academic definitions of information. Information usually refers to non-mental, mind-independent entities that are embedded in the physical (e.g. a book or brain states). Pioneers of information theory like Claude Shannon and even colloquial usage of the term information agree with this definition. An important distinction between these two definitions is that while physical things encode and embed information, they themselves are not information. A book by itself is just an arrangement of paper and ink but it may carry information like the concept of Dante’s Inferno. Thus, whatever Chalmers claims to be ‘information’ cannot be the same information everyone else refers to1 and so his conclusions on the basis of information cannot be valid.\nWe then consider the seriality/stream problem in the context of Chalmers’ process theory. The ‘stream’ character of human conscious experience seems to almost be at odds with the parallel distributed model of the mind with its various synapses and neurons that have no central center for keeping order. Process theories of consciousness must therefore account for how seriality arises from the distributed nature of the mind6. Chalmers fails to do address this in his theory all-together. It is important to note that behaviourist theories avoid this all-together as behaviour is a series (or at least, very limited parallelism) of agent-environment interactions. Agents do not perform multiple complex interactions at once (e.g. eating and playing). Even for multi-tasking of simple interactions, most theories propose the concept of a bottle-neck or limiting capacity — more complex behaviours take more bandwidth and thus require more focus, required the need for serial execution.\nLast but not least, we turn to how Chalmers’ refutes information carrying in non-conscious objects. From earlier, Chalmers defines the ability to contain information states as the capacity for consciousness4. Yet, there are clearly non-conscious objects, books for example, that clearly carry information but are not widely accepted as being conscious. Chalmers provides two options:\n\nPerhaps only some kinds of “physically realized information spaces” are conscious.\nPerhaps thermostats are conscious.\nChalmers’ chooses the second option and suggests that “the level of organization at which consciousness ‘winks out’ might be lower than a thermostat but higher than a rock.”1 The resolution that Chalmers’ chose is quite unsatisfying.\n\nTononi attempts to improve on Chalmers’ theory by proposing IIT1. In this theory, information is defined as information that is specified by a system that is irreducible to that specified by its parts. That is, information is integrated information. In making this distinction, Tononi explicitly rejects Chalmer’s choice of distinguishing information-carriers as conscious and instead chooses to define a subset of physically-realized information spaces (integrated information) as conscious. In doing so, IIT avoids the first and third pitfalls of Chalmers’ theory.\nHowever, IIT still has a major flaw in that it only claims to correlate integrated information Φ with consciousness: “To recapitulate, the theory claims that consciousness corresponds to the capacity to integrate information.”5 Yet, we know that correlation is most definitely neither definition nor causation. Even while this is a glaring hole in what IIT claims to be, we can continue to show that even the definition of Φ itself is problematic.\nRoughly, Φ is large if the system has a lot interconnection between its components. In more technical terms, it is “minimizing, over all subdivisions of your physical system into two parts A and B, some measure of the mutual information between A’s outputs and B’s inputs and vice versa.” 7 It is worth noting then that any sort of device that has some level of interconnection would be slightly conscious. According to Aaronson, Tononi seemed to accept this panpsychist implication and agree that thermostats have small but nonzero levels of consciousness. This clearly suffers the same unsatisfying conclusion that Chalmers arrived at earlier.\nHowever, even more problematic, is the fact that as this is a mathematical formula, it is susceptible to optimization (see: Goodhart’s Law). Aaronson shows that we can construct almost trivial examples where systems that are clearly not conscious exhibit ridiculously large values of integrated information. For example, we can hook together a large number of logic gates together all in ways that are highly interconnected and achieve levels of Φ that imply that over half of the information in the system is integrated information. As these logic gate systems (Aaronson details these as bipartite expander graphs) can be infinitely scalable, one could theoretically construct such a system with unbounded Φ. Surely there is something problematic going on if we can say that a graph of logic gates is infinitely conscious.\nIt is clear that information and information-processing based methods are brittle. Of course, there are alternatives to consider like Boris Kotchoubey’s behaviourist approaches to consciousness that I believe are more sound, it is outside the scope of this paper to discuss their viability. Earlier, we posited that cognitive theories consciousness rely on consciousness as information or information-processing. In conclusion, I have shown that key cognitivist theories of consciousness like Chalmer’s theory and Tononi’s IIT have glaring flaws in attempting to measure and define consciousness. Thus, consciousness should not be considered a kind of information.\nFootnotes\n\n\nPockett, Susan (2014). Problems with theories that equate consciousness with information or information processing ↩ ↩2 ↩3 ↩4\n\n\nVelmans, M. (1991). Is human information processing conscious? ↩ ↩2 ↩3\n\n\nAtkinson, A. P., Thomas, M. S. C., and Cleeremans, A. (2000). Consciousness: mapping the theoretical landscape ↩ ↩2\n\n\nChalmers, D. J. (1996). The Conscious Mind: in Search of a Fundamental Theory ↩ ↩2 ↩3\n\n\nTononi, Giulio (2004). An information integration theory of consciousness ↩ ↩2 ↩3\n\n\nKotchoubey, Boris (2018). Human Consciousness: Where Is It From and What Is It for ↩\n\n\nAaronson, Scott (2014). Why I Am Not An Integrated Information Theorist in https://scottaaronson.blog/?p=1799 ↩\n\n\n"},"thoughts/CouchDB":{"title":"CouchDB","links":["thoughts/CAP-Theorem"],"tags":["seed"],"content":"NoSQL database with great replication capabilities.\nMulti-master replication (you can write to any server, even if it cannot see any other servers). Of course, as per CAP Theorem this means trading off consistency for availability and partition tolerancy.\nReplication\nReplication can be\n\nunidirectional or bidirectional\none-time or continuous\nperiodic or on-demand\n\nDuring replication, CouchDB will compare the source and the destination database to determine which documents differ between the source and the destination database. It does so by following the Changes Feeds on the source and comparing the documents to the destination.\nA replication task will finish once it reaches the end of the changes feed. If its continuous property is set to true, it will wait for new changes to appear until the task is canceled. Replication tasks also create checkpoint documents on the destination to ensure that a restarted task can continue from where it stopped, for example after it has crashed.\nChanges Feeds\nresults is the list of changes in sequential order. New and changed documents only differ in the value of the rev; deleted documents include the &quot;deleted&quot;: true attribute."},"thoughts/Curse-of-Dimensionality":{"title":"Curse of Dimensionality","links":[],"tags":["seed","CPSC340"],"content":"Volume grows exponentially with dimension. Our nearest neighbour in high-dimensions might be really really far away\nIf want every location on to have a “neighbor” with distance ϵ,\n\nIn 1D, we need O(1/ϵ) points\nIn 2D, we need O(1/ϵ2) points\nIn DD, we need O(1/ϵ3) points\n"},"thoughts/DHCP":{"title":"DHCP","links":["thoughts/Protocol","thoughts/IP-Address","thoughts/UDP"],"tags":["seed","CPSC317"],"content":"Dynamic Host Configuration Protocol so they can join a network. Assigns IP addresses to hosts. Main reason was when people started moving devices across networks\nBasic Messages\n\nDiscovery, host looks for a DHCP server\n\nSender is 0.0.0.0 port 68\nReceiver is 255.255.255.255 port 67 (this is the broadcast address!)\n\n\nRouter responds with an offer valid for the next n seconds\n\nThis is broadcasted (255.255.255.255) as router does not know where receiver is\n\n\nHost responds with acceptance\n\nBroadcast which address picked\n\n\nRouter responds OK\n\nACK message, still broadcast\nIncludes\n\nUnique IP Address for Client\nNetmask for local network\nLease time\nRouting information\nHost name, domain name (optional)\nName (DNS) Server\n\n\n\n\n\nFields (all UDP)\n\nsrc: source IP address (usually 0.0.0.0 for no meaningful IP)\ndest: destination IP address\nyiaddr: your IP address\ntransaction ID: for matching request with response\n"},"thoughts/DHT":{"title":"DHT","links":["thoughts/Kademlia-DHT","thoughts/Sloppy-Hashing-DHT","thoughts/Theseus-DHT","thoughts/BitTorrent","thoughts/Sybil-Attack"],"tags":["seed"],"content":"\nThe big hash table in the sky\n\nOne solution for ‘decentralized’ registry of peers. Each node holds a small shard of the DHT, so the burden of participation isn’t painful for any one agent. The DHT stores multiple redundant copies of each entry so that the information is available even when the author and a portion of the authorities are offline.\nAny good implementation tries to answer 2 questions:\n\nWhich nodes take which part of the hash map\nIf a key is not on a node, how does it go and get it?\n\nKeys are opaque, 160-bit quantities (e.g. an SHA-1 hash). Peers store data with similar IDs.\nJoining a DHT requires knowledge about at least one member of the DHT (the bootstrap node)\nImplementations include\n\nKademlia DHT\nChord DHT\nSloppy Hashing DHT\nTheseus DHT\n\nApplications to Torrent Software\nOld way was to use a tracker, you announce which file you are going to download to the tracker and the trackers sends back a list of peers. However, these trackers tend to go down easily (get sued)\nDHT proves to be a more reliable way of replicating this behaviour.\nSee also: BitTorrent\nProblems\n\nDHTs can be crawled and mined for profit\n\nPerform a Sybil attack by simulating 1000+ clients and just wait for values to come in, cheaply capturing 90%-99% of the DHT\n\n\nHow do we make DHTs work where member count is low?\n"},"thoughts/DID":{"title":"DID","links":["thoughts/Self-sovereign-Identity-(SSI)","thoughts/cryptography","thoughts/decentralization","thoughts/bitcoin","thoughts/blockchain","thoughts/DHT","thoughts/RSA","thoughts/CID","thoughts/Sidetree"],"tags":["seed"],"content":"Summarized from W3C Proposal\nDecentralized identifiers (DIDs) are a new type of self-sovereign identity that enables verifiable, decentralized digital identity through the use of cryptography.\n\nThey are designed to enable individuals and organizations to generate their own identifiers using systems they trust. These new identifiers enable entities to prove control over them by authenticating using cryptographic proofs such as digital signatures.\n\nIdentity is important to identify things. The digital economy relies on proper identification to combine information from different sources. Uniqueness is vital here!\nGoals:\n\nEase of creation\nDecentralized\nPersistent\nResolvable\nCryptographically verifiable\n\nHigh level overview\nA DID is a unique string that has a specific syntax. It can be resolved to a DID Document (also called a DDO - DID descriptor object) in a global, decentralized, key-value database (Verifiable Data Registry).\nFormat: did:xyz:abcde123456\n\ndid: fixed string, this is a DID\nxyz: method name (e.g. btcr which is built on top of Bitcoin, acts sort of like a namespace)\nabcde123456: method specific identifier\n\nCan be thought of like a public decentralized keychain. It binds a public/private key pair to an identity, even when those keys are rotated out and replaced!\nThe VDR can be hosted/based on any platform (e.g. on distributed blockchains, DHTs, or just hosted files on GitHub)\nA DID Document can have arbitrary content. It contains references to “controllers” which are entities that have permission to make changes to a DID Document. It can also contain various cryptographic data delated to the DID subject (e.g. RSA, keys, etc.)\nWhat it enables\n\nLogin without usernames or passwords\nDigitally sign documents and transactions to prove it’s you\nEasily send encrypted messages\nAuthorize delegates\nUsage and ownership on your own terms — without surveillance or middlemen\n\nCreating DIDs using IPLD\n\nIn IPID, associating the DID document with a DID is accomplished by cryptographically publishing the CID to the IPNS public key associated with the identity owner (DID method specific identifier). Any updates to the DID document are saved to IPLD and the resulting hash is published to IPNS cryptographically associating the new CID with the DID (for IPID this is the multihash of the public key). IPID uses a PubSub model for realtime updates to the DID.\n\nThis is self-attesting and does not facilitate consensus of the document across peers\nSometimes described as the “microledger” approach\nEven though IPFS could be used for content addressing there would not be a need to connect to a wider IPFS network.\n\nNot resolvable without hosting (which might actually be good for relational DIDs)\n\n\n\n\nMore reading in RWoT 7, 2018\n\nDID Method Key\nThe did:key format\nThis DID Method is purely generative, requiring no look ups in a registry. Since did:key values are not stored in any registry, they cannot be updated or deactivated.\nDWN\nA Decentralized Web Node (DWN) is a data storage and message relay mechanism entities can use to locate public or private permissioned data related to a given Decentralized Identifier (DID).\nDecentralized Web Nodes are a mesh-like datastore construction that enable an entity to operate multiple nodes that sync to the same state across one another, enabling the owning entity to secure, manage, and transact their data with others.\nMethods\ndid:key\nGreat for burner DIDs\nSidetree\nSee Sidetree"},"thoughts/DNS":{"title":"DNS","links":["thoughts/Internet","thoughts/IP-Address","thoughts/Bluesky","thoughts/IPFS"],"tags":["seed","CPSC317"],"content":"Domain name: an identification string that defines a realm of administrative autonomy, authority, or control within the Internet.\nDNS currently has ~300 million DNS registrations. Both query and reply messages follow the same message format. Both always include Name, Type, Class tuples — Class is usually IN. Names cannot be wildcarded but type and class can\nHow do we resolve domain names to IP addresses? Resolves starting from the root and makes it way down the network hierarchy\n\nRoot (13 of these worldwide)\nTop-level Domains (e.g. .com, .net, .org, etc.)\nSecond-level Domains (e.g. UBC)\nSubdomains (e.g. www)\nIndividual machines\nLocal DNS Servers (not actually a part of the hierarchy, just caches data)\n\nAuthoritative DNS server is the server with the actual jurisdiction of the domain name you are looking for. The authoritative server of cs.ubc.ca is the cs server under UBC.\nTypes of queries\n\nRecursive Query — if the name server doesn’t know the answer, it asks a downstream server (recursively) for the answer on your behalf.\nIterative Query — if the name server doesn’t know the answer, it tells you where to look at next, you do all the querying\n\nDNS servers store resource records (RRs)\nTypes:\n\nA (address records)\n\nname: hostname\nvalue: IPv4 address\n\n\nNS (name server)\n\nname: domain\nvalue: name of DNS server for domain\n\n\nMX (mail exchanger)\n\nname: domain of email address\nvalue: name of mail server\n\n\nAAAA (addressx4 record)\n\nname: hostname\nvalue: IPv6 address\n\n\nCNAME (canonical name)\n\nname: alias\nvalue: canonical name (e.g. foo.com)\n\n\nTXT (just plain text)\n\nname: domain\nvalue: plain text in the format of attribute=value. The TXT record was originally intended as a place for human-readable notes but now often used for domain ownership verification (see: dnslink and Bluesky)\n\n\n\nInserting records into DNS\n\nRegister name with a registrar\n\nProvide registrar with name and IP address for your authoritative name server (usually a primary and secondary)\nRegistrar inserts two resource records into the top-level domain server for each authoritative name server\n\n(example.com, dns1.example.com, NS)\n(dns1.example.com, 212.212.212.1, A)\n\n\n\n\nAdd appropriate records into our own authoritative name server\n\n(www.example.com, &lt;server-ip&gt;, A)\n(www.example.com, &lt;server-ip&gt;, MX)\n\n\n\nDNSLink\nDocumentation\nDNSLink uses DNS TXT records to map a DNS name, like en.wikipedia-on-ipfs.org, to an IPFS address\nBecause you can edit your DNS records, you can use them to always point to the latest version of an object in IPFS.\n$ dig +noall +answer TXT _dnslink.docs.ipfs.tech\n_dnslink.docs.ipfs.tech.  34  IN  TXT &quot;dnslink=/ipfs/QmVMxjouRQCA2QykL5Rc77DvjfaX6m8NL6RyHXRTaZ9iya&quot;"},"thoughts/DSL":{"title":"DSL","links":["thoughts/program-analysis","thoughts/compiler","thoughts/software-principles","thoughts/design-goals","thoughts/programming-models","thoughts/linguistic-relativism","thoughts/notation"],"tags":["seed"],"content":"Implementation Stages\n\nTokenization String -&gt; [Token]\n\nMakes defining and recognizing correct sequences easier\nSometimes called lexing\n\n\nParsing [Token] -&gt; ParseTree\n\nA tree that represents a successful parsing of a sequence of tokens\n\n\n(optional) AST Conversion ParseTree -&gt; AST\n(optional) Static Checks AST -&gt; AST\n\nSee also: program analysis\n\n\nEvaluate AST -&gt; Result\n\nRun the input or generate code for it\n(optional) Dynamic Checks\n\n\n\nSee: compiler\nGrammar Rules\ne.g. for BNF, EBNF\n\nGenerally matches left to right\nSingle-quoted strings are literal\nGrammar rules end with semicolons\n\nANTLR Lexer\nlexer grammar TinyHTMLLexer;\n// DEFAULT_MODE is the implicit defualt\nTITLE_START: &#039;Title:&#039; WS* -&gt; mode(TEXT_MODE) ;\nTABLE_START: &#039;Table:&#039; ;\nROW_START  : &#039;[&#039; WS* -&gt; mode(TEXT_MODE) ;\nROW_END    : &#039;]&#039; ;\nSEP        : &#039;|&#039; WS* -&gt; mode(TEXT_MODE) ;\nWS         : [\\r\\n\\t] -&gt; channel(HIDDEN) ;\n \nmode TEXT_MODE;\nTEXT       : ~[[|\\]\\r\\n]* -&gt; mode(DEFAULT_MODE) ;\n// cant infinite match because as soon as we match, we exist TEXT_MODE\nANTLR Parser\n\nParser grammar rules have lower-case non-terminal symbols\nParser rule bodies can use both parser non-terminals and lexer ones\n\nThough, we should avoid doing this and keep parser and lexer rules separate\n\n\nParser rule bodies may not include regex character classes (e.g. [0-9] or \\d)\n\nparser grammar TinyHTMLParser;\noptions { tokenVocab = TionyHTMLLexer; }\n \nprogram: title table+ EOF ;\ntitle  : TITLE_START TEXT ;\ntable  : TABLE_START boldrow row+ ;\nboldrow: row ;\nrow    : ROW_START (item (SEP item)*)? ROW_END ;\nitem   : TEXT ;\n3 Parsing Guidelines\n\nThe grammar cannot be ambiguous: any given input string has at most one parse tree that accepts it\nNo left recursion: each rule cannot start with itself (even indirect)\n\nShould not allow T ::= T ...\n\n\nGrammar must be locally deterministic: for each choice, we must be able to choose between them based only on the next token (avoid common prefixes, factor them out into separate rules)\n\nLanguage Principles\n\nLearnability (how quickly can you pick it up; feels like “common sense”?)\nEfficiency (once you’ve learned it, how efficiently can you perform tasks)\nMemorability (coming back to the language, how easy to regain proficiency)\nErrors (how many do users make, how severe, how easily can they recover)\nSatisfaction (subjective, but very important for perseverance and adoption)\n\nSee also: software principles, design goals, programming models\n\nMaximize information hiding\n\nMake classes, members as private as possible\nPublic classes should have no public fields (with the exception of constants)\n\n\nDon’t confuse users\n\nKeep things simple\nName things well\nKeep things consistent\nHave good documentation\nAvoid unnecessary boilerplate\nMake it boring (intuitive, expected)\n\n\n\nWhat is the purpose of a language?\n\nWe think in a particular language and it determines how you think (see: Sapir-Whorf)\nLanguages should help us think better\n\nSee also: notation\nEvaluation\nRecursive Evaluation\nEach node has an evaluate method. Recursively traverse the tree and evaluate each node.\nBut this only supports a single type of traversing the AST. What if we want to support other types of checkers? We have many different operations that traverse the AST\nPutting all functionality into AST methods violates SRP. We can instead, implement the visitor pattern.\nVisitor Pattern\nThe visitor design pattern is a way of separating an algorithm from an object structure on which it operates.\nBasically, you are passing this visitor object to a node’s accept function.\n\nVisitor defines a visit for each concrete node type to detail how to visit that node + its children (functionality depends on visitor)\nIf it needs to visit another node, it calls accept on itself (functionality also depends on the node type)\n\nWe perform double dispatch as the functionality depends on two things:\n\nthe type of AST object (via the accept call) and\nthe type of visitor object (via the visit call)\n\nWe could just evaluate each AST node, but this places the responsibility on the nodes for how to do this.\n\nSupport multiple kinds of “evaluation” for our AST without having to edit every node every node every time\nEvaluation is in a separate file from the AST implementation\n\nexport class Client() {\n\tnodes: Element[]\n\tdoSomething() {\n\t\tconst visitor: Visitor&lt;T, U&gt; = // idk some visitor to do something\n\t\tfor (node in this.nodes) {\n\t\t\tnode.accept(visitor)\n\t\t}\n\t}\n}\n \nexport interface Element {\n\taccept: (visitor: Visitor&lt;T, U&gt;, param: T): U,\n}\n \n// same for ConcreteB\nclass ConcreteA implements Element {\n\taccept(visitor: Visitor&lt;T, U&gt;, param: T): U {\n\t\treturn visitor.visit(this, param)\n\t}\n}\n \nexport interface Visitor&lt;T, U&gt; {\n\t// where ConreteA and ConcreteB both inherit from Element\n\t// error checks here are runtime checks\n\tvisit: (a: ConcreteA, param: T): U,\n\tvisit: (b: ConcreteB, param: T): U,\n\t...\n}\n\nCreate a Visitor interface under AST and define a bunch of visit methods for each concrete node type\nUnder the abstract Node class, create an abstract accept method\nCreate a new visitor class that implements the Visitor interface\n"},"thoughts/DWeb-Camp-2023":{"title":"DWeb Camp: Gather by the Campfire","links":["posts/agentic-computing","thoughts/cozy-software","thoughts/Tools-for-Conviviality","thoughts/Design-Justice","thoughts/A-Pattern-Language"],"tags":["fruit"],"content":"\n\nFire is a chief metaphor for the Internet: it is metaorganic; it extends the range of (informational) food; it empowers people to explore new time zones (the night) and territories of knowledge; it increases some kinds of sociability, demands ongoing maintenance, and produces dangers and externalities that did not exist before. Fire was the first World Wide Web, a fragile system for contagious spreads.\n(The Marvelous Clouds: Toward a Philosophy of Elemental Media)\n\nThus it was only fitting that at DWeb Camp 2023, we gather around a literal campfire to talk about this metaphorical fire that has seeped into so much of our lives.\n\nDWeb originates as a technical term, short for the ‘decentralized web’. It refers to new web technologies and communities seeking to reduce or eliminate central points of control on the web.\nThe internet’s original creators intended decentralization to be a core value, aiming to make information accessible to all and connect people worldwide. They believed, rightfully so, that the web is a genuine social accomplishment and that we should look after it by improving and preserving it. They believed that the ability to ‘modify source’ — the ability to make and remake software to better serve them — was critical to a thriving, diverse, and resilient web.\nYet, looking at the web today we see very little of these patterns. It’s no longer a controversial statement to say that, as citizens of the internet, we have lost our ability to shape the web and make it a home.\nHowever, just blindly decentralizing the web won’t fix it. DWeb also asks the questions of what we are decentralizing and for whom1?\nComputer scientists and academics see decentralize the web as a call for architectural and maybe logical decentralization: we should decentralize the actual physical hardware of the system and ensure that there is no central point of failure. If you cut the system in half, will both halves continue to fully operate as independent units?\nTechnology critics, philosophers, and activists see decentralize the web as a call to interrogate the power structures behind this infrastructure. How many individuals or organizations ultimately control the computers that the system is made up of? After all, what’s the point of decentralizing all the hardware if 5% of all the users own 95% of it anyways?\nThese approaches are not mutually exclusive and in fact should be considered in unison. To me, DWeb as a term has evolved beyond just the actual technical term ‘decentralized web’ and is now a banner for people who also believe that these two questions are both asking and in conjunction.\nDWeb Camp exists because the founders believed that we couldn’t just have the technologists and entrepreneurs building away in one realm and the academics, philosophers, and critics theory-making in another. Rather, they wanted to create a space where both of these types of people could co-exist, learn from each other, and leave eventually having become a bit of the other — the technologist a little more critical, and the critic a little more pragmatic.\nTo me, this is what makes DWeb so special. So rarely do you find a space where you can find people who are on-the-ground activists during the day and avid protocol researchers by night. People who care about both the “what” and the “why”2. These are people that don’t just preach the gospel of ‘decentralization good’, but themselves live in contexts where these values actually make a tangible difference in their lives.\nAnd, perhaps most importantly, they know how to do it all while being kind, empathetic, and quirky in the way that you know they’ve spent too long growing up on internet to not care about the future of it.\n\n\nThis year at DWeb Camp, Spencer Chang and I hosted a session asking campers to gather their collective imaginations and dreams for what a communally-owned internet could look like.\n\nWe have a lot of dystopian fiction and a lot of reminiscing, but not a lot of forward-looking dreams for the web. Dreaming is an important piece of fiction that rallies people to articulate a vision they want to make a reality. In hosting this session, we recall Ruha Benjamin: “to see things as they really are, you must imagine them for what they might be”.\nOur session focused around circulating 5 sheets of paper, each with a question on it:\n\nWhat do you wish the Internet evoked for you?\nWhat would co-owning digital spaces look like?\nWhat is your digital neighbourhood?\nWhere have you felt agency online?\nWhat is/was your favourite place on the internet?\n\n\nWe started by reminising: remembering sacred spaces like Club Penguin, obscure math mailing lists, torrent communities, and hobby forums. We discussed the feeling that existing on the internet feels like living on rented ground. We recalled architects and urban designers like Christopher Alexander who first noticed that rental areas are always the first to turn to slums and the importance of owning your own home3, drawing parallels to our digital spaces.\nWe wondered how to make it possible for the average layperson to be able to change and adapt software for their own needs; for them to experience creating software not like a professional chef, but a home cook. One group noticed how users consistently still subvert these platforms and create folk usages of software, paving our their own desire paths. We forayed into the realms of convivial software and the movement to make building software fun again and wondered how to give people the ability to be architects of their own digital homes again.\nTo us, the session was a reading of ‘decentralized’ not in the pure technical or power sense of the word. Rather, we read ‘decentralized’ to mean communally governed and owned infrastructure.\nWe wondered about what digital infrastructure coops could look like; hosting little websites and apps for just our communities rather than at global scale. We thought about how community-owned infrastructure could mean that digital autonomy enables more physical and bodily autonomy rather than take away from it. Many participants mentioned how their own projects and organizations used decentralizated infrastructure and governance to help their local communities to better provide financial support for gender-affirming care or speak-out about tyrannical governments more safely, consistently, and reliably.\nWe wondered how data can be used as something to share as a gift between friends rather than extracted without consent. We also thought about how cool and whimsical it would be if we could have tiny little windows into the digital homes of our friends, and be hosted just like we would be able to be in the real world.\nReclaiming our agency and ability to communally construct our digital spaces starts from people willing to dream and fight for it.\nIn many ways, this session (and the greater DWeb Camp as a whole) felt like gathering of people who haven’t given up on the inherent good of the internet and are fighting for this future where anyone can be an architect of their own digital experience. It’s a pretty special place.\nFootnotes\n\n\nVal Elefante, another fellow, wrote about this in-depth in their piece on Coalition-building Across the Tech Stack. If you are interested in a more critical analysis of decentralization, go read their piece! ↩\n\n\nThese terms come from a wonderful session hosted by Fight for the Future on technology that supports various abortion and gender affirming care funds and their beneficiaries. See also: Design Justice ↩\n\n\n“Rental areas are always the first to turn to slums. The mechanism is clear and well known. See, for example, George Sternlieb, The Tenement Landlord (Rutgers University Press, 1966). The landlord tries to keep his maintenance and repair costs as low as possible; the residents have no incentive to maintain and repair the homes – in fact, the opposite – since improvements add to the wealth of the landlord, and even justify higher rent. And so the typical piece of rental property degenerates over the years. The landlords try to build new rental properties which are immune to neglect – gardens are replaced with concrete, carpets are replaced with lineoleum, and wooden surfaces by formica: it is an attempt to make the new units maintenance-free, and to stop the slums by force; but they turn out cold and sterile and again turn into slums because nobody loves them.” (p.394, Your own home, A Pattern Language) ↩\n\n\n"},"thoughts/Dark-Forest-Theory-of-the-Internet":{"title":"Dark Forest Theory of the Internet","links":["thoughts/Moving-Castles","thoughts/cozy-software","thoughts/Internet","thoughts/digital-mindfulness","thoughts/identity"],"tags":["seed"],"content":"Related: Moving Castles, cozy software\nSource: The Dark Forest Theory of the Internet by Yancey Strickler\n\nImagine a dark forest at night. It’s deathly quiet. Nothing moves. Nothing stirs. This could lead one to assume that the forest is devoid of life. But of course, it’s not. The dark forest is full of life. It’s quiet because night is when the predators come out. To survive, the animals stay silent.\n\nCozy Web\nIt’s unsafe to reveal yourself to them in any authentic way. So we retreat into private spaces. We hide in the cozy web. Ephemeral content is the content in the dark forest. Fireflies not lamps.\n“These are all spaces where depressurized conversation is possible because of their non-indexed, non-optimized, and non-gamified environments.”\nIn these cases, the specific context and audience is well-defined. More users are also turning to ‘finstas’ which are accounts focused for a closed group of friends and family rather than the entire public internet.\nHow can we create digital mindfulness in communication and conversation?\nIllustration from Maggie Appleton’s Cozy Web\nHandmade Web\nFrom J.R. Carpenter’s A Handmade Web\n\nI evoke the term ‘handmade web’ to refer to web pages coded by hand rather than by software; web pages made and maintained by individuals rather than by businesses or corporations; web pages which are provisional, temporary, or one-of-a-kind; web pages which challenge conventions of reading, writing, design, ownership, privacy, security, or identity.\n\nThe Small Web\nSource\nIn the 1990s, browser design took nearly the opposite approach, using iconography associated with travel to convey the feeling of going on a journey. Netscape Navigator, which used a ship’s helm as its logo, made a very direct link with the nautical origins of the prefix cyber-, while Internet Explorer’s logo promised to take the user around the whole globe.\n\nA painter wouldn’t add more red to her painting or change the composition because market data showed that people liked it better. It’s her creative vision; some people might like it, others might not. But it is her creation, with her own rules. The question of “performance” is simply irrelevant. It’s the same thing with the small web.\n\nIf the commercial web is “industrial”, you could say that the small web is “artisanal”. One is not better than the other. They serve different needs and both can co-exist in an open web."},"thoughts/Data-Capitalism":{"title":"Data Capitalism","links":[],"tags":["seed"],"content":"A form of capitalism (sometimes also known as surveillance capitalism) where the selling and exchange of data is the source of economic benefit. It is Associated with rise of Internet and Web 2.0; post 9/11.\nSearch\n\n“Currently, the predominant business model for commercial search engines is advertising. The goals of the advertising business model do not always correspond to providing quality search to users.”\n“We believe the issue of advertising causes enough mixed incentives that it is crucial to have a competitive search engine that is transparent and in the academic realm”\n(Sergey Brin &amp; Lawrence Page, Google cofounders)\n\nAlmost ironic that they said this when writing the paper on what would become PageRank and Google Search.\nThieves of Experience: How Google and Facebook Corrupted Capitalism\nBy Nicholas Carr in LARB\n“Surveillance capitalism’s real products, vaporous but immensely valuable, are predictions about our future behavior — what we’ll look at, where we’ll go, what we’ll buy, what opinions we’ll hold”\n“Unlike financial derivatives, which they in some ways resemble, these new data derivatives draw their value, parasite-like, from human experience.”\nZuboff coins the term extraction imperative: To improve its predictions, it had to mine as much information as possible from web users.\nThe bullying style of TOS agreements also characterizes the practice, common to Google and other technology companies, of threatening users with a loss of “functionality” should they try to opt out of data sharing protocols or otherwise attempt to escape surveillance.\n\n“If you disable this app, other apps may no longer function as intended.”\n"},"thoughts/Database":{"title":"Database","links":["thoughts/Designing-Data-Intensive-Applications","thoughts/RDF","thoughts/State-Machine-Replication-(SMR)"],"tags":["seed"],"content":"Excerpts from Designing Data-Intensive Applications\nData Models\nData models are perhaps the most important part of developing software, because they have such a profound effect: not only on how the software is written, but also on how we think about the problem that we are solving.\nRelational model\nData is organized into relations (called tables in SQL), where each relation is an unordered collection of tuples (rows in SQL).\n\nUpsides\n\nBetter support for joins, many-to-one, and many-to-many relationships\n\n\nDownsides\n\nMuch application development today is done in object-oriented programming languages, which leads to a common criticism of the SQL data model: if data is stored in relational tables, an awkward translation layer is required between the objects in the application code and the database model of tables, rows, and columns. We generally use ORMs to reduce the boilerplate for this translation layer.\nIn relational databases there isn’t a standard way of representing reorderable lists, and various tricks are used\n\n\n\nDocument Model\nUsually represents data as JSON.\n\nUpsides\n\nSchema flexibility\nBetter performance due to locality\nThe relational technique of shredding — splitting a document-like structure into multiple tables — can lead to cumbersome schemas and complicated application code\n\n\nDownsides\n\nMany-to-one and many-to-many relationships do not easily fit within one self-contained JSON document\n\n\n\nDocument databases are sometimes called schemaless but a more accurate term is schema-on-read (the structure of the data is implicit, and only interpreted when the data is read)\nGraph Model\nWhat if many-to-many relationships are very common in your data? As the connections within your data become more complex, it becomes more natural to start modeling your data as a graph.\nA graph consists of two kinds of objects: vertices (also known as nodes or entities) and edges (also known as relationships or arcs).\nGraph models lets us ask questions that contain a variable number of joins which is very difficult to express in traditional SQL (requires the use of recursive common table expressions).\nProperty Graphs\nIn the property graph (also known as labeled property graph) model:\nEach vertex consists of:\n\nA unique identifier\nA label (string) to describe what type of object this vertex represents\nA set of outgoing edges\nA set of incoming edges\nA collection of properties (key-value pairs)\n\nEach edge consists of:\n\nA unique identifier\nThe vertex at which the edge starts (the tail vertex)\nThe vertex at which the edge ends (the head vertex)\nA label to describe the kind of relationship between the two vertices\nA collection of properties (key-value pairs)\n\nImportant notes:\n\nAny vertex can have an edge connecting it with any other vertex. There is no schema that restricts which kinds of things can or cannot be associated.\nGiven any vertex, you can efficiently find both its incoming and its outgoing edges, and thus traverse the graph—i.e., follow a path through a chain of verti‐ ces—both forward and backward.\n\nTriple Stores\nSee also RDF\nIn a triple-store, all information is stored in the form of very simple three-part statements: (subject, predicate, object).\nFor example, in the triple (Jim, likes, bananas), Jim is the subject, likes is the predicate (verb), and bananas is the object.\nGenerally queried by making a set of constraints.\nEvent Sourcing\nIn complex applications it can sometimes be difficult to find a single data representation that is able to satisfy all the different ways that the data needs to be queried and presented.\nIn such situations, it can be beneficial to write data in one form, and then to derive from it several representations that are optimized for different types of reads.\nThe simplest, fastest, and most expressive way of writing data is an event log: every time you want to write some data, you encode it as a self-contained string (perhaps as JSON), including a timestamp, and then append it to a sequence of events. Events in this log are immutable: you never change or delete them, you only ever append more events to the log (which may supersede earlier events).\nThe principle of maintaining separate read-optimized representations and deriving them from the write-optimized representation is called command query responsibility segregation. Similar ideas can be found in SMR\nNormalization\nNormalization refers to how many ways there are of representation the same underlying information. This is typically done by giving an ID to each piece of data so that there is only one ‘canonical’ way of referring to it.\nLooking up an ID and replacing it with the actual information it refers to is often called hydration\n\nWhen you use an ID, your data is more normalized: the information that is meaningful to humans is stored in only one place, and everything that refers to it uses an ID.\nWhen you store the text directly, you are duplicating the human-meaningful information in every record that uses it; this representation is denormalized, there are multiple potential referring to the same information.\n\nTradeoffs:\n\nIn a denormalized representation, the information in each document is self-contained meaning we don’t need to make another lookup to figure out what the ID refers to. However, if we do need to change the underlying information, we then need to go and find all the occurrences of the old information and update it.\n\nTLDR; faster read, slower write\n\n\nIn a normalized representation, updating the information is as easy as changing the information that the ID refers to. However, each reference to the ID requires looking the ID up to get the corresponding information.\n\nTLDR; faster write, slower read\n\n\n\nQuery Languages\n\nDeclarative Query Languages allow you to specify the pattern of the data you want—what conditions the results must meet, and how you want the data to be transformed (e.g., sorted, grouped, and aggregated)—but not how to achieve that goal\nImperative Query Languages require you to write an algorithm —i.e., telling the computer which operations to perform in which order\n"},"thoughts/Datalog":{"title":"Datalog","links":["thoughts/CRDT"],"tags":["seed"],"content":"Source: What You Always Wanted to Know About Datalog (And Never Dared to Ask)\nDatalog is basically a simplified version of general Logic Programming.\nIn the formalism of Datalog, both facts and rules are represented as Horn clauses of the general shape L0 :- L1, ..., Ln which is equivalent to L1​∧⋯∧Ln​⟹L0​. Each L is a predicate symbol p(t1, ..., ti) where t are the terms. A term can either be a constant or a variable.\nA logic program consists of a finite set of:\n\nFacts: assertions about a relevant piece of the world. These are represented as clauses with empty bodies (e.g. friends(alice, bob).)\nRules: statements which allow us to deduce facts from other facts. These usually contain variables (e.g. mutual(X, Y) :- friends(X, Z), friends(Z, Y).)\n\nNormally, constants and predicate symbols are strings that begin with a lower-case character and variables are strings that begin with an upper-case character.\nA clause that does not contain any variables is called ground. To guarantee that the set of all facts that can be derived from a Datalog program P is finite, we require:\n\nEach fact of P is ground\nEach variable which occurs in the head of a rule of P must also occur in the body of the same rule\n\nWhen using facts stored in an external relational database (sometimes called the Extensional Database or EDB), we distinguish these from facts stored within the logic program P (sometimes called the Intensional Database or IDB).\nWe can consider the program P as a query against the EDB. The EDB is normally considered a time-varying collection of information. A query then is a time-invariant mapping which maps the EDB to result states.\nA program P might produce a large number of intermediate IDB-relations. However, users are only interested in a subset of these relationships. We can express this as a goal to a Datalog program. To Prolog users, this is familiar. It is a single predicate that looks like ?-friends(alice, X). which would, for example, get all people who alice is friends with.\nMore notation/terminology:\n\nHerbrand base (HB) is the set of all facts we can express in the language of Datalog\nIf S is a finite set of Datalog clauses, we denote cons(S) the set of all logical consequences of S\nA fixpoint is a state of the evaluation process in which no more rules can be applied and all of the predicates in the program have reached their final, “fixed” values.\n\nInference Rules\nBottom-up Evaluation\nOne example is the Elementary Production Principle (EPP). Given a rule that looks like friends(X, X) :- person(X)\nIf we can substitute facts (e.g. person(john).) for terms in the body such that the entire body is true and ground, then the head of the rule becomes a new fact. In the example above, friends(john, john). is then added as a fact, using the substitution θ = {X &lt;- john}.\nThe sequence of applications of EPP which is used to infer a ground fact F from S is called a proof of F from S. We say S⊢F (F can be inferred from S) iff:\n\nF∈S\nF can be obtained by applying EPP a finite number of times\n\nUsing this to derive cons(S) is also sometimes called forward-chaining (it follows the logical implication sign from premises to conclusions\nTop-down Evaluation\nSometimes called backward-chaining. This method is particularly appropriate when a goal is specified together with a Datalog program.\nRules are seen as problem generators. Each goal is considered as a problem that must be solved. The initial goal is matched with the left hand side of some rule, and generates other problems corresponding to the right-hand side predicates of that rule; this process is continued until no new problems are generated.\nOne example of this is called the Query-Subquery (QSQ) Approach (this feels quite similar to chained currying in Haskell). Prolog uses this!\nDatalog goals seem more naturally executed through breadth-first techniques, as the result of the computation is neither affected by the order of predicates within the right-hand sides (RHS) of rules, nor by the order of rules within the program.\nTable of Methods\n\nEvaluation methods: effective evaluation strategies (improvements at runtime)\n\nBottom-up\n\nNaive: On each iteration, the program takes its current database of facts and computes all new facts that can be produced by one step of deductive inference, by iterating over all of the rules and exhaustively unifying them. If any new facts were produced, it then merges those with the previous set and repeats the process with another iteration; otherwise, it terminates.\nSemi-naive: Semi-naive evaluation solves this inefficiency by only attempting to evaluate rules where at least one term on the right-hand side of the rule was generated (i.e., is new) since the previous iteration.\nHenschen-Naqvi\n\n\nTop-down\n\nQuery-subquery (QSQ)\n\n\n\n\nRewriting method: program transformation which yields a more efficient computation (improvements at compile time)\n\nLogic\n\nMagic sets\nCounting\nMagic Counting\nStatic Filtering\n\n\nAlgebraic\n\nVariable reduction and constant reduction\n\n\n\n\n\nExpressivity\n\nPositive relational algebra (RA+) is equivalent to non-recursive Datalog\nDatalog can express recursive queries which RA can’t express\nFull relational algebra (RA) can express negation which Datalog can’t express\n\nHowever, Datalog can be enriched to support logical negation ¬\n\n\n\nNegation\nIncorporating negation can be allowed by adopting the Closed World Assumption (CWA). That is, if a fact does not logically follow from a set of Datalog clauses, then we conclude that the negation of this fact is true.\nThat is, we assume our facts completely describe the domain we are interested. For the purposes of CRDTs, this unfortunately is not true.\nEven with negation in Datalog, we can’t derive new facts from these negations. That is, we can’t express premises that contain a negative in the formulation (e.g. “if X is a student and X is not a graduate student, then X is an undergraduate student”).\nThis is possible in relational algebra using the set-difference operator.\nWe can extend Datalog to support negated literals in rule bodies using stratified Datalog\nStratification\nConsider a rule such as boring(chess) :- !interesting(chess)\nWe try to stratify the clauses of the program such that when evaluating a predicate in a rule head, it is always possible to completely evaluate all the predicates which occur negatively in the rule body or in the bodies of some subsequent rules.\nIn the above case, it is always possible to fully evaluate the interesting(chess) predicate before evaluating bording(chess). Formally, a stratified program P can be partitioned into disjoint sets of clauses P=P1∪⋯∪Pn\nLet’s consider an example program P where d is the only EDB-predicate:\nr1: p(X,Y) :- !q(X,Y), s(X,Y).\nr2: q(X,Y) :- q(X,Z), q(Z,Y).\nr3: q(X,Y) :- d(X,Y), !r(X,Y).\nr4: r(X,Y) :- d(Y,X).\nr5: s(X,Y) :- q(X,Z),q(Y,T), X != Y\nWe can turn these statements into an extended dependency graph (EDG) by\n\nMaking nodes of the IDB-predicate symbols in P\nCreate a directed edge ⟨p,q⟩ iff the predicate symbol q occurs positively or negatively in a body of a rule with head predicate p\n\nIf the symbol q occurs negatively, we mark the edge with ¬ (that is, an edge is marked ¬ if there is at least one rule with head predicate p such that q occurs negatively in the body)\n\n\n\nThe program can be stratified iff the EDG does not contain any cycle involving an edge labeled ¬.\nWe can then stratify P into 3 layers by doing a topological sort on the dependency graph:\n\nr4\nr2, r3, r5\nr1\n\nThis is one of many possible stratifications (however, they are all equivalent).\nMisc\n\nThree-valued logic? A fact can be true, false, or undefined\nComplex objects\n\nNF2 model (Jaeschke and Schek)\nNested Relations (Fisher and Thomas)\nModel of Abiteboul and Beeri\nALGRES\n\n\nModularization and structure types?\n"},"thoughts/Decision-theory":{"title":"Decision theory","links":["thoughts/causal-decision-theory","thoughts/Pascal's-Wager","thoughts/utility","thoughts/causality","thoughts/Order-theory","thoughts/interval-scale","thoughts/decision-tree","thoughts/Decisions-under-ignorance"],"tags":["seed","PHIL321A"],"content":"Choices by one agent in which background conditions are independent of what other agents are doing. We usually represent these decisions with a decision matrix or decision table. Sometimes called evidential decision theory.\nSee also: causal decision theory\ne.g. Pascal’s Wager\nComponents:\n\nRows are possible acts\n\nActs are functions that map states to outcomes\n\n\nColumns are possible states of the world\n\nProbabilities are sometimes included for decisions under risk.\nShould not depend on agent action\nStates should be\n\nMutually exclusive\nExhaustive: no possibility is left out\nRelevant partition: distinctions that actually have impact on probability or utility of outcomes\nIndependence: (optional) each state should be causally and probabilistically independent of the acts\n\nDominance principle only holds if independent holds\n\n\n\n\n\n\nCells are outcomes.\n\nCan be described using\n\nVerbal description\nPreference ranking on an ordinal scale\n\nDefines a partial ordering of outcomes\n\nx≽y is a weak preference\nx≻y is a strong preference\nx∼y is indifference between x and y\n\n\n\n\nUtility (numerical value) using an interval scale\n\n\n\n\n\nDecision tables\n\nArt: providing a good formalization of a decision into a table\nproviding a justified recommendation based off of the formalization\n\nThey can also be represented using decision trees\nWe can transform decision tables between each other using “reasonable transformations”\n\nPIR: assign equal probabilities to all states (if we have no knowledge of probabilities, aka DUI)\nMerger: if two states yield identical columns, then we can merge them into one state and add probabilities if we know them\n"},"thoughts/Decisions-under-ignorance":{"title":"Decisions under ignorance (DUI)","links":["thoughts/interval-scale","thoughts/Decisions-under-risk","thoughts/Pascal's-Wager","thoughts/Precautionary-Principle"],"tags":["seed","PHIL321A"],"content":"Decision rules when the agent is ignorant of all probabilities\nRules\nDominance\n\nWeak Dominance: act a is as good or better than b for each possible state and there is at least one state where it is strictly better\nStrong Dominance: act a is strictly better than b for all possible states\n\nPrinciple: 1. Avoid dominated acts and prefer dominant acts 2. Can only use dominance principle if states are independent of acts 3. Gold standard, use this whenever possible\nMaximin/Leximin\n\nFind the minimum value of each act\nChoose the act with the least bad worst-case outcome\n\nNote: can violate dominance if there are rows with the same minimum\n\nLeximin can help resolves ties by removing the minimum value in case of ties. Note that this violates dominance!\nLeximin* only strikes out a single minimum value in case of ties. This does *not* violate dominance\n\nThis rule is extremely conservative, avoids the worst-case scenario\nOptimism/Pessimism and “Best Average”\n\nMaximax (pick the best of the best-case outcomes)\nBest Average: take the best of each row and worst of each row and average it, pick the act with best average\nOptimism/Pessimism: Uses a weighted average (a linear combination) of the minimum and maximum values (α=0 is pessimistic, α=1 is optimistic): Vα​(A)=αmax(A)+(1−α)min(A)\n\nObjections:\n\nRequires interval scale instead of ordinal scale\n\nMinimax regret\nBasis: it is irrational to reject an act with a chance of a great gain, where the cost is slight.\n\nRegret value for each outcome = value of the outcome - maximum value in that column\nMax regret for each act is the most negative regret for each row A\nChoose the act with the minimum max regret\n\nObjections\n\nRequires interval scale instead of ordinal scale\nAdding irrelevant alternative acts potentially affects recommended acts\n\nPrinciple of Insufficient Reason (PIR)\nIf there are n possible states and you have no reason to believe any of them more likely than any other, then it is rational to assign each state equal probability (namely, 1/n)\n\nAssign each of the n states probability 1/n and maximize the expected value\nFor an act A, calculate ∑i=1n​n1​value(A,Si​)\n\nThis turns the problem into a DUR\nObjections\n\nRequires interval scale instead of ordinal scale\nArbitrary partitions of states (can result in incoherence)\nDoesn’t apply outside games of chance (e.g. Pascal’s Wager)\n\nRationality Constraints\nFind criteria that any rational decision rule should satisfy. Use these to rule out one or more decision principles\nMilnor proposes a few axioms for rules under DUI:\n\nMixture condition (randomization)\n\nIf a rational agent is indifferent between A1 and A2, then the agent must be indifferent between A1, A2 and the mixed strategy [21​A1,21​A2]\n\nPresupposition that the agent has a neutral attitude to risk\n\n\nEliminates Maximin (as above)\nEliminates Minimax Regret (as above)\nEliminates Optimism-pessimism rule with α=21​\nEliminates Best Avg if we allow other mixtures\nOnly PIR survives\n\n\nIndependence of Irrelevant Alternatives\n\nA rational agent’s choice will be invariant under an irrelevant expansion B: if A1≥A2 before adding option B, then A1≥A2 after adding B.\nEliminates Minimax Regret\n\n\n\nPerhaps we should use different rules for DUI in different situations. Can we be systematic?\n\nFor decisions under partial ignorance, see: Precautionary Principle"},"thoughts/Decisions-under-risk":{"title":"Decisions under risk (DUR)","links":["thoughts/interval-scale","thoughts/value","thoughts/utility","thoughts/Utilitarianism"],"tags":["seed","PHIL321A"],"content":"Decision rules when the probability of each outcome is known.\nRequires an interval scale\nWhen evaluating, we often need to quantify what outcomes are more valuable than others\n\nExpected Value (EV)\nExpected Monetary Value (EMV)\nExpected Utility (EU)\n\nTypically, we use EU. Sometimes we use EMV under the assumption that it is equivalent to EU in a free market (utility is a positive linear transformation of monetary value)\nTwo arguments for why EU Max:\n\nIn the long run, no strategy can be expected to do better than maximizing expected utility.\n\nObjections\n\nThere is no long run for humans\nGambler’s Ruin: a gambler with finite wealth, playing a fair game, eventually goes broke with probability 1\nHow is the long-run argument relevant to unique decisions that can’t be repeated?\n\n\n\n\nAxiomatic Approach. You should maximize expected utility, because you are also maximizing utility.\n\nThis conclusion does not depend on the long run argument, so it applies even to the single case.\n\n\n\nMaximizing Expected Utility\nUtility is a numerical representation of the agent’s preference ranking\nGenerally, we prefer using EU over EMV.\nSee also: utilitarianism\nUtility theory allows for outcomes without monetary value, or whose value can’t be measured solely in terms of monetary payoff.\nUtility of Money\nNote that generally, there is a diminishing marginal utility of money\nFor example, in a lottery where you\n\nA) win $1M for sure or\nB) 50% chance to win $3M and 50% chance to get nothing\n\nMost people would choose A. The change in utility from $1M to $3M is not enough to offset the drop in probability from certainty to 50%.\nParadoxes and Puzzles\nAllais Paradox\n\nSituation A: Choose between\n\n$1 million for sure\nA lottery:\n\n10% for $5 million\n89% for $1 million,\n1% for nothing.\n\n\n\n\nSituation B: Choose between\n\n10% for $5 million\n11% for $1million\n\n\n\nParadox arrises if you choose a) over b) in situation A and d) over c) in situation B. But, we can show that these two situations are equivalent.\nSolutions:\n\nSavage: fix irrational preferences of people!\nRevise the table: utilities of the same amount of money is not the same in the two situations (not very plausible)\nPriority heuristics (Gigerenzer): people have a hierarchy of goals and avoiding uncertainty is high on the list (hard to defend as a normative principle)\n\nEllsberg Paradox\nSame idea as Allais Paradox but shifts probabilities instead of values\nUrn with 90 balls\n\n30 red balls (R), 60 black (B) or yellow balls (Y)\nLet p∗60 be the number of black and (1−p)∗60 is the number of yellow balls\nA ball will be draw and you make a bet on its colour\n\n\nSituation A: Bet R or B. Receive $100 if right, $0 if wrong\nSituation B: Bet (R or Y) or (B or Y). Receive $100 if right, $0 if wrong\n\nMany people bet R in Situation A but bet (B or Y) in situation B. People like to avoid uncertainty! However, this is inconsistent with EU Max\nSt. Petersburg Paradox\nA fair coin is tossed until it comes up Heads. If Heads appears for the first time on toss n, you are paid n dollars\nWhat is the EMV of this game? Technically, the St. Petersburg game involves infinitely many states\nEMV=(1/2)$2+(1/2)2$2+⋯=1+1+⋯=∞\nYou should be willing to pay any price to play!\nSolutions\n\nBernoulli: diminishing marginal utility of money\n\nRefutation: change payout to 2n\n\n\nBuffon: de minimis condition\n\nIgnore tiny probabilities\n\n\n\nTwo-Envelope Paradox\nA trustworthy informant tells you that one of the envelopes contains exactly twice as much as the other, but the informant does not tell you which is which. Since this is all you know, you decide to pick an envelope at random. Let us say you pick envelope A. Just before you open envelope A you are offered to swap and take envelope B instead.\nGiven what you know, both possibilities are equally likely. Hence, the expected monetary value of swapping to B is 21​2x+21​2x​=45​x. Since 45​x&gt;x, it is rational to take B instead of A. But you can apply the same argument to arrive at a contradictory claim that you should take A instead of B.\nLimit Counterargument\n\nThe present formulation of the paradox presupposes that there is no upper limit to how much money there is in the world\nSuppose that there indeed is some upper limit L to how much money there is in the world.\n\nIt then follows that no envelope can contain more than (2/3)L\nThe other envelope would be certain to contain (1/3)L\n\n\n"},"thoughts/Degraded-Blockchain-problem":{"title":"Degraded Blockchain problem","links":["thoughts/trust","thoughts/cryptography"],"tags":["sapling"],"content":"One thing that I still don’t understand about blockchain is the ‘trustless’ aspect of blockchains. Blockchains are not ‘trustless’, rather it shifts the trust balance away from trusting people and corporations to trusting an algorithm.1\nSource\nThe blockchain really only stores a pointer to a good 99% of content that supposedly lives ‘on-chain’, most of it is never 100% on chain. (This is also why people meme on NFT owners which just link to a PNG, it’s literally just a weak pointer. If the image hosting service goes down, that pointer is useless as it is immutable). The problem with this model is that if most of the value comes from the ‘off-chain’ content, then what use is this proof of ownership if I can’t do anything with it?\nThe only thing that actually connects the blockchain with the ‘off-chain’ value is… you guessed it… trust.\nFootnotes\n\n\nAlso, how do we go about trusting the algorithm that is ‘cryptographically secure’ when a good 99.5% of the users don’t actually understand how blockchain and cryptography works? ↩\n\n\n"},"thoughts/Descartes'-Meditations":{"title":"Descartes' Meditations","links":["thoughts/trust","thoughts/qualia","thoughts/the-Self","thoughts/emptiness"],"tags":["seed","PHIL240A"],"content":"Cartesian doubt refers to the opposite of trust in Nguyen’s sense\nDescartes, in his meditations, wishes to find a foundation of knowledge that will still stand strong even after doubting the most basic facts about the world. In doing so, he starts by methodically doubting all things that he has “less than complete certainty” of. He casts asides sensory experience, claiming that these can be deceived. Even objects close by could be illusory as he could be dreaming. Even basic axioms of the world like mathematics could be false because of some evil genius that has “employed all his energies in order to deceive me”. Iteratively, he tries to re-prove existence of many things, starting from the existence of self (“cogito, ergo sum”) and progressing towards an argument for the existence of God (and god as a perfect being who cannot be a deceiver.)\nFirst Meditation\nDescartes wants to find a foundation of knowledge that will still stand strong even after doubting the most basic facts about the world. Everything he has accepted has true has come through senses, but senses can deceive\nFirst Argument\n\nSome experiences are deceptive (e.g. visual illusions, mirages)\nAny particular experience I have might be deceptive\nIt is possible that all my experiences are deceptive\n\nCounterexample to first argument\n\nSome paintings are forgeries\nAny particular painting might be a forgery\nIt is possible that all paintings are forgeries\nThat can’t be possible because all paintings if all paintings are forgeries, what are they based off of?\nCannot dream things that have no component real parts (even mermaids, for example are part women and fish)\nSome universal axioms still hold\n\n“For whether I am awake or asleep, two plus three make five, and a square does not have more than four sides.” (p. 15)\n\n\n\nDescartes’ objection to the First Argument\n\nPremise 2 doesn’t work!\nI am sure of my own thoughts. To have these doubts, one must exist. For an evil demon to mislead him in all these insidious ways, he must exist in order to be misled.\nTherefore, thought above all else is inseparable from being. The Meditator concludes that, in the strict sense, he is only a thing that thinks.\n\nDream Argument\n\nI have dreamt at being at my desk\nWhen I dreamt it, I believed it was true\nWhen I dreamt it, it was false\nThere is no way to tell whether you are dreaming or awake\nConclusion: I don’t know that I am here at my desk right now\n\nAs premise 4 is roughly equivalent to the conclusion, this argument is invalid. It begs the question (argument which has a premise as a conclusion, circular reasoning)\nThe God argument\nSuppose there exists an evil demon, just as powerful as God, but which deceives me about everything he can\nHowever, it seems possible for there to be such a person. If there were such a person, then everything I believe would be false. I can’t tell that there isn’t such a person. So, I don’t really know anything I thought I knew.\nSecond Meditation\nI am, I exist is necessarily true whenever it is put forward by me or conceived in my mind. For one to be deceived, one must first exist\nWhat is “I”? Soul and body can be deceptions. Thought then, above all else, is inseparable from being\nThird Meditation\nTo assure himself that he is not deceived, he must inquire into the nature of God. Before doing this, the Meditator needs to classify his thoughts\n\nIdeas: images of things\n\n3 sources for ideas\n\nInnate\nAdventitious: coming from ‘outside’ us (as with our sensory perceptions)\n\nHis will has no effect on adventitious ideas: he cannot prevent himself from feeling hot when it is hot simply through the will, for instance.\n\n\nInvented\n\n\n\n\nVolitions, emotions, judgements: idea is the object of the thought, and a further thing such as an affirmation or a fear which is directed towards the object of thought\n\nOne of the grave mistakes is to judge the ideas in one’s mind to be accurate resemblances of things outside the mind (if they do even exist). Thus the meditator considers ideas in the mind only as modes of thought. Ideas, as modes of thought, all have the same amount of formal reality (reality intrinsic to themselves) but their objective reality differs greatly\nFor Descartes and the Scholastics, ideas are the link that connect mind and world because they have both formal and objective reality. We can think about this like dividing reality into a scale where infinite substances (like God) have the most reality, followed by finite substances (bodies and minds), followed by modes (modifications of body and mind — e.g. colour, shape, size, etc.).\nIdeas then, have the formal reality of modes (as they are modifications of the mind) but objective reality of a finite substance (car is a body). No effect can have a greater amount of reality than its cause. The idea of a stone, then, could be caused by a stone or a large rock but it could not be caused by a colour. If the meditator can locate an idea with more objective reality than he has formal reality (finite substance). The only thing with more reality is infinite substance or the idea of God\nQualia can only be perceived in a confused and obscure way, so if they are things, they must have small degree of reality as to originate unproblematically from the Meditator himself\nExistence of God\nAs the Meditator cannot have originated the concept of God (which has infinite substance), God must be the cause of this idea and must therefore exist\nCould God not just be in contrast to his own finite being? We would not be aware of a lack unless we were aware of a more perfect being and God is the ultimate perfect being. You can’t actually get the idea of infinity just from endlessly increasing what’s finite, so the infinite must independently exist (p. 32)\nCould the Meditator themselves be supremely perfect? If this is the case, it is plausible that the idea of God could be conceived in him without any outside cause\nRejects this for 3 reasons\n\nGod is all actual and not potential\nIf he is constantly improving, he will never attain perfection while there is room for improvement\nPotential is not being at all\n\nThe Meditator seems committed to claiming both (a) that we can only be sure of our clear and distinct perceptions if God exists and (b) we can know that God exists because we clearly and distinctly perceive the idea of God. If both (a) and (b) are true, Descartes is guilty of circular reasoning.\nFourth Meditation\nIf God has endowed him with infallible judgment, how is it that he can be mistaken, as he undoubtedly is from time to time\nIf God is a perfect creator, God should be able to create perfect beings. The Lord works in mysterious ways — we should not seek to understand the true motives of God (?) Perhaps we are only a small part of a much larger creation\nDescartes is a proponent of free will. The will is free to affirm or deny whatever it wishes — as such, free will is the source of error. If there was no free will, we would never make mistakes\nFifth Meditation\nIn the Fifth Meditation, Descartes presents the argument as follows:\n\nSuppose there is a supremely perfect being — a being that has every possible ‘perfect’ trait. A supremely perfect being must have every possible perfect trait because to exclude any or all perfections from a supremely perfect being is to stumble into a contradiction or to “conceive of a mountain without a valley”.\nNecessary existence is a perfection. It is an existence where one depends only on the self for existence (similar to the concept of independent origination in emptiness).\nNecessary existence cannot be separated from the essence of a supremely perfect being.\nTherefore, a supremely perfect being exists — it is God.\n\nHowever, this argument is flawed as it begs the question — the premise (that there exists a supremely perfect being) is also the conclusion (a supremely perfect being exists). This inherent circular argument then does not hold.\nFor the sake of continued examination of the argument, let us assume that this argument is sound, and that God indeed does exist.\nDescartes goes on to utilize the statement that God exists to prove that God cannot be a deceiver.\n\nGod exists and is perfect\nDeception is imperfect, therefore God cannot deceive\nWhatever I perceive “clearly and distinctly” must be true\nAs my perception is clear, the material world must exist\n\nHowever, this argument also begs the question as it relies on the first proof that God necessarily exists as a premise. Unfortunately, Descartes first proof relies on the fact that Descartes has a clear and distinct idea of God, which presupposes clear and distinct perception (the conclusion of this argument). Thus, to show that his perception is clear, he must assume that his perception is clear — clearly an invalid argument."},"thoughts/Design-Justice":{"title":"Design Justice","links":["thoughts/systems-design","thoughts/Technosolutionism","thoughts/counterculture","thoughts/Hackers","posts/hackathons","thoughts/Do-Artifacts-Have-Politics","thoughts/human-centered-design","thoughts/creation-vs-maintenance","thoughts/maintenance","thoughts/attention-economy","thoughts/Post-It-Note-City","thoughts/teaching","thoughts/ethics"],"tags":["seed","book"],"content":"Summary\nDesign Justice focuses not just on design in the visual and aesthetic sense, but also on the design of systems.\nTo quote from the book, “Design justice rethinks design processes, centers people who are normally marginalized by design, and uses collaborative, creative practices to address the deepest challenges our communities face.” The norms, values, and assumptions that are encoded and reproduced in the design of systems can be changed by rethinking our design processes.\nIn less than 400 short pages, Sasha Costanza-Chock covers a breadth of topics ranging from intersectionality, bias, and universal design to maintenance, design sites, and technosolutionism — all through detailed case studies of real-world design practices and social movements.\nShe guides the reader into how the ‘unmarked user’ and universal design erases certain groups of people within the matrix of domination (through ableist, eurocentric, and classist assumptions) and refutes the argument that ‘design by committee produces mediocrity.’\nDesign Justice is a book that invites us to “center people who are too often marginalized by design”. More importantly, it urges us to work towards an equitable world for everyone: one which treats design justice not as a funnel that we use to limit ourselves to a minimal set of supposedly universal design choices, but rather as a prism through which to generate a far wider rainbow of possible choices, each better tailored to reflect the needs of a specific group of people.\nReflection\nDesign sites are valorized as places of learning, making, and building and the intersection of social movements and the counterculture. Why then, have they become increasingly corporate places of extraction of free labour?\nThis cooptation of hacker culture, hackathons as design sites in particular, by neoliberalism has been on the back of my mind ever since reading the chapter on design sites in Design Justice. As someone who first got their footing in computer science through hackathons, it pains me to see that this is the rep that hackathons have slowly gotten over time, moving from safe spaces for idea exploration to increasingly corporate, time-bound, events where hackers spin up apps to test company products in exchange for the slim chance of winning prizes and recognition.\nHackathons reshape precarious and unpaid work. Writing code and building apps for free becomes an extraordinary opportunity and a collective imagination for fictional expectations of innovation that benefits all. Do we so necessarily need to tie these rituals of play in building and tinkering to the recruiting and product testing pipeline for large corporations?\nAs a hackathon organizer, Design Justice has helped me to more actively think about what hackathons are trying to motivate. Having more of the tools to articulate and locate exactly why hackathons have felt increasingly corporate is the first step to reinstate hackathons as third spaces not as places of creation or competition, but as places of play and exploration.\nRead more: hackathons\nQuotes\nDefining Design Justice\nHow larger systems — including norms, values, and assumptions — are encoded in and reproduced through the design of sociotechnical systems. (Do Artifacts Have Politics)\nDesign justice rethinks design processes, centers people who are normally marginalized by design, and uses collaborative, creative practices to address the deepest challenges our communities face\nDesign justice is a framework for analysis of how design distributes benefits and bridges between various groups of people.\nDesign (noun): A plan or scheme conceived in the mind and intended for subsequent execution; the preliminary conception of an idea that is to carried into effect by action; a project. (Oxford English Dictionary)\nTrue, everyone designs, but only certain kinds of design work are acknowledged, valorized, remunerated, and credited. Though all humans design, not everyone gets paid to do so.\nThe Unmarked User\nDesigners most frequently assume that the unmarked user has access to several very powerful privileges, such as US citizenship, English language proficiency, access to broadband internet, a smartphone, a normatively abled body, and so on.\nFor broader reasons of structural inequality, the universe of real-world users falls within a limited range compared to the full breadth of potential users, then user-centered design reproduces exclusion by centering their needs.\nDisability Simulation\nThese ‘simulations’ produce an unrealistic understanding of the life experience of disability for a number of reasons: the nondisabled person does not have the alternate skill sets developed by [disabled people], and thus overestimates the loss of function which disability presents, and is furthermore likely to think of able-normative solutions rather than solutions more attuned to a [disabled person’s] life experience\nDisability simulation is discredited; lived experience is nontransferable. “Don’t start by building a new table; start by coming to the table”\nDismantling Existing Systems\ni.e. what’s wrong with colour blindness\n“Under this new rhetoric of colour-blindness, equality means treating all individuals the same, regardless of differences they brought with them due to the effects of past discrimination or even discrimination in other venues”\nNew Jim Code: Algorithmic decision systems based on historical data sets reinforce white supremacy and discrimination even as they are positioned by their designers as “fair”\nRacial hierarchies can only be dismantled by actively antiracist systems design, not by pretending they don’t exist.\nFar too often, user personas are created out of thin air by members of the design team, based on their own assumption or stereotypes about groups of people. When this happens, user personas are literally objectified assumptions about end users.\nMaintenance\nCreating new vs maintaining old\n\nContributing to an existing project requires contacting and negotiating with the existing developers, maintainers, and community. Creating something new produces attribution, credit, and visibility for its developers, whereas attribution, credit, and visibility for participating in an existing project must, at the very least, be shared.\nSupport Maintenance, not just “innovation.” Significant resources are necessary to maintain and improve existing movement tech, but most focus is on the creation of new projects.\n\n“Those of us working to promote universal access to clean water and sanitation must keep our eyes not just on the competition and prizes, but on the less glamorous work of encouraging adoption, usage, and maintenance”\nTechnosolutionism\nNeoliberal, technocentric ideas about the city as a machine or as a software system waiting to be optimized have become increasingly prominent. Citizens should not be reduced to users through the lens of neoliberal governmentality.\nFirst consider what already works at the community level, and to steer students away from the pitfalls of technosolutionism and technochauvanism\nAttention\nMediated visibility has become an important form of capital. Attention (time) is a scarce resource within late-stage informational capitalism, and its allocation has significant symbolic and material impacts (this is the Attention economy).\n“Design challenges” in which dozens, sometimes hundreds, of people do free labour and submit ideas in hopes that they’ll be the lucky one chosen to receive visibility, recognition, and possibly even compensation.\nDesign Sites\nTracing the cooptation of hacker culture by neoliberalism\nInvisibility of subaltern communities may also be strategic. Sometimes, they shield their practices and innovations from mainstream visibility to avoid incorporation and appropriation.\n“Alternative spaces and forms of living provided interesting ideas could be milked and marketed. So certain structural features of these ‘indie’ movement outputs were suddenly highly acclaimed, applied, and copy-pasted into capitalist developing laboratories” (Post-It Note City)\nHackerspaces in the European context, which they describe as originally being “third spaces” outside of the logic of both the communist state and the capitalist market.\nThe assumption that making sites “open” makes them inclusive, without specifically addressing race, class, gender, and/or disability dynamics, is common to many privileged design sites.\nMany who are active in these design sites feel themselves to be participants in commons-based peer production, or “decentralized, collaborative, and non-proprietary; base on sharing resources and outputs among widely distributed, loosely connected individuals who cooperate with each other without relying on either market signals or managerial commands”\nWithout intentional intervention, these spaces find it very difficult to fulfil even their own liberal democratic rhetoric because they end up dominated by white cis men and by middle-class people with free time and disposable income.\nMore on hackathons: hackathons\nHackathons: The Bad\nHackathons are understood by corporate managers as potentially effective ways to identify new talent, and therefore as a possible mechanism in the tech sector hiring pipeline.\n“Hackathons, time-bounded events where participants write computer code and build apps, have become a popular means of socializing tech students and workers to produce ‘innovation’ despite little promise of material reward… [they] reshape unpaid and precarious work as an extraordinary opportunity, a ritual of ecstatic labour, and a collective imaginary for fictional expectations of innovation that benefits all, a powerful strategy for manufacturing works’ consent in the ‘new’ economy.” (Sharon Zukin and Max Papadantonakis)\nHackathons provide excellent opportunities for the extraction of free labour.\nThe assumption that a “hackathon for good” will be successful if it produces a new app that can help “solve” a social problem runs deep.\nHackathons nearly always focus on problems and rarely build on existing community assets; and people thing hackathons can do things that they usually can’t, such as solve big or even little problems, create new products overnight, or ‘level the playing field’ of innovation through meritocracy. “A one day hack for homelessness takes away from the complexity of social justice issues. … You can’t just come up with an app and solve the world’s problems”\n“Hackathon spaces cultivate a culture that marginalizes hackers with specific needs, including but not limited to women, people with disabilities, people with non-traditional backgrounds, and even individuals with specific dietary restrictions. By consistently ignoring the health, diet, and care needs of diverse attendees, along with needs based on skill, class, and gender identities, hackathons create an exclusive and hostile environment.”\nHackathons: The Good\nThey are often crucibles of intense and focused learning, making, problem-solving, community building, and play.\n“In the old days people used to form teams and rush in and try to fix things, without really even knowing what was broken … it is no longer just a bunch of programmers in a room. There are now hackathons where actual community members are learning to code and interacting. … Community members are also teaching programmers about the things they need to sustain and build for the future. That’s a really good thing happening”\nSuch hackathons push hackers to reflect on why they are doing the work they do, push for the ideas and welfare of marginalized communities in the tech sphere, and do so on the terms of their wellbeing and safety.\nOrganizers should pay attention to participants as whole human beings. For example, this means that it is important to consider food, bio breaks, accessible bathrooms that are friendly to all body types and genders, comfortable spaces to nap or relax, and decent lighting, etc.\nDesign Justice Pedagogies\n“Critical pedagogy, where the role of the educator is to pose problems, create spaces for the collective development of critical consciousness, help to develop plans for action to make the world a better place, and develop a sense of agency among learners”\n“No one knows everything, but together we know a lot, if we listen to each other”\nKey elements\n\nTeach data science in a way that honours context, respects situated knowledge, and makes it clear that data is never “raw”.\nEmphasize the use of data to create shared meaning over individual mastery.\nTeach data science that values not only reason, but ethics and emotions as well\n\nCritiques\n“Design by committee produces mediocrity” or “we don’t want to end up with the lowest-common-denominator design!”\nIt’s true that design justice practitioners have to take care that critique does not become our primary activity; an overemphasis on testing, evaluation, and critique can indeed be ultimately disempowering. At the same time, explicit critique paired with alternative proposals can be very productive.\nDesign justice doesn’t imply that we must somehow reduce our options to only those that satisfy all accessibility criteria for the most marginalized within the matrix of domination. It is not meant to be a filter that we use to eliminate most design possibilities from consideration because they fail an accessibility checklist. In fact, design justice as a framework is meant to do the opposite: to act not as a funnel that we use to limit ourselves to a minimal set of supposedly universal design choices, but rather as a prism through which to generate a far wider rainbow of possible choices, each better tailored to reflect the needs of a specific group of people."},"thoughts/Designing-Data-Intensive-Applications":{"title":"Designing Data-Intensive Applications","links":["thoughts/Universal-Scaling-Law","thoughts/Do-Artifacts-Have-Politics"],"tags":["seed","book"],"content":"Reviewing the second edition by Martin Kleppmann. This is the first book I’m helping revise in a formal capacity!\nPreface\n\nPreface should skip that entire first page (xiii) this doesn’t appear to add/contextualize anything\nStart with what data-intensive applications are (top of xiv) then provide examples\n\nIf we want to include the driving forces list on xiii, this is a good place to move it\n\n\n“We call an application data-intensive if data is its primary challenge—the quantity of data, the complexity of data, or the speed at which it is changing—as opposed to compute-intensive, where CPU cycles are the bottleneck.”\n\nThis is a key line that defines data-intensive applications. To me, this should reflect the majority of the contents of the book.\nSpecifically: data replication and consistency should be emphasized more here! Especially after seeing that a good chunk of the book focuses on this.\nI don’t actually think ‘the speed at which it is changing’ is talked about at all in the book, maybe remove that part\nWould remove the comparison to compute-intensive, doesn’t seem to aid the definition.\n\n\nLove the part on why terminology and underlying principles are important — it’s true!\nWho should read this book section\n\nAxe first sentence of second paragraph\n\n\nScope section\n\nReferences and further reading seem implied (and references are already mentioned in the scope section), we can probably axe\n\n\n\nBig-picture Content\n\nWould love sections on tradeoffs in data storage!\nAlso either directly preceding or after, there should be a section on latency tradeoffs (seems like this exists in the section on “Describing Performance” on page 54)\n\ne.g. deciding between hot/cold storage\ncost vs latency tradeoff\n\nfactoring in speed of light when we talk about distances about how far data is away (e.g. cpu caches to memory to disk to network)\n\n\ncaching as a means to move along the tradeoff spectrum\n\nmaybe move section on systems of record and derived data here\n\n\n\n\nIt felt like there were quite a few cases where terminology was used before it was defined, just a flag for some potential reordering of chapters/content.\n\nE.g. I feel like Chapter 1 should actually be much later, Chapter 2 seems more ‘fundamental’\nIf we drew a dependency graph of what each chapter depends on, this could help us sort out book order better (e.g. Types and Programming Languages)\n\n\n\n\n\n\n\nChapter 1\n\nI really like the clarity and writing style here already. It feels a lot more mature than the writing in the preface (though that’s likely because the preface hasn’t been updated)\n\nI feel like there’s a lot of duplication in the first few pages of the preface and the start of Chapter 1, we should just keep it in one place or the other\nOr have one of those animal markers that tell you to skip ahead to Chapter 1 if they want the definition of data-intensive, etc.!\n\n\nBullet point list on p.21-22 probably needs to include zonal affinity (how do we make sure data is close to users)\np.22 paragraph 1: glue also includes network requests!\n\nMaybe link to Distributed versus Single-Node systems\n\n\nAnnotated diagram for OLTP vs OLAP access patterns would be more helpful than the table on p.25 I think.\nConfusing part about “Analytic systems are often only accessible to employees of the company that owns the data” (p.26)\n\nParagraph feels disjoint, doesn’t add anything to the chapter, would just cut\n\n\nData warehousing section, what caused the transition “for companies to stop using their OLTP systems for analytics purposes, and to run the analytics on a separate database system instead”? Adding historical context here would be really cool\nLink to stream processing in “Beyond the data lake”\nSection on Systems of record and derived data\n\nFun quote to consider adding: “Never go to sea with two chronometers; take one or three.”\nMention ‘canonical’ alongside system of record/source of truth\nDerived data systems definition mentions indexes, materialized views, etc. but these aren’t defined. It should at least link out to the appropriate sections in the book.\nHave a section on why redundant systems are essential for getting good perf on read queries or link out to the proposed chapter/section on latency tradeoffs\nDiagram for systems of record and derived data (and maybe even levels of caching)\n\n\nCloud versus Self-hosting section feels way too long\n\nNo good definition of ‘bursty’ workloads — there should probably be a section on types of load distribution\nI think biggest pro of cloud services is that they take on the maintenance burden! Without it, you need to hire a team of SREs to maintain uptime, updates, and infrastructure\n\nMove section on Operations in the Cloud Era (p.37) here\n\n\nInclude diagram of layering of cloud services\nAlso highlight “deploying to the edge” as a way of keeping data/compute close to users\n\nAgain, good place to mention/link to latency tradeoffs\n\n\n\n\nSeparation of storage and compute\n\nI feel like there needs to be a section on the fundamentals of what data is and how it is stored\nMention poor performance and poor scalability (paragraph 2 of the section, p.36) but why? This feels important to link out to that section on latency tradeoffs (sorry if I sound like a broken record here, but this feels important!)\n\n\nDistributed vs Single-Node systems\n\nScalability and elasticity aren’t clearly distinguished (they seem roughly the same by description)\n\n\nMicroservices and serverless\n\nMention driving principles of microservices: Single Responsibility Principle (SRP) and having well-defined interfaces as contracts\nCoordination overhead section, mention Universal Scaling Law: http://www.perfdynamics.com/Manifesto/USLscalability.html\nClarify that serverless/FaaS does in fact have servers but rather it abstracts more of the service lifecycle.\n\nIn addition to start up/shutdown, it also manages scaling and connection management\n\n\n\n\nProblems with Distributed Systems\n\nLinks to multiple chapters but doesn’t preview what it is talking about (e.g. link to Chapter 9 in first paragraph and links the Chapter 6 and 8 in 4th paragraph). I wouldn’t want to interrupt my reading flow to jump to the chapter and see if it’s relevant, it should at least include the chapter title.\nIn the part about more nodes not always faster, explain the intuition behind why here. Is it the resource contention? The actual network bandwidth? The speed of light?\nMore of a personal question: does observability and telemetry count as analytic systems? It doesn’t seem directly operational or analytical, what type of data is this?\n\n\nCloud computing versus Supercomputing\n\nI had no idea what a Clos topology was!! I had to look this up.\nAlso, what is bisection bandwidth?\n\n\nData Systems, Law, and Society\n\nYessss, I’m so glad this section was included\n\n\n\nChapter 2\n\nMy personal definition of requirements: Requirements are stable descriptions of users’ aspirations, goals, constraints, expectations etc that form a sound basis from which to start designing around\n\nfunctional requirements: describe what the product will do\nnonfunctional requirements: describe the characteristics (sometimes called constraints) of how it will do it\n\n\nI appreciate the case studies in Chapter 2! Makes it much more concrete to follow\nIs it worth talking about SQL views CREATE VIEW? This helps contextualize materialization and materialized views.\nDescribing performance\n\nResponse time\n\nFirst mention of response time should also mention latency as a sometimes interchangeable term and then mention that a more formal and specific definition can be found under the “Latency and Response Time” section\n\n\nProbably should mention jitter/variance in the first bullet point under describing performance (top of p.55)\nThroughout\n\n“somethings per second” → Hz\nGoodput: rate at which useful data arrives\n\n\nI’ve seen network latency be broken down a bit further\n\nProcessing/routing delay: time to figure out where a packet should go but this is almost always small enough to be negligible\nQueueing delay (which you’ve included but I think separated from the rest of network latency): waiting time to get access to a link\n\nDelayQueueing​=1−US​−S\n\n\nTransmission delay: time to write packet to the wire\n\nDelayTransmission​=BandwidthMessage size×8bits/byte​ for each segment (as each router needs to receive the entire packet before adding it to the queue)\n\n\nPropagation delay: time to move bits over the wire\n\nDelayPropagation​=Link speedTotal distance​\n\n\n^ the above is probably too much detail but worth considering!\n\n\n\n\nShould also do histogram with vertical bars for percentiles in addition to the time domain diagram (figure 2-5, top of p.58)\nReliability and Fault Tolerance\n\nSoftware faults: sometimes good to have multiple implementations of the same software to prevent single point of failure (e.g. Ethereum client diversity)\nDefine sociotechnical system (not all readers will know what this is or why it matters)\n\nMaybe link out to or briefly summarize Do Artifacts Have Politics\n&quot;&quot;Seemingly innocuous design features in mass transit systems, water projects, industrial machinery, and other technologies actually mask social choices of profound significance.” (societal impact is treated as an externality, those which do not matter when ‘just considering efficiency’)”\n\n\n\n\nScalability\n\nAlso to add to things in discussion (p.67):\n\nHow might scale affect the cost of running the service? Do the unit economics still make sense?\n\n\n\n\nDescribing Load\n\nGraph of linear scalability\nWhat are typical types of workloads that have linear scalability? Sublinear? Superlinear?\n\n\nShared-memory, Shared-disk, and shared-nothing architecture\n\nTalk about resource contention somewhere! As far as I can tell, there hasn’t been a section on what type of workloads this may crop up in (i.e. mutual exclusion, etc.)\nGood segue into share-nothing architecture!\nIt would be really awesome to insert a case study comparing what these two look like in the wild\n\n\nMaintainability\n\nMaybe give a brief definition of technical debt: choices that are easier today at the expense of time and money in the future. Prefer taking on technical debt that is particularly high leverage (it is easy to pay down relative to the amount of immediate benefit)\n\n\nOperability: Making Life Easy for Operations\n\nDefine self-healing system in terms of a data system. Does this mean recovering from faults via redundant copies?\n\n\nSimplicity: Managing Complexity\n\nReally love the clarity in distinguishing essential vs accidental complexity, probably the clearest definition of it I’ve read\nMaybe include a warning about how pursuing the ‘right’ abstraction before you need it is also dangerous! There are lots of case studies about how the wrong abstraction is even worse than no abstraction at all\n\n\nEvolvability: Making Change Easy\n\nGood place to talk about forward and backward compatability and schema evolution (famously difficult and especially relevant data systems)!\n\nLink to schema flexibility section in chapter 3?\n\n\n\n\n\nChapter 3\n\nGood history of SQL and NoSQL, contextualizes a lot of the next chapter!\nExplain the fundamentals behind an index. What is it precomputing, how do I know if I need one?\nUpper size of a table? How big is too big?\n\nI see a lot of HN discussion about “what is too big” and how tailscale at one point was using a big JSON file to store everything\n\n\nNo major comments here, content is excellent and clear\n"},"thoughts/Do-Artifacts-Have-Politics":{"title":"Do Artifacts Have Politics","links":["thoughts/software-and-politics"],"tags":["seed","pattern"],"content":"by Langdon Winner\n\n“Politics” is defined as “arrangements of power and authority in human associations as well as the activities that take place within those arrangements.” (see: software and politics)\n\n“In controversies about technology and society, there is no idea more provocative than the notion that technical things have political qualities”\n“What matters is not technology itself, but the social or economic system in which it is embedded.”\n“Seemingly innocuous design features in mass transit systems, water projects, industrial machinery, and other technologies actually mask social choices of profound significance.” (societal impact is treated as an externality, those which do not matter when ‘just considering efficiency’)\n“The things we call ‘technologies’ are just ways of building order in our world.”\n“In that sense technological innovations are similar to legislative acts or political foundings that establish a framework for public order that will endure over many generations.”\nHow artifacts can contain political properties\n“Technological change expresses a panoply of human motives, not the least of which is the desire of some to have dominion over others, even though it may require an occasional sacrifice of cost-cutting and some violence to the norm of getting more from less”\nThe mechanical tomato harvester\nA remarkable device perfected by researchers at UC from the late 1940s to present. The harvesters replace the system of handpicking.\nThe machine reduces costs by approximately five to seven dollars per ton as compared to hand-harvesting.\n“The suit charges that University officials are spending tax monies on projects that benefit a handful of private interests to the detriment of farmworkers, small farmers, consumers, and rural California generally, and asks for a court injunction to stop the practice. The University has denied these charges, arguing that to accept them ‘would require elimination of all research with any potential practical application’”"},"thoughts/Dreams":{"title":"Dreams","links":["thoughts/linguistic-relativism"],"tags":["seed","PHIL451A"],"content":"Dream State\nImmersion in a dream\n\nDreams\nHere we are all, by day; by night, we’re hurled\nBy dreams, each one into a several [separate] world\n— Robert Herrick (1591 - 1674)\n\n\nWe experience being in the dream\nTwo ways of experience\n\nIdentification with dream ego from first-person (field perspective)\nIdentification with dream ego from third-person (observer perspective)\n\n\nWe cannot inspect (nonlucid) dreams directly\n\nWe can inspect only our waking memories of dreams\nWe have certain cultural and linguistic practices of dream reporting whereby we make stories of our dreams (linguistic relativism for dreams)\n\n\n\nDifferent views (Simulation Models of Dreaming)\n\nOrthodox View\n\nPrecepts: dreaming involves senses that we experience when we are waking, except the experiences of things that are not there or have weak correlation with what is there\nBeliefs: when we dream p we believe p to be true. In most cases, these are false so dreaming involves mainly false beliefs\n\n\nHallucination model\n\nDreaming is immersive spatiotemporal hallucination\nImmersive: full immersion in the dream world\nSpatiotemporal: full immersion in a here and now\nHallucination: experience that seems exactly like a perception but has weak stimulus correlation with the environment\n\n\nImagination model\n\nDreaming involves experiences of the sort we have when we imagine (mental images)\nWhen we dream that p, we imagine that p (however, imagining that p does not entail believing that p)\nDreams can be indeterminate in their sensory features (e.g. indeterminate in colour)\nObject: what about emotions? Some emotions can only arise from belief\n\nWhen I dream that p, I experience fear, elation, etc.\nSuch emotions arising from an attitude that p can only arise from a belief that p\nSo when I dream that p, I believe that p\n\n\nCounterargument: contradiction, you still feel these emotion reading fiction\n\nSame premises as above, but final conclusion is that: when I read in a fiction that p, I do NOT believe that p.\nWay out of this contradiction is to deny premise 2)\n\n\nEye movements during lucid-REM sleep resemble waking perception more than they resemble waking imagination\nUpshot: to dream is to imagine a dream world and to identify with the dream ego immersed in that world\n\n\n\nLucid Dreaming\n\nA dream in which you can direct your attention to the dreamlike quality of the state\nFeatures\n\nGreater clarity/vividness\nRealism\nEmotional exhilaration\nSense of freedom\n\n\nSense of self in lucid dream state\n\nSelf-as-dreamer: “I am dreaming” (knowledge of being asleep in bed)\nSelf-as-dreamed (dream ego): “I am flying” (default conceptualizations of self)\n\n\nIs lucid dreaming knowing you’re dreaming or dreaming you’re dreaming?\n\nDid they just dream that they were aware that they were dreaming?\nKnowing you’re dreaming seems to involve a certain kind of attention and cognitive control that is missing when you dream you’re dreaming\nSeems to be a tell-tale LRLR eye moment signal during REM sleep when participants realize they are dreaming\n\n\n"},"thoughts/Dunbar's-Number":{"title":"Dunbar's Number","links":["thoughts/Making-and-Maintenance-of-OSS","thoughts/Universal-Scaling-Law","thoughts/web3"],"tags":["sapling"],"content":"Source: The Limits of Friendship by Maria Konnikova\n\nThere’s only so big a social group can get before it decays into smaller ones.\nSocial Brain Hypothesis: primates have large brains because they live in socially complex societies: the larger the group, the larger the brain\nApplied to humans, Dunbar computed a theoretical maximum of 150 for human social groups.\nA range (by a factor of 3)\n\nCasual Friends: 150\n‘Dinner’ Friends: 50\nClose Friends: 15\nIntimate Friends: 5\n\nCan we scale trust beyond the Dunbar number? Is this what makes large orgs so sluggish and boring to work at?\nThis applies to scaling orgs and projects too. How do we ensure that open source software works when more than say 150 people are contributing? (see also: Universal Scaling Law)\nWhat about in web3? Structure helps us scale beyond ‘natural’ community sizes but this seems difficult in a group where the ethos is very much against said centralized structure."},"thoughts/Dutch-Book":{"title":"Dutch Book","links":["thoughts/fair-betting-quotient","thoughts/probability"],"tags":["seed","PHIL321A"],"content":"A Dutch Book is a set of bets that you consider individually fair, but which collectively guarantee a loss\nThis usually happens when people commit probabilistic fallacies (e.g. the conjunction fallacy, believing P(A∧B∣E)&gt;P(A∣E) when this can never be the case). Another common mistake is double counting probabilities\nFor example, if J believes that P(heads)=P(tails)=32​, we can propose two bets\n\nPay 2;win3 if heads, $0 if tails\nPay 2;win3 if tails, $0 if heads\n\nBoth bets make sense for J. However, if J takes both bets, then he faces a guaranteed loss of $1\nHave the agent bet for propositions with credences (or FBQs) that are too high, and against propositions with credences (or FBQs) that are too low\nFor any given bet (set p to be 1−p for the against case):\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlayer wins betPlayer loses betS−pS−pS\nDutch Book Theorem\nBased on the Kolmogorov probability axioms,\n\nIf any axiom is violated, a Dutch Book can be made.\nIf no axiom is violated, then no Dutch Book can be made.\n"},"thoughts/Elliptic-curve-Cryptography-(ECC)":{"title":"Elliptic-curve Cryptography (ECC)","links":["thoughts/Asymmetric-Key-Cryptography","thoughts/RSA"],"tags":["seed"],"content":"A form of asymmetric cryptography which uses much smaller key-sizes than RSA\nProperties\nSpecifically looking at Ed25519\nThe formula follows something like y2=x3+ax+b (which is symmetric about the x-axis)\nDrawing a straight line through the curve will intersect no more than 3 points. A line between any two points will also intersect the curve at another place\nLet the starting point on the curve point be point A. The “dot” function is kind of like a game of billiards.\nIn this game of billiards, you take a ball at point A, shoot it towards point B. When it hits the curve, the ball bounces either straight up (if it’s below the x-axis) or straight down (if it’s above the x-axis) to the other side of the curve. The point it lands on is C\nIf the value C is over some maximum value (usually a prime), we modulus it with the maximum to end with a valid number.\nIllustrated example of A dot B = C:\n\nIt turns out that if you “dot” an initial point with itself n times to arrive at a final point, finding out n when you only know the final point and the first point is hard.\nn is then the private key, point A dotted with itself n times is the public key\nKey Exchange\nECDHE stands for Elliptic Curve Diffie Hellman Ephemeral and is a key exchange mechanism based on elliptic curves\nCurve25519 is a popular set of elliptic curve parameters and reference implementation by Daniel J. Bernstein in C Bindings and alternative implementations are also available."},"thoughts/Energy-Maximalism":{"title":"Energy Maximalism","links":["thoughts/causal-decision-theory","thoughts/Jevons-Paradox","thoughts/Where-is-My-Flying-Car","thoughts/Nuclear-Fusion","thoughts/Mass-Hysteria"],"tags":["seed"],"content":"\n“Power is our only lack. We generate all we can with the materials and knowledge at our disposal, but we never have enough. Our development is hindered, our birth-rate must be held down to a minimum, many new cities which we need cannot be build and many new projects cannot be started, all for lack of power.” — E.E. “Doc” Smith, Skylark Three\n\nPoverty is ameliorated by cheap energy. “If you want to improve the situation of the poorest two billion on the planet, having the price of energy go down substantially would be the best thing you could do for them.” (Bill Gates)\nAdditionally, energy consumption Granger-causes real GDP per capita and vice versa in the long run, which implies that an increase in energy consumption leads to an increase in economic growth and vice versa.\n\nIf, as is perfectly possible, we are short of energy two generations from now, it will be through our own incompetence. We will be like Stone Age men freezing to death on top of a coal bed (Arthur C. Clarke)\n\nSee: Jevons Paradox\nErgopobia\nExcerpt from Where is My Flying Car:\n\nErgophobia technically means a neurotic fear of doing work. But the term comes from the same Greek root as ‘energy’ and you will find that physicists use ‘work’ and ‘energy’ to mean the same thing. So I feel justified in using the word to refer to the almost inexplicable belief that there is something wrong with using energy\n\nRegulation\nThe Nuclear Regulatory Commision (NRC) found that there is no evidence of a carcinogenic effect on humans for accute irradiation at doses less than 100 millisieverts (mSv). Yet, the same NARC also sets the limit for public radiation at just 1% of that number at 1mSv. This is like setting a speed limit of one mile per hour because people have died doing 100. Yes, nuclear today is expensive. Shipping would be, too, if trucks had to operate with a speed limit of one mile per hour.\nEnergy Source Mortality Rates:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSourceDeaths/Terawatt-hourRemarksCoal (world average)170Coal (US)150Over 10k/yrOil150Air pollution (e.g. LA smog)Biofuels/Biomass24Natural Gas4Solar0.44Falls from rooftopsWind0.15Hydro0.1Dam burstsNuclear0.04Including Chernobyl\nSee also:  Mass Hysteria (re: fearmongering around nuclear)"},"thoughts/Ensemble-method":{"title":"Ensemble method","links":["thoughts/emergent-behaviour","thoughts/XGBoost","thoughts/Random-Forest"],"tags":["seed","CPSC340"],"content":"Ensemble methods are classifiers that have classifiers as input (and often have higher accuracy than regular input classifiers). This is also called “meta-learning” and it only works if the individual classifiers make independent errors\nSee also: emergent behaviour\nBoosting/Stacking\nImproves training error of classifiers with high Etrain​\nModels that use the boosting ensemble method:\n\nXGBoost (regularized regression trees)\n\nAveraging/Voting\nImproves approximation error of classifiers with high Eapprox​\nModels that uses the averaging ensemble method:\n\nRandom Forest\n\nMethods\n\nVoting: take the mode of the predictions across the classifiers\nStacking: fit another classifier that uses the predictions as features\n"},"thoughts/Equality-Saturation":{"title":"Equality Saturation","links":["thoughts/compiler"],"tags":["seed"],"content":"In a traditional compilers, optimizations are applied sequentially, with each optimization taking as input the program produced by the previous one.\nOne of the drawbacks is that the order in which optimizations are run affects the quality of the generated code, a problem commonly known as the phase ordering problem. That is, optimization stages are not commutative. The local nature of these optimization heuristics makes it difficult to take into account the effect of future optimizations.\nInstead, the set of candidate optimized programs is computed by repeatedly inferring equivalences between program fragments, thus allowing us to represent the effect of many possible optimizations at once. Equality analysis effectively looks to instantiate equality axioms or rules that we know are equivalent. For example, simplifying a * 0 to 0 but they can also get more complicated (e.g. inlining, tail recursion elimination).\nOptimizations can work as before, except that when the optimization would have performed a transformation, it now simply records the transformation as an equality. After a program is converted into IR, we repeatedly apply equality analyses to infer new equalities until no more equalities can be inferred, a process known as equality saturation.\nPseudo-code\ndef equality_saturation(expr, rewrites):\n  egraph = initial_egraph(expr)\n \n  # hot loop\n  while not egraph.is_saturated_or_timeout():\n    for rw in rewrites:\n      # read step -&gt; find substitutions\n      # ematch searches for the pattern in the egraph\n      # returns a list of pairs of substitutions and the eclass it was found in\n      for (subst, eclass) in egraph.ematch(rw.lhs):\n        # write step, ensure we restore invariants here\n        eclass2 = egraph.add(rw.rhs.substitute(subst))\n        # restore congruence!\n        egraph.merge(eclass, eclass2)\nreturn egraph.extract_best()\nThings to note:\n\nrewrites are ordered\nreads and writes are interleaved which means we need to do more invariant maintenance\n\nWhat egg does (deferred invariant maintenance):\ndef equality_saturation(expr, rewrites):\n  egraph = initial_egraph(expr)\n  while not egraph.is_saturated_or_timeout():\n    # difference here is we partition the reads/writes separately\n    matches = []\n \n    # all reads, we can parallelize\n    for rw in rewrites:\n      for (subst, eclass) in egraph.ematch(rw.lhs):\n        matches.append((rw, subst, eclass))\n \n    # all writes\n    for (rw, subst, eclass) in matches:\n      eclass2 = egraph.add(rw.rhs.substitute(subst))\n      egraph.merge(eclass, eclass2)\n    \n    # restore invariants\n    egraph.rebuild()\nreturn egraph.extract_best()"},"thoughts/Equivalence-Graphs":{"title":"Equivalence Graphs","links":["thoughts/Equality-Saturation","thoughts/Hash-consing"],"tags":["seed"],"content":"Source\nAn e-graph compactly represents many equivalent programs. Commonly used in Equality Saturation and modern theorem-provers.\nIt’s a data-structure that represents equivalence relations over terms.\nIt is made up of equivalence classes (or e-classes) which are sets of equivalence nodes (e-nodes). e-nodes are n-ary operators from whatever domain you are operating in (e.g. / or * for math).\nInstead of destructive rewrites over the tree of nodes, we can grow the e-graph by representing the transformed program as new equivalences. The following is an example of rewriting (2∗a)/2 to be (a≪2)/2.\nOriginally, the e-class containing * just has one item which points to a and 2. We extend the e-class by adding &lt;&lt; which points to a and 1, semantically saying that a * 2 is equivalent to a &lt;&lt; 1. Note the Hash consing on the nodes to prevent duplication.\n\nFinally, we can pick one ‘best’ e-node in each e-class. This is called ‘extracting’ the optimized term."},"thoughts/Evolutionary-game-theory":{"title":"Evolutionary game theory","links":["thoughts/positive-sum","thoughts/zero-sum","thoughts/game-theory","thoughts/ethics","thoughts/rationality","thoughts/Social-Contract-Theory"],"tags":["seed","PHIL321A"],"content":"Focuses on the dynamics of strategy change by agents over time.\nEvolutionary game theory has helped to explain the basis of altruistic behaviours in Darwinian evolution. It is salient to note that the evolutionary pressures on what we consider moral behaviour arise only in positive sum interactions. In a dynamic, growing society, people can interact cooperatively and both come out ahead.\nIn a static, zero sum society, people can pressures toward morality and cooperation vanish: You can only improve your situation by taking from someone else.\nEvolutionarily Stable Strategy (ESS)\nCompute the fitness of each group. Watch how each group fares in indefinitely repeated interactions. Fitness of a strategy is Fitness(S)=Avg EUEU(S)​\nAn evolutionarily stable strategy is one that, once fixed in a population, has higher fitness than any other available strategy\nIn formal terms, x is an ESS iff\n\nEU(x,x)≥EU(y,y) for any strategy y\nEU(x,x)&gt;EU(y,x) or EU(x,y)&gt;EU(y,y) for any y\n\nCake Division Problem\nWhereas rational choice theory has multiple pure equilibrium strategies here, there is only one evolutionarily stable pure strategy: share equally\nPolymorphic state\nDifferent strategies are played by different proportions of the population; however, these equilibria are not commonly found\nSequential Rationality and Trembling Hand\nA plan involving a sequence of choices exhibits sequential rationality if it specifies a rational (i.e., utility-maximizing) choice at each point, relative to the choice situation at that point.\nTrembling Hand: sequential rationality is necessary and sufficient for optimality if we allow a small probability of error by other players. In picking a strategy, we should take “mistakes” into account — errors that occasionally lead an opponent to choose a dominated strategy\nEvolutionary analogue: mutation and recombination are analogues of the trembling hand. Both allow for new strategies to emerge\nReplicator Dynamics\n\nCompute expected payoff (fitness) for each strategy. A weighted average of payoff against each other strategy in play: U(A)=u(A vs A1​)p(A1​)+⋯+u(A vs An​)p(An​)\nCompute the average fitness of the whole population, U. This is the weighted average of the individual fitnesses: U=u(A1​)p(A1​)+⋯+u(An​)p(An​)\nCompute the relative fitness of each strategy. Relative fitness of A is just U(A)/U\nCompute the new proportion of the population using each strategy (for the next round). p′(A)=p(A)⋅[U(A)/U]\n\nEthics\nCan game theory give us substantive ethical/political recommendations?\nGauthier’s Theory\n\nThe Nash bargaining solution provides a solution to the problem of fair division, based entirely on assumptions about rationality.\n\nObjection: Hume’s Principle, we can’t derive what we ought to do from what is true\n\n\nInclude a ‘bridge-premise’ that whatever a rational group of people agree upon should be implemented (similar to Rawl’s Theory of Justice)\n\nObjection: Exploitation may be beneficial… but is it moral\n\n\n"},"thoughts/Extended-Mind-Hypothesis":{"title":"Extended Mind Hypothesis","links":["thoughts/Theory-of-Niche-Construction","thoughts/epistemology","thoughts/generational-learning","thoughts/trust","thoughts/language"],"tags":["sapling"],"content":"Extended view of the mind\nNot an internal control system, enclosed in the human body, receiving data from human sensory system and directing human action.\nInstead, the mind is a systems that extends far beyond the body of the human organism, systems that include extra-somatic resources: environmental fuels for adaptive action\nIt suggests that human cognitive systems include those resources that are importantly, robustly, reliably, or persistently supportive of decisions making\nEnvironmentally Supported Cognition\nDerives from the Theory of Niche Construction, helps to emphasize the active role of the agent in explaining the adaptive fit of agent and environment\nOver time, agents adapt to environments but also adapt their environment to them\n\n“Animals construct nests, burrows and dams, thus protecting themselves from predators and from the violence of the world.”\n\nEpistemic action is a form of niche construction too. For example, ants lay scent trails between nest and food source. Humans also partake in generational learning\nAgential Gullibility\nIn which we too readily bolt external processes onto our own agency. To rephrase, to rely too heavily on the external faculties and trust in them blindly and fully.\nOtto — the man who lost his memory\nClark and Chalmers (extended mind) argued that information in his notebook should be amongst Otto’s memories\nParity principle: if an external resource plays the same functional role in supporting action as an action-supporting internal resource that is uncontroversially cognitive, then the external resource is part of the cognitive system of the agent\nHowever, there is a functional difference between Otto’s notebook and internally represented information.\nExternal representations are:\n\nsubject to interference from other parties and manipulation\nonly accessed via other intentional states (one must believe that the book contains memories that you’ve written before, for example)\n\nAnd thus, they can’t replace internal, embodied wants (e.g. relational and sexual preferences). The notebook might be a prompt or a cue, but can’t replace motivation and desires\nEnvironmental fuels for cognition — three dimensions\n\nTrust\n\nreliability of their access to a resource and the reliability of the resource itself\nthe more agents trust a resource, the less they will see themselves needing redundancy against failure\nOtto’s competitors have the opportunity to steal his notebook, erase passages in it and add deceptively to it. If Otto is rational, he will be aware of such a danger and will be wary of committing himself to a high-stakes action on the basis of his notebooked beliefs alone\n\n\nInterchangeability, individualization, entrenchement\n\nexample of stick for a blind person, extension of their hand and for them, phenomenologically the interface between body and world is at the end of their stick rather than at the end of their hand\nstick is individualized (custom weight, balance, length, etc.) and entrenched (switch it out, they won’t be used to it)\ncan apply to cognitive resources like book too\n\nmost books are interchangeable (standard books) but some are heavily individualized (long marginalia, etc.)\nnone are entrenched → no single work is sufficiently salient, they dont read them enough that they adapt to their resource\n\n\n\n\nThe individual and the collective\n\ndistinction between individual and collective resources\n\ncollective resources have distinct individual and intergeneration dynamics\nlanguage have almost certainly transformed the internal processes of human minds\nWe adapt the expressive powers of language to our own purposes, but no doubt we have also adapted to it (not to any one individual, but to society as a collective)\n\n\n\n\n"},"thoughts/FLP-Result":{"title":"FLP Result","links":["thoughts/system-model","thoughts/CAP-Theorem","thoughts/safety","thoughts/consistency","thoughts/liveness","thoughts/HoneyBadgerBFT","thoughts/longest-chain-consensus"],"tags":["seed"],"content":"Consensus in distributed systems cannot be asynchronous due to the FLP Result: there is no deterministic consensus algorithm that is guaranteed to terminate in an asynchronous crash-stop system model\nThis holds even if f=1\nSimilar to tradeoffs made in the CAP Theorem, when under attack, we need to choose between\n\nsafety\nconsistency\nliveness/availability\n\nThis can somewhat be abated by randomized protocols (see: HoneyBadgerBFT, Nakamoto consensus)"},"thoughts/FOAF":{"title":"FOAF","links":["thoughts/hypertext","thoughts/RDF"],"tags":["seed"],"content":"Summarized from the xmlns FOAF specs\nFOAF stands for ‘friend of a friend’\n\nFOAF is a project devoted to linking people and information using the Web… If people publish information in the FOAF document format, machines will be able to make use of that information. If those files contain “see also” references to other such documents in the Web, we will have a machine-friendly version of today’s hypertext Web.\n\nIt is a way of creating a semantically meaningful network of objects, useful for enabling the Semantic Web.\n\nFOAF descriptions are published as linked documents in the Web\nThe result of the FOAF project is a network of documents describing a network of people (and other stuff).\n\nExample FOAF describing a person (using RDF)\n&lt;foaf:Person rdf:about=&quot;#danbri&quot; xmlns:foaf=&quot;http://xmlns.com/foaf/0.1/&quot;&gt;\n  &lt;foaf:name&gt;Dan Brickley&lt;/foaf:name&gt;\n  &lt;foaf:homepage rdf:resource=&quot;http://danbri.org/&quot; /&gt;\n  &lt;foaf:openid rdf:resource=&quot;http://danbri.org/&quot; /&gt;\n  &lt;foaf:img rdf:resource=&quot;/images/me.jpg&quot; /&gt;\n&lt;/foaf:Person&gt;\nIt basically says, “there is a foaf:Person with a foaf:name property of ‘Dan Brickley’; this person stands in foaf:homepage and foaf:openid relationship to a thing called http://danbri.org/ and a foaf:img relationship to a thing referenced by a relative URI of /images/me.jpg\nAdoption\nSource\n\npeople will only add semantic markup to their web pages if doing so is easier than not.\n\nNow imagine this world for a second:\n\nI want to insert a book into my blog post\nI type /book\nA search box appears where I start typing in the title of my book and choose from an autocomplete list.\nOnce I find the book, a block gets inserted in my blog post showing details of the book in a format I like, with nice semantic markup behind the scenes.\n"},"thoughts/Farcaster":{"title":"Farcaster","links":["thoughts/DID","thoughts/git"],"tags":["seed"],"content":"\nFarcaster is a sufficiently decentralized social network.\n\nTwo main components:\n\nOn-chain registry for identity registration (like DID VDRs). Table of username, address, and host_url\nOff-chain hosts where users store social data\n\nDistributed Host Architecture\nFarcaster allows users to host their content on any web server as long as they sign everything with their private key.\nThere are two options for hosting: self-hosting and using a managed host (like Gmail does for email and Github does for Git)."},"thoughts/Filecoin":{"title":"Filecoin","links":["thoughts/decentralized-marketplace","thoughts/Kademlia-DHT","thoughts/clocks","thoughts/Network-Time-Protocol","thoughts/gossip"],"tags":["seed"],"content":"\nA decentralized storage network\n\nSummarized from Filecoin Specs\nEssentially a decentralized marketplace (see: storage market) with storage providers and storage users. Providers advertise space and cost and client selects winning storage provider (creating competition). Provider stores the content and is paid with Filecoin on an ongoing basis as long as they can prove they are storing the data properly.\nFilecoin Blockchain + VM\nThe majority of Filecoin’s user facing functionality (payments, storage market, power table, etc) is managed through the Filecoin Virtual Machine (Filecoin VM). The network generates a series of blocks, and agrees which ‘chain’ of blocks is the correct one. Each block contains a series of state transitions called messages, and a checkpoint of the current global state after the application of those messages.\nNode Types\nAny node participating in the Filecoin network should provide the chain verification service as a minimum. Depending on which extra services a node provides on top of chain verification, it gets the corresponding functionality and Node Type “label”.\nNetworking\nMostly reuses existing libp2p library bits\n\nGraphsync: used for syncing metadata and blockchain data\nGossipsub: propagating block headers + messages\nKademlia DHT: peer discovery + peer routing\nBootstrap list: list of nodes to connect to upon joining the network, bootstrap nodes and their addresses are defined by the users (i.e. applications)\n\nClocks and Time\nSee also: clocks\nUses the concept of epochs where epoch=⌊epoch time(current time−genesis time)​⌋\nClocks used as part of the Filecoin protocol should be kept in sync, with offset less than 1 second so as to enable appropriate validation. Nodes SHOULD run an NTP daemon (e.g. timesyncd, ntpd, chronyd) to keep their clocks synchronized to one or more reliable external references.\nNodes have strong incentive to prevent their clock skewing ahead more than one epoch to keep their block submissions from being rejected. Similarly have a strong incentive to prevent their clocks skewing behind more than one epoch to avoid partitioning themselves off from the synchronized nodes in the network.\nAlgorithms\nProof of Storage\nThe proof that a storage miner indeed keeps a copy of the data they have promised to store is achieved through “challenges”, that is, by providing answers to specific questions posed by the system.\nChallenge properties:\n\ntarget a random part of the data and\nbe requested at a time interval such that it is not possible, profitable, or rational for the miner to discard the copy of data and refetch it when challenged.\n\nTwo components\n\nProof of replication (PoRep): extends the basic concept of proof-of-retrievability by proving that multiple copies of the data are stored\nProof of spacetime (PoSt): extends PoRep by proving that replicas are stored for a given period of time. It involves a series of PoReps\n\nWinningPoSt: The answer to the WinningPoSt challenge has to be submitted within a short deadline, making it impossible for the miner to seal and find the answer to the challenge on demand. This guarantees that at the time of the challenge the miner maintains a copy of the data.\nWindowPoSt: This involves submitting proofs regularly (see details below) and makes it irrational for a miner to not keep a sealed copy of the data as it is more expensive to seal a copy of the data every time they are asked to submit a WindowPoSt challenge.\n\n\n\nThe sectors a miner has pledged to store, the more the partitions of sectors that the miner will need to prove per deadline. This requires ready access to sealed copies of each of the challenged sectors and makes it irrational for the miner to seal data every time they need to provide a WindowPoSt proof. If this proof is not completed in time, the storage miner supplying that sector in the proof has their collateral slashed and storage power reduced.\nGossipPub\nGossipSub is a gossip-based pubsub protocol that is utilising two types of links to propagate messages:\n\n\nmesh links that carry full messages in an eager-push (i.e., proactive send) manner and\n\n\ngossip-links that carry message identifiers only and realise a lazy-pull (i.e., reactive request) propagation model.\nIn gossip propagation, only message headers are sent to inform them of messages that they might not have received before. Nodes then ask for the full message, hence, realizing a reactive request, or “lazy pull” model.\n\n"},"thoughts/Fishbowl-effect":{"title":"Fishbowl effect","links":["thoughts/collaborative-software"],"tags":["seed","pattern"],"content":"Jonathan Zdziarski (aka @NerveGas) on user demands: “There is definitely a place for users and their demands, however that’s not inside the community (unless they’re also contributing devs); the community, as in practicing any art form, is vulnerable; you wouldn’t sit and criticize a painter while they’re still painting their piece. The user base needs to be moved outside of the artistic realm and into the museum, where you software is on display.”\nReal-time collaboration like in Google Docs creates stress as writers feel watched by their co-authors\n\n“Writers don’t want first drafts visible to the editor.” — Journalist\n\nWriters often need to initially ideate or experiment with new ideas in private, and then share the new material with their collaborators when they are ready to do so.\nSupport diverge-converge workflows!\nSee also: A spectrum"},"thoughts/Franz-Kafka":{"title":"Franz Kafka","links":[],"tags":["seed"],"content":"Letter to his father\nExcerpts from Maria Popova in The Marginalian\n\nThe anguish resulting from a disparity of temperaments coupled with a disparity of power between parent and child is familiar to all who have lived through a similar childhood — the constantly enforced, with varying degrees of force, sense that the parent’s version of reality is always right simply by virtue of authority and the child’s always wrong by virtue of submission, and thus the child comes to internalize the chronic guilt of wrongness. \nA heartbreaking effect of these disorienting double standards is that we as children grows utterly confused about right and wrong, for they seem to trade places constantly depending on who the doer is, and comes to internalize the notion that he or she is always at fault.\nThe most devastating pathology of such relationships is the child’s compulsive effort — be it by vain hope or by concrete action — to eradicate the abusive parent’s demons and make the paltry angels endure, only to be disappointed over and over again every time the demons re-rear their undying heads.\nQuotes\n“you do charge me with coldness, estrangement, and ingratitude. And, what is more, you charge me with it in such a way as to make it seem my fault, as though I might have been able, with something like a touch on the steering wheel, to make everything quite different, while you aren’t in the slightest to blame, unless it be for having been too good to me.”\n“You can only treat a child in the way you yourself are constituted, with vigor, noise, and hot temper, and in this case this seemed to you, into the bargain, extremely suitable, because you wanted to bring me up to be a strong brave boy.”\n“I was continually in disgrace; either I obeyed your orders, and that was a disgrace, for they applied, after all, only to me; or I was defiant, and that was a disgrace too, for how could I presume to defy you; or I could not obey because I did not, for instance, have your strength, your appetite, your skill, although you expected it of me as a matter of course; this was the greatest disgrace of all.”\n“Now you are, after all, at bottom a kindly and softhearted person (what follows will not be in contradiction to this, I am speaking only of the impression you made on the child), but not every child has the endurance and fearlessness to go on searching until it comes to the kindliness that lies beneath the surface. You can only treat a child in the way you yourself are constituted, with vigor, noise, and hot temper, and in this case this seemed to you, into the bargain, extremely suitable, because you wanted to bring me up to be a strong brave boy.” \n“Hence the world was for me divided into three parts: one in which I, the slave, lived under laws that had been invented only for me and which I could, I did not know why, never completely comply with; then a second world, which was infinitely remote from mine, in which you lived, concerned with government, with the issuing of orders and with the annoyance about their not being obeyed; and finally a third world where everybody else lived happily and free from orders and from having to obey” \n“It is also true that you hardly ever really gave me a whipping. But the shouting, the way your face got red, the hasty undoing of the braces and laying them ready over the back of the chair, all that was almost worse for me. It is as if someone is going to be hanged. If he really is hanged, then he is dead and it is all over. But if he has to go through all the preliminaries to being hanged and he learns of his reprieve only when the noose is dangling before his face, he may suffer from it all his life. Besides, from the many occasions on which I had, according to your clearly expressed opinion, deserved a whipping but was let off at the last moment by your grace, I again accumulated only a huge sense of guilt. On every side I was to blame, I was in your debt.”"},"thoughts/From-Counterculture-to-Cyberculture":{"title":"From Counterculture to Cyberculture","links":["thoughts/counterculture","thoughts/tribe-flourishing","thoughts/Internet","thoughts/boundary-object","thoughts/computability","thoughts/research-institutions","posts/collaborative-thinking","thoughts/peer-to-peer"],"tags":["seed","book"],"content":"See also: counterculture\nQuotes\nDigital Communalism\nSee: tribe flourishing\n“Ubiquitous networked computing had arrived, and in its shiny array of interlinked devices, pundits, scholars, and investors alike saw the image of an ideal society: decentralized, egalitarian, harmonious, and free”\n“In The Gutenberg Galaxy McLuhan described the new age in tribal terms: electronic media had linked all of humanity into a single ‘global vilage’”\n“Nor does the fact that individuals can come together by means of computer networks necessarily require that their gatherings become ‘virtual communities’”\n“Dyson and Barlow, as well as many other commentators at the time, saw the Internet serving as a rhetorical prototype for new, flexible, and mobile ways of working and living.”\nTogether, the Catalog and the Supplement became textual forums within which a geographically dispersed collection of individuals and groups could come together, in text and sometimes pictures, and recognize each other as members of a single community. In a sense, Catalog and Supplement became town squares.\nComprehensive Designers\nContact Language which to exchange ideas and techniques in linguistically distinct tribes (e.g. scientists, technologists, and administrators)\n“The Comprehensive Designer not only did not need to don a gray flannel suit when he went to work; he actually needed to become an artist and intellectual migrant. To a generation preoccupied with the fear of becoming lockstep corporate adults on the military model of Brand’s imagined Soviet Army, Buckminster Fuller offered a marvelously playful alternative”\n“How you get energy is, you take polarities and slap them next to one another. If you get into cybernetics and your head is just a minute ago full of organic gardening and ecology, then cybernetics starts to come alive for you in a different way.”\nNetwork Forums\nNetwork Forum — a place where members of different communities come together, exchanging ideas and legitimacy, and in the process, synthesizing new intellectual frameworks and new social networks.\nNetwork entrepreneur: one who migrates from one intellectual community to another and, in the process, to knit together formerly separate intellectual and social networks.\n“We thought of the WEC as a print version of what the Internet was going to be”\nComputational Metaphor\nOn the computational metaphor (seeing everything as computable)\n“We are compiling a vocabulary and a syntax that is able to describe in a single language all kinds of phenomenon that have escaped a common language until now. It is a new universal metaphor. It has more juice in it than previous metaphors: Freud’s dream state, Darwin’s variety, Marx’s progress, or the Age of Aquarius. And it has more power than anything else in science at the moment. In fact the computational metaphor may eclipse mathematics as a form of universal notation.”\n“Society as a whole, as well as its constituent organizational parts, functioned much like organisms and machines.”\n“The principles governing the world of the soft — the world of intangibles, of media, of software, and of services — will soon command the world of the hard — the world of reality, of atoms, of objects, of steel and oil, and the hard work done by the sweat of brows.” — New Rules for the New Economy\nTechnocracy, technostructure, or technological society: society’s rapid process of centralization and rationalization as both supported by new technologies and designed to help build them.\nSanta Fe Institute: founded in 1984 by a group of scientists who had come to believe that since WWII, the biological, physical, and social sciences had begun to converge. Computers, they argued, had made this convergence possible in two ways: first, they had served as tools for examining and modeling the world, and, second, the algorithms with which they organized information mimicked the algorithmic patterning of life itself by means of biological “technologies” such as DNA.\nNeocolonialism\n“[The hippie’s] arrival tapped into memories of very old patterns of colonization and migration. A chicano member of New Mexico’s Reality Construction Company commune told a visiting reporter, ‘every time a white hippie comes in a buys a Chicano’s land to escape the fuckin’ city, he sends that Chicano to the city to go through what he’s trying to escape from, can you dig it?… Then when that money’s gone, see, the Chicano has to stay in the city, cause now he ain’t got no land to come back to.”\nPower Hierarchies and Individualism\n“Brand suggests that top-down politics (i.e. the kind where Mr. Advantage tells Mr. Disadvantage what to do) is bankrupt. The center of change must be the individual, acting with other likeminded individuals. This emphasis on local action echoes the notion of the individual’s local role in maintaining universal systems.”\nWEC seems to promote an incredibly individualistic approach; Indians must work with Indians, the Third World with the Third World, blacks with blacks, and so on. No group should count on help from any other… Such segregation might seem to conflict with the WEC’s celebration of ‘whole’ systems. This seems to be an artifact of the fact that all members of the WEC were white and relatively young, with a high level of education and easy access to social and financial resources.\n“The great machines of empire had been miniturized and turned over to individuals, and so transformed into tools with which individuals could improve their own lives.”\n“As a variety of economic sociologists have noted, the mid-1980s saw hierarchical firms in many industries and several nations reorganize themselves as project oriented networks. They laid off workers, broke component elements of firms into semi-independent project teams, and decentralized their management structure.”\n“Although the hive had a queen, he pointed out it was governed by the rule-driven behaviour of its many members. In the hive one could see ‘the true democracy and all distributed governance’. One could also see the faded image of New Communalism. Leveled, collaborative, linked by invisible signals and shared feelings, Kelly’s hive was a sort of natural commune.” (more on collaborative-thinking)\n“Together, Wired suggested, this digital generation would do what the New Communalists had failed to accomplish: they would tear down hierarchies, undermine the sorts of corporations and governments that had spawned them, and, in the hierarchies’ place, create a peer-to-peer, collaborative society, interlinked by invisible currents of energy and information.”\n“The urge to ‘hack’ politics by bringing governance down to a mangeable local level and by basing social integration on technologically facilitated forms of consciousness was one of the driving impulses behind the New Communalist movement”\n“In many industries today, and in some parts of military and academic life as well, hierarchies have been replaced by flattened structures, long-term employment by short-term, project-based contracting, and professional positions by complex, networked forms of sociability.”\nMedia Labs\n“The sponsors were not allowed to demand that any particular research be done on their behalf. Rather, they were buying permission to watch as [they built]”\n“Media Lab personnel were never required to produce artifiacts that could be mass-produced or that would feed directly into sponsors’ lines of business per se. Instead, they were expected to produce ‘demos.‘”\nSee also: research institutions\nMisc\n“I always thought tools were objects, things: screw drivers, wrenches, axes, hoes. Now I realize that tools are a process: using the right-sized and shaped object in the most effective way to get a job done.”\nOn CompuServe and elsewhere, developers largely treated information as a commodity to be exchanged and users as consumers of information goods\n“Biological life does not want to keep speeding up like a chip design, cycling ever faster year by year.”\n“Behind the fantasy of unimpeded information flow lies the reality of millions of plastic keyboards, silicon wafers, glass-faced monitors, and endless miles of cable. All of these technologies depend on manual laborers, first to build them and later to tear them apart.”\nReboot Discussion\n\nSteward Brand in 1984 as an Apple Mac, create a society of consciousness\n\nMac (as a metaphor for tech) is the new LSD: ability to remove the importance of the physical self\n\n\n\nHow much of the shift was a result of WEC? How much you see importance of media / social networking vs technical revolution?\n\nWEC was not a marketplace, rather an aggregator of information (a pre-Google search engine)\nWhat tools do you want to bring to a commune (ex: farming commune)\n\nFood, housing, basics\n\n\nWhy would you need a calculator and a book on cybernetics on a farm?\n\nTools which allow you to fundamentally understand the world in a different way\n\n\nCatalogue provides the infrastructure → leads to conferences which lead to development of more infrastructure\nNetwork Entrepreneur sees several decentralized groups that aren’t connected and services as a connector\n\nConnection between WEC and WELL\n\npicture yourself as a hippie in the 1980’s, communes will save the world and end warmaking America\n\nCalifornia is tech driven, everything is failing\n\n\nElectronic Frontier → empty society/land of consciousness\nLarry Bird, most influential people in silicon valley\n"},"thoughts/GDPR":{"title":"GDPR","links":[],"tags":["seed"],"content":"From Antonio García Martínez in The right to never be forgotten\nTwo foundational concepts in GDPR:\n\npresence and persistence of personal data\nwho keeps that data around (further distinguished into two classes of data holders)\n\ncontrollers (first-party). liability falls on this party to get end-user opt-in and face consequences if found in violation\nprocessers (third-party)\n\n\n\nPersonal data is “an identifier such as a name, an identification number, location data, an online identifier or to one or more factors specific to the physical, physiological, genetic, mental, economic, cultural or social identity of that natural person.” In other words, it’s things that can be traced unambiguously to the real you"},"thoughts/GLSL":{"title":"GLSL","links":[],"tags":["seed"],"content":"Types of variables:\n\nuniform: constant across all vertices (normally textures)\nattribute: per vertex value (normally positions, normals, UVs)\nvarying: per pixel (fragment) value (normally colours, UV coordinates)\n\nSpecial variables shaders can write to:\n\nVertex shader:\n\ngl_Position\n\n\nFragment shader:\n\ngl_FragColor\ngl_FragDepth\n\n\n"},"thoughts/GOFAI":{"title":"GOFAI","links":["thoughts/symbolic-system","thoughts/semantics","thoughts/potemkin-village"],"tags":["seed"],"content":"Historically, development of AI has been about systems that represent the world through symbols and manipulate those tokens in a systematic way to arrive at a result. Within these GOFAI systems, symbols are representative of aspects of our world. That is, it is a symbolic system. This type of AI was coined Good Old-Fashioned AI (GOFAI) by John Haugeland.\nA very common example of GOFAI systems are expert systems, which are computer systems that emulate the decision making ability of a human expert. They solve problems via decision-tree reasoning, figuring out whether to perform certain actions based off of if-then rules.\nAt its core, GOFAI can be considered ‘artificially intelligent’ because of semantic interpretation. If the symbols represent aspects of our world, the result, which is also a symbol sequence, can be translated back into aspects of our world. This is called semantic interpretation, which “seeks to construe a body of symbols so that what they mean (‘say’) turns out to be consistently reasonable and sensible, given the situation” (see semantics)\nHowever, because of how symbols map to the world, GOFAI is very narrow-minded and vulnerable to unexpected variations and oddities in the problems and information they were given. That is, the potemkin village that a GOFAI system may construct will hold up if only seen from the intended angles, but any slight deviation from an intended or expected input would shatter the illusion immediately."},"thoughts/Gall's-law":{"title":"Gall's law","links":["thoughts/complexity"],"tags":["seed","pattern"],"content":"\n“One day I will find the right words, and they will be simple.” ― Jack Kerouac, The Dharma Bums\n\nGall’s Law\nA rule of thumb for complex system design: simple alphabets produce complex behaviors, complex alphabets produce stupid behaviors\n\nA complex system that works is invariably found to have evolved from a simple system that worked. A complex system designed from scratch never works and cannot be patched up to make it work. You have to start over with a working simple system**\n\nGall-Meadows Ladder\nAdewale Oshineye coins the concept of a Gall-Meadows ladder\n\nSuccess needs what I think of as a Gall-Meadows ladder: a mechanism for jumping from one stable system to another … in order to evolve your project\nThe Gall-Meadows ladder suggests that to get from a simple working system to a complex working system you have to find a sequence of working systems that increase the level of complexity.\n"},"thoughts/Games-Agency-as-Art":{"title":"Games: Agency as Art","links":["thoughts/games","thoughts/The-Grasshopper,-Games,-Life-and-Utopia","thoughts/social-contracts","thoughts/game-design","thoughts/self-effacing-ends","thoughts/Seeing-like-a-State","thoughts/quantization","thoughts/information","thoughts/Goodhart's-Law"],"tags":["seed","book"],"content":"See also: games, The Grasshopper, Games, Life and Utopia\nBook\nBook, written by C. Thi Nguyen\n\nPainting lets us record sights, music lets us record sounds, stories let us record narratives and games let us record agencies\n\n\nAchievement play: pursuing winning for the sake of winning or the sake of something that follows from winning such as fame, goods, or money\nStriving play: pursuing winning for the sake of the struggle and the intrinsic act of playing the game itself.\n\nTwo clear parts:\n\nIn order to engage ourselves in striving play, I must be able to take on a disposable end that is treated as final. It builds the capacity to submerge ourselves in narrowed agential modes.\n\nBeing unable to do this leads to the diffident player, who can’t bring themselves to care about the game “What’s the point? It’s just a game”\n\n\nI must be able to bring myself to temporarily care about an end, and for that end to appear to me as final. But I must also be able to dispose of that end afterwards. This disposal helps build the capacity to step back and reflect on the value of these narrower states from a wider, less artificially clarified perspective. This turns out to be protective against the stickiness of narrowed agential modes and ends\n\n\nA successful striving player is able to do both of the above\nSuits calls this the lusory attitude\nStenros builds on Salen and Zimmerman’s ‘magic circle’, arguing that this game state is an explicitly negotiated social contract — an agreement to treat the in-game events as separated from the world\n\n\nAesthetics of games\n\nClimbers praise particular climbs for having interesting movement or beautiful flow\nWe can justify our pursuit of an arbitrary-seeming goal in terms of the aesthetic value of that struggle.\nAesthetics of harmony: 3 levels\n\nHarmony of solution: strictly a harmony between the solution and the obstacle. e.g. wow, what a brilliant and beautiful Chess move\nHarmony of action: your agency and action fitting the demands of the environment. e.g. during a difficult climb, figuring out you need to slide your hips over just enough to balance on a tiny foothold\n\nThis is a strict superset of the harmony of solution. It concerns not only how the solution fits the problem, but how my decision making and action generation were just right to generate the harmony of solution\nEffectively ‘flow state’\n\n\nHarmony of capacity: the experience of engaging your abilities to their fullest potential. Arises from a fit between one’s maximum skill level (their limit) and the demands of the task\n\nWe want to hit this in game design as much as possible\n\n\n\n\n\n\nCrafting Agency\n\nA game designer crafts for players a very particular form of struggle, and does so by crafting both a temporary practical agency for us to inhabit and a practical environment for us to struggle against. Games are the art of agency\nWhatever is created has to be open, flexible, and malleable to allow players to appropriate, express, act and interact, make and become part of the form itself (Sicart 2014)\nGames are as interesting as their constraints. On a smaller scale, restrictions can actually help constitute entirely new actions. The action of “making a basket” in basketball is only meaningful because of the constraint of the game of basketball\nIts important to consider the “ludic loop” when designing games. A continuous stream of gentle challenges and in-game rewards, offered at the right pacing and tempo, seems to produce something of an addictive response\n\n\nAgential Fluidity\n\nWe need to have final ends to avoid being bored. However, having final ends is no insurance against being bored. Instead, to remain interested in the world, we must be able to fluidly be able to change our interests\nI acquire my ends from my experience of value in an activity\nGame playing is a way to practical agential fluidity. Game playing builds familiarity with different agential modes — to help us build our inventory and know which one to pic — and the fluidity to shift easily in and out of our chosen mode\n\n\nself-effacing ends\nThe value contradiction that makes games valuable\n\nWhen we succeed in games, we treat them as normal contexts in which success matters. But when we fail at games, we treat them as deflated contexts, telling ourselves that success and failure in games doesn’t really matter anyway (Juul, 2013)\n\n\nLearning new modes of agency\n\nShouldn’t we develop autonomy on our own? How can games help with this?\nThe counter argument is that any genuinely plausible view of autonomy and freedom must make room for the fact that we learn from others and using a variety of techniques at that\nLearning from others involves temporarily giving up our own agencies in a controlled and consensual way. When I take an art class, I put my attention in another’s hands for a while. I look where they tell me to look, attend to the features they tell me to.\nSimilarly, games are a temporary constriction of our own agencies leads us to develop more flexible agencies in the future\nRigidity in the short term is sometimes crucial for flexibility in the long term.\n\n\nAgential Distance\n\nThe gap that the game designer explicitly leaves for the player to occupy. This gap is shaped by the rules/constraints of the game\nCreative/sandbox games like Minecraft have a big agential distance whereas strict games like osu! have very little agential distance.\n\n\nGamification and Value Capture\n\nA la Seeing like a State, metrics arose from the bureaucratic need to collate information. They also foster game-like motivations. They look a lot like points! But if we are too eager to recapture the pleasures of games in ordinary life, we may be excessively drawn to using such simplified measured in our practical reasoning.\nThis is the danger of exporting back to the world a false expectation: that values should be clear, well-delineated, and uniform in all circumstances. Games threaten us with a fantasy of moral clarity.\nThe right values may not be the clearest values. The false clarity of values that games provide may seduce us into oversimplifying our own values\nConsider a phenomenon I call value capture\n\nOur values are, at first, rich and subtle (e.g. we value the happiness of a country’s citizens)\nWe encounter simplified (often quantified) versions of those values (e.g. we use GDP as a measure of its capacity to satisfy its citizen’s desires)\nThose simplified versions take the place of our richer values in our reasoning and motivation (e.g. we begin valuing GDP itself and try to increase it in whatever way we can)\nOur lives get worse\n\n\nAgain, abstractions and simplifcations are not necessarily a bad thing\n\nThe arts, Dewey suggests, reach into the welter of practical life and create crystallized versions of practical experience. The arts create little unities. The value clarity and harmonious agency of game life is, in a sense, no worse than the unnatural harmoniousness of music, or the narrative clarity and unity of fictions. But value clarity becomes problematic when we export a need for it outside the game.\n\n\n\n\n\nPodcast\nOn The Ezra Klein Show: A Philosophy of Games that is really a Philosophy of Life with C. Thi Nguyen\nQuantization\n\n“The most important thing in my game designer toolbox is the point system because the point system tells the players what to care about.” — Reiner Knizia\n“Quantified measures are extremely good tools for large-scale bureaucracies to organize themselves”\nMetrics and quantities carve out all of the subtle nuance and all the weird little information that needs a lot of shared context to understand so that it can travel and be transported between contexts and let it aggregate easily\n\n“So if you have large-scale bureaucracies that need to be organized and function coherently, then you need these kind of simple, nuance-free packets of information”\n\n\n“if you spend your life playing games, you’ll expect that value systems will be crisp, clear, well-defined, and quantified. And then you leave games, you’ll start looking around for— I don’t know— things to do, or institutions to be a part of, or jobs to do where the outcomes are clear, crystallized, quantified, and shared between people”\n\nOn social media\n\nA lot of times people click like because something made them laugh for a second, not because it moved them two weeks later.\n\nAesthetics\n\nObject aesthetics: when an artist makes a thing like a painting. And you look at the thing and the thing is beautiful\nProcess aesthetics: the artist makes a thing and you interact with the thing and you’re beautiful. Your actions are beautiful, or comic, or thrilling. (I think games fall under this later category)\n\nGames are the crystallization of doing\n\nThe visual arts are the crystallization of seeing… music is a crystallization of hearing… fiction is a crystallization of story telling… games are the crystallization of doing.\n\nThe game designer tells you what abilities you have and what obstacles you’ll face, but most importantly, what goals you’ll have: games are the art form that works in the medium of agency itself\n\n\nFollows Bernard’s definition of a game in The Grasshopper, Games, Life and Utopia: to play a game is to voluntarily take on unnecessary obstacles for the sake of making possible the activity of overcoming them\n\nAgency\n\nI feel like games are like an existential balm for the horror of life. A lot of life is you don’t fit. You have to do things. And it sucks and it’s horrible and it’s boring. And in games, for once in your life, you know exactly what you’re doing and you know exactly that you can do it. And then you have just the right amount of ability to do it. It’s a feeling of concentrated, crystallized action\nGame designers have sculpted these little action universes so that we can step into them and just have this ecstasy over and over again.\n“I’m more worried about games breeding more Wall Street profiteers than I am about their breeding serial killers.”\n\nThe greatest power of games is that you can explore this landscape of different agencies. The greatest danger of games is that you can get sucked into this experience of just craving and wanting to be in a clear, crisp and gentle universe where you know exactly what to do and exactly how well it’s measured (similar concerns to Goodhart’s Law).\nSo I think that the body of games is a kind of library of agencies. The real promise of games, if you take them seriously, is that by playing a ton of them, you can traverse all the different possibilities of agency.\n\n\n\nMisc\n\n“‘Train’ … looks like, in many ways, a standard European board game. You’re building a railway network. You’re trying to optimize it. And over time the game reveals to you that what you’re actually doing is it’s Nazi Germany and you’re building the railway network to move people to concentration camps.”\nhttps://buriedwithoutceremony.com/the-quiet-year\n"},"thoughts/Gaussian-RBF":{"title":"Gaussian RBFs","links":[],"tags":["seed","CPSC340"],"content":"Non-parametric basis. Can think about it as a sum of gaussian ‘bumps’.\nReplace xi​=(xi1​,xi2​,…,xin​) with\nzi​=n features(g(∥xi​−x1​∥),g(∥xi​−x2​∥),…,g(∥xi​−xn​∥))​​\nwhere g(x)=exp(−2σ2x2​)\nGaussian RBFs are universal approximators\n\nEnough bumps can approximate any continuous function to arbitrary precision.\nAchieve optimal test error as ‘n’ goes to infinity.\n"},"thoughts/Genetic-Selection":{"title":"Genetic Selection","links":[],"tags":["seed"],"content":"How might we do gene editing without getting dangerously close to eugenics?\nHeinlein Solution\nWhen a couple goes to a genetic specialist to have an improved baby, they are allowed to select among the genes they bear but not introduce new ones, so their baby is one that could haver possibly had naturally."},"thoughts/Goodhart's-Law":{"title":"Goodhart's Law","links":["thoughts/feedback-loops","thoughts/paperclip-optimizer"],"tags":["seed","pattern"],"content":"\n“When a measure becomes a target, it ceases to be a good measure” — [Goodhart’s Law]\n\n“When quantifying things, people naturally focus on things that can easily be measured. Measuring the final result doesn’t provide enough quantitative data, so it’s tempting to include the data from intermediate steps. This is an attempt to shorten the feedback loop, and trying to shorten feedback loops is very dangerous in complex systems.”\nSee also: paperclip optimizer"},"thoughts/Gordian-Knot":{"title":"Gordian Knot","links":[],"tags":["seed"],"content":"It is often used as a metaphor for an intractable problem (untying an impossibly tangled knot) solved easily by finding an approach to the problem that renders the perceived constraints of the problem moot"},"thoughts/HTTP":{"title":"HTTP","links":["thoughts/Application-Layer","thoughts/IP-Address","thoughts/Braid-HTTP","thoughts/TCP"],"tags":["seed","CPSC317"],"content":"Main method of communication at the Application Layer for the web. A state-transfer protocol. Sometimes called the second waist of the Internet (after IP Address)\nSee also: Braid HTTP for HTTP as state-synchronization\nConnections\n\nNon-persistent HTTP (v1.0)\n\nAt most one object is sent over each TCP connection\nConnection is closed as soon as data is transferred\nInclude an additional round trip for the TCP handshake for each object = 2n RTTs where n is number of resources\n\n\nPersistent HTTP (v1.1)\n\nMultiple requests can be sent over single connection\nKeep-Alive header\nRound trip for each resource = n+1 RTTs where n is number of resources\nPipelining:\n\nCan send all the requests at once!\nProcess them all sequentially but get all the responses in the same order as the requests\nRound trip = 3 RTTs (one for handshake, one for initial resource, one for the rest of the resources)\n\n\nServer has the right to close any connection\n\nUsual heuristic is to close the connection after 5 seconds of inactivity\n\n\n\n\n\nCache\n\nBrowser may maintain a cached version of page\nCache can also be maintained by a separate host (a proxy e.g. Varnish)\nWeb servers can provide cache policy\nCalculating cache speedups\n\nCache time = rRTTcache​+(1−r)(RTTserver​+RTTcache​) where r is the cache hit rate\n\n\n\nCookies\n\nUsed for storing per-user state\n\n“Remember me” authentication\nSession state\n\n\nApplicable for a particular hostname\nResponse from server includes Set-Cookie header\nBrowser saves cookie associated with the server\nNext request to the same server will include Cookie header with the same value\nTwo styles of using cookies\n\nAll the state is in the cookie\n\nTotal header size limited by servers (~8KBytes)\nIndividual cookie size limited by browsers (~4KBytes)\n\n\nState (or part of it) may be stored server-side: cookie is used to identify entry in server database\n\n\n\nClient/server model\n\nClient: browser that requests, receives, displays Web objects\nServer: sends objects in response to requests\nUses TCP port 80 (443 for HTTPS)\nFor each object\n\nClient sends one request message at once\nServer sends full response message at once\n\n\nServer is stateless\n\nMessage Format\n\nRequest\n\nFirst line: method, URL, version\nHeaders:\n\nHeader-Name: value&lt;CR-LF&gt;\nRequired\n\nHost: &lt;domain&gt;\nUser-Agent: &lt;browser/version&gt;\n\n\nEmpty line to end header section\n\n\nBody here\n\nSize determined by Content-Length header\n\n\n\n\nResponse\n\nFirst line: version, response code, response text\n\n\n\nRequest Methods\n\nGET\n\nrelevant data is in URL\nform data (if needed) is in URL\n\n\nPOST\n\nincludes form input in message body\nused in forms that submit new data\n\n\nHEAD\n\nsimilar to GET but only returns header\nused to check if existing content was modified\n\n\n\nCodes\n\n2xx: success\n3xx: additional action required\n4xx: client problem\n5xx: server problem\n\nWWW\n\nA web page consists of objects\nEach object is addressable by a URL\n"},"thoughts/Hackers":{"title":"Hackers","links":["thoughts/decentralization","thoughts/creation-vs-maintenance","thoughts/quantization","thoughts/academia","thoughts/Vanilla-Ice-Cream-effect","thoughts/maintenance"],"tags":["seed"],"content":"Steven Levy\nSource: Hackers by Steven Levy\nThe hacker ethic: sharing, openness, decentralization, and getting your hands on machines at any cost to improve the machines and to improve the world. All information should be free.\nSqueezing in time where no one else would (sniping people even 1 min late at 3am in the morning). This culture seemed to almost arise bottom-up from the scarcity of compute resources at the early 60s and 70s, especially around the MIT area where there seemed to be a lot more spare compute time and just really smart, curious people.\n\nWhat was a computer but something that benefited from a free flow of information?\n\nDecentralization\nThe best way to promote free exchange of information is to have an open system, one with no boundaries attached. The worst thing to encourage this is to have a bureaucracy.\nThe difference between bureaucrats and and logical algorithms are that bureaucrats follow arbitrary rules whereas algorithms follow concrete and transparent rules.\nPaul Graham\nSource: Hackers and Painters by Paul Graham\nHacking as a creative art\n\nThey seemed to think that hacking and painting were very different kinds of work— that hacking was cold, precise, and methodical, and that painting was the frenzied expression of some primal urge.\n\nPG is positing that hacking (traditionally seen as more as more mechanical) and painting (seen as more creative) have a lot in common — most importantly, the face that they make things.\nFor hackers, computers are just the medium through which they express themselves.\nNamely, they don’t exclusively try for exclusively creating or maintaining but rather to make ‘good things’ and if either of those happen along the way, all the better.\nLabels\n\nBut often this mismatch [between labels and what they do] causes problems. It’s easy to drift away from building beautiful things toward building ugly things that make more suitable subjects for research papers.\n\nLabels are logically decentralized so if it no longer is useful, then it will phase itself out. While yes, language is useful for defining norms externally, for those strongly intrinsically motivated (which arguably are the people PG is trying to speak to), those labels don’t matter because external incentives matter very little to them. If that label continues to be useful to them (e.g. to secure funding or to talk about it to laypeople) then why not continue to use it?\nI think my case for this argument would be slightly different if PG focused more on the public perception of the label.\nIndustry\n“Universities and research labs force hackers to be scientists, and companies force them to be engineers… Programmers were seen as technicians who translated the visions (if that is the word) of product managers into code.” (re: academia)\nThis feels like a problem of scale: only a small percentage of hackers can actually design software and it’s hard for people running a company to pick these out\nBig companies want to damp oscillation/standard deviation of results, they have shareholders to which they are accountable. They win by sucking less than their competitors, not making great products.\n“But when you damp oscillations, you lose the high points as well as the low.” (Vanilla Ice Cream effect)\nFiguring things out as you go\n\nIf I had only looked over at the other makers, the painters or the architects, I would have realized that there was a name for what I was doing: sketching.\n\nThe traditional approach is to logically plan things out and to create with intention. PG argues that sketching — figuring out the program as one writes it — is just as valid.\nA programming language is for thinking of programs, not for expressing programs you’ve already thought of. The argument against strong typing is really interesting. I like to think of strong typing as ruler lines or a guide to ensure that the perspective of my drawing is right; a good aid to ensure that I’m on the right track.\nDemand\n\nPrices are determined by supply and demand, and there is just not as much demand for things that are fun to work on as there is for things that solve the mundane problems of individual customers.\n\nSimilar to creation vs maintenance, there seems to be a need to artificially incentivize the difficult work (e.g. maintenance or mundane problems)"},"thoughts/Hard-problem-of-consciousness":{"title":"Hard problem of consciousness","links":["thoughts/mind-body-problem","thoughts/Materialism","thoughts/consciousness","thoughts/emergent-behaviour","thoughts/the-Self","thoughts/Nagel's-Bat-Argument"],"tags":["seed","PHIL451A"],"content":"Related: mind body problem, Materialism\nHow do we bridge objective description and subjective experience (qualia)? Is consciousness explainable in terms of brain processes?\nWhat gives rise to consciousness (rel: emergent behaviour), first-person experiences, and a sense of self? Do inanimate and non-human intelligences have personal experiences?\nChalmers\n\nCertain phenomena are functionally definable — explanation of them requires only the explanation of the relevant functions and the mechanisms that implement them (e.g. hereditary passing of genes and DNA)\nConsciousness is not functionally definable\n\nDefining information discrimination, integration, and reporting does not fully explain consciousness\n\n\n\nExplanatory Gap\nThere is no cognitive function such that we can say in advance that explanation of that function will automatically explain subjective experience\nSee Nagel’s Bat Argument"},"thoughts/Hash-consing":{"title":"Hash consing","links":[],"tags":["seed"],"content":"A technique used to memoize values that are structurally equal.\nWhen a value is constructed, the technique checks if such a value has been constructed before, and if so reuses the previous value and points to it, avoiding a new memory allocation.\nThen, two structures can be tested for equality in constant time via pointer equality."},"thoughts/Heidegger":{"title":"Heidegger","links":["thoughts/ontology","thoughts/context","thoughts/Tools-for-Conviviality","thoughts/small-technology","thoughts/convex"],"tags":["seed"],"content":"See also: ontology\nDasein\nLiterally means ‘Being-there’. Through the use of this expression, Heidegger calls to attention the fact that a human cannot exist or be taken into account without existing in context of a world with other things — “to be human is to be fixed, embedded, and immersed in the physical, literal, tangible day to day world”\nNear-ness\nHeidegger observes that because of technology, “all distances in time and space are shrinking” and “yet the hasty setting aside of all distances brings no nearness; for nearness does not consist in a small amount of distance.”\nSee also Tools for Conviviality, small technology\nContext\nValues are just meaningless facts (hammer is for hammering). It leaves out information defining the relation of hammers to nails and the rest of the environment (readiness-to-hand)\nSee also: convex"},"thoughts/Hold-the-Fries":{"title":"Hold the Fries","links":[],"tags":["fruit","writing"],"content":"“Shaun, you’re doing that thing again. Staring into some far off land. Hurry up loopyhead, figure out what you wanna get.”\nA fog seemed to lift from Shaun’s brain and he realized he had been staring at the menu for a while.\n“Same as usual I guess. Deluxe Burger, hold the fries. I just want the coke.”\n“You haven’t changed a smidge, huh” Cassidy chuckled. “I gotcha. On me this time.”\n“Nah, Cass you really don’t need to…” he hesitated because he know it wasn’t true. The tattered, oil covered t-shirt and beat up Nikes told Cassidy otherwise.\n“Two Deluxe Burger Combos, no fries please!”\nShaun shuffled his shoes a little bit, almost as if he suddenly became a little more conscious of how he looked beside her neatly pressed blazer and slightly waxed hair. After all, she was the one who looked out of place, a stark contrast to the greasy dinge of the burger shop and the unearthly pale glow of fluorescent lights.\n“So, uh. You been up to much?” Cassidy asked.\n“Not much, no.” Shaun answered, a little startled at the sound of his own voice. “I’ve been working at a gas station for the past few months. It pays the bills, I guess.”\nCassidy raised an eyebrow. “An’ your mom? She’s doing alright?”\nShaun shrugged. “I guess so. She’s working at a fast food place. She’s been takin’ her meds regularly so problems haven’t been too bad.”\nCassidy nodded. “That’s good. That’s good.”\n“Yeah.” Shaun said, looking away from Cassidy’s gaze and staring at the wall, where a poster of a bikini clad girl with a burger and a coke in her hands was taped to the peeling wallpaper.\n“Cass?”\n“Yeah?”\n“What happened to that promise?”\nCassidy froze. “The friendship one?”\n“Yeah.”\nAn awkward silence rested between the two of them, suddenly broken by the sound of an ambulance siren. The blue and red lights lit up their faces like the bruising of a recent injury.\n\n“Order 512! Chicken Deluxe with fries! Last call!” Shaun rang the bell a few more times.\n“Well, I guess that guy isn’t showing up. It’s past closing anyways, do you wanna just split the food?”\nCassidy glanced at the clock just as it hit 10:00pm.\n“Yeah sure, I’m starving.”\nThe two of them sat down in the only two chairs in the room that weren’t coated in grime, neither of them saying a word.\nShaun took a bite and chewed, feeling the greasy chicken inch down his throat. “How did you find Mr. Saltsworth’s calc test? I definitely feel like I got slaughtered. That one with the double integral was real tricky, how did he expect us to actually know that one?”\nCassidy spoke between bites. “Well silly, if you actually did the practice problems you would know. In any case, you should start taking school seriously. College application season is soon. How do you expect to do great things in the world if you don’t even take this stuff seriously?”\n“Well, maybe not everyone wants to change the world.”\n“Come on Shaun. Everyone wants to matter. What better way to do that than to make an impact?” Cassidy said, putting her burger down and wiping her lips with a tissue.\n“What if I just want to work a regular job and care for my mom?”\nThey both sat, facing each other, a world of understanding apart. Both not understanding why the other wanted what they wanted.\n“Cass, you ever thought that this is what I want?” Shaun said, feeling a lump form in his throat. “It’s all I’ve ever really wanted.”\n\n“I remember that you said we would be friends until we both grayed out. I don’t think we’re too far from that, are we Shauney?”\n“I said not to call me that. And are you kidding? We are far from that. Don’t fucking joke with me. You’ve got a nice new job, a wonderful relationship, hell you probably have enough dough to buy up this whole place!” Shaun said a little too loudly.\n“You are treating time spent with me like some sort of fucking charity. Oh I’ll go spend some time with poor old Shauney so I can rub it in his face how successful and cool I am”\n“-that really wasn’t my intention!” Cassidy butted in, her voice faltering. “I just wanted to catch up like old times.”\n“Old times are old times. They’re gone. You shouldn’t dig them up. They’re just going to make you feel sad. And you don’t need that, do you? You’re happy now. You have no problems. You’re doing great.” Shaun said, staring at the empty plates.\n“Look. I barely can pay the rent every month. I sell drugs to stay alive and take care of Katie and you’re moving to the gentrified part of the neighbourhood which is the reason why people like me can barely afford to stay alive!”\n“I…I didn’t know that. I’m so sorry.” Cassidy cried, at first droplets streaming down the contours of her face, but eventually flowing streams that reddened her cheeks.\n“You’re sorry! You’re sorry!” Shaun stood up and shouted, his voice echoing throughout the restaurant. If anyone else had been around, they would have turned to look at them too.\n“Things will never be the same again, Cass. Y’know, I used to think we would end up together one day. We would take two different paths to get there, but the destination would be the same.” Shaun said, his voice trembling. “But I realize now that things just won’t be the same. I mean, look at us. I’m a me and you’re a ‘professional’. What could we possibly have in common? Things just aren’t what they used to be. They can’t be.”\nShaun stood up and began to walk away, but Cassidy grabbed his arm. “Wait, Shaun. Please don’t go. I know you are going through a lot. But we can figure this out together. We always have.”\nIn that moment, a ringtone pierced the air and they both froze, eyes wide and lips parted. The sound rang, twice, three times, through the empty restaurant.\n“It’s work.” Cassidy said. “I have to take this. Please, can we talk about this tomorrow?”\n“No Cass. I’ve had enough. Who are you going to choose? What’s the life you want to live? What do you really care about, Cass?”\nCassidy stared at Shaun, with her lips quivering. I’m sorry she mouthed.\n“Hello… Cassidy speaking. How can I help you?”"},"thoughts/Holochain":{"title":"Holochain","links":["thoughts/cryptography","thoughts/DHT","thoughts/content-addressed-storage","thoughts/RDF"],"tags":["seed"],"content":"\nHolochain is an open-source application development framework and peer-to-peer networking protocol. It allows you to create truly serverless applications with high levels of security, reliability, and performance. Every user runs the application on their own device, creates and stores their own data, and talks directly to other users. The security of the application is supported by both cryptography and peer accountability.\n\nSummarized from Holochain Docs\nWe start with users, not servers or data, as the primary system component. The application is modeled from the user perspective, which we call agent-centric computing.\nHolochain has two main value pillars:\n\nIntrinsic data validity: Empowered by the Holochain runtime, each user runs their own copy of the back end code, controls their identity, and stores their own private and public data. Half of the problem is already solved—because everyone has the ‘rules of the game’ in their copy of the code, they can verify that their peers are playing the game correctly just by looking at the data they create.\nPeer witnessing: Each piece of public data is witnessed, validated, and stored by a random selection of devices using a DHT. Together, all cooperating participants detect modified or invalid data, spread evidence of corrupt actors or validators, and take steps to counteract threats.\n\nTerminology\n\nConductor: Runtime that sandboxes and runs hApp code. Responsible for all network communication and routing things to the right place.\nhApp: Holochain application. Composed of slots for cells which provide an aspect of functionality.\nCell: occupies slots in an hApp. Alive version of the DNA. Contains state, settings, etc. that is specific to that user\nDNA: A bundle of Zomes that makes a unit of functionality in a hApp.\nZome: (chromosome). Made up of DNA define the core business logic in a DNA, exposing their functions to the conductor.\n\nDHT\nOf course, the DHT is really large. You need to know its address in order to retrieve a piece of data that matters to you.\nBut because Holochain’s DHT is content-addressed, you can only know the address if you can calculate it from the data (in which case you don’t need to retrieve it), or if you discover the address somehow.\nAddress discovery in Holochain works through anchors and links.\n\nAnchors: ‘known’ things you can use as starting points\nLinks: an RDF triple\n\nSource Chain\nEach user in a Holochain network creates and stores their own data in a journal called a source chain. Each journal entry is cryptographically signed by its author and is immutable once written.\nIdentifiers are based off of a public/private key pair. All messages posted to the source chain are signed so they are tamper-proof. Each message refers the previous one in the order so it order can’t be tampered with either.\nBut unfortunately anyone can modify their own source chain, regenerate the hashes and signatures, and create a perfectly valid, but wrong, alternate history for themselves. The DHT resolves this sharing your source chain actions and public entries with a random selection of your peers, who witness, validate, and hold copies of them."},"thoughts/Homotopy-Theory":{"title":"Homotopy Theory","links":[],"tags":["seed"],"content":"Two continuous functions from one topological space to another are said to be homotopic (f≅g) if one can be “continuously deformed” into the other over time.\nIt is a form of abstraction, allowing us classify entire groups of things as exhibiting the same characteristics.\n"},"thoughts/HoneyBadgerBFT":{"title":"HoneyBadgerBFT","links":["thoughts/system-model","thoughts/message-broadcast"],"tags":["seed"],"content":"\nAn asynchronous system model total-order message broadcast protocol\n\nKey features:\n\nAsynchrony: no timing assumptions. Messages can arrive at any time, and network speed determines the transaction rate\nLeaderless consensus: every node is a proposer. This eliminates potential attacks where a leader node can be stalled indefinitely, bringing the entire network to a halt\n\nHoneyBadgetBFT (HBBFT) can be decomposed into nested subproblems:\n\nQueuing Honey Badger (QHB)\nHoney Badger (HB)\nSubset\nReliable Broadcast (RB)\nBinary Agreement (BA)\nThreshold Sign\n\nAn example flow of how message broadcast might work with a transaction x\n\nQHB places this transaction, along with others it has received, into a queue of pending transactions\nA random process determines which transactions to include in the next block\n\nWhen x is included in the contribution, it is submitted to Honey Badger. HB encrypts the list using threshold cryptography, creating a garbled version that contains your transaction but can’t be read. This is submitted to the Subset algorithm\n\nSubset puts it into a Reliable Broadcast instance for Node 1. This distributes the contribution to every other node in the network\n\nOnce every node has received the encrypted contribution via RB, they know that all other correct nodes will also receive this contribution. They vote Yes in the Binary Agreement instance labelled Node 1, meaning that this contribution should be included as part of the next block\n\n\nGoing back up to Subset, we now have &gt;32​contributions from all the nodes which are all encrypted\n\n\nIn HB, we now have enough contributions to decrypt all of them. Each node gets n lists of decrypted transactions, including x\n\n\nQHB then makes a union of the contributions and creates a single finalized list of transactions that all nodes have agreed on\nThis final list is sent out of QHB and back to the application client\n\nSome faster and more efficient alternatives have also been developed:\n\nDumbo1 and Dumbo2\nBEAT\n"},"thoughts/HotStuff":{"title":"HotStuff","links":["thoughts/Byzantine-Faults","thoughts/State-Machine-Replication-(SMR)","thoughts/system-model","thoughts/PBFT","thoughts/Tendermint","thoughts/Casper-FFG","thoughts/blockchain","thoughts/authenticator-complexity","thoughts/digital-signatures"],"tags":["seed"],"content":"\nA byzantine fault-tolerant state machine replication protocol for the partially synchronous system model. It can express other known protocols (e.g. PBFT, Tendermint, Casper FFG) in this common framework.\n\nSource Paper\nThe Scaling Challenge\nOriginal BFT SMR protocol were designed with a typical target system size of n=5 or n=7 for local-area deployments. As such, they don’t scale well to high n as required by permissioned and permissionless blockchains.\nHotStuff aims to overcome this by improving the bound of total number of authenticators communicated from O(n4) to O(n2)\nThe first BFT SMR protocol with the following properties:\n\nLinear view change: After GST, any correct leader, once designated, sends only O(n) authenticators to drive a consensus decision in the best-case. In the worst-case, communication costs to reach consensus after GST is O(n2) authenticators in the worst case of cascading leader failures\nOptimistic Responsiveness: After GST, any correct leader, once designated, needs to wait just for the first n−f responses to guarantee that it can create a proposal that will make progress\n\nHotStuff does this by adding another phase to each view, with the assumption that the network delay is less than the Δ required to wait for GST. This solves the hidden QC problem.\n\nIf a leader doesn’t wait for the Δ expiration time of a round. Simply receiving n−f replies from participants (up to f of which may be Byzantine) is not sufficient to ensure that the leader gets to see the highest QC\n\nSuch an impatient leader may propose a lower QC value than what is accepted and this may lead to a liveness violation. In order not to wait the maximum Δ expiration time of a round, HotStuff introduces another round, Pre-commit, before the actual Commit round.\nBoth Casper and Tendermint wait the full Δ period instead of incurring the cost of a new round.\nCryptographic Primitives\nUses thresholded signatures with a threshold of k=2f+1\nThree-phase Protocol\nHotStuff is a view-based protocol. Each view v has a unique leader known to all. Each replicas has a tree of pending commands (as opposed to a list used by more classical BFT protocols).\nDuring the protocol, a monotonically growing branch becomes committed. To become committed, the leader of a particular view proposing the branch must collect votes from a quorum of (n−f) replicas (the QC) in three phases: prepare, pre-commit, and commit.\nChained HotStuff\nNote that each of the three-phases have very similar structure and that the protocol isn’t doing “useful” work except collecting votes from replicas. To optimize this, we can pipeline the phases, similar to what Casper FFG does.\n\nCommit Rule\nHotStuff uses the concept of chains which maps nicely onto Chained HotStuff.\nThe decision when a block is considered committed rests purely on a simple graph structure, a three-chain.\n\nThe three-chain commit rule provides the following guarantee.\n\nThe first link (corresponding to prepare) in the chain QC|B&#039; -&gt; QC|B guarantees n−f votes on a unique block B.\nThe second link (corresponding to pre-commit) in the chain QC|B&#039;&#039; -&gt; QC|B&#039; guarantees n−f replicas have a QC on a unique block.\nThe last link (corresponding to commit) QC|B&#039;&#039;&#039; -&gt; QC|B&#039;&#039; guarantees that n−f replicas have the highest QC of any two-chain that has a vote.\n"},"thoughts/How-to-do-Nothing":{"title":"How to do Nothing","links":["thoughts/attention-economy","thoughts/The-Grasshopper,-Games,-Life-and-Utopia","thoughts/creation-vs-maintenance","thoughts/Do-Artifacts-Have-Politics","thoughts/censorship"],"tags":["seed","book"],"content":"On the attention economy. Very heavily related to Grasshopper\n\nWhat does it mean to construct digital worlds while the actual world is crumbling before our eyes?\n\nA man living in the attention economy attempting a humble and ethical life would certainly appear ‘backward’: for him, good would be bad, up would be down, productivity would be destruction, and indeed, uselessness would be useful.\nResistance-in-place: to resist in place is to make oneself into a shape that cannot so easily be appropriated by a capitalist value system. To do this means refusing the frame of reference: in this case, a frame of reference in which value is determined by productivity, the strength of one’s career, and individual entrepreneurship. It means embracing and trying to inhabit somewhat fuzzier or blobbier ideas: of maintenance as productivity, of the importance of nonverbal communication, and of the mere experience of life as the highest goal.\nHow do we even begin to define productivity? Productivity that produces what? Successful in what way, and for whom? The happiest, most fulfilled moments of my life have been when I was completely aware of being alive, with all the hope, pain, sorrow that that entails for any mortal being. In those moments, the idea of success as a teleological goal would have made no sense; the moments were ends in themselves, not steps on a ladder.\nSelf-care\n“I hope that the figure of ‘doing nothing’ in opposition to a productivity-obsessed environment can help restore individuals who can then help restore communities, human and beyond.”\n“Caring for myself is not self-indulgence, it is self preservation, and that is an act of political warfare.”\n“In the context of health and ecology, things that grow unchecked are often considered parasitic or cancerous… The life force is concerned with cyclicality, care, and regeneration; the death force sounds to me a lot like ‘disrupt’.”\nGardens\n“I find myself gravitating toward these kinds of spaces — libraries, small museums, gardens, columbaria — because of the way they unfold secret and multifarious perspectives even within a fairly small area.”\nCensorship\nInformation DDOS-ing in the attention economy: Instances of censorship are rather marginal when compared to what is essentially an immense informational overload and an actual siege of attention, combined with the occupation of the source of information by the head of the company”"},"thoughts/Hyper-Hyper-Space":{"title":"Hyper Hyper Space","links":["thoughts/CRDT","thoughts/causality","thoughts/DHT"],"tags":["seed"],"content":"\nMake all data local, communicate only through data sync\n\nSummarized from the site and whitepaper\nProvides:\n\na local data store, both in-browser using IndexedDB and server-side\na data representation format, based on Merkle-DAGs and CRDTs\na secure data sync protocol over WebRTC and WebSockets\n\nThe Hyper Hyper Space project proposes a framework for universal information access\nSpaces\nApplications organize their information using spaces — a bit like a file but for the internet age. It’s a file that is opened and modified locally on your devices but synchronized automatically over the internet. They can be universally looked up using 3-word codes, like suburb-suburb-awake.\nFinality\nTo preserve operation commutativity, these untimely capability uses would need to be accepted, hence preventing the application from truly enforcing capability revocation.\nLack of finality is worrying for vast majority of applications. They resolve this in a weird manner by introducing causal relationships (e.g. this action is only valid if this previous one is valid). Even then, this weird form of causality doesn’t actually solve finality.\nConnection\nWebRTC as underlying transport layer. Uses a signalling server that each peer runs. Not ideal, doesn’t run a DHT so requires users to know address of other’s signalling server."},"thoughts/Hypercore":{"title":"Hypercore","links":["thoughts/peer-to-peer","thoughts/consensus","thoughts/DHT","thoughts/Kademlia-DHT","thoughts/Hypercore"],"tags":["seed"],"content":"\nStreaming based append-only log that aims to be the lego-block of distributed applications.\n\nHypercore Protocol is a peer-to-peer data network built on the Hypercore logs. Hypercores are signed, append-only logs. They’re like lightweight blockchains without the consensus algorithm\nConnects peers using the Hyperswarm DHT which is based off of Kademlia\nThoughts\n\nGreat developer experience, super simple to understand and use\nComprehensive library of data structures\nNot amazing availability, no incentive system for people to run nodes (though Dat is working on this using a blockchain-based reward system)\nNot exactly great local first support. Continues working locally without an internet connection but new users cannot connect or get an up-to-date version of your data. If the user wants to send data to someone else, both devices need to be online simultaneously\nHypercore also does not guarantee long-term write-once storage\nMulti-writer support is still being worked on\n\nHypercore is inherently single-writer due to it’s append only log structure, and while they have some work on multiwriter it’s very tied to the data model\n\n\n"},"thoughts/I-Confluence":{"title":"I-Confluence","links":["thoughts/Byzantine-Faults","thoughts/Datalog","thoughts/semilattice","thoughts/CALM-Theorem"],"tags":["sapling"],"content":"Martin Kleppmann and Heidi Howard: Byzantine Eventual Consistency and the Fundamental Limits of Peer-to-Peer Databases\nI-confluence is a necessary and sufficient condition for the existence of a byzantine fault-tolerant eventual consistency replication algorithm\nThey define an invariant is a predicate over replica states, i.e. a function I(S) that takes a replica state S and returns either true or false.\nA set of transactions is I-confluent with regard to an invariant I if\n\nEach replica can execute a subset of the transaction with I preserved on that replica\nMerging the updates from the transactions still preserves I\n\nA few examples:\n\nI-confluent: consider an invariant I(S) that is true if every user in S has a non-negative balance\n\nIf each transaction only increases a user’s account balance, then any combination of transactions will still satisfy I\nNote that this example is no longer I-confluent if transactions can deduct from a user’s account balance\n\nSay A has a balance of $50\nIf T1​ results in deducting $40 from A and T2​ results in deducting $25 from A, each transaction is valid on its own\nBut when combined, it violates the invariant I\nAs a result, we can’t model anything like a cryptocurrency using CRDTs\n\n\n\n\nNot I-confluent: consider an invariant I(S) that is true if there are no duplicate values in S (i.e. ensure that S is a set)\n\nIf T1​ and T2​ are both transactions that create data items with the same value in that attribute, each of transaction preserves the constraint\nHowever the combination of the two does not\n\n\n\nI conjecture that if a data structure is I-confluent, then it can be expressed in monotonic Datalog. That is, I-confluence holds if and only if states S can be represented as a join semilattice.\nThis shows equivalence with the CALM conjecture (proof left as an exercise for the reader)."},"thoughts/IP-Address":{"title":"IP Address","links":[],"tags":["seed","CPSC317"],"content":"IP addresses are 32 bits (4 bytes) split into 4 chunks. Obviously 232 is an incredibly large address space so we compress the table using IP prefixes.\n\nIt is generally recognized that the current approach of using the IP address as a locator and as an identifier was a poor design choice. (Clark, 2018, “Designing an Internet”)\n\nIPv4\nSpecial Addresses\n\nFirst address (generally all 0s): network itself, or not assigned\nLast address (generally all 1s): broadcast\n\nCIDR Notation\nIP/# where # is the number of bits in the network ID\ne.g. 18.0.0.0/8 means first 8 bits are network ID, and 18.x.x.x is the space of all possible addresses (23∗8=224)\nBy default, routers will take the most specific one (longest network ID).\nTo prevent loops, we set a TTL (time-to-live) for packets to expire after a certain time."},"thoughts/IPFS":{"title":"IPFS","links":["thoughts/peer-to-peer","thoughts/CID","thoughts/git","thoughts/content-addressed-storage","thoughts/Merkle-DAG","thoughts/DHT","thoughts/Filecoin","thoughts/authorization"],"tags":["seed"],"content":"IPFS is a decentralized storage and delivery network which builds on fundamental principles of p2p networking and content-based addressing (see CID)\n\nCan seen as a single BitTorrent swarm, exchanging objects within one big Git repository\n\nMuch like how we look up sites on the internet using URIs, we can look for specific pieces of content by their content-address.\nUnder the hood, IPFS uses\n\nlibp2p: network layer, takes care of host addressing, content and peer discovery, and structures like DHTs and pubsub\nIPLD: data layer, standards and formats to build Merkle-DAG structures (quasi-filesystem)\nMultiformats: formatting structures for self-describing values\n\nPublishing content\n\nChunk the content and deduplicate chunks\nObtain CID\nAdd the content to the network\n\nNot the actual content, just the provider record to the DHT\n\n\n\nConsuming content as a peer\n\nGet CID (out of band)\nUsing DHT, resolve CID to peer\nContact peer to ask for CID content\nFetch content and cache a copy\nServe local copy upon subsequent request\n\nEncoding/decoding\nHow does the system decode the hashes that it gets into the component data structures?\nCodecs! IPLD codecs are functions that transform IPLD Data Model into serialized bytes so you can send and share data, and transform serialized bytes back into IPLD Data Model so you can work with it. The CID includes an indicator called a multicodec (opens new window)to tell us which codec to use!\nSystems can build abstractions on top of this. For example, IPFS encodes the UnixFS using DAG-PB (which is a IPLD codec).\n\nBecause the CID can describe different codecs relating to different systems, all sorts of systems can interoperate using CIDs, and IPLD and process and cross-link data from any of them.\n\nBlock Exchange using BitSwap\nBitSwap peers are looking to acquire a set of blocks (want_list), and have another set of blocks to offer in exchange (have_list). Unlike BitTorrent, BitSwap is not limited to the blocks in one torrent. BitSwap operates as a persistent marketplace where node can acquire the blocks they need, regardless of what files those blocks are part of. The blocks could come from completely unrelated files in the filesystem. Nodes come together to barter in the marketplace.\nIncentivizing rare blocks\nOf course, there will not be perfect overlaps between nodes’ have_list and want_lists . Nodes must work for their blocks. In the case that a node has nothing that its peers want (or nothing at all), it seeks the pieces its peers want, with lower priority than what the node wants itself. This incentivizes nodes to cache and disseminate rare pieces, even if they are not interested in them directly.\nThis barter system implies a virtual currency could be created, this would require a global ledger to track ownership and transfer of the currency, which is exactly what Filecoin provides.\nIncentivizing satisfied nodes to seed\nThe protocol must also incentivize nodes to seed when they do not need anything in particular, as they might have the blocks others want. Thus, BitSwap nodes send blocks to their peers optimistically, expecting the debt to be repaid.\nA simple credit-like system solves the problem:\n\nPeers track their balance (in bytes verified) with other nodes.\nPeers send blocks to debtor peers probabilistically, according to a function that falls as debt increases. The probability function, given a debt ratio r=bytes received+1bytes sent​ is P(send∣r)=1−1+exp(6−3r)1​. The debt ratio is a measure of trust: lenient to debts between nodes that have previously exchanged lots of data successfully, and merciless to unknown, untrusted nodes. This also strongly disincentivizes Sybil attacks by making it hard for new nodes to request a lot of blocks without the intention of paying them back.\n\nPinning\nThis ensures the objects are kept in the node’s local storage. Pinning can be done recursively, to pin down all linked descendent objects as well. All objects pointed to are then stored locally. This is particularly useful to persist files, including references\nWNFS\nUnder the Fission project, see specs\nFile system built on top of IPFS. Uses a DAG instead of a hierarchy, meaning that a given child can have more than one parent.\nEach user has their own WNFS and consists of a public and private tree.\n\nThe public tree is “live” and publicly accessible on the Internet.\nThe private tree is encrypted so that only the owner can see the contents.\n\nUses UCAN for authorization."},"thoughts/In-Over-Our-Heads":{"title":"In Over Our Heads: The Mental Demands of Modern Life","links":["thoughts/ethics","thoughts/group-limits","thoughts/social-contracts","thoughts/seeing","thoughts/friendship","thoughts/self-confidence","thoughts/selfish","thoughts/teaching"],"tags":["seed","book"],"content":"In Over Our Heads is about the societal expectations of higher orders of consciousness that most of us do not have.\n\nAll of us will spend some portion of our adult lives overmatched by the demands of modernism, the compulsory “major” in our culture’s curriculum.\n\nThe book details Robert Kegan’s model of adult development — basically a series of increasingly sophisticated ways one can approach ethical reasoning.\nGrowth is a process of ‘leaving home’. It is a process of leaving the mental homes they have furnished and made familiar.\nStages of Adult Development\nStage 2: Self\n\nthe subject (self) is a collection of short-term practical interests\nethics in this mode is “instrumental”: aimed at satisfying your own needs, while working with or around other people’s. Relationships are “transactional”: transient alliances for mutual benefit.\n\nStage 3: Communal\n\nno longer a collection of interests but you have interests\nyou are in relationships and find yourself defined by them\nthe other’s point of view matters to us intrinsically, not just extrinsically as a means of satisfying our more egocentric purposes — one gains the ability to put oneself in the other person’s shoes\nstage 3’s limitation is that it cannot resolve conflicts between responsibilities to different relationships. If one person wants you to do something, and another person wants you to do something different, there is no good basis for decision\nit’s impossible to base a large-scale society on the communal mode, because it’s so ineffective at coordinating complex group activities. (If individuals frequently fail to do their specific, agreed tasks, nothing can get done.) Modern societies are based on the systematic mode — see group limits and social contracts\n\nStage 4: Systematic\n\nyou no longer are in relationships that define you; you have relationships\nyou are “self-authored”: you choose your own principles, projects, and commitments\nit means seeing the other person for who they really are. Emotions are just something people have, from time to time. Those need to be dealt with, but should not be taken too seriously. Relating to the other person’s principles, projects, and commitments means supporting what they most care about in the longer run\n\nStage 5: Fluid\n\nWe do not have one fixed “self” we can simply “be”\n\n\nsystems are relativized. They move from subject to object, and are subordinated to, and organized by, the process of meaning-making itself\nyou are no longer defined as a system of principles, projects, and commitments. You have several such systems, “multiple selves,” none of them entirely coherent, and which have different values—and this is no longer a problem, because you respect all of them\nthere is sometimes an uncomfortable middle between systematic (stage 4) and fluid (stage 5) stages often referred to as stage 4.5 where nihilism and postmodernism commonly emerges. Understanding that there is no ultimate meaning, one comes to the wrong conclusion that there are no meanings at all\nsees systems as nebulous (intangible, interpenetrating, transient, amorphous, and ambiguous) and patterned (reliable, distinct, enduring, clear, and definite)\n\nIndependence and love\n\nTo want to find the answer for oneself is not to reject the advice of others\n\n“is not just rejecting the assumptions of her husband or church or culture; she is rejecting her relationship to these assumptions as truths”\nthe discovery here is not that one may have different ideas, values, or beliefs but rather ideas, values, and beliefs are by their very nature assumptive\n\n\nWhat brings two people together in the first place?\n\n“we find each other in love out of some correspondence we feel with our deepest personal and primordial themes of longing, hoping, and desiring.”\n“these deeply felt desires and needs animate our personal images, themes, stories, and myths. And the images, themes, stories, and myths inside our skins look for people to portray and enact them outside”\n\n\nMarriage is a partnership between two distinct individuals who do not share one mind, heart, and soul\n\nDifferences need not signal the failure of their closeness or bond but rather the reality of their distinctness\nCorollary: successful couples do not give up the pursuit of closeness in their intimate relationship but reconstruct the very definition of what closeness is about\nRilke: “once the realization is accepted that even between the closest human beings infinite distances continue to exist, a wonderful living side by side can grow up, if they succeed in loving the distance between them which makes it possible for each to see the other whole against the sky”\n\n\nOn autonomy\n\nIncreasing autonomy does not have to be a story of increasing aloneness\nDeciding for myself does not have to equal deciding by myself\nIn seems true, for example, as many have argued, that North American culture promotes and expects individuation and separation while many South American, Africa, and Asian cultures promote and expect the self-in-the-collective and the maintenance of attachments.\nIt may not be true that South American, Africans, or Asians partake any less than their North American counterparts in processes of increasing psychological differentiation, self-regulation, or even autonomy. Rather, they may partake of such processes in the context of the collective.\n\n\nOn mismatched love (e.g. familial)\n\n“Suppose you have a dog. A big-hearted, high-energy dog who begins to bark, and won’t shut up, every time someone approaches your door. Now one day your dog starts into howling something first. He sounds a terrible alarm. You look out the window and it’s just your friendly neighbourhood mailman. So what do you do? You aren’t going to shoot your dog dead. He’s a pain but you wouldn’t think of it. Your dog loves you. He barks to warn you when anyone approaches. He wants nothing bad to happen to you. That’s just how he is. Problem is, he’s completely indiscriminate. He thinks everyone’s a danger, barks at anyone who approaches… You’re going to have a look for yourself. You’re going to bend over and stroke your dog. ‘Down boy,’ you say. ‘It’s just the postman. No harm here, silly guy’”\nThis is very much a metaphor for many familial relationships (especially in Asian-Canadian/Asian-American families) in which family is incredibly overprotective of their children, despite the pleas of the children to have their own independence and to not be constantly told of what to do (I am guilty of this as well)\nHere it is noteworthy that suggested way of navigating this relationship is one that puts you the relation between you and the parents as an object rather than the subject — to realize that you no longer are in relationships that define you; you have relationships (A stage 3 to stage 4 development)\n\n\n\nOn self-confidence and being whole\n\nWholesomeness not in the aesthetic sense but as in the wholeness of the self\n\nWhen American POWs from the Vietnam era were first released, nearly all performed the same two first acts after being flown to Wiesbaden, Germany: they took showers and called loved ones.\nInterestingly, the men were far more likely to shower first and then to call loved ones. The women were more likely to call loved ones first and then to shower.\nThe difference is not necessarily that the men are more selfish and care more about their own bodily comfort than about their loved ones — in fact, they could have well thought what was most important was to talk to their loved ones but couldn’t do that in a ‘self’ that doesn’t feel cleansed or psychologically restored.\nThe difference is not between “selfish” and “altruistic” — both groups may have been doing first what they needed to do to restore the self — in that sense, could be said to be ”selfish“. The difference is in how the self is made whole. For some, the self is restored by itself and is not until then capable or fit for precious connection. For others, the self is restored in and through connection.\nOn therapy\n\nMany times the efforts in individual therapy to convert a client’s stance toward the past or present story of their life from one of helpless victim to creative agent not only fails but can cause them to feel unhappier still\nThe implicit message in such reconstructions is that “you are responsible for your life”\nIf the client already constructs the world at the fourth order, they will more likely hear this as a confirmation of their personal authority\n\nA reminder that while the things that others do to me or that happen to me may not be in my control, the meaning I make of them can be\nA reminder that while I cannot change the wind, I can change my sails\nThis is a confirmation of my own power\n\n\nIf the client constructs the world at the third order however, they are likely to hear the same message as a declaration of my blameworthiness\n\n“You are responsible for your own life” is then less an inspiring rallying call to self-authorship than a humiliating and dispiriting judgement that I have only myself to blame for the fix I am in\n\n\nThis is the essence of “In Over Our Heads” — that the systems that we are expected to function in expect much higher orders of consciousness that the vast majority of us have attained\nRelevant: Kierkegaard on teaching — “instruction begins when you put yourself in his place so that you understand what he understands and in the way he understands it”\n\nTrying to teach someone who is has a third order consciousness assuming they have fourth order is like trying to explain a three-dimensional sphere to a Flatlander: it neither sees a sphere nor has any sense that there is more than what it sees — namely, a two-dimensional circle, that piece of a sphere its plane runs through.\nThe one being taught will only recognize the fourth order teachings in third order structures.\n\n\n"},"thoughts/Integrated-Information-Theory-of-Consciousness-(IIT)":{"title":"Integrated Information Theory of Consciousness (IIT)","links":["thoughts/Consciousness-is-not-Information","thoughts/consciousness","thoughts/Panpsychism","thoughts/information"],"tags":["seed","PHIL451A"],"content":"See also: a refutation against IIT\nFirst-order sensory theory of consciousness, a form of panpsychism\nIIT\n\nConsciousness is integrated information\n\nIn this context, information refers to information that is specified by a system that is irreducible to that specified by its parts.\nThe amount of information ϕ that the entire complex system has beyond the information available from the sum of its parts (i.e. Information that can’t be localized in the system’s individual parts.)\nAcknowledges that one cannot infer the existence of consciousness starting from physical systems. Instead, IIT starts from the experience itself, identities its essential properties (axioms), and then infers what kind of properties physical systems must have to account for its essential properties (postulates), and determines ‘how conscious’ a system is based off of these postulates.\nAxioms of IIT\n\nIntrinsic Existence: consciousness exists, each experience is real and actual (my experience exists independent of external observers)\nComposition: Each experience is composed of multiple phenomenological distinctions\nInformation: each experience is the particular way it is (it is different from other possible experiences)\nIntegration: each experience is irreducible to non-interdependent, disjoint subsets of phenomenal distinctions (I experience the whole scene, not just the left side independent of the right side)\nExclusion: consciousness is definite\n"},"thoughts/Internet":{"title":"Internet","links":["thoughts/Postel's-Law","thoughts/niche-at-scale","thoughts/decentralization","thoughts/Mangrove-Theory-of-the-Internet","thoughts/Dark-Forest-Theory-of-the-Internet","thoughts/digital-permanence","thoughts/friction","thoughts/Moving-Castles","thoughts/epistemology","thoughts/epistemic-authority","thoughts/trust","thoughts/gate-keeping","thoughts/Moderation","thoughts/democracy","thoughts/censorship","thoughts/ephemereal-content","thoughts/creation-vs-maintenance"],"tags":["sapling"],"content":"Not to be confused for the web. While the terms web and Internet are often used interchangeably in the media, they refer to different systems. The web is an information network, whose nodes are documents. In contrast the Internet is an infrastructural network, whose nodes are routers.\nCapital I Internet\nSource\n\nWhen I first joined the Internet in the 1990s, I found some now-long-lost introductory tutorial. It talked about the difference between an internet (lowercase i) and the Internet (capital I). An internet is “any network that connects smaller networks together.” The Internet is… well… it turns out that you don’t need more than one internet. If you have two internets, it is nearly unavoidable that someone will soon figure out how to connect them together. All you need is one person to build that one link, and your two internets become one. By induction then, the Internet is the end result when you make it easy enough for a single motivated individual to join one internet to another, however badly.\n\nSee: Postel’s Law\nNiche at scale\n\nThe Internet allows you to scale any niche obsession\n\nLocation-based scaling before usually meant smaller audiences. Now, larger cities (and the internet at large) has allowed audience sizes to grow to ridiculous amounts.\nCentralization of Content and Services\nImagine the surface of the web as a representation of its potential activity. A few heavyweight players have dug into the web surface, dragging activities down their slopes, activities that could have remained independent and decentralized.\nInstead of creating a new webpage, Internet professionals and private users tend to go to a Facebook Page and therefore open content hosted on the slope of a dominant curve.\n\nTheories / Models of Thinking\n\nMangrove Theory of the Internet\nDark Forest Theory of the Internet\nThe Internet Is a Collective Hallucination (digital permanence)\nA friction-ful Internet\nMoving Castles and Wizards\n\nInternet Epistemology\nOn epistemic authority\nTraditional\n\nexpertise-based\nhierarchical\ninstitutionally-mediated\neducation, positions of authority\n\nInternet\n\ncrowd-based\nbroad and distributed\ntechnologically-mediated\npopularity, usefulness, influence\ndata and metrics\npost-truth?\n\ntruth and facts as constructed, contested notions\nproduct of low trust and highly polarized information environment\ninformation becomes fact\n\n\nsee: gate keeping\n\nModeration\nSource: The Internet Is Rotting in The Atlantic\n\n“So the internet was a recipe for mortar, with an invitation for anyone, and everyone, to bring their own bricks.”\n\nThis absence of central control, or even easy central monitoring, has long been celebrated as an instrument of grassroots democracy and freedom.\nGap of responsibility: Their designs naturally create gaps of responsibility for maintaining valuable content that others rely on.\n“It’s not trivial to censor a network as organic and decentralized as the internet. But more recently, these features have been understood to facilitate vectors for individual harassment and societal destabilization, with no easy gating points through which to remove or label malicious work not under the umbrellas of the major social-media platforms, or to quickly identify their sources.”\n“10 years ago, a third-party bookseller offered a well-known book in Kindle format on Amazon for 99 cents a copy, mistakenly thinking it was no longer under copyright. Once the error was noted, Amazon—in something of a panic—reached into every Kindle that had downloaded the book and deleted it. The book was, fittingly enough, George Orwell’s 1984. (You don’t have 1984. In fact, you never had 1984. There is no such book as 1984.)”\n“Indeed, Wikipedia suffers from vandalism, and over time, its sustaining community has developed tools and practices for dealing with it that didn’t exist when Wikipedia was created. If they’d been implemented too soon, the extra hurdles to starting and editing pages might have deterred many of the contributions that got Wikipedia going to begin with.”\nCurious about how this relates to ephemereal content. Is moderation and managing rot thereby a form of maintenance?\nFire as a Metaphor for the web\n\nFire is a chief metaphor for the Internet: it is metaorganic; it extends the range of (informational) food; it empowers people to explore new time zones (the night) and territories of knowledge; it increases some kinds of sociability, demands ongoing maintenance, and produces dangers and externalities that did not exist before. Fire was the first World Wide Web, a fragile system for contagious spreads. Young people now stay up late looking at flickering firelights—TV screens, computer monitors, smart phones—as they once tended the communal well of flames. (Television has always been compared to the family hearth.)\n…\nInternet videos for restoring the power of the embodied voice speaking around the campfire. We “burn” discs on our computers. Memes and themes tear through the Internet like prairie fires, or are retarded by censorship “firewalls” such as those of the Chinese government. The server farms that are key to the material infrastructure of the Internet generate vast amounts of heat, requiring air conditioning in addition to the electricity their processing takes up. (Data centers are often built in cold climates to save on cooling costs.) Touchscreen technologies fulfill a certain fantasy of touching flame. As Paul Frosh notes, “Television and computer screens (including iPads etc.) have some of the qualities of fire, especially self-illumination; unlike cinema and print, they are lit from within.” Information is irreducibly connected with heat and burning.\n(The Marvelous Clouds: Toward a Philosophy of Elemental Media)\n"},"thoughts/Jestermaxxing":{"title":"Jestermaxxing","links":["thoughts/The-Grasshopper,-Games,-Life-and-Utopia","thoughts/trust","thoughts/in-group-bias","thoughts/play","thoughts/self-effacing-ends","thoughts/exploit-explore","thoughts/Turing-Test"],"tags":["seed"],"content":"Playfulness vs Epistemic Traps\nC. Thi Nguyen on Playfulness vs Epistemic Traps\nIntellectual playfulness, loosely, is the disposition to try out new ideas, perspectives and systems of thought (involves perspective shifting) for the sheer joy of it (autotelic behaviour). It is a disposition to explore ideas for the value of exploration itself. \nIntellectual playfulness also has some clear epistemic functionality for us\n\nintellectually playful exploration sometimes can better serve the goal of finding the truth, than will exploration that is strictly aimed at finding the truth\nit functions against epistemic traps: a belief system that re-directs good-faith inquiry to bad results. An epistemic trap manipulates background beliefs to fend off contrary evidence.\n\nExamples of epistemic traps:\n\nanti-reflective traps: belief systems that operate by preventing their adopters from reflecting on their belief system at all (e.g. unswerving and unthinking obedience to a leader)\ninquiry trap: belief systems which encourage, but re-direct, various intellectual processes. A kind of intellectual judo, flipping earnest intellectual efforts and sending down the wrong paths (e.g. echo chambers: a community which creates a significant trust disparity between members and non-members — in-group bias. Members of echo chambers come equipped with the intellectual machinery needed to dismiss contrary evidence coming in from the outside. Outside sources are, after all, untrustworthy, malicious, and corrupt)\ninsensitivity trap: hybrid anti-reflective and inquiry trap. Belief system that selectively cuts off attention to certain areas of life by attributing valuelessness to those areas by narrowly specifying what counts as valuable. (for example, the businessperson who believes the only thing of importance is money. They spend their time thinking about strategies to make more money and unlikely to attend to pursuits which might put them into contact with other expressions of value)\n\nThe truth-seeker’s explorations are guided by the current belief system; they will typically check out the most plausible alternatives. The intellectually playful person doesn’t care about plausibility. They care about more aesthetic qualities of ideas. They care about cool ideas, or elegant ones, or thrilling joy-rides of discovery. They care about exploring where exploration is joyful.\nIntellectual playfulness, like play, involves a form of perspective shifting — trying on and (at least temporarily) inhabiting alternate belief systems, which includes trying out alternate beliefs, values, and norms for belief-acquisition.\nThis is notably different from open-mindedness. Open-mindedness makes a weaker demand than perspective shifting. An open-minded person ought to take some challenges seriously, when their background belief system gives them good reason to, but their standing belief system is a very active participant in the process (this is problematic as their belief system shapes 1) which challenges one takes seriously 2) and how to view them as valid or not). However, open-mindedness is weak to epistemic traps.\nImagine that a belief system is a boat. Open-mindedness is the willingness to pull out any particular plank and to inspect it to see if it makes sense on the boat. But that assessment occurs while standing on all the other planks of that boat. Perspective shifting involves jumping ship and trying out a whole new boat.\nNext, we argue that the pursuit of truth may be a self-effacing end — that it cannot be acquired through direct pursuit. Even if you are trying out alternative systems of belief, the choice of those systems will still be influenced by your standing system of beliefs. The trouble is that a well-designed epistemic trap can undermine the plausibility of alternative perspectives.\nIn the exploit-explore tradeoff, random walks are actually good. Surely, one may argue, that if going on occasional random walks is the best path for rationality, then wouldn’t the rational person go on random walks? Yet, one cannot take a truly random walk without being guided by prior beliefs.\nIdealogical Turing Test\nhttps://twitter.com/liron/status/1551205692152975360\n\nThe Ideological Turing Test is an exercise where you try to pretend to hold an opposing ideology convincingly enough that outside observers can’t reliably distinguish you from a true believer.\n\nPassing the ideological Turing test is a sign that you understand the opposing ideology on a deep level.\nSee also: Turing Test"},"thoughts/Jevons-Paradox":{"title":"Jevons Paradox","links":[],"tags":["seed"],"content":"The more efficiently energy has been used, the more, not less, total energy has been consumed. This apparent paradox was first noted by economist William Stanley Jevons in 1856, in his book The Coal Question.\nPeople were using the new, efficient — and thus less expensive — things to do many more things than they had been using the old, inefficient - and thus more expensive — things to do."},"thoughts/K-means":{"title":"K-means","links":["thoughts/quantization","thoughts/latent-factor-model","thoughts/convex"],"tags":["seed","CPSC340"],"content":"Assumption that we know how many clusters there are as a prior (k in K-Means). Designed for vector quantization: replacing examples with the mean of their cluster (collapsing a bunch of examples of a class down to a single example)\nCan also be seen as a really bad latent-factor model\nK-means partitions the space into convex regions, but clusters in the data might not be convex\nMinimize ∑i∈clusters​{∑j∈ithcluster​∣∣xj​−μi​∣∣2}\n\nPick some k\nAssign a cluster to k different points randomly\nIterate\n\nCenter update → calculate average for each cluster (using euclidian distance)\nLabel update → re-assign the data to the closest cluster center\nIf no labels changed, finish (model has converged)\n\n\n\nWarning: the clustering is initialization dependent and converges to a local minimum. Often requires some amount of random runs to approximate a good solution, pick best one.\nLimited to compact/spherical clusters in high-dimensions (which is poor for modeling clusters with the same mean but different distributions)\nAdvantages\n\neasy to implement and interpret\nsimple to understand\ncomputationally more efficient than other clustering algorithms\n\nDisadvantages\n\nneed to specify K\ndependent on initialization\nsensitive to scale of features (need to normalize/standardize)\n\nCost\n\nDominated by calculating distance from each xi​ to each mean wc​\n\nK-means, unlike the classification and regression models we studied in previous chapters, can get “stuck” in a bad solution. For example, if we were unlucky and initialized K-means with the following labels. To solve this problem when clustering data using K-means, we should randomly re-initialize the labels a few times, run K-means for each initialization, and pick the clustering that has the lowest final total WSSD."},"thoughts/KNN":{"title":"k-Nearest Neighbours (KNN)","links":["thoughts/fundamental-tradeoff","thoughts/linear-algebra","thoughts/Curse-of-Dimensionality"],"tags":["seed","CPSC340"],"content":"To classify an example, we find the k examples closest to the example and take the mode of the k examples.\nWorks based off of the assumption that similar features are likely to have similar labels\nEffects on fundamental tradeoff:\n\nAs k grows, training error increases and approximation error decreases.\nAs n grows, model complexity increases\n\nWe measure distance using the “norm” between feature vectors. The most common norm is the L2-Norm or Euclidean Norm\nPerformance\n\nO(1) training (just relies on training data)\nO(nd) predictions (O(d) distance calculations for all n examples)\nO(nd) space to store each training example in memory\n\nThis is non-parametric\n\n\n\nKNN can suck in high dimensions (see: curse of dimensionality)"},"thoughts/Kademlia-DHT":{"title":"Kademlia DHT","links":["thoughts/peer-to-peer","thoughts/DHT","thoughts/IP-Address"],"tags":["seed"],"content":"Summarization of the Kademlia paper\n\nA peer-to-peer distributed hash table (DHT)\n\nParticipating computers each have a node ID in the 160-bit key space. Key-value pairs are stored on nodes with IDs “close” to the key for some notion of closeness. A node-ID-based routing algorithm lets anyone efficiently locate servers near any given target key\nA get(key) operation traverses the identifier space and, upon hitting a node storing key, returns the key’s corresponding contact list. Then, the requesting node can contact these nodes, in parallel or in some application-specific way, to download the stored data.\nCore ideas\n\nUniform ID Space: names for both data and nodes share the same ID space\n\nBut collisions are ok as they mean different things depending on the context\n\n\nDecide what machine to place data on depending on a measure of ‘closeness’ between the name of the data and name of a machine\n\nEuclidean distance can be ambiguous, two entries could have the same distance\nXOR distance between x and some arbitrary y will always be unique\n\n\nEach node only needs information about its neighbours.\n\nIf the structure of the network is correctly chosen, then global properties apply to the whole network\nNested routing similar to IP routing\nEach machine has a lot of info about machines closest to it, less about machines far away from it\n\n\n\nSecurity\n\nIt is very vulnerable to Sybil attacks, which can result in the modification or erasure of any data in the network.\nIt also uses no message encryption whatsoever\n"},"thoughts/Kafka":{"title":"Kafka","links":["thoughts/TCP"],"tags":["seed"],"content":"Apache Kafka is a publish–subscribe based messaging system over TCP. In event streaming, an event (also called a message or record) is simply a record of a state change in the system.\n\nTwo main actors:\n\nProducers are applications that write data into topics (which each have their own message log)\nConsumer are applications that read data from topics\n\nA Kafka broker arranges transactions between producers and consumers. Brokers handle all requests from clients to write and read events. A Kafka cluster is simply a collection of one or more Kafka brokers."},"thoughts/Kant":{"title":"Kant","links":["thoughts/ethics","thoughts/privacy"],"tags":["seed","CPSC430"],"content":"The ethical theory of the German philosopher Immanuel Kant\nGood will and the Categorical Imperative\n\n“What is always good without qualification? Intelligence and courage can be used for bad. The only thing in the world that can be called good without qualification is a good will”\n\nAccording to Kant, what we want to do is of no importance. Our focus should be on what we ought to do (dutifulness).\nFor Kant, an imperative is a way in which reason commands the will. There are two major kinds of imperatives\n\nHypothetical: a conditional rule of the form “if you want X then do Y”\nCategorical: unconditional rule, always applies\n\n\nWhat’s good for the goose is good for the gander.\n\nHe believes that the only categorical imperative is the moral imperative: you should act only on moral rules that you can imagine everyone else following without deriving a logical contradiction.\nOr, the second formulation: act so that you always treat both yourself and other people as ends in themselves, and never only as a means to an end.\nAgainst Kantianism\nKant holds that every action is motivated from a rule. The appropriate rule depends upon how we characterize the action. Once we know the rule, we can test its value using the Categorical Imperative. What happens when no single rule fully explains the situation? Suppose I’m considering stealing food from a grocery store to feed my starving children. How should I characterize this action? Am I stealing? Am I caring for my children? Am I trying to save the lives of innocent people? Until I characterize my action, I cannot determine the rule and test it against the Categorical Imperative.\nIf we allow multiple rules to be relevant to a particular action then what do we do what relevant rules conflict? Kant distinguished between perfect duties and imperfect duties\n\nPerfect Duties are duties we are obliged to fulfil in every instance (e.g. telling the truth)\nImperfect Duties are duties we are obliged to fulfill in general but not in every instance\n\nPerfect duties prevail over imperfect duties. However, if there is a conflict between perfect duties, Kantianism does not provide us a way to choose between them.\nApplied to privacy\n\nGovernment believes it is morally right to monitor suspects\nThen what if everyone could monitor who they suspect\nThen anyone could monitor anyone, including government officials\nGovernments require a basic level of privacy to conduct their business (security clearance is a thing)\nThis is not possible if anyone could monitor government officials without consent/permission, leading to a contradiction\nThus, being able to surveil suspects without consent is immoral\n"},"thoughts/Kernel-Curriculum":{"title":"Kernel Curriculum","links":["thoughts/trust","thoughts/money","thoughts/meaning","thoughts/value","thoughts/security","thoughts/ethereum","thoughts/blockchain","thoughts/questions","thoughts/debt","thoughts/banking","thoughts/intentionality","thoughts/freedom","thoughts/tools-for-thought","thoughts/attention-economy"],"tags":["seed"],"content":"\n“Too often, the people disrupting any industry don’t understand deeply what it is they’re disrupting. This is definitely the case with cryptocurrencies and the current financial system. It really is well worth your time to stop and become familiar with more history so that you can understand why we are where we are, what led us here, and - only then - what solutions might actually benefit you and those you care about.”\n\nModule 0\n\nTrust\nMoney\n\nModule 1\n\nMeaning\nValue\nSecurity\nUnderstanding Ethereum\nThe promise of the Blockchain\n\nModule 2\n\nQuestions\nMoney and Speech\nDebt\nBanking\nEngineering Money\n\nModule 3\n\nIntentionality\nFreedom\nRemember\nTime\n"},"thoughts/Key-Sharing-Problem":{"title":"Key Sharing Problem","links":[],"tags":["seed"],"content":"\nThe key exchange problem describes ways to exchange whatever keys or other information are needed for establishing a secure communication channel so that no one else can obtain a copy.\n\n\nAlice locks the box with her lock and sends it to Bob\nBob locks the box with his lock and sends it back to Alice\nAlice removes her lock and sends it back to Bob\nBob removes his lock\nDouble Encryption Transfer\n\nBoth parties generate random keys KA​ and KB​\nSender sends encrypted message KA​(m)\nReceiver encrypts received message and sends it back KB​(KA​(m))\nSender decrypts message and sends it again KA−​(KB​(KA​(m)))=KB​(KA+​(KA​(m)))=KB​(m) (only valid if operation is associative!)\nReceiver decrypts final message KB−​(KB​(m))=m\n\n\n\nDiffie Hellman\nA way of performing key exchange over an insecure channel."},"thoughts/Knowledge-Argument":{"title":"Knowledge Argument","links":["thoughts/semantics","thoughts/Chinese-room-argument","thoughts/consciousness","thoughts/language","thoughts/potemkin-village"],"tags":["sapling","PHIL240A","PHIL451A"],"content":"One cannot learn what it is like to subjectively experience something without actually experiencing it\nFrank Jackson’s Knowledge Argument1\n\nMary is confined to a black-and-white room, is educated through black-and-white books and through lectures relayed on black-and-white TV. Through this, she learns everything there is to know about the physical nature of the world, including all there is to know about the causal and relational facts consequent upon all this, including of course functional roles.\nIf physicalism is true, she knows all there is to know.\nYet, if Mary is let out of her black-and-white room or given a colour TV, she will learn what it is like to see something red.\n\nThe implication here is that Mary cannot learn what it is like to subjectively experience red from reading physical facts (similar argument re: semantics in Chinese room argument).\nDennett’s Refutation of the Knowledge Argument1\n\n“And so, one day, Mary’s captors decided it was time for her to see colours. As a trick, they prepared a bright blue banana to present as her first colour experience ever. Mary took one look at it and said ‘Hey! You tried to trick me! Bananas are yellow but this one is blue!’ Her captors were dumbfounded. How did she do it? ‘Simple,’ she replied. ‘You have to remember that I know everything—absolutely everything— that could ever be known about the physical causes and effects of colour vision.\n\n\nNo double transduction\n\nTransduction is a conversion of one for another\nPeripheral/internal transducers are sense organs (e.g. eyes) that transform physically detectable properties to electrochemical signals\nThere is no further conversion of neural signals into a qualitative medium\n\n\nNo Cartesian theatre in the brain\n\nThere is no central place where all things ‘come together’ and consciousness happens\nPoint of view of the observer is smeared over a large volume of the brain with different networks (distributed model of representation)\n\n\nConsciousness is like fame\n\n“Just as becoming famous is not a precisely datable event like being transduced into a medium (like being televised), so achieving consciousness in the brain is not a precisely datable transition in the brain.\n\n\nNo qualia if intrinsic properties are instantiated by rather than represented by neural activity\n\nEssay\n\nCritically evaluate Frank Jackson’s “Knowledge Argument.” Does Mary learn something new, and if so, what exactly does she learn and what are the implications for physicalism? If she does not learn anything new, explain how and why this is the case.\n\nIn this paper, I posit that Frank Jackson’s “Knowledge Argument” is logically sound but hinges on many bold and often presumptuous premises to be true. As such, the conclusion should not be taken for granted without more clarifications around premises and terminology.\nI propose that Mary has all of the ‘know-that’ knowledge but none of ‘know-how’ knowledge. The act of Mary ‘seeing’ colour for the first time means that she learns new ‘know-how’ knowledge which she did not have. Physicalism, as a result, is preserved as Mary only has ‘know-that’ knowledge so does not know all physical facts about colour perception.\nThis relies on three facts:\n\nThere is a distinction between physical fact and physical knowledge\nThere are unknowable physical facts that give rise to new experiences\nKnow-how cannot be learned through know-that (e.g. reading and watching videos)\n\nBriefly, the conclusion that Frank Jackson posits is as follows:\n\nIt is possible for someone to know all physical information there is to know about x, still experiencing the act of seeing colour will teach her something about x.\nCertain experiences (e.g. experiencing colour) still teach Mary something new. Then, this means that all physical information there is to know about x do not completely describe all information about x.\nThis contradicts the theory of physicalism: that everything, including mental states, has a physical explanation. Extending this argument: if we could know every physical detail about someone else’s brain, one would still not understand what it is like to experience things for them (Nagel’s Bat Argument).\n\nHowever, this conclusion relies heavily on the first premise being true, that is, it being possible for one to “acquire … all the physical information there is to obtain about what goes on when we see [x]”. Let us first examine the explicit distinction between ‘physical knowledge’ and ‘physical facts’ here.\nKnowledge, specifically, entails all ‘knowable’ things. In the context of this paper, I define ‘knowledge’ as justified true belief. Subject S knows x if and only if1:\n\nx is true\nS believes that x\nS is justified in believing that x.\n\nI posit that there are facts (read true statements about object x) that cannot be known, let alone proven. For example, let us imagine that it is a fact that aliens do not exist. As of now, we have no suitable methodology or measurements that would allow us to confirm this fact (this would involve an exhaustive search of the universe which, as far as we know, is impossible). Thus, the set of all true statements about object x is equal to or larger than the set of all obtainable knowledge about x. To know all physical knowledge about x then does not necessarily imply knowing all physical facts about something.\nSpecifically, we note that the original Mary’s Room argument phrases knowing as knowing “all the physical information there is to obtain about what goes on when we see [x]” (emphasis added). Then, there may be some physical facts Mary may not know that contribute to her experience of experiencing ‘yellowness’ when seeing a banana for the first time that she may not have known ahead of time, despite her having complete physical knowledge of the banana. As a result, Mary experiences something new (namely, seeing yellow) but does not learn anything new.\n\nThe sharp-eyed may note that the previous argument relies on the fact that the set of all true statements about object x is explicitly larger than the set of all obtainable knowledge about x. However, the statement originally assumed that the set of all true statements is larger than or equal to the set of all obtainable knowledge. This exposes an edge case which could be problematic where the set of all true statements is exactly equal to the set of all obtainable knowledge (i.e. subject S knows all obtainable knowledge about x, which coincidentally is also the set of all true statements about x).\nI will show how this does not prove to be a problem for our argument. I raise one main question: Why is it obvious that she will learn something new about the world through an experience?\nTo be explicit, let us examine the framing of the Mary’s room2:\n\nAn epistemic subject A appears to have no access to particular items of knowledge about a subject B\nA cannot know that B has an experience of a particular quality Q on certain occasions\nThis particular item of knowledge about B is inaccessible to A because A never had experiences of Q herself\n\nThis is quite similar to Thomas Nagel’s Bat Argument3. Despite everything that Mary knows about seeing colour, she doesn’t know what it is like to see colour.\nSpecifically, the Knowledge Argument relies on knowledge that is explicit or codified and can be communicated via language. I argue that, conceptually, one cannot ‘know’ all there is to know about seeing colour through purely linguistically communicated language. I refer to this ‘linguistically unknowable’ knowledge as tacit knowledge.\nWe first examine three different kinds of knowledge4:\n\nAcquaintance knowledge: we get to know the characters of others (like friends) by being around them. This is a form of tacit knowledge.\nKnowledge-that: propositional knowledge, facts about the world obtained through reading, talking, and consuming content. This can be linguistically communicated.\nKnowledge-how: truly knowing how to do something, speaking, reading, etc. This is a form of tacit knowledge.\n\nMy position is that knowledge-how cannot be learned through only knowledge-that. Ryle calls this the Sufficiency Argument: “how could propositional knowledge be sufficient for knowing how to do something?”5\nFrom this list above, knowledge-that is the only form of transferrable or communicable knowledge. Knowledge-how is a form of tacit knowledge which is inexpressible via language. Knowledge-how, by definition, is only acquired through practical experience in the relevant context. To attempt to reconstruct knowledge-how using knowledge-that is to build a potemkin village, a sort of ‘facade’ of understanding. One can read all about playing a piano — the music theory, the muscles to actuate, the feel of the keys — yet fail to actually play the piano. Learning to play piano requires an embodied experience of feeling the keys and wiring the feedback between the concept of music in your brain to the motion of playing the notes.\nThus, in Frank Jackson’s Knowledge Argument, Mary has all the ‘know-that’ knowledge about the perception of colour but none of ‘know-how’ knowledge. The argument depends heavily on Physicalism stating that Mary knows all physical facts about any given object. I have shown through this paper that the premises of Jackson’s argument is dubious at best, namely the ability for Mary to know all physical facts about x. Thus, Physicalism is preserved.\nFootnotes\n\n\nhttps://plato.stanford.edu/entries/knowledge-analysis/ ↩ ↩2 ↩3\n\n\nhttps://plato.stanford.edu/entries/qualia-knowledge/#BasiIdea ↩\n\n\nhttps://warwick.ac.uk/fac/cross_fac/iatl/study/ugmodules/humananimalstudies/lectures/32/nagel_bat.pdf ↩\n\n\nhttps://plato.stanford.edu/entries/knowledge-how/#LingArgu ↩\n\n\nKnowing How and Knowing That: The Presidential Address, Ryle 1946 ↩\n\n\n"},"thoughts/LDP":{"title":"Linked Data Platform (LDP)","links":["thoughts/RDF","thoughts/HTTP"],"tags":["seed"],"content":"Think of an LDP as a way of interacting with RDF resources in a way that is similar to a web site with folders and documents in a RESTful manner.\nThe term “Linked Data” refers to an approach to publishing data that puts linking at the heart of the notion of data, and uses the linking technologies provided by the Web to enable the weaving of a global distributed database. By naming real world entities - be they web resources, physical objects such as the Eiffel Tower, or even more abstract things such as relations or concepts - with http(s) URLs, whose meaning can be determined by dereferencing the document at that URL, and by using the relational framework provided by RDF, data can be published and linked in the same way web pages can.\nRules of LDP:\n\nUse URIs as names for things\nUse HTTP URIs so that people can look up those names\nWhen someone looks up a URI, provide useful information, using the standards (RDF, SPARQL)\nInclude links to other URIs, so that they can discover more things\n\nWhat it enables\nLDP enables a very connected way of working with RDF data. You make HTTP requests to URI’s and they get resolved to resources (Containers or RDFSources), which you can then consume to get at all the triples. And of course you can create resources, update them, list members of a container, etc. In this way, you can build web applications, that use RESTful requests."},"thoughts/LLMs":{"title":"LLMs","links":["thoughts/autoregressive-model","thoughts/machine-learning","thoughts/NLP","thoughts/emergent-behaviour","thoughts/intelligence","thoughts/observer-expectancy-effect","thoughts/transformers","thoughts/teaching","thoughts/writing","thoughts/garbage-in-garbage-out","thoughts/epistemology","thoughts/plurality","thoughts/cozy-software"],"tags":["sapling"],"content":"Large Language Models (LLMs) are autoregressive foundational machine learning models that use machine learning to process and understand natural language. They seem to have emergent properties of intelligence, though this could just be the observer-expectancy effect\nSee also: transformers\nTeaching\nThe widespread use of ChatGPT poses a pedagogical question: how do we assess thinking?\nI suspect ChatGPT will do to writing what calculators did to math. That is, they made it much more accessible to the masses but in the process of doing so, lost the value in the actual process of doing math.\nWe do math by hand to help internalize it in our minds, to naturalize and practice the mind to thinking in that manner. Similarly, we write to naturalize the mind to critical and thorough thought.\n\n“The hours spent choosing the right word and rearranging sentences to better follow one another are what teach you how meaning is conveyed by prose. Having students write essays isn’t merely a way to test their grasp of the material; it gives them experience in articulating their thoughts.”\n\nAI generated content\nWill produce an influx of AI generated content and be modern day automated content mills. However, this is concerning for a variety of reasons.\nDon’t shit where you eat! Garbage in garbage out! When it comes time to train GPTx it risks drinking from a dry riverbed.\nMaggie Appleton calls this ‘human centipede epistemology’: models are trained on generated content and ideas lose provenance. “Truth” loses contact with reality and degrades even more.\nTed Chiang on ChatGPT: “the more that text generated by large language models gets published on the Web, the more the Web becomes a blurrier version of itself … Repeatedly resaving a jpeg creates more compression artifacts, because more information is lost every time”\nProgrammers won’t be asking many questions on StackOverflow. GPT4 will have answered them in private. So while GPT4 was trained on all of the questions asked before 2021 what will GPT6 train on?\nA cautionary tale on AIs to replace human connection: all the better to see you\nGood-enough content\nAI is helpful in situations where you need ‘good enough’ code/art/writing where the value of the output outweighs the process.\n\nhttps://twitter.com/gordonbrander/status/1600469469419036675\nhttps://twitter.com/jachiam0/status/1598448668537155586\n\nI don’t think it’s ready to replace anything that requires rigorous thought or reasoning quite yet because it is still very prone to confidently hallucinating wrong answers. LLMs should acts as an atlas and not a map (see: plurality)\nEnd-user programming\nSee also: personal software\nSource\n\nI think it’s likely that soon all computer users will have the ability to develop small software tools from scratch, and to describe modifications they’d like made to software they’re already using\n\nGeoffrey Litt’s talk at Causal Islands\n\nShowed off a really interesting demo which integrated LLMs into Potluck, allowing a bidirectional binding between a natural language description of a pattern/search and the actual code behind it\n\nThis also helps with learnability. Using the AI helps you understand the underlying system by seeing how the LLM translates the concepts into code\n\n\nQuestions to keep asking: how do we recover from a state where the LLM produces a wrong result but is confident in its answer?\n\nHow might we nudge LLMs to produce more correct answers under human feedback in a non-text environment?\n\n\n"},"thoughts/LR-Permissionless-Result":{"title":"LR Permissionless Result","links":["thoughts/consensus","thoughts/system-model","thoughts/access-control"],"tags":["seed"],"content":"Shown by Lewis-Pye and Roughgarden in 2022\nDeterministic consensus is not possible for decentralized protocols with a Byzantine, permissionless system model.\nPermissionless means that it does not enforce access control and allows the number and identity of participants to change without notice (under some number of participants bounded by N)."},"thoughts/LSP-Server":{"title":"LSP Server","links":["thoughts/Unicode"],"tags":["seed"],"content":"Source\nThe idea behind the Language Server Protocol (LSP) is to standardize the protocol for how tools and servers communicate, so a single Language Server can be re-used in multiple development tools, and tools can support languages with minimal effort.\nA language server runs as a separate process and development tools communicate with the server using the language protocol over JSON-RPC.\nCommon signals:\n\ntextDocument/didOpen: notifies the language server that a document is open. From now on, the truth about the contents of the document is no longer on the file system but kept by the tool in memory. The contents now has to be synchronized between the tool and the language server using textDocument/didChange\ntextDocument/didChange: tool notifies the server about the document change and the language representation of the document is updated by the language server.\ntextDocument/publishDiagnostics: the language server analyses this information and notifies the tool with the detected errors and warnings\ntextDocument/definition: the user executes “Go to Definition” on a symbol of an open document. Has two parameters:\n\nthe document URI and\nthe text position from where the ‘Go to Definition’ request was initiated to the server\n\n\ntextDocument/didClose: notification is sent from the tool informing the language server that the document is now no longer in memory. The current contents are now up to date on the file system.\n\nGotchas\n\nColumn number/length should be calculated using utf16 offsets instead of Unicode codepoints\n"},"thoughts/LSTM":{"title":"LSTM","links":["thoughts/convolutional-neural-networks"],"tags":["seed","CPSC340"],"content":"Long short term memory (LSTM) models are variant of RNNs. They are modified to try to remember short-term z and long-term dependencies c. The purpose of memory cells is to remember things for a long time.\nLSTMs were the practical analogy of convolutional neural networks for RNNs\n\nForget gate ft​\n\nIf element ‘j’ of ft​ is 0, then we clear element ctj​ from the memory (set it to 0).\nIf it is 1, then we keep the old value.\n“Given the input and previous activation, are the elements in memory still relevant?”\n\n\nInput gate it​\n\nIf element ‘j’ of it​ is 0, then we do not add any new information to ctj​ (no input).\nIf it is 1, then we “value” to the memory (where “value” is also a function of input and previous at ).\n“Given the input and previous activation, should I write something new to memory?”\n\n\nOutput gate ot​\n\nIf element ‘j’ of ot​ is 0, then we do not read value ctj​ from the memory (no output).\nIf it is 1, then we load from the memory.\n“Given the input and previous activation, should I read what is in memory?”\n\n\n"},"thoughts/Lambda-Calculus":{"title":"Lambda Calculus","links":["thoughts/notation","thoughts/functional-programming"],"tags":["seed"],"content":"λ-calculus is a notation for functions and applications. The main ideas are applying a function to an argument and forming functions by abstraction.\nFor example, to represent f(x)=x2−2x+5 in λ-calculus, we can write a λ-term\nλx[x2−2x+5]\nFor those coming from programming languages like JavaScript, this is equivalent to\nx =&gt; x^2 - 2*x + 5\nThis can be read as an expression ‘waiting’ for a value a for the variable x. When given a value a, it is substituted to become a2−2a+5. The act of receiving a value a is referred to as applying the λ-term to the argument a. This is written in notation Ma to denote applying function M to argument a.\nThe central principle of the λ-calculus is β-conversion or β-reduction which is effectively saying that you can substitute values into arguments of functions, effectively binding them.\nMulti-argument operations\nFor example the Pythagorean theorem:\nHypotenuse Length:=λa[λb[a2+b2​]]\nOr in JavaScript:\na =&gt; b =&gt; Math.sqrt(a^2 + b^2)\nThis is what currying does in any functional programming language.\nBinding\nWe write M[x:=N] to denote the substitution of N for the free occurrences of x in M.\nλx[M] can then be thought of as binding x in the term M.\nThe following examples are in Scheme/Lisp/Racket:\n\n(lambda (x) x), the x is a bound variable. There are no free variables so the expression can be considered a combinator.\n(lambda (x) y), the y is a free variable and thus the expression is not a combinator.\n(lambda (x) (lambda (y) x)), there is only one variable x (y exists but it is a formal argument of the expression and not used).\n\nCombinators\nA λ-term with no free variables. Effectively a self-contained or completely specified operation.\n\nS combinator: λx[λy[λz[xz(yz)]]]\n\nconst fn = x =&gt; y =&gt; z =&gt; (x(z))(y(z))\n\n\nK combinator (constant function):  λx[λy[x]]\n\nconst fn = x =&gt; _y =&gt; x (e.g.) f(5)(_any) = 5\n\n\nI combinator (identity function): λx[x]\n\nconst fn = x =&gt; x\n\n\nB combinator (right associative operator): λx[λy[λz[x(yz)]]]\n\nconst fn = x =&gt; y =&gt; z = x(y(z))\nThis is disambiguated from the C combinator\n\n\nC combinator (left associative application): λx[λy[λz[xyz]]]\n\nconst fn = x =&gt; y =&gt; z = (x(y))(z)\n\n\nω combinator: λx[xx]\n\nconst fn = x =&gt; x(x)\n\n\nΩ combinator: ωω\n\nEquivalent to application of the self-application combinator to itself: λ(λx[xx])[λx[xx]λx[xx]]\n\nλ(ω)[ωω]≡ωω (unsure if this is actually true)\n\n\nconst fn = (x =&gt; x(x)) =&gt; (x =&gt; x(x))(x =&gt; x(x))\n\n\nY combinator: λf[(λx[f(xx)])(λx[f(xx)])]\n\nHigher order function that takes in a function that isn’t recursive and returns a version of the function that is recursive\nconst fn = f =&gt; (x =&gt; f(x(x)))(x =&gt; f(x(x)))\n\n\n\nY Combinator in detail\nSource\nImplicit recursion operator, how might you define a recursive function without naming it?\nIt’s a tried-and-true principle of functional programming that if you don’t know exactly what you want to put somewhere in a piece of code, just abstract it out and make it a parameter of a function.\n(define factorial\n\t(lambda (n)\n\t  (if (= n 0)\n\t\t  1\n\t\t  (* n (factorial (- n 1))))))\nWe can abstract out the recursive call with another function that is provided as a parameter\n(define almost-factorial\n\t(lambda (f)\n\t\t(lambda (n)\n\t\t\t(if (= n 0)\n\t\t\t\t1\n\t\t\t\t(* n (f (- n 1)))))))\nThe Y combinator then effectively turns (Y almost-factorial) into something equivalent to factorial\nIn lazily evaluated languages, you can actually pass itself in as an argument and it will actually work correctly. That is, (define factorial (almost-factorial factorial)) will produce an actual factorial function.\nUnfortunately, this will infinitely loop for languages that use strict evaluation.\nLet us imagine that we want to make some function fn recursive. We can\n\nDo what we did above to create almost-fn by abstracting out the recursive call to fn as an argument f.\n(define almost-fn-0 (almost-fn identity)) where identity is just (lambda x x). This works for the base case of fn.\nThen, we construct almost-fn-1 by (define almost-fn-1 (almost-fn almost-fn-0) which is equivalent to (define almost-fn-1 (almost-fn (almost-fn identity))). This works for the base case and one level of recursion.\nThe natural extension of this is to create (define almost-fn-infinity (almost-fn (almost-fn (almost-fn ... identity)))) which will work for all levels of recursion. almost-fn-infinity is the fixpoint of almost-factorial. That is, fixpoint-fn = (almost-fn fixpoint-fn).\nWe can try to reverse engineer the definition of fixpoint-fn by repeatedly substituting the right-hand side of the equation. That is, we get fixpoint-fn = (almost-fn (almost-fn (almost-fn ...))). Wouldn’t it be great to get a function that takes in almost-fn and produces fixpoint-fn? This is the Y combinator.\nY combinator then is (define Y fn) = fixpoint-fn.\n\nDeriving the lazy Y combinator\n\nWe know that (define fn fixpoint-fn) = fixpoint-fn\nBy reflexivity, we get (define Y fn) = fixpoint-fn = (fn fixpoint-fn)\nSubstituting, we get (define Y fn) = (fn (Y fn)). This is a definition of Y but it will only work in a lazy language and, because it uses Y in its own definition, is not a true combinator.\n\nIn a strict language, we need to avoid using Y in its own definition.\nLet’s modify the factorial function to take itself as an extra argument when you call the function:\n(define part-factorial\n\t(lambda (self)\n\t\t(lambda (n)\n\t\t\t(if (= n 0)\n\t\t\t\t1\n\t\t\t\t(* n (self (- n 1)))))))\nYou would, however, need to call (part-factorial part-factorial 5) to calculate the factorial. To take this into account, we also need to modify the recursive call.\n(define part-factorial\n\t(lambda (self)\n\t\t(lambda (n)\n\t\t\t(if (= n 0)\n\t\t\t\t1\n\t\t\t\t(* n (self self (- n 1))))))) ; extra self\nThe self self call here isn’t a problem as it only happens in the non-base case. We can try to make the inner part of part-factorial more like almost-factorial by creating a let binding for self self\n(define part-factorial\n\t(lambda (self)\n\t\t(let ((f (self self)))\n\t\t\t(lambda (n)\n\t\t\t\t(if (= n 0)\n\t\t\t\t\t1\n\t\t\t\t\t(* n (f (- n 1)))))))) ; extra self\nNote that this won’t actually work because the let binding makes self self get evaluated regardless of base case so will send us into an infinite loop in strictly evaluated languages. We can make it lazy because we can turn any let binding into a lambda expression\n(let ((x &lt;expr1&gt;)) &lt;expr2&gt;)\n==&gt; ((lambda (x) &lt;expr2&gt;) &lt;expr1&gt;)\n; equiv as you are basically making an IIFE that binds &lt;expr1&gt; to x\n; scoped to the lambda\nRewriting the above,\n(define part-factorial\n\t(lambda (self)\n\t\t((lambda (f) ; this is `almost-factorial`\n\t\t\t(lambda (n)\n\t\t\t\t(if (= n 0)\n\t\t\t\t\t1\n\t\t\t\t\t(* n (f (- n 1))))))\n\t\t\t(self self))))\nThen after pulling out almost-factorial, part-factorial is\n(define part-factorial\n\t(lambda (self) (almost-factorial (self self))))\n \n(define factorial (part-factorial part-factorial))\nInlining part-factorial,\n(define factorial\n\t(let (part-factorial)\n\t\t(lambda (self) (almost-factorial (self self)))\n\t(part-factorial part-factorial)))\nReplacing the let binding with a lambda expression like above,\n(define factorial\n\t((lambda (part-factorial) (part-factorial part-factorial))\n\t(lambda (self) (almost-factorial (self self)))))\n \n; renaming part-factorial x to make it more concise\n; we can also rename self to x as the scope is different from x\n \n(define factorial\n\t((lambda (x) (x x))\n\t(lambda (x) (almost-factorial (x x)))))\nFinally, we can abstract  away the call to almost-factorial (which is actually the lazy Y combinator!)\n(define Y\n\t(lambda (f)\n\t\t((lambda (x) (x x))\n\t\t(lambda (x) (f (x x)))))))\n \n(define factorial (Y almost-factorial))\n \n(define almost-factorial\n\t(lambda (f)\n\t\t(lambda (n)\n\t\t\t(if (= n 0)\n\t\t\t\t1\n\t\t\t\t(* n (f (- n 1)))))))\nWe can apply the argument of the outer lambda to its definition to arrive at the common definition of the normal-order Y combinator.\n(define Y\n\t(lambda (f)\n\t\t((lambda (x) (f (x x)))\n\t\t(lambda (x) (f (x x))))))\nAgain, this application in JS to make it more clear:\n(x =&gt; x(x))(x =&gt; f(x(x)))\n// replacing x in the fn with (x =&gt; f(x(x)))\n(x =&gt; f(x(x)))(x =&gt; f(x(x)))"},"thoughts/Lattice-Proposal":{"title":"Lattice Proposal","links":["thoughts/tools-for-thought","thoughts/Rhizome-Proposal","thoughts/idea-list","thoughts/memory-palace","thoughts/git","thoughts/information-retrieval","thoughts/interoperability"],"tags":["seed"],"content":"Potential exploration on my gripes with existing tools for thought using principles from Rhizome. Extracted from my idea list\n\nTransclusion of blocks supported by default\nFull revision history with the granularity of single operations\nWrite first, organize later: always-on ‘default note’\n\nMake it super easy to split out content into a new note\n\n\nGood daily notes, choose what to carry over from the previous day\n\n\n\n\nBlock based editors are neat, it allows more drafting/moving stuff around really easily\n\nIf you drag outside the page it makes a new note, super easy to break down big complex topics into more granular/atomic notes\nSpatially consistent? Memory palace vibes → https://twitter.com/jordanmoore/status/1418942880941477891\n\n\nCard based backlinks/outgoing links\n\nOn the left, you can see all the notes that link to the current page\nOn the right, you can see all outgoing links\nHovering on each card will reveal another layer which those ones are connected to\n\n\nTimeline view\n\neach note will come with a date so you can chronologically order notes and view them that way (kind of like a git history) if you brain works better that way\n\n\nGood global search (see: information retrieval)\nInteroperability of data, easy export\n"},"thoughts/Law-of-Requisite-Variety":{"title":"Law of Requisite Variety","links":["thoughts/plurality"],"tags":["seed"],"content":"Ashby’s Law of Requisite Variety: If a system is to be stable, the number of states of its control mechanism must be greater than or equal to the number of states in the system being controlled.\nOnly variety can absorb variety.\nSee also: pluralism"},"thoughts/Lindy-effect":{"title":"Lindy Effect","links":[],"tags":["seed","pattern"],"content":"Lindy Effect: future life expectancy of a bit of technology is proportional to its current age (i.e. the longer something has survived, the more likely it is to have a longer remaining life expectancy).\nLongevity implies a resistance to change, obsolescence or competition and greater odds of continued existence into the future.\nMany traditional materials have the attractive property that they look bad before they act bad and, furthermore, the problems with traditional materials are well understood."},"thoughts/Link-Layer":{"title":"Link Layer","links":["thoughts/Network-Layer","Physical-Layer","thoughts/MAC","thoughts/UDP","thoughts/Overlay-Network"],"tags":["seed","CPSC317"],"content":"Layer 4, the layer below the Network Layer and layer above the Physical Layer\n\nHardware\nUnit: Frame\nResponsibilities: Routes frames to adjacent machines (“direct” connection) on a local area network (LAN). Defines the format of data on the network\nDetails\n\nBreaks up chunks into frames, contains some metadata\nHub model (share the same medium) means that we don’t need to run wires between every computer (implicit broadcasting). Downside is we have to now specify who the message is for (usually using 48 bit media access control (MAC) addresses)\n\n\n\nVXLAN\nLAN but across local networks… spooky\nIt encapsulates the MAC frame into a UDP datagram for transport across an IP network. This creates an overlay network"},"thoughts/Locutus":{"title":"Locutus","links":["thoughts/local-first-software","thoughts/WebAssembly","thoughts/CRDT"],"tags":["seed"],"content":"Locutus is a local-first decentralized key-value database. It uses the same small world routing algorithm as the original Freenet design, but each key is a cryptographic contract implemented in WASM, and the value associated with each contract is its state.\nLocutus is not append-only and has mutable state.\nSplits are merged using CRDTs"},"thoughts/MAC":{"title":"MAC","links":["thoughts/Asymmetric-Key-Cryptography"],"tags":["seed"],"content":"\nMessage authentication code\n\nNot to be confused with MAC addresses.\n\nAdd a secret to the end of each message that is also hashed. It is extremely unlikely that anyone who doesn’t know the secret to come up with an appropriate hash\nShared secret s (this is a symmetric key)\nHash is computed not on message m, but on m+s\n\nBob sends message h=H(m+s)\nAlice receives (m,h) and computes H(m+s)\nIf h=H(m+s), message is considered signed\n\n\nFast because asymmetric key crypto is not necessary\n"},"thoughts/Making-and-Maintenance-of-OSS":{"title":"The Making and Maintenance of Open Source Software","links":["posts/paid-open-source","thoughts/creation-vs-maintenance","thoughts/attention-economy","thoughts/infrastructure","thoughts/utility","thoughts/governance","thoughts/feedback-loops","thoughts/positive-sum","thoughts/types-of-goods","thoughts/incentives","thoughts/decentralization"],"tags":["seed","book"],"content":"Full post: A case for funding Open Source\nQuotes\nMaintenance\nA lot of initiatives like HacktoberFest are widely championed as ‘good for open source’ yet frequently cause maintainers to seize up with anxiety because such initiatives often attract low-quality contributors looking to snag a free t-shirt. “Open source code is public, but it doesn’t have to be participatory: maintainers can buckle under excess demand for their attention”\nThis is akin to a writer being asked to edit and make changes to the same book every day, into perpetuity, long after they’ve reaped the initial financial and reputational rewards from its creation. What’s more, unlike other content, open source code is relied upon by people, companies, and other institutions that need it to keep working, long after the maintainer’s interest may have waned.\nWhen Richard Stallman first described free software as “free as in speech, not free as in beer,” the distinction he wished to make is that the term “free” referred to what one could do with the software, rather than to its price.\nThe common misconception of software is that it is often characterized as “zero marginal cost,” meaning that it can be distributed for nearly nothing, regardless of how many additional people consume it. The problem is not that simple. Code is nearly free to distribute, but maintenace can still be expensive. External contributions don’t necessarily reduce the burden of maintenance either, because they still require someone to review and merge them.\nThe work of an open source developer goes beyond the initial costs of creation. Maybe developers can’t help but make things, and share the things they make, but every time they do, and ever time they find success, a tiny, invisible clock begins to tick, and they’re tasked with managing the care and feeding of their code into perpetuity.\nDependencies and Infrastructure\nThere’s even a term called the bus factor, where project health is a measured by the number of developers that would need to get hit by a bus before the project is in trouble.\nHowever, this ‘theory’ no longer matches up with empirical observation. In fact, less than 5% of developers were responsible for over 95% of code and social interactions on GitHub. The average software developer these days easily relies on hundreds of open source projects to write code these days, it’s inevitable that they’ll only be able to passively consume most of them.\nInfrastructure is recursively defined by public consensus. It’s the set of structures that we’ve collectively decided are most valuable in any given moment, and, therefore, its boundaries and definitions are expected to change over time.\nLike a bridge that needs to support more traffic that it was built for, every social platform is scrambling to upgrade its infrastructure to accommodate the volume of social interactions we’re dealing with today. Platforms need to build skyscrapers where there were once villages.\nIdentity\nLike joining a club, it’s not about how many times you’ve attending meetings but how you self-identify, and how others identify you, that makes you a ‘member’. Some attendees might come every week for years and still not be considered part of the group.\nCode as Content\nCode, like any other type of content available online today, is trending toward modularity: a mille-feuille layer cake of little libraries instead of one big, jiggling, Jell-O mold.\n“Like a book or video, code is just a bunch of information, packaged up for distribution. But its role as a utility is more explicit.”\nToday, “content” is better understood not as a thing we set out to make — as an automaker might exist solely to produce cars — but as “an externality from [our] existing social systems.” Content is a snapshot of our civilization.\nPlatforms and governance\nA “Benevolent Dictator for Life” or BDFL for short, describes authors of open source projects who retain control even as the project grows. A great example of this is Linus Torvalds, who even after 14,000 unique contributors to the Linux kernel, still is the only person allowed to merge contributions into the main branch.\n“Like a talent agency, platforms add value to creators by first improving their distribution, exposing them to potentially millions of people. … This feedback loop is positive sum, encouraging more creators to join. So long as more people keep using the platform, there’s no sense that any one creator will ever suck up all the oxygen in the room.”\nCommons-based Peer Production\n“At a company, only employees can do the work, limited by their job function. But in a commons, anyone can stumble upon an advertised task and volunteer themselves. By removing ‘property and contract’, the commons will theoretically select for the best person for the job at a lower cost.”\nSee also: types of goods\nNon-rivalry: If I download code from GitHub, my decision doesn’t diminish your ability to download that same code. (By contrast, if I bite into an apple and hand it to you, there is now less apple for you to eat) — a key point here is that open source software isn’t necessarily non-rivalrous especially when it comes to marginal increase in maintenance for each additional user\nNon-excludability: If someone owns my copy of my code, it is difficult for me to prevent them from sharing it with others. (By contrast, if I build a theme park, I can prevent people from entering by putting up a turnstile or charging admission)\nThe implication is that support can be handled in a fully decentralized manner that will distribute its costs among users — but someone still needs to review, manage, and process these reports. This maintenance cost is referred to as the “servicing costs” of software, noting the “asymmetry between the low cost of community participation and the high cost that others’ participation places on the leaders of the community.” She compares the problem to traffic congestion, where each person wants to drive their own car, but in doing so increases the congestion experienced by others, and, eventually, themself.\nIncentive Structures\n\n“Creation is an intrinsic motivator, maintenance usually requires extrinsic motivation” — @balupton\n\nIf production runs on intrinsic motivation, money is an extrinsic motivator that is thought to interfere with an already well-coordinated system. Although the commons might not be as profitable as the firm, it’s also more resilient, because the currency of its transactions is the desire to participate, rather than money.\n\nExternal, expected rewards diminish the intrinsic motivation of the fundraising open-source contributor. It risks transporting a community of peers into a transactional terminal. And that buyer-seller frame detracts from the magic that is peer-collaborators.\n\n“Whereas active contributors show interest in adding value to others early on, casual contributors demonstrate an acute, personal need at the outset.”\nThere are two types of funders that care enough to spend money on open source:\n\nInstitutions: usually companies, but also governments and universities. They spend money to influence and access a project they care about — especially gaining priority for issues and pull requests that concern them. They can also pay to gain brand influence through the open source project\nIndividuals: usually developers who are direct users. Politicians who fund their campaigns from grassroots donations are generally viewed more favorably by the public than are those who are funded by corporate donations. I’m not sure that open source is so different.\n\nProjects and Production Models\nSimilar to governance models\n“While some open source developers write code in public from the very beginning, many prefer to do their initial creative work in private, so they can properly articulate their ideas before opening the project up for feedback. Even if developers do publish their code early on, they may not advertise it widely until they have something ready for release.”\nBased off of arelationship between contributors and users, we can think of projects in terms of their contributor growth and user growth.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHigh user growthLow User GrowthHigh contributor growthFederations (e.g. Rust)Clubs (e.g. Astropy)Low contributor growthStadiums (e.g. Babel)Toys (e.g. ssh-chat)\n\nFederations: ‘bazaars’, the epitome of open source projects. Small percentage of overall # of projects. Complex to manage (typically develop decentralized governance structures)\nClubs: roughly overlapping group of contributors and users. May not have huge reach, but loved and built by a group of enthusiasts.\nStadiums: powered by a few main contributors, generally widely-depended upon packages. Centralized structure.\nToys: effectively personal projects\n"},"thoughts/Mangrove-Theory-of-the-Internet":{"title":"Mangrove Theory of the Internet","links":["thoughts/digital-commons","thoughts/digital-mindfulness","thoughts/pace-layers","thoughts/ephemereal-content"],"tags":["sapling","pattern"],"content":"Tools for Communication\nSource: vgr on Twitter\nRelated: tools for digital commons\nStream interfaces like Discord and Slack basically create a single global timeline that moves at a certain rate. If you miss a certain conversation, it flows away.\nSomeone once described Slack as the online version of the open office arrangement — it puts the pressure of everything being visible all the time to everybody on the individual.\nWe do “gardening” in our tools for thought but it’s not exactly social first. Sure, we can leave messages for each other but this is not communication. It’s more like background cueing/prompting.\nHow can we create mangroves? Garden/forest type ecosystems which have gently flowing multi-branched stream systems, designed for digital mindfulness and non-linearity?\nCan we create different pace layers for interaction?\nRelated: ephemereal content"},"thoughts/Mass-Hysteria":{"title":"Mass Hysteria","links":["thoughts/Where-is-My-Flying-Car","thoughts/Scientific-Freedom"],"tags":["seed"],"content":"Xhosa Memetic Plague\nThis, in a weird way, could also potentially explain the greatly increased numbers of extremist opinions that are seemingly so widespread.\nThe following excerpt is from Where is My Flying Car:\nThe Xhosa are a southeast African tribe whose economy and culture were traditionally based on cattle-herding. In the spring of 1856, Nongqawuse, a 15-year-old girl, heard the voices of her ancestors telling her that the Xhosa had to kill all their cattle and destroy their hoes, pots, and stores of grain. Channeling the ancestors, Nongqawuse explained that once this had been done the very ground would burst forth with plenty, the dead would be resurrected, and the interloping Dutch-German Boers would be driven from their lands. Surprisingly enough, the beliefs found fertile ground among the Xhosa and spread like wildfire, within months receiving the imprimatur of their king.\nMany members of the tribe slaughtered their cattle; by the end of 1857, over 400,000 had been killed. The Xhosa also refrained from planting for the 1856-1857 growing season, and as a consequence, there was no harvest. No cattle, no harvest. It is estimated that 40,000 Xhosans starved to death and that about an equal number fled the country in search of food. By the end of 1858, three-quarters of the Xhosa were gone.\nIt stands as a stark reminder that when social feedback, superstition, hopes and desires, and the suppression of doubt and skepticism in the name of faith line up, the resulting movement can make an entire people believe and do horribly self-destructive things that are completely at odds with common sense.\nEaster Island\nExcerpted and lightly edited from Scientific Freedom\nPolynesians arriving on this remotest of Pacific islands around 900 AD found it hospitable and covered in dense forest. However, drawing on a wide range of archaeological and forensic science, we conclude that the forest finally disappeared by about 1600 AD to provide the most extreme example of forest destruction in the Pacific.\nThis deforestation came about largely because the islanders needed trees for the construction and transport of an escalating number of huge stone statues that played an essential role in the islanders’ religious and social lives.\nUnfortunately, the island’s ecology is totally dependent on trees, and demand for them eventually outstripped Nature’s supply. Without. trees, the islanders could not build long-range canoes, and so deep-sea fish, dolphins, porpoises, and tuna became inaccessible. Loss of trees also led to increased soil erosion and poor crop yields.\nIt is estimated that by 1774 AD, when Captain Cook visited the island, its once prosperous population had declined by about 70% since its break between 1400 AD and 1600 AD, and relates that Cook described the islanders as “small, lean, timid, and miserable”.\n\nWhat did the Easter Islander who cut down the last palm tree say as he was doing it?\n\nIt is likely that one or two dissenting voices would always have been railing about the madness of trying to cover the island with statues. But who would have listened? Statue building was apparently the principal mode of competition among the island’s dozen or so factions. If one faction withdrew from the statue-building race, it would effectively be throwing in the towel. The leadership might have pointed out that the island still had a viable number of trees and a much larger number of saplings. We do not have to worry, therefore, their leaders might have said, the saplings will grow in due course and more than replace the trees we take.\nUnfortunately, the island is subject to very strong winds. An exceptionally strong gale could destroy the entire tree cover if it had already been seriously depleted, and the saplings would suddenly be left with no cover and soils reduced by erosion. Tree cover would most likely have come to a catastrophic end, therefore, rather than dwindling away imperceptibly.\nTheir actions had caused the island’s ecology to become precariously and uncontrollably balanced. Once tree cover had fallen below a certain critical level, the islander’s fate would have been sealed."},"thoughts/Materialism":{"title":"Materialism/Physicalism","links":["thoughts/mind-body-problem","thoughts/qualia"],"tags":["seed","PHIL451A"],"content":"Materialism/physicalism is the view that the universe is made up of particles and behaviour (esp. consciousness) arises out of it. There is nothing special distinguishing mind from matter.\n\nTo be conscious is to have subjective human experience, to have qualia.\n\nNo one person experiences red, the smell of garlic, or the warmth of the sun in the same way.\nYet, it feels like consciousness is in intrinsic property. We don’t know if someone or something is consciousness unless we ask them. However, the inability to report a subjective experience is not the same thing as not having one. (locked-in syndrome). For all we know, that rock could be conscious.\nAs an extended thought experiment, how similar do minds have to be to human ones to be considered conscious?\nOn why physicalism is a poor theory\n“Physical” is not well-defined\nHempel’s Dilemma:\n\nIf we define “physical” as what contemporary physics tells us is the physical, then physicalism is likely to be false\nIf we define “physical” by what the ideal, completed physics tells us is the physical, then physicalism is empty (because don’t know what that physics will be)\n"},"thoughts/Matrix":{"title":"Matrix","links":["thoughts/Raft-Consensus-Algorithm"],"tags":["seed"],"content":"\nAn open network for secure, decentralized communication\n\nWebsite\nMainly for messaging + voice + video but theoretically can handle any type of real-time data.\nA decentralized conversation store; when you send a message in Matrix, it is replicated over all the servers whose users are participating in a given conversation. (helps to tackle data availability)\nModel of event log delivery and consensus feels very similar to Raft\nUses HTTPS + JSON by default but also supports other transports like WebSockets or Matrix via CoAP + Noise\nSDK seems well-maintained"},"thoughts/Matthew-Effect":{"title":"Matthew Effect","links":[],"tags":["seed"],"content":"The Matthew effect of accumulated advantage, Matthew principle, or Matthew effect for short, is sometimes summarized by the adage “the rich get richer and the poor get poorer”"},"thoughts/Merkle-DAG":{"title":"Merkle-DAG","links":["thoughts/git","thoughts/IPFS","thoughts/docker"],"tags":["seed"],"content":"A directed acyclic graph where nodes correspond to versions of the content and edges correspond to changes (diffs).\nEach node has an identifier which is the result of hashing the node’s content.\nMerkle DAG nodes are immutable. Any change in a node would alter its identifier and thus affect all the ascendants in the DAG, essentially creating a different DAG\nExamples of DAGs include:\n\ngit\nIPFS\nDocker images\n"},"thoughts/Microworld":{"title":"Microworld","links":[],"tags":["seed"],"content":"\nCreating a simplified world model to explain a system because the real one is too complex to grasp\n\nRestricted microworlds give students the freedom to explore, without needing to ask for permission or reassurance from teachers\nNewton “understood” the universe by reducing whole planets to points that move according to a fixed set of laws of motion. Is this grasping the essence of the real world or hiding its complexities?"},"thoughts/Milieu":{"title":"Milieu","links":["thoughts/taste","thoughts/reading"],"tags":["seed"],"content":"Source\nA milieu is the culture contained in your unique set of connections.\nIt is an ever-shifting, individual configuration of information flows. The Twitter feed you have curated is a milieu. Your friend group (which is not the same as the friend groups of the other people in that group!) is a milieu.\nCurating mileau\nSelecting these sources and crafting that culture is a form of taste\n\nWhat sets Frusciante apart as a guitarist is that he is uncommonly undistractable (“he can really focus and concentrate and apply himself to his craft in a way where the noise of the world has less of an impact”). In the words of this essay, this can be rephrased as: Frusciante has an uncommon discipline about what he lets into his senses; he is uncommonly deliberate about curating his milieu\n…\nHe knows how their playing styles interrelate and where he wants to position himself in that landscape of playing. He has a map of guitarists. The map is organized by technique but also by the effect they have on him, the kind of songwriting they open up. He tweaks this web of influences to change his playing.\n\nReading\nSpeaking with authors through their written work triggers the same neural circuits that produce imitation of desire. By stocking a bookshelf judiciously, you can express a preference over preferences — “what should I value? What do I want to spend my waking hours thinking about?” — and act on it through careful, honest reading. This engineering is safe: most authors exert their influence slowly, over hundreds of pages, and if the effect turns out to be undesirable, you need only put the book down. It’s cheap and reliable — if you want to emulate someone, start by reading what they read. Most importantly, it’s powerful, because authors form a large part of the meta-peer group that determines which communities and games you engage with."},"thoughts/Mindstorms":{"title":"Mindstorms","links":["thoughts/constructionist","thoughts/linguistic-relativism","thoughts/Design-Justice","posts/agi","thoughts/multiple-realization","thoughts/knowledge-distillation","thoughts/teaching"],"tags":["seed","book"],"content":"Lots of really good constructionist theory here :))\nQuotes\nComposition\nWhen knowledge can be broken up into “mind-size bites,” it is more communicable, more assimilable, more simply constructable.\n“New commands, once defined, can be used to define others.”\nComputing as a medium/language\n“Seymour saw the computer not just as a problem-solving tool but as an expressive medium. He believed that learning to program was analogous to learning to write, providing children with new ways of organizing and expressing their ideas.”\nDoes language matter when programming? “A programming language is like a natural, human language in that it favors certain metaphors, images, and ways of thinking. The language used strongly colors the computer culture” (linguistic relativism)\nMicroworlds and simplification\nNewton “understood” the universe by reducing whole planets to points that move according to a fixed set of laws of motion. Is this grasping the essence of the real world or hiding its complexities?\nRestricted microworlds give students the freedom to explore, without needing to ask for permission or reassurance from teachers\nIn school, false theories are no longer tolerated. Our educational system rejects the “false theories” of children, thereby rejecting the way children really learn.\nDanger of ‘simplicity’: Design Justice: “Imagine a suggestion that we invent a special language to help children learn to speak. This language would have a small vocabulary of just fifty words, but fifty words so well chosen that all ideas could be expressed using them. Would this language be easier to learn? Perhaps the vocabulary might be easy to learn, but the use of the vocabulary to express what one wanted to say would be so contorted that only the most motivated and brilliant children would learn to say more than ‘hi’.”\nOn mechanical thinking\nPeople often fear that using computer models for people will lead to mechanical or linear thinking: They worry about people losing respect for their intuitions, sense of values, powers of judgement. They worry about instrumental reason becoming a model for good thinking. I take these fears seriously but do not see them as fears about computers themselves but rather as fears about how culture will assimilate the computer presence.\nRelevant to AI systems: The definition of artificial intelligence can be narrow or broad. In the narrow sense, AI is concerned with extending the capacity of machines to perform functions that would be considered intelligent if performed by people. Its goal is to construct machines, and, in doing so, it can be thought of as a branch of advanced engineering… In order to make a machine capable of learning, we have to probe deeply into the nature of learning. And from this kind of research comes the broader definition of artificial intelligence: that of cognitive science.\n“The question to ask about the program is not whether it is right of wrong, but if it is fixable. If this way of looking at intellectual products were generalized to how the larger culture thinks about knowledge and its acquisition, we all might be less intimidated by our fears of ‘being wrong.‘”\nMultiple realization\n“If we had to base our opinions on observation of how poorly children learned French in American schools, we would have to conclude that most people were incapable of mastering it. But we know that all normal children would learn it easily if they lived in France.”\nIn retrospect, we know that the road that led from nineteenth-century transportation was quite different. The invention of the automobile and the airplane did not come from a detailed study of how their predecessors, such as horse-drawn carriages, worked or did not work.\nLogical consistency in systems\nThe conflicts are regulated and kept in check rather than “resolved” through the intervention of special agents no less simple-minded than the original ones. Their way of reconciling differences does not involve forcing the system into a logically consistent mold.\nOn brain being compartmentalized\nWe put our skills and heuristic strategies into a kind of tool box — and while their interaction can, in the course of time, give rise to global changes, the act of learning is itself a local event.\nKnowledge distillation\nOn the role of knowledge distillers and teaching:\nThis problem goes deeper than a mere short supply of such people. The fact that in the past there was no role for such people has been cast into social and institutional concrete; now there is a role but there is no place for them.\nIt is a step toward a situation in which the line between learners and teachers can fade\nA stage of unconscious work, which might appear to the mathematician as temporarily abandoning the task or leaving the problem to incubate, has to intervene… On the contrary, the problem has been turned over to a very active unconscious which relentlessly begins to combine the elements supplied to it by the\nMisc\nOn self-correcting systems like bikes\n“Thus learning to ride does not mean learning to balance, it means learning not to unbalance, learning not to interfere”\nPart of the fun is sharing, posting graphics on the walls, modifying and experimenting with each other’s work, and bringing the “new” products back to the original inventors.\nOn rejecting the false dichotomy of verbalizable versus nonverbalizable knowledge\n“No knowledge is entirely reducible to words, and no knowledge is entirely ineffable”\n“As knowing how to use a computer becomes increasingly necessary to effective social and economic participation, the position of the underprivileged could worsen, and the computer could exacerbate existing class distinctions.”\nOn the split between ‘humanities’ and ‘science’\n“It is self-perpetuating: The more the culture is divided, the more each side builds separation into its new growth”\nThe printed page cannot capture either the product or the process: the serendipitous discoveries, the bugs, and the mathematical insights all require movement to be appreciated."},"thoughts/Minecraft-End-Poem":{"title":"Minecraft End Poem","links":[],"tags":["seed"],"content":"Source\nProbably one of the most impactful pieces of poetry I’ve read. It’s embedded into end credits after they finish the canonical ‘end goal’ of beating the Ender Dragon in Minecraft.\nThe poem takes the form of a scrolling dialogue between two speakers who are discussing the player’s accomplishments, dreams, and relation to the rest of the universe as if they come from a different universe from the player.\nEverything is a dream and a story and the life we live is a long one. Within it is the possibility of many worlds within it. See also: The Egg by Andy Weir + Kurzgesagt\nHighlights\nOf things beyond the game\nBut what true structure did this player create, in the reality behind the screen?\nIt worked, with a million others, to sculpt a true world in a fold of the [scrambled], and created a [scrambled] for [scrambled], in the [scrambled].\nIt cannot read that thought.\nNo. It has not yet achieved the highest level. That, it must achieve in the long dream of life, not the short dream of a game.\nOf suffering\nDoes it know that we love it? That the universe is kind?\nSometimes, through the noise of its thoughts, it hears the universe, yes.\nBut there are times it is sad, in the long dream. It creates worlds that have no summer, and it shivers under a black sun, and it takes its sad creation for reality.\nTo cure it of sorrow would destroy it. The sorrow is part of its own private task. We cannot interfere.\n…\nand the universe said the darkness you fight is within you\nand the universe said the light you seek is within you\nOf charting your own path\nTo tell them how to live is to prevent them living.\nI will not tell the player how to live."},"thoughts/Moderation":{"title":"Moderation","links":["thoughts/communities","thoughts/infrastructure","thoughts/Protocol","thoughts/r-K-Selection-theory","thoughts/trust","thoughts/transitive-closure"],"tags":["seed"],"content":"What is the role of moderation in communities? What does good/bad moderation look like? (posts/comments/membership)\nI think a key part of answering this is trying to define what role a community occupies. Is it just a collection of people gathering around a shared identity? Or is it supposed to be a platform for moderated idea discussion/generation? Or something else in between.\nThinking about this at the infrastructure level too, if a community’s purpose is to serve as infrastructure so that it enables conversations, should infrastructure be regulated/enforced or something that should just play out naturally?\nBroadly defined, I see moderation as having two main categories: moderation of users and moderation of content. Moderation then, is to ensure that the set of values that is being conveyed by the users/content roughly aligns with that of the group/community.\nModeration of users involves regulating the membership of individuals within the community whereas moderation of content regulates what can/can’t be said in the community.\nModeration gets problematic when it is used to target and discriminate against individuals because of personal relations or vendettas. I was thinking about also saying it becomes problematic when it discriminates against characteristics but I’m hesitant on this bit. Is it categorically different to exclude people who are misogynistic vs those of a certain race? Probably yes but I don’t have a solid argument for it yet.\nProtocols for Moderation\nFrom Protocols Not Platforms\nSee also: protocols\nThe key to making this work is that while there would be specific protocols for the various types of platforms we see today, there would then be many competing interface implementations of that protocol.\nMuch like email, you don’t need to build an entirely new Facebook if you already have access to everyone making use of the “social network protocol” and just provide a different, or better, interface to it.\nThe problem is leaving it up to platforms to decide what ‘abusive’ behaviour looks like.\n\nNearly everyone recognizes that there is such behavior online and that it can be destructive, but there is no agreement on what it actually includes.\n\nUnder such a system, both Type I (“false positive”) and Type II (“false negative”) errors are not only common; they are inevitable. Content that a large body of people believe should be taken down is left up, while content that many people believe should remain up is taken down.\nRather than relying on a single centralized platform, with all of the internal biases and incentives that that entails, anyone would be able to create their own set of rules—including which content do they not want to see and which content would they like to see promoted.\nIn such a world, we can let a million content moderation systems approach the same general corpus of content—each taking an entirely different approach—and see which ones work best. r-selected moderation.\nTrust-based Moderation Systems\nSource\nSee also: trust\nIn the centralized context, removing a malicious participant is the action of a moderator. Usually it is one or two clicks, and the malicious participant has been removed for all other participants.\nIn a distributed context, there are many possible answers to this problem. The first and naive solution is to delegate the responsibility of removing the malicious participant to each individual participant. Thus everyone participating has to individually hide offenders.\nA subjective system, where parti`cipants can themselves decide who moderates on their behalf, sidesteps the mentioned problems. Additionally, the mechanism of freely allowing multiple people to moderate also spreads out the invisible care-giving labour required to keep a community free from abuse.\nThe proposed system is transitive, meaning that there will be indirect trust relations by way of those whom you trust directly."},"thoughts/Moving-Castles":{"title":"Moving Castles","links":["thoughts/design-goals","thoughts/metalabel","thoughts/composable","thoughts/interoperability","thoughts/games","thoughts/access-control","thoughts/Internet","thoughts/causality"],"tags":["seed"],"content":"Source: Moving Castles: Modular and Portable Multiplayer Miniverses by ARB and GVN908\n\n“With a last squeak the castle lifted, the crew cheered as we could see across the dark forest for the first time, our eyes locked on the arid wastelands on the horizon. Now back to chat, this machine won’t move itself.”\n\nCozyweb is non-indexable because of a lacking interconnection between material, creating unintentionally disconnected islands populated by isolated communities.\nIn our vision of this new media format, Moving Castles are modular and portable multiplayer miniverses; inhabited by communities that use them to manage their lore, ecosystems and economies. They are collective machines controlled by governance mechanisms and allow low-barrier participation.\nMoving Castles should reflect the following principles (adapted from the design goals described in Modular Politics)\n\nCollective: many contributors share control through transparent and real-time governance mechanisms. (see: Metalabel)\nPortable: the ability to move freely between platforms, standards and protocols, from private to public, without losing any value, knowledge, or lore in the process.\nModular: ability to construct Moving Castles by creating, importing, and arranging composable parts together as a coherent whole while making these parts available for others to reuse and adapt.\nInteroperable: ability to interact with other communities; communicating, playing games, and sharing knowledge &amp; skills in order to help these communities become Moving Castles themselves.\n\nThe Story of All Powerful Wizards\nSource: A story of all powerful wizards - 20th of Nov 2021 @ 0xPARC Boston by Justin Glibert\nEarly internet: individual gardens linked by hyperlinks, tended to by all-powerful wizards who could build any worlds they wanted. Very libertarian\nProposing: a bazaar, no access control and everybody plays by the same rules. Where each visitor has the power to change the world around them, as long as they respect the physics of the world.\nYou could visit these other gardens that wizards built but in doing so, you lose all of your wizardly powers in their domain.\nAre there any ways we can remove the distinction between all-powerful hosts and powerless guests?\n\nWith the power of a host, it only takes one bad actor to destroy an entire garden\n\nSystems of magic\n\nSpecificity is good\nRules should be able to be composed\nShould not be too powerful\n\nProgramming is magic, and it’s too powerful. There is no OS for the Internet telling you what you can and cannot do (only limitation is not running out of memory and time).\nWhat is interesting with systems is not what you can do, but what you cannot do.\nWhat if programming was closer to mechanical engineering and physics?\n\nEnergy conversation\nSymmetries\nSpatial computation\nCausality, speed of light, etc.\n\nThe dream is to be wizards in the same world together. We want to be able to protect things like person A wiping all of person B’s work out of existence with the snap of their finger"},"thoughts/Mutual-Aid":{"title":"Mutual aid","links":[],"tags":["seed"],"content":"The puzzle: A Darwinian explanation can only account for behaviour that promotes the survival and reproductive success of the individual organism. How do we account for altruism and cooperation given this constraint?\nOne explanation is gene selection. The unit of selection is the gene rather than the individual. That is, altruism and cooperation promote the survival of your genes."},"thoughts/Myth-of-the-Infrastructure-Phase":{"title":"Myth of the Infrastructure Phase","links":["thoughts/Protocol","thoughts/infrastructure"],"tags":["seed","pattern"],"content":"Source\n**Apps inspire infrastructure. Then that infrastructure enables new apps.**\nThe history of new technologies shows that apps beget infrastructure, not the other way around. It’s not that first we build all the infrastructure, and once we have the infrastructure we need, we begin to build apps. It’s exactly the opposite.\n\n“You can’t build railroads before it is railroad time. (Chuck Thacker)”\n\nFor example, light bulbs (the app) were invented before there was an electric grid (the infrastructure). You don’t need the electric grid to have light bulbs. But to have the broad consumer adoption of light bulbs, you do need the electric grid, so the breakout app that is the light bulb came first in 1879, and then was followed by the electric grid starting 1882.\nPlanes (the app) were invented before there were airports (the infrastructure). You don’t need airports to have planes. But to have the broad consumer adoption of planes, you do need airports, so the breakout app that is an airplane came first in 1903, and inspired a phase where people built airlines in 1919, airports in 1928 and air traffic control in 1930 only after there were planes.\n\nYou can open the door to the next room, but you can’t really skip steps and open the back door from the front porch.  It is hard to successfully build infrastructure that is too far ahead of the apps market.\nSee also: Protocol, infrastructure"},"thoughts/NAT":{"title":"Network Address Translation (NAT)","links":["thoughts/Transport-Layer","thoughts/IP-Address","thoughts/Protocol","thoughts/UDP","thoughts/decentralization","thoughts/peer-to-peer"],"tags":["seed","CPSC317"],"content":"Network Address Translation\nBoth network and transport layer (violation of abstraction/layering) More devices than IP addresses! What do we do?\n\nHome router gets assigned an IP (public IP) by the ISP\nDevices connected on the local network are assigned a private IP address (usually starts with subnet mask 196.168.x.x or 10.x.x.x)\nChanges the private IP address to the public address of the router\n\nChanges source port to some available port\n\n\nAdds the mapping to the NAT forwarding table\n\nEntries correspond to private side (192.168.1.3:42301) to public side (12.13.14.15:24604)\nIncludes\n\nSource IP\nSource Port\nDestination IP\nDestination Port\nProtocol\nNAT IP (Router public IP address)\nNAT Port (Unique)\n\n\nActually, Port Forwarding just adds entries to the NAT forwarding table! You can set remote IP and remote port to wildcard entries (i.e. any web requests made to this port go to the specified machine)\n\nMax number of rows is 65535\nAny requests that come in then clone the wildcard rule and make it specific to that conversation (to avoid collisions to the NAT port)\nEntries are removed when a conversation is coming to a close (stream based protocol, detect termination packets)\n\n\n\n\nDoes the inverse when it receives a packet\n\n\nHole-punching and NAT Traversal\n\nHole punching (or sometimes punch-through) is a technique in computer networking for establishing a direct connection between two parties in which one or both are behind firewalls or behind routers that use network address translation (NAT).\n\nGenerally done using UDP. You can do NAT traversal with TCP, but it adds another layer of complexity\nMostly used in decentralized or peer-to-peer communication as the latency incurred by relaying through a central server is prohibitively expensive for real-time activity like voice calling, file syncing, etc.\nHole-punching usually involves the use of third-party hosts that run STUN or ICE to figure out the public address of the NAT.\nThe idea is that to allow packets to come in from a remote endpoint, the computer behind the NAT or firewall needs to send something to the remote endpoint first. By doing so it creates a “hole” in the NAT or firewall through which communications can proceed. This even works when both sides are behind a NAT/firewall, when both sides start by punching a hole in their respective NAT/firewalls.\nTypes of NAT/Firewall combinations\n\nEndpoint-Independent Mapping (EIM): all outgoing connections from the same IP address and port have the same modified IP address and port\nAddress-Dependent Mapping (ADM): each outgoing connection to a different address from the same IP address and port has a different modified IP address and port\nAddress and Port-Dependent Mapping (APDM): same as ADM, instead of just address, it’s address + port\nEndpoint-Independent Filtering (EIF): all packets from remote addresses are allowed for a single endpoint on the NAT/Firewall\nAddress-Dependent Filtering (ADF): only allows packets to a specific endpoint from a specific address if a computer behind the NAT/Firewall has sent a packet to that address\nAddress and Port-Dependent Filtering (APDF): same as ADF but instead of just address, it’s address + port\n\nEfficacy\nFrom UDP NAT and Firewall Puncturing in the Wild by Gertjan Halkes and Johan Pouwelse\n\nOver 79% of peers on the Internet are not directly connectable\n\nUsing a simple redez-vouz mechanism decreases this to ~15% of peers\nKeep-alive messages should be sent at least every 55 seconds to ensure that mappings/holes will remain open on almost every NAT/firewall\n\n\nFirewalls and NAT makes establishing P2P connections directly between any two machines hard\n\nFirewalls are frequently configured to allow only outgoing connections, based on the assumption of the client-server model of communication\nNAT hides the ‘true’ port and IP combination of any machine that is behind it, meaning that connecting to an arbitrary port is often not allowed\n\n\nMostly useful for UDP traffic as setting up a connecting for TCP when NAT/firewall requires unusual or non-standard use of TCP and IP mechanisms, and may rely on specific NAT/firewall behaviour to work\nDetection\n\nNAT\n\nA and B report the remote address and port they see when a connection is set up. They now know their own external address and port\nIf all other peers agree on peer A’s remote address and port, peer A determines they have no NAT or the NAT has EIM behaviour\nIf peer A’s remote address and port are reported differ among peers, peer A determines they are behind a A(P)DM NAT\n\n\nFiltering Behaviour\n\nCheck order of requests received when connecting to a rendez-vous server\n\nSide note: this many not always be true. The reason for this is that the direct connection request is not always faster than the reverse connection request which is sent through the rendez-vous peer. This is known as a Triangle Inequality Violation (TIV)\nThis can happen as often as 40% of the time although lower numbers are more common\n\n\nWhen a peer A tries to set up a connection using rendezvous R, it will always first send a direct connection request to the remote peer B\n\nIf the direct connection request from A to B arrives first, there is no filtering behaviour or it uses EIF\nIf the rendez-vouz connection request from R to B arrives first, the filtering behaviour is most likely A(P)DF\n\n\n\n\n\n\n\n\n\nPossible explanations for a non-100% connection rate even in EIM-EIF to EIM-EIF peers\n\nUDP packets being dropped under high UDP packet load (especially in consumer-grade NAT/firewalls)\nRouters stop functioning when mapping tables are full (not out of the realm of possibilies, only 65535 entries)\nUse of CGNAT (NAT at the ISP level instead of home-router)\n\nTerminology Translation Table\nNAT Cone Types\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEndpoint-Independent NAT Mapping (EIM)Endpoint-Dependent NAT Mapping (ADM/EDM)Endpoint-Independent Firewall (EIF)Full cone NATn/aEndpoint IP-Dependent Firewall (ADF)Restricted Cone NATn/aEndpoint IP and Port-Dependent Firewall (APDF)Port-restricted Cone NATSymmetric NAT\nFlow\nLet A and B be the two hosts, each in its own private network; NA​ and NB​ are the two NAT devices with globally reachable IP addresses EIPA​ and EIPB​ respectively. S is a public server with a well-known, globally reachable IP address.\n\nA and B both begin a UDP conversation with S\nNAT devices NA​ and NB​ create UDP translations and assign temporary external ports EPA​ and EPB​\nS looks at UDP packets to get source ports of NA​ and NB​ (through EPA​ and EPB​)\nS makes pairs of external IP addresses assigned by the NAT along with temporary external ports and exchanges them between A and B (S passes EIPA​:EPA​ to B and EIPB​:EPB​ to A)\nA and B send packets to each other and their appropriate NAT devices create the entries in their lookup tables\n\nA sends a packet to EIPB​:EPB​.\nNA​ examines A’s outgoing packet and adds (Source-IP-A, EPA​, EIPB​, EPB​) to its translation table.\nB sends a packet to EIPA​:EPA​.\nNB​ examines B’s outgoing packet and adds (Source-IP-B, EPB​, EIPA​, EPA​) to its translation table.\n\n\nBest case scenario NA​ and NB​ should have made the entry in the translation. Worst case, both NAT devices have not yet made the entry and drop the first packet sent from B\nAt worst, the second packet from both A and B make it to each other. Holes have been “punched” in the NAT and both hosts can directly communicate through NA​ and NB​ without needing S\n\nStrategies for Robust NAT Traversal\nFrom Tailscale\n\nA UDP-based protocol to augment\nDirect access to a socket in your program\nA communication side channel with your peers\nA couple of STUN servers\nA network of fallback relays (optional, but highly recommended)\n\nThen, you need to:\n\nEnumerate all the ip:ports for your socket on your directly connected interfaces\nQuery STUN servers to discover WAN ip:ports and the “difficulty” of your NAT, if any\nTry using the port mapping protocols to find more WAN ip:ports\nCheck for NAT64 and discover a WAN ip:port through that as well, if applicable\nExchange all those ip:ports with your peer through your side channel, along with some cryptographic keys to secure everything.\nBegin communicating with your peer through fallback relays (optional, for quick connection establishment)\nProbe all of your peer’s ip:ports for connectivity and if necessary/desired, also execute birthday attacks to get through harder NATs\nAs you discover connectivity paths that are better than the one you’re currently using, transparently upgrade away from the previous paths.\nIf the active path stops working, downgrade as needed to maintain connectivity.\nMake sure everything is encrypted and authenticated end-to-end.\n\nPeer Discovery in a purely distributed manner\n\nPeer A sends an introduction-request to peer B. Peer B is chosen from an existing pool of neighboring peers.\nPeer B sends an introduction-response to peer A containing the address of peer C.\nPeer B sends a puncture-request to peer C containing the address of peer A.\nPeer C sends a puncture to peer A, puncturing its NAT.\n\nWhen a peer doesn’t yet have a list of neighboring peers, it will select a bootstrap server for peer B. Bootstrap servers have the same peer discovery protocol as regular peers except they respond to introduction-requests for anyone.\nSTUN (Session Traversal Utilities for NAT)\nServers like S usually run STUN. Recognized using a stun or stuns resource record.\nDetails how a server can determine what kind of NAT and firewall is between itself and the public Internet.\nICE (Interactive Connectivity Establishment)\nUses STUN. Provides a structured mechanism to determine the optimal communication path between two peers\nTURN (Traversal Using Relays around NAT)\nTURN places a third-party server to relay messages between two clients when direct media traffic between peers is not allowed by a firewall."},"thoughts/NFAI":{"title":"NFAI","links":["thoughts/connectionist-networks","thoughts/neural-networks","thoughts/GOFAI"],"tags":["seed"],"content":"An approach to the development of AI focused on statistical methods and connectionist networks like artificial neural networks. Haugeland dubbed this approach to AI design New Fangled AI (NFAI).\nThis approach relies on computers the same way a weather service does, to simulate digitally systems that are not in themselves digital. It excels in finding various sort of similarities among patterns, recognizing repeated (or almost repeated) patterns, and filling in missing parts of incomplete patterns.\nNFAI typically learns from examples (but not in the same way humans do). Their architectures are inspired by the structure of the brain, but more deeply, by the importance and ubiquity of non-formal pattern reasoning.\nNFAI is a very grab-bag term, it practically means anything that isn’t GOFAI."},"thoughts/NFT":{"title":"NFTs","links":["thoughts/Degraded-Blockchain-problem"],"tags":["sapling"],"content":"Source: vgr on Magic Beans\nAn NFT need not to contain something. All it guarantees is that it has an “identity and can be traded, transferred, mixed and matched indiscriminately with other things in a social context.” See the Degraded Blockchain problem.\n3 (progressively more ambitious) ways to think about NFTs\n\nSignifier-value mental models: right to represent things\nAgency-value mental models: right to do things\nRight-to-future-rights mental models: right to expect things\n\nI specifically like the last version of these: the right to expect things. “An NFT represents an access pass to an unspecified, generative possible future associated with an object. It is a key to a possible world.”\nOwnership\nRight-clicking is not a problem for the NFT world because securing exclusive possession rights is not the problem NFTs aim to solve.\n\nDo you really think 9/10ths of the value associated with the Mona Lisa accrues to the Louvre? I can put a photograph of it in an article, write a book discussing it, even make a movie about it, without paying the Louvre anything. The Louvre has the right to hold the original physically, but every other meaningful right is in the public domain. Not only are these other rights collectively far more valuable, physical possession only has value by way of derivation from these other rights.\n"},"thoughts/NLP":{"title":"NLP","links":["thoughts/LLMs","thoughts/explainability","thoughts/OODA"],"tags":["seed"],"content":"See also: LLMs\nChain of Thought Prompting\nArXiv Link\n\nFlat scaling curves—simply increasing model scale does not lead to substantive performance gains\nChain of thought prompting facilitating multistep reasoning in large language models\n\nThe intuition is that a chain of thought allows language models to decompose a multi-step problem into intermediate steps that are solved individually, instead of solving an entire multi-hop problem in a single forward pass\nAnother intuition behind chain of thought reasoning is that it allows the model to spend more computation (i.e., intermediate tokens) solving harder problems (though later sections of the paper rule this out as the primary factor for performance improvements)\nNotably, chain of thought prompting only does better than standard prompting only at the scale of ~100B params\n\n\nReally cool side-effect is more explainable decision making processes\n\nFully characterizing a model’s computations that support an answer remains an open question\n\n\n\nSee: OODA"},"thoughts/NVIM-Cheatsheet":{"title":"NVIM Cheatsheet","links":[],"tags":["seed","technical"],"content":"Text-objects\n\nadjectives\n\ni - inner\na - whole thing\n\n\nnouns\n\nl - character\nw - word\nt - html tag\n{ or [ or &lt; or {\n&quot; or &#039; - quoted string\nq - parameter\n\n\n\nMappings\n\n&lt;Ctrl&gt;c - leave insert mode\n&lt;leader&gt;o - open file by name\n&lt;leader&gt;g - live grep\n&lt;leader&gt;f - format file\n&lt;leader&gt;ca - code actions\nf - hop by 2 chars\n&lt;ctrl+/&gt; - comment line or block\n[[ - jump to previous parameter\n]] - jump to next parameter\nWindow Navigation\n\n:vsp - vertical split\n:hsp - horizontal split\n&lt;leader&gt;&lt;direction&gt; - move to window in that direction (one of wasd)\n\n\nTabs\n\n&lt;alt-&lt;&gt; - previous tab\n&lt;alt-&gt;&gt; - previous tab\n&lt;alt-q&gt; - close tab\n&lt;alt-#&gt; - go to tab #\n\n\nLanguage server\n\ngD - go to declaration\ngd - go to definition\ngt - go to type\n&lt;shift&gt;K - show type hint\n&lt;space&gt;rn - rename\n\n\nMoving\n\n% to jump to matching paren\n\n\nSurrounds\n\nys[text-object][char] - surround with char\nds[text-object][char] - delete surrounding char\ncs[char1][char2] - change surrounding char1 to char2\n\n\nGit conflicts\n\n&lt;leader&gt;co - choose ours\n&lt;leader&gt;ct - choose theirs\n&lt;leader&gt;cb - choose both\n&lt;leader&gt;c0 - choose none\n]x - move to previous conflict\n[x - move to next conflict\n&lt;leader&gt;m - open diff view\n\n\nShow Keybindings\n\n:Telescope keymaps\n\n\n"},"thoughts/Nagel's-Bat-Argument":{"title":"Nagel's Bat Argument","links":["thoughts/Materialism","thoughts/qualia","thoughts/Knowledge-Argument"],"tags":["seed"],"content":"Nagel’s Bat Argument (against Physicalism)\n\nPhysicalism is the thesis that everything that exists is physical.\nPhysical facts then, are objective truth (“the kind that can be observed and understood from many points of view and by individuals with different perceptual systems”)\nEven if we knew everything about how the bat’s sonar system works, we would not know what is is like for the bat to perceive using this system.\n\n‘What it’s like’: in this case, something is conscious (a bat) if and only if there is something it’s like to be that being (only a bat knows what it is like for a bat to be a bat)\nOne, for example, cannot imagine a chair to know what it is like for a chair to be a chair.\nFor a state to be conscious is for it to have a subjective character (to seem or feel a certain way to the subject). A conscious experience is a state that is both subjective and qualitative.\n\n\nTherefore, complete knowledge of the physical facts about a bat’s perceptual system would not yield knowledge of certain facts about a bat’s experiences\n\nPhysicalism leaves out the subjective facts, so it’s a mystery how it could be true (given that qualia and the subjective experience exists).\nRelated: Frank Jackson’s Knowledge Argument\nFishes\nA famous Taoist story about happiness and knowing what it is like to be a fish\n\nZhuangzi and Huizi were strolling on a bridge over the River Hao, when the former observed, “See how the minnows dart between the rocks! Such is the happiness of fishes.”\n“You not being a fish,” said Huizi, “how can you possibly know what makes fish happy?”\n“And you not being I,” said Zhuangzi, “how can you know that I don’t know what makes fish happy?”\n“If I, not being you, cannot know what you know,” replied Huizi, “does it not follow from that very fact that you, not being a fish, cannot know what makes fish happy?”\n“Let us go back,” said Zhuangzi, “to your original question. You asked me how I knew what makes fish happy. The very fact you asked shows that you knew I knew—as I did know, from my own feelings on this bridge.”\n\nWhy did Zhuangzi, who wrote it down, show himself to be defeated by his logician friend?"},"thoughts/Naive-Bayes":{"title":"Naive Bayes","links":["thoughts/probability"],"tags":["seed","CPSC340"],"content":"An example of a probabilistic classifier. Commonly used in spam filters (classifies as spam if the probability of spam is higher than not spam)\nTo model this, it uses Bayes rule:\nP(yi​=spam∣xi​)=P(xi​)P(xi​∣yi​=spam)P(yi​=spam)​\nWhere\n\nP(yi​=spam) is the marginal probability that an e-mail is spam\nP(xi​) is the marginal probability than an e-mail has the set of words xi​\n\nHard to approximate (lots of ways to combine words)\n\n\nP(xi​∣yi​=spam) is the conditional probability that a spam e-mail has the words xi​\n\nOptimizations\nDenominator doesn’t matter\nWe can actually reframe this to avoid calculating P(xi​) as Naive Bayes just returns spam if P(yi​=spam∣xi​)&gt;P(yi​=not spam∣xi​)\nRoughly, denominator doesn’t matter\n∝P(xi​∣yi​=spam)P(yi​=spam)\nConditional Independent Assumptions\nAdditionally, we assume that all features xi​ are conditionally independent given label yi​ so we can decompose it.\n≈∏j=1d​P(xij​∣yi​)P(yi​)\nLaplace Smoothing\nIf we have no spam messages with lactase, then P(lactase∣spam)=0 so spam messages with lactase automatically get through!\nOur estimate of P(lactase∣spam)=0 is # spam messages# spam messages with lactase​=# spam messages0​\nWe can add β to the numerator and βk to the denominator, which effectively adds βk fake examples: β for each k where k is a possible class (2 for a binary classifier)\nSo for our binary spam classifier (with β=1):\n# spam messages+2# spam messages with lactase+1​"},"thoughts/Nash-equilibrium":{"title":"Nash equilibrium","links":["thoughts/rationality","thoughts/Pareto-optimality"],"tags":["seed","PHIL321A"],"content":"If no player can do better by unilaterally changing strategy, given the other players’ strategies. This is a criterion of individual rationality (see Pareto optimality for group rationality)"},"thoughts/NeRF":{"title":"NeRF","links":["thoughts/coordinate-system","thoughts/colour","thoughts/rendering","thoughts/supervised-learning"],"tags":["seed"],"content":"A neural radiance field (NeRF) is a fully-connected neural network that can generate novel views of complex 3D scenes, based on a partial set of 2D images.\nWe represent a static scene as a continuous 5D function that outputs the radiance emitted in each direction (θ,ϕ) at each point (x,y,z) in space (see: coordinate system)\nThe radiance function is represented as a 4D function, with volume density value (opacity, represented as σ) and a view-dependent RGB colour.\nWe train a simple MLP to map from position and direction to radiance. We also encourage the representation to be multiview consistent by restricting the network to predict the volume density σ as a function only of the location. That is:\n\nWe train an MLP on the input 3D coordinate with 8 fully connected layers (with 256 channels per layer) to output σ and a 256 dimensional feature vector\nThis feature vector is then concatenated with the camera ray’s viewing direction and is passed to one additional fully-connected layer (with 128 channels) that output the view-dependent RGB colour\n\nIt can be seen as interpolating between the input images to render new views\n\nTo render an image, we\n\nGenerate a ray from the camera viewpoint to the viewing plane\nFor each ray, pass the position and viewing direction as input into the neural network to produce a set of output colours and densities\nAccumulate this into a final colour and collapse it into a 2D image\n\nBecause this process is naturally differentiable, we can use supervised learning to optimize this model by minimizing the error between each observed image and the corresponding views rendered from our representation\nMinimizing this error across multiple views encourages the network to predict a coherent model of the scene by assigning high volume densities and accurate colors to the locations that contain the true underlying scene content\nOptimizations\n\nHierarchical volume sampling\n\nOur rendering strategy of densely evaluating the neural radiance field network at N query points along each camera ray is inefficient: free space and occluded regions that do not contribute to the rendered image are still sampled repeatedly\nInstead of just using a single network to represent the scene, we simultaneously optimize two networks: one “coarse” and one “fine”\nWe first sample a set of Nc locations using stratified sampling, and evaluate the “coarse” network at these locations. Given the output of this “coarse” network, we then produce a more informed sampling of points along each ray where samples are biased towards the relevant parts of the volume\n\n\n\nPlenOctrees\nFor Real-time Rendering of Neural Radiance Fields\nSource, Demo\nWe propose a framework that enables real-time rendering of NeRFs using plenoptic octrees, or “PlenOctrees”. Our method can render at more than 150 fps at 800x800px resolution, which is over 3000x faster than conventional NeRFs, without sacrificing quality."},"thoughts/Network-Block-Device":{"title":"Network Block Device","links":[],"tags":["seed"],"content":"Network block device (NBD) is a network protocol that can be used to forward a block device (typically a hard disk or partition) from one machine to a second machine\nWe can think of a block device as a big array of bytes that file system drivers can build file/directory support on top of\nA network block device is realized by three components:\n\nthe server part,\nthe client part, and\nthe network between them\n"},"thoughts/Network-Layer":{"title":"Network Layer (IP)","links":["thoughts/Transport-Layer","thoughts/Link-Layer","thoughts/IP-Address","thoughts/Internet","thoughts/Application-Layer"],"tags":["seed","CPSC317"],"content":"Layer 3, the layer below the Transport Layer and layer above the Link Layer\n\nUnit: Packet (datagram)\nResponsibilities: Routes packet through routers to destination machine (not necessary if two devices are on the same network)\nTwo main functions\n\nForwarding: move packets from router’s input to appropriate router output (process of getting through a single interchange)\nRouting: determine route taken by packets from source to destination (process of planning trip from source to destination)\n\n\n\nPacket Definition\nContains information about the packet itself (metadata) and the body/content\nBGP Advertisement\n\nIP Address: the one they are advertising they can reach\nGateway Next Hop: address of the entry point\nAS Path: Sequence of AS’s a packet would need to travel through\n\nNetwork Tiers\nThe structure of the internet is organized into entities called autonomous systems (ASs).\nEach AS is\n\n\nassigned a range/collection of IP addresses\n\n\nresponsible for routing to addresses it “owns”\n\n\nresponsible for routing to addresses that are not its responsibility\n\n\nPeering vs Transit\n\nTransit: AS pays for the right to transit traffic across another AS\nPeering: mutual exchange of traffic between networks\n\n\n\nTier 1 Networks\n\nA network that can exchange traffic with other Tier 1 networks without paying any fees (transit-free) for the exchange of traffic in either direction\n\n\n\nTier 2 Networks\n\nA network that peers for free with some networks, but still purchases IP transit or pays for peering to reach at least some portion of the Internet\n\n\n\nTier 3 Networks\n\nA network that solely purchases transit/peering from other networks to participate in the Internet. Everybody else\n\n\n\nRouting\nBoth IGP and EGP run at the application layer\nInternal Gateway Protocols (IGP)\n\nrouting within a single AS, under the control of a single administrative entity\nLink State\n\neach router tells every other router about all its links\nthis gives other routers complete info about the entire network\nevery so often, each router uses Dijkstra’s to find shortest path to all routers, then it updates its forwarding table\nOSPF (open-shortest-path-first)\n\nmost used IGP in the internet\nuses link-state protocol (each router has complete topological map of the entire AS)\nsupports extensions such as areas (support hierarchy and scaling)\n\n\n\n\nDistance Vector\n\nevery so often, each router tells its neighbours about the cost of its best routes to the networks it knows about\na receiving router checks if any of the broadcasted routes would shorten their path to destination\nif so, it updates its routing table to route through the first router\n\n\n\nExternal Gateway Protocol (EGP)\n\nrouting between different AS, no control over the routing policies of other AS (External Gateway Protocols - EGP)\nBGP\n\nthe protocol that all ASs use for inter-AS routing\npackets are not routed to specific destination address, but to CIDRized prefixes, with each prefix representing a subnet or collection of subnets\nenables each router to\n\nobtain prefix information from neighbouring ASs\ndetermine “best” routes to the prefixes\n\n\n\n\n"},"thoughts/Network-Theory":{"title":"Network Theory","links":["thoughts/Internet","thoughts/cascading-failures"],"tags":["seed"],"content":"\nWhenever nature seeks robustness, it resorts to networks.\n\nThe internet played a huge role in developing network theory with over a trillion documents N≈1012\nPower Laws and Scale-Free Networks\nFrom the Network Science Book\nA random network is a network where the degrees of connections of the nodes follow a Poisson distribution.\nA scale-free network is a network where the degrees of connections of the nodes follows a power law (modelled by the Power Law pk​∼k−γ, this is also where the concept of the 80/20 rule comes from). For example, roughly 80% of the web point to only 15% of the web pages.\nMain difference is that power-law distributions (scale-free networks) have long tails (i.e. some nodes have lots of connections — these are called hubs).\n\nLargest hubs in scale-free networks have degree in the order of kmax​∼Nγ−11​\nLargest hubs in random networks have degree in the order of kmax​∼lnN\nLargest hubs in a complete graph have degree exactly N−1\n\nOnce hubs are present, they change the way we navigate the network. In random networks, we usually need to make many hops. On scale-free networks, however, we can reach most destinations via a single hub. Scale-free networks mean that even as the sizes may differ widely between networks, navigation time across the networks is very slow to grow (ultra-small world network).\n\n\nAn almost universal property of most real-world networks. For example:\n\nInternet at the router level\nProtein-protein interaction network\nEmail network\nCitation network\n\nHowever, the scale free property is absent in systems that limit the number of links a node can have, effectively restricting the maximum size of the hubs.\nSee also: On the Power of (even a little) Centralization in Distributed Processing\nDistances in Networks\nThe dependence of the average distance ⟨d⟩ on the system size N and the degree exponent γ are captured by the formula\n⟨d⟩=⎩⎨⎧​constlnlnNlnlnNlnN​lnN​γ=22&lt;γ&lt;3γ=3γ&gt;3​\n\nγ=2: Hub and spoke model. kmax​∼N so all nodes connect to a single central hub. The average path length is constant.\n2&lt;γ&lt;3: Ultra-small world model. Hubs radically reduce the path length.\nγ=3: Critical point\nγ&gt;3: Small-world and random networks. Extremely unlikely to have large hubs, traversal time is on the order of lnN\n\n\nNetwork Robustness\nSee also: cascading failures\nPercolation Theory\nHow many nodes do we have to delete to fragment a network into isolated components, assuming deletion is random?\nWe can model network breakdown as inverse percolation.\nThinking about this using the metaphor of forest fires helps to imagine what these variables mean. If we randomly ignite a tree, what fraction of the forest burns down? How long it takes the fire to burn out?\nAs a forest is roughly similar to a random network, the answer depends on the tree density, controlled by the parameter p. For small p the forest consists of many small islands of trees (p =0.55), hence igniting any tree will at most burn down one of these small islands. Consequently, the fire will die out quickly. For large p most trees belong to a single large cluster, hence the fire rapidly sweeps through the dense forest (p =0.62). But there also exists a critical pc​ at which it takes extremely long time for the fire to end.\nHowever, this breaks down once we consider scale-free networks. Scale-free networks observe unusual robustness to failure: we must remove all of its nodes to have likely destroyed its giant component.\n\nUnder Attack\nThe removal of a small fraction of the hubs is sufficient to break a scale-free network into tiny clusters. See more on cascading failures in networks\nThe probability that a node belongs to the largest connected component in a scale-free network under attack (purple) and under random failures (green).\nKnocking out even a few hubs quickly breaks down the network. Y-axis is the ratio P∞​(0)P∞​(f)​ provides the relative size of the largest connected subgraph"},"thoughts/Network-Time-Protocol":{"title":"Network Time Protocol","links":[],"tags":["seed"],"content":"The known problem with this mechanism is that it can drift away from the actual time of the day, based on how fast or slow the crystals oscillate. To fix this, computers typically have a service like NTP which synchronizes computer clocks with well known time sources on the internet.\n\nNTP Client sends out a request at t1​\nNTP Server receives request at t2​\nNTP Server sends a response at t3​\nNTP Client receives a request at t4​\nRound-trip network delay = δ=(t4​−t1​)−(t3​−t2​)\nEstimated single-trip network delay = δ/2\nEstimated server time when client receives response, so clock skew is θ=(t3​+δ/2)−t4​\nIf θ&lt;125ms, slew the clock: speed it up/slow it down by 500ppm until clocks are in sync\nIf 125ms≤θ&lt;1000s, step the clock: suddenly reset client clock to estimated server timestamp\nIf θ≥1000s, panic and do nothing (leave it to the humans!)\n"},"thoughts/Neural-Correlates-of-Consciousness-(NCC)":{"title":"Neural Correlates of Consciousness (NCC)","links":["thoughts/explainability","thoughts/neural-networks","posts/agi"],"tags":["seed","PHIL451A"],"content":"\nExperimentally, content-specific NCCs are identified by comparing conditions where specific conscious contents are present versus absent.\n\nSimilar to probing for network activation in explainable neural networks / AI systems\n\n\nThe full NCC is the union of all content-specific NCCs.\nEven for ambiguous figures (those weird visual illusions that have differing things depending on how you perceive it), there is a rivalry for a dominant interpretation\n\nWhen faced with ambiguous visual information, you don’t normally experience a combination of the different interpretations. Rather, you only see one at a time, often switching back and forth between the two.\nBinocular rivalry\n\n\n\nDoes phenomenal awareness overflow access?\n\nWhen reading, we are conscious only of the letters attended to and the impression that there are other items displayed whose identity we don’t know. Once the cue is presented, we gain access to the unconscious information before it decays and can recall the letters.\nGreat example of why foveated rendering works so well\n\n\nThis content is sourced from Professor Evan Thompson’s course materials for PHIL451A at UBC. All rights to this content is retained by Evan Thompson.\n"},"thoughts/Newcomb's-Problem":{"title":"Newcomb's Problem","links":[],"tags":["seed","PHIL321A"],"content":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n$1M in Box 2$0 in Box 2Take only box 2$1M$0Take both boxes1M+1000$1000\nBackground\nThere is a predictor that is 99% accurate is predicting whether people will only take box 2 or both boxes\nProcedure\n\nPredictor makes their prediction\nThey put $1000 in box 1 (which you know)\nThey put 0inbox2iftheythinkyouwilltakebothand1M if they think you will only take box 2\nChoose either both boxes or box 2\n\nArguments\n\nTwo-box argument: dominance argument\n\nWe can perhaps rule out the dominance argument because it only applies when states are independent of our actions (which is not the case here)\n\n\nOne-box argument: taking only box 2 almost guarantees $1M. Calculate using expected utility\n\nEU(both) = 0.01(1M + 1k) + 0.99(1k) = 11k\nEU(box 2) = 0.99(1M) + 0.01(0) = 990k\n\n\n"},"thoughts/No-Free-Lunch-Theorem":{"title":"No Free Lunch Theorem","links":[],"tags":["seed","CPSC340"],"content":"\nAll optimization algorithms perform equally well when their performance is averaged across all possible problems.\n\nThere is no “best” machine learning model. This question is like asking which is “best” among “rock”, “paper”, and “scissors”.\nCaveat\nThe proof of the no-free-lunch theorem assumes any map from xi​ to yi​ is equally likely.\nThis may not be true in the real world: some datasets are more likely than others. Model A really could be better than model B on every real dataset in practice."},"thoughts/Nodejs":{"title":"Node.js","links":["thoughts/TCP"],"tags":["seed"],"content":"Event Loop\nSource\n\nThe event loop is what allows Node.js to perform non-blocking I/O operations — despite the fact that JavaScript is single-threaded\n\nSince most modern kernels are multi-threaded, they can handle multiple operations executing in the background. When one of these operations completes, the kernel tells Node.js so that the appropriate callback may be added to the poll queue to eventually be executed.\n   ┌───────────────────────────┐\n┌─&gt;│           timers          │\n│  └─────────────┬─────────────┘\n│  ┌─────────────┴─────────────┐\n│  │     pending callbacks     │\n│  └─────────────┬─────────────┘\n│  ┌─────────────┴─────────────┐\n│  │       idle, prepare       │\n│  └─────────────┬─────────────┘      ┌───────────────┐\n│  ┌─────────────┴─────────────┐      │   incoming:   │\n│  │           poll            │&lt;─────┤  connections, │\n│  └─────────────┬─────────────┘      │   data, etc.  │\n│  ┌─────────────┴─────────────┐      └───────────────┘\n│  │           check           │\n│  └─────────────┬─────────────┘\n│  ┌─────────────┴─────────────┐\n└──┤      close callbacks      │\n   └───────────────────────────┘\n\nEach phase has it’s own FIFO queue of callbacks to execute\nOn entering each phase, it will perform any operations specific to that phase, then execute callbacks in that phase’s queue until the queue has been exhausted or the maximum number of callbacks has executed\n\nPhases\n\nTimers: execute any setTimeout() and setInterval() callbacks given that enough time has passed since they were scheduled\nPending callbacks: certain types of I/O callbacks (i.e. TCP ECONNREFUSED)\nIdle, prepare: Node internals\nPoll: wait for system to call us back for I/O events (normally, this is where Node chooses to block)\nCheck: setImmediate()\nClose callbacks: socket.on(&#039;close&#039;, ...)\n"},"thoughts/Nozick's-Experience-Machine":{"title":"Nozick's Experience Machine","links":["thoughts/virtual-worlds"],"tags":["sapling"],"content":"Experience machine\nSuppose there was a machine that would give you any experience you desired. Would you plug in?\n\nIf “how things seem to be” is the only thing that matters to us, we have no reason to refuse this offer\nWe are hesitant to take up this offer (we have reasons to refuse it)\n\n“We want to do certain things, and not just have the experience of doing them.” (43)\n“… we want to be a certain way, to be a certain sort of person.” (43)\nWe want to leave ourselves open to contact with a deeper reality.\n\n\nWhat if we made machines to tackle all of this reasons? e.g. a transformation machine, result machine, etc.\nThese machines are disturbing to us because they are living our lives for us\n\n\nTherefore, experiences aren’t the only thing that matter to us\n\nPryor’s analysis of The Matrix\n\nMatrix implies that there’s something bad about being inside the Matrix\nWho is the matrix supposed to be bad for?\nMachines are using the Matrix to keep humans docile to use them as a source of energy\n\nThis is a form of slavery\nWhat if it was instead benevolent and philanthropic? Would this change how we see the situation?\n\n\n“if in every respect it seems to you that you’re in the good situation, doesn’t that make it true— at least, true for you — that you are in the good situation?\n3 possibilities for what would be “bad” about living in the matrix\n\nLack of real scientific and/or objective knowledge\nInterpersonal relationships\nBeing slaves — albeit content ones. The matrix is computer-generated dream world, built to keep us under control\n\n\nMeaning in the matrix (and virtual worlds more broadly)\n\nDo the things in the matrix refer to their “real” counterparts?\n“steak” and “air” refer to the actual things inside of the matrix for those who have spent their whole life in the matrix, but mean something very different for outsiders who just visit\n\n\n"},"thoughts/Nuclear-Fission":{"title":"Nuclear Fission","links":["thoughts/Nuclear-Fusion"],"tags":["seed"],"content":"A nuclear reaction that can release large amounts of energy both as electromagnetic radiation and as kinetic energy of the fragments.\nSee also: Nuclear Fusion"},"thoughts/Nuclear-Fusion":{"title":"Nuclear Fusion","links":["thoughts/nuclear-binding-energy"],"tags":["seed"],"content":"A reaction in which two or more atomic nuclei combine to form one or more different atomic nuclei and subatomic particles (neutrons or protons).\n\nIf the fusion process produces atomic nuclei lighter than iron-56 or nickel-62 then the excess is emitted as energy: an exothermic process\nIf the fusion process involves atomic nuclei heaver than iron-56 or nickel-62 then the excess is used to bind the resulting nucleons together: an endothermic process\n\nSee page on nuclear binding energy for more details\nMagnetic Confinement\nElectrically charged particles (such as fuel ions) will follow magnetic field lines. The fusion fuel can therefore be trapped using a strong magnetic field.\nInertial Confinement\nApply a rapid pulse of energy using a driver to a large part of the surface of a pellet of fusion fuel, causing it to simultaneously “implode” and heat to very high pressure and temperature. If the fuel is dense enough and hot enough, the fusion reaction rate will be high enough to burn a significant fraction of the fuel before it has dissipated.\nVarious drivers exist like lasers, ions, electron beams, or Z-pinches"},"thoughts/Nyāya":{"title":"Nyāya","links":["thoughts/philosophical-realism","thoughts/ontology","thoughts/testimony","thoughts/trust","thoughts/Descartes'-Meditations","thoughts/seeing"],"tags":["seed","PHIL240A"],"content":"\n“Perception is a cognition that has arisen from the contact of sense faculty and object and is inexpressible, not erroneous, and determinate in nature.”\n\nNyāya is a major proponent of philosophical realism — cognition is only possible because it is dependent on the objects in this world.\nNyāya adopts the Vaiśeṣika ontology, which posits that seven different types of things:\n\nUniversals (node)\nQualities (node)\nMotions (node)\nSubstances (node)\nInherence (edge)\nIndividuators (node)\nAbsenses (later addition)\nThis ontology of a directed graph in which inherence relations connect things in inheror-inheree pairings.\n\nPramānas are means of knowledge and provide it through mode like perception, inference, and testimony. The objects of knowledge are called ‘knowables’ (prameya).\nStuff has to exist, if you investigate well, you get answers. Pramāna-generated knowledge is knowledge gained through “close examination of objects through cognition.”\nRational inquiry requires purpose. We do not doubt everything, lest we not trust the ground beneath us. Debate must proceed based off of shared axioms.\nCounter against the dream argument (perception of knowledge is akin to conception of objects in dreams) — similar to Descartes’ Dream Argument\n\nDreams aren’t real because we can wake up\nAlternatively, dream objects fall apart under close examination\nDream objects implies the existence of non-dream objects\n\nWhat about incorrect understandings of the world? Nyāya argues that these are akin to dream objects, where destruction of false perception is akin to the destruction of conceptions of dream objects upon waking. (NS p. 68)\n\nStuff existing != existing how you think it does\nError, seeing a post as a person or vice versa, occurs when their differences are elided while their similarities are grasped.\n\nIn order to be wrong about something, we need to mistake something not-F for something F (to see a post as a person)\nI could not mistakenly see a person in the distance if I didn’t have the concept of a person, and Nyāya argues that in order to have such a concept, I must have had knowledge of people in the past.\n\n\nNot true that incorrect understands have convergent content, the essence of things implies natural diversity in the similarity of things. (Argument against the fact that ‘nothing can be explained’)\nErroneous and dream cognition are both real, they have specific content arising from specific causes\n"},"thoughts/OODA":{"title":"OODA","links":["thoughts/information-behaviour","thoughts/context","thoughts/LLMs"],"tags":["seed","pattern"],"content":"OODA stands for observe, orient, decide, act. Initially developed by a military strategist, it is now applied to understand commercial operations and information behaviour\nWilson’s Information-seeking Model\n\nPerceived information need is bounded in a context (column 1). This perceived need may not be enough if there isn’t enough to activate actions if the stress isn’t high enough (column 2). Intervening variables may become either barriers to or in support of information seeking activities (column 3). The actual activation mechanism itself is driven by social learning theory.\n8 main types of information activities\n\nStarting: initial info-gathering (e.g. searching or asking colleagues)\nChaining: going down the rabbit-holes found in Starting\nBrowsing: casual search for info in different sources (generally non-intentional)\nDifferentiating: rough grouping and separating of different identities/origin of sources\nMonitoring: keeping updated with developments\nExtracting: finding relevant materials in source\nVerifying: making sure info is correct\nEnding: final search\n\nLLMs\nSource\nInstead of asking GPT to simply do smart-autocomplete on your text, you prompt it to respond in a thought/act/observation loop. So you ask GPT to respond like:\n\nThought: Let’s think step by step. I need to find out X and then do Y.\nAct: Search Wikipedia for X\nObservation: From the Wikipedia page I have learnt that …\nThought: So the answer is …\n\nAnd it is allowed to repeat as many times as necessarily, iterating towards its goal.\nThis allows LLMs to go from a constant number of compute per token (a single forward pass) to effectively unbounded computation. This feels significantly closer to how humans process information, where some queries take more brain power to mull over the possibility space rather than responding directly."},"thoughts/Operational-Transform":{"title":"Operational Transform","links":[],"tags":["seed"],"content":"Source\nIt defines a way to describe changes that has two properties:\n\nYou can transform changes relative to other changes. So if user A inserted an “O” at offset 1, and user B concurrently inserted a “T” at offset 10, user A can transform B’s change relative to its own change, an insert the “T” at offset 11, because an extra character was added in front of the change’s offset.\nNo matter in which order concurrent changes are applied, you end up with the same document. This allows A to transform B’s change relative to its own change, and B to transform A’s change similarly, without the two users ending up with different documents.\n\nAn Operational Transformation (OT) based system applies local changes to the local document immediately, and broadcasts them to other users\nIn its simplest form, a transformation function that takes two changes A and B, which both apply to the same document, and produces a new pair Aᴮ (a version of A that applies to the document produced by B) and Bᴬ (B but applies to the document created by A), such that A + Bᴬ and B + Aᴮ (where + indicates change composition) produce the same document.\n    Docˢ\n A ↙   ↘ B\nDocᴬ    Docᴮ\nBᴬ ↘   ↙ Aᴮ\n    Docᴬᴮ\n\nFor text\nSource\nGenerally, OT for text involves 3 main types of operations:\n\nretain(n: number), equivalent to right arrow\ninsert(chars: string), equivalent to typing text\ndelete(chars: string), equivalent to deleting forward\n\nIn practice, retain() tends to be the most commonly used component by a wide margin. The trick is that every operation must span the full width of the document.\nWhen evaluating the operations, the cursor will start at index 0 and walk forward through the existing document and the incoming operation one item at a time. Note that the cursor here is a strictly OT concept and isn’t the same as the editor caret!\nPosition Mapping\nSomething that comes up quite a lot in an editor is the need to transform a document position in an original document into a corresponding position in the changed document. If text is inserted somewhere before the selection, the selection should move forward with the surrounding text. Some OT systems call this “cursor transformation”, but it also applies to things like ranges of collapsed code, lint warnings, breakpoint markers, and so on."},"thoughts/OrbitDB":{"title":"OrbitDB","links":["thoughts/peer-to-peer","thoughts/IPFS","thoughts/CRDT"],"tags":["seed"],"content":"OrbitDB is a distributed, peer-to-peer database with IPFS as its data storage and IPFS Pubsub to automatically sync databases with peers. Provides eventual consistency with CRDTs for conflict-free merges."},"thoughts/Order-theory":{"title":"Order theory","links":["thoughts/causality","thoughts/clocks","thoughts/CRDT","thoughts/A-City-is-not-a-Tree","thoughts/semilattice"],"tags":["seed"],"content":"From An Abstract Plane: CRDT Primer 1\nAn order is a binary relation ≤ on a set S, written &lt;S,≤&gt;.\n\nIf two things a and b are incomparable, we write it a∥b\nTotal order:  for all a and b in the set, either a≤b or b≤a\nPartial order: at least one pair a and b in the set, where a≤b or b≤a\n\nSee also: message ordering, Vector clocks\nJoins\nAn upper bound is an element of the set that is ≥ every other element in the set in terms of that relation\nWhen we take the join of a and b (written a∨b), we’re looking for some element x for which a≤x and b≤x where x is the smallest element that satisfies that condition\nJoin has\n\nCommutativity: a∨b=b∨a\nAssociativity: (a∨b)∨c=a∨(b∨c)\nIdempotence: a∨a=a\n\nWhen it comes to CRDTs, what we’re looking for is the ability to apply an operation in any order and as many times as we want without corrupting the result. The laws obeyed by joins give us exactly this.\nA join semi-lattice then essentially does a topological sort or causal ordering of its elements except all of the elements can be joined (i.e. have a single shared ancestor)\nWe can illustrate the semi-lattice using a Hasse Diagram\n"},"thoughts/Ostrich-Algorithm":{"title":"Ostrich Algorithm","links":[],"tags":["seed"],"content":"Ignoring potential problems on the basis that they may be exceedingly rare. It is named after the ostrich effect which is defined as “to stick one’s head in the sand and pretend there is no problem”. It is used when it is more cost-effective to allow the problem to occur than to attempt its prevention."},"thoughts/Overlay-Network":{"title":"Overlay Network","links":["thoughts/HTTP","thoughts/Internet","thoughts/VPN","thoughts/peer-to-peer","thoughts/distributed-systems","thoughts/DHT"],"tags":["seed","pattern"],"content":"\nA network that is layered on top of another network\n\nNormally on top of HTTP. Fun fact! The Internet was originally built as an overlay upon the telephone network\nExamples:\n\nVPN\n\nWireGuard\n\n\nOpenZiti\nZeroTier\nTailscale (or the open source version Headscale)\nAlmost all peer-to-peer distributed systems\n\nMost notably, DHTs\n\n\n"},"thoughts/Overton-Window":{"title":"Overton Window","links":[],"tags":["seed"],"content":"A window into the world of ideas that frames what people are prepared to entertain and consider possible. Ideas outside the window are not seriously considered.\n\n“Really revolutionary ideas simply roll off men’s minds like water off a duck.”\n\nThus, the act of ‘shifting’ the Overton window involves the work from people and ideas outside this window to shift what if considered acceptable by the general public. This is mostly talked about in the realm of policy but applies to other fields as well."},"thoughts/Overtone":{"title":"Overtone","links":["thoughts/colour"],"tags":["seed"],"content":"An overtone is any resonant frequency above the fundamental frequency of a sound. While the fundamental is usually heard most prominently, overtones are actually present in any pitch except a true sine wave. The relative volume or amplitude of various overtone partials is one of the key identifying features of timbre, or the individual characteristic of a sound.\nIf the fundamental frequency is f then the overtones are nf where n∈N+\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrequencyOrderName 1Name 2f=440Hzn=1fundamental tone1st harmonic2f=880Hzn=22nd overtone2nd harmonic3f=1320Hzn=33rd overtone3rd harmonic4f=1760Hzn=44th overtone4th harmonic\n\nTimbre\nBoth instruments can sound equally tuned in relation to each other as they play the same note, and while playing at the same amplitude level each instrument will still sound distinctively with its own unique tone color.\nThe concept of tristimulus originates in the world of colour, describing the way three primary colors can be mixed together to create a given colour. By analogy, the musical tristimulus measures the mixture of harmonics in a given sound, grouped into three sections.\n\nThe first tristimulus measures the relative weight of the first harmonic;\nthe second tristimulus measures the relative weight of the second, third, and fourth harmonics taken together;\nand the third tristimulus measures the relative weight of all the remaining harmonics.\n\nT1​=∑h=1H​ah​a1​​\nT2​=∑h=1H​ah​a2​+a3​+a4​​\nT3​=∑h=1H​ah​∑h=5H​ah​​"},"thoughts/PBFT":{"title":"PBFT","links":["thoughts/State-Machine-Replication-(SMR)","thoughts/system-model","thoughts/Byzantine-Faults","thoughts/liveness","thoughts/safety","thoughts/33-percent-Impossibility-Result","thoughts/Asymmetric-Key-Cryptography","thoughts/digital-signatures","thoughts/FLP-Result","thoughts/SBFT","thoughts/Raft-Consensus-Algorithm"],"tags":["seed"],"content":"Practical Byzantine Fault Tolerance by Miguel Castro and Barbara Liskov\nTLDR; one of the first state machine replication algorithms with an asynchronous system model that can tolerate Byzantine faults (although it has a weak synchrony assumption where all messages are guaranteed to be delivered after a certain time bound by using timeouts).\nIt can drive a consensus decision in two rounds of message exchanges.\n\nThe first phase guarantees proposal uniqueness through the formation of a quorum certificate (QC) consisting of (n−f) votes.\nThe second phase guarantees that the next leader can convince replicas to vote for a safe proposal.\n\nIt offers both liveness and safety under the 33 percent Impossibility Result and only uses public-key cryptography during faults to prevent major speed bottlenecks (typically just uses signed message digests). This circumvents the FLP Result because it relies on a synchrony assumption to guarantee liveness, not safety.\nFor a faster alternative, consider SBFT (which provides a reduction from O(n2) to O(n) normal-case communication and a best-case latency of only a single round of communication)\nThe primary of a view is replica p such that p=vmod∣R∣ where R is the set of replicas. Note that this explicitly allows for faulty primaries while the algorithm properly handles.\nThe algorithm works as follows\n\nA client c sends a request for operation o to a node in the cluster: ⟨Request,o,t,c⟩σc​​\n\nIf a replica receives the request, it forwards it to the leader\n\n\nThe primary multicasts the request to the backup nodes in a three-phase protocol. The pre-prepare and prepare phases are used to totally order requests sent in the same view even when the primary, which proposes the ordering of requests, is faulty\n\nPre-prepare: mainly used to ensure request was assigned sequence number n in view v. Primary p assigns a sequence number n to the request and multicasts a pre-prepare message with digest m: ⟨⟨Pre-prepare,v,n,d⟩σp​​,m⟩\n\nA replica i accepts a pre-prepare message iff:\n\nIt is in view v\nd is the digest for m and the signature is correct\nhas not seen a pre-prepare message with the same v and n with a different digest d\nsequence number is between some h and H\n\n\n\n\nPrepare\n\nIf replica i accepts the pre-prepare message, it enters the prepare phase by multicasting a ⟨Prepare,v,n,d,i⟩σi​​\nWe define a replica i as prepared iff i has in its log:\n\nThe request m\nA pre-prepare for m in view v with sequence number n\n&gt;2f prepares that match the pre-prepare based on v, n, and d\n\n\n\n\nCommit\n\nIf we are prepared, we have supermajority agreement which means all honest replicas agree on the requests in view v\nReplica i multicasts ⟨Commit,v,n,D(m),i⟩σi​​\nReplicas accept commit messages and insert them in their log provided they are properly signed, the view number in the message is equal to the replica’s current view, and the sequence number is between h and H\nThis phase ensures that if a message is considered committed locally, then it should have been committed for the cluster\n\nA message m is considered committed locally on replica i if it has a log entry indicating it has prepared m with view v and sequence number n and has also accepted 2f+1 commits that match the pre-prepare for m\n\nAfter a m is considered committed locally and all m′ with lower n have been executed, i executes m and applies the state change (as we don’t assume ordered message delivery, keeping messages until ready is critical to ensuring message ordering)\ni then sends a reply to the client: ⟨Reply,v,t,c,i,r⟩σi​​\n\n\nA message m is considered committed on the cluster if for all i in some f+1 honest replicas, it has a log entry indicating it has prepared m with view v and sequence number n\n\n\n\n\n\n\nThe client waits for f+1 replies from different replicas with the same result; this is the end result\n\nIf the client doesn’t receive replies in a timely manner, it broadcasts the request to all replicas. If the request has already been processed, the replicas simply re-send the reply (as replicas cache the last reply sent to each client)\n\n\n\nReplica 0 is the primary, replica 3 is faulty, and C is the client\nGarbage Collection\nAs we assume only asynchronous model, we can’t assume any unresponsive node won’t rejoin at some later point. So either, we need to keep all log entries around potentially forever (not idea), or have some way to transfer state between nodes (which requires nodes to prove correctness of state).\nGenerate state correctness proofs are expensive so only happen once every 100 sequence numbers (a stable checkpoint).\nProof generation:\n\nWhen a replica i reaches a checkpoint, it multicasts a message ⟨Checkpoint,n,d,i⟩σi​​ where d is the digest of the state\nA replica collects checkpoint messages until they have 2f+1 messages for the same sequence number n and digest d\n\n2f+1 messages are the proof of correctness for the checkpoint\nA checkpoint with a proof means that the replica is safe to discard all log messages related to sequence number n\n\n\n\nAdditionally, this checkpointing determines what the waterlevel h and H are\n\nh is the sequence number of the last stable checkpoint\nH=h+k where k is generally twice the gap between stable checkpoints (e.g. 200)\n\nView Changes\nSimilar to the concept of term changes and heartbeats in Raft\nIf the timer of replica i expires in view v, it can broadcast a message to move the system to view v+1\n\nIt stops accepting messages (other than checkpoint, view change and new view messages)\nMulticasts a ⟨View-change,v+1,n,C,P,i⟩σi​​ to all nodes\n\nn is the sequence number of the last stable checkpoint s known to i\nC is the set of 2f+1 certificates for s\nP is a set of sets Pm​ for each message m with a sequence number higher than n\nEach Pm​ is the set containing\n\nA valid pre-prepare message for m\n2f matching valid prepare messages from different replicas\n\n\n\n\nWhen the initiator p of view v+1 receives 2f valid view-change messages, it multicasts a new ⟨New-view,v+1,V,O⟩σp​​\n\nV is the set of all received 2f view-change messages along with the initial view-change message p sent out\nO is the set of pre-prepare messages computed as follows:\n\nmin-s: latest stable checkpoint in V\nmax-s: highest sequence number in a prepare message in V\nFor each n between min-s and max-s\n\nIf there is some message in P where the sequence number matches n\n\nCreate a new message ⟨Pre-prepare,v+1,n,d⟩σp​​\n\n\nIf there isn’t\n\nCreate a new message ⟨Pre-prepare,v+1,n,dnull⟩σp​​ where dnull is the digest of a noop request\n\n\n\n\n\n\nLeader appends all messages in O to its log and enters view v+1\n\n\n"},"thoughts/PMOG":{"title":"PMOG","links":["thoughts/information-retrieval"],"tags":["seed"],"content":"\nGeocaching meets Pokemon Go for the Web\n\nSummarized from their retrospective here.\nThe premise of The Nethernet came from the fact that internet users spend a large portion of their time multitasking, browsing information, or contacting other people online. The Nethernet aimed to classify and allocate an individual’s internet use and then utilize the gathered information in a unique and playful manner.\nMaking the casual act of information retrieval fun and playful\nThe Nethernet was originally an in-browser toolbar that compensated users as they browsed the World Wide Web. The game evolved as a HUD overlay in the Firefox web browser."},"thoughts/PSL-FLM-Impossibility-Result":{"title":"PSL-FLM Impossibility Result","links":["thoughts/Byzantine-Broadcast","thoughts/system-model","thoughts/Public-key-Infrastructure"],"tags":["seed"],"content":"Byzantine Broadcast is impossible in the synchronous system model if you have too many byzantine nodes (for f≥3n​).\nVague intuition for the result, imagine 3 nodes A (Byzantine), B (honest), and C (Byzantine)\n\nNode A is the sender\nA could tell B + C conflicting things\nB + C can compare histories but C can try to frame A\nB can’t distinguish which of A or C are responsible for conflict\n\nThis result breaks down in the presence of PKI (you can’t forge signatures from other nodes!!)\n\nThus, with PKI we get BB for all f\nWithout PKI, we get BB only if f&lt;3n​\n"},"thoughts/Pacing-Problem":{"title":"Pacing Problem","links":["thoughts/Chesterton's-Fence"],"tags":["seed"],"content":"The “pacing problem” refers to the notion that technological innovation is increasingly outpacing the ability of laws and regulations to keep up, first explained in Larry Downes’ 2009 book The Laws of Disruption, in which he famously states that “technology changes exponentially, but social, economic, and legal systems change incrementally”.\nSee also: Chesterton’s Fence"},"thoughts/Panpsychism":{"title":"Panpsychism","links":["thoughts/Materialism","thoughts/emergent-behaviour","thoughts/monism","thoughts/philosophical-realism"],"tags":["seed","PHIL451A"],"content":"Opposite take to that of Materialism. Panpsychists deny that the mind is fundamentally non-mental.\n\nEverything physical is also mental or phenomenal or experiential.\n\nIn other words, everything is slightly conscious.\nSee the Combination Problem\nEddington, Russell, Strawson’s argument for panpsychism\n\nPhysical science tells us about only the structural and relational properties of physical phenomena.\nRelational properties are determined by intrinsic properties\n\nPotentially problematic\nCould be the case that relational processes determine the placeholders of the relation\n\n\nCertain configurations of physical phenomena generate/constitute phenomenal states\nIntrinsic properties of physical phenomena must encompass this power\nOwn inner awareness reveals that phenomenality is an intrinsic property\n\nPotentially problematic\nSupposes the introspection gives us access to the essential nature of our conscious states\nA careful phenomenological investigation of qualia reveals that they are always situated and invested with vital, affective, and embodied value so are relational, not intrinsic. Qualia must have content\n\n\nPhenomenality is the only intrinsic property we know of\nSo phenomenality must be an intrinsic property of physical phenomena, or at least of certain organized physical systems\n\nStrawson’s Panpsychism\nPhysicalism actually entails panpsychism\n\nMonism is true (there is only one kind of ‘stuff’ in the universe)\nExperiential phenomena are real phenomena (denies illusionism: see philosophical realism))\nAll concrete phenomena are physical\nTherefore experiential phenomena are physical\nThe emergence of experiential being from non-experiential beings does not make sense. If this were true, the experiential would need to wholly depend on the non-experiential. This can’t possible be the case: we have no model for it and can’t make sense of it\nSo, at least some elementary physical phenomenal must also be experiential (micropsychism)\nIf at least some elementary physical phenomena must also be experiential, it’s far more likely that all of them are (otherwise there would be radical heterogeneity edging on dualism and we are assuming that monism is true)\nTherefore, panpsychism is true\n"},"thoughts/Pareto-optimality":{"title":"Pareto optimality","links":["thoughts/rationality","thoughts/Nash-equilibrium","thoughts/positive-sum"],"tags":["seed","PHIL321A"],"content":"If there is no profile where one player’s payoff is better but no player’s payoff is worse. Pareto optimality is a criterion of ‘group rationality’, not a criterion for individual rationality (see: Nash equilibrium)\nUsually only applicable for positive sum games"},"thoughts/Parkinson's-Law":{"title":"Parkinson's Law","links":[],"tags":["seed"],"content":"Work expands to fill the time allotted for its completion, regardless of the real amount of work that needs to be done.\nIt was first published in 1955 by the naval historian C. Northcote Parkinson as an essay in The Economist as a commentary on bureaucracy.\nA current form of the law is not the one to which Parkinson referred by that name in the article, but rather a mathematical equation describing the rate at which bureaucracies expand over time.\nHe explained this growth using two forces:\n\n“An official wants to multiply subordinates, not rivals”, and\n“Officials make work for each other.”\n\nCorollaries:\n\nIf you wait until the last minute, it only takes a minute to do.\nBerglas’s corollary: no amount of automation will have any significant effect on the size or efficiency of a bureaucracy.\n\nThe law can be generalized further as: the demand upon a resource tends to expand to match the supply of the resource (If the price is zero)."},"thoughts/Pascal's-Wager":{"title":"Pascal's Wager","links":["thoughts/Precautionary-Principle"],"tags":["seed"],"content":"In Pascal’s Pensées (1623-1662), he outlines an argument that one should always believe in God so long as the probability that God exists is nonzero.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGod existsGod does not existBelieve∞FiniteDon’t believe−∞Finite\nSimilarities\nSee: Precautionary Principle\nIf we draw a decision table for the Catastrophe Principle, we see they follow a similar structure.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKnowledge Condition is metKnowledge Condition is not metEnact e-remedyFiniteFiniteDo nothing−∞Finite\nCriticisms\nPascal’s Wager has been subjected to a number of philosophical criticisms. Manson specifically focuses on the “many gods” objection as a way to dismantle both Pascal’s Wager and the Catastrophe Principle.\nWhat if, for example, there are other gods aside from God that also promise an infinite payout? If you chose to believe in God and He does not exist but the other god does, it is plausible (non-zero possibility) that they could sentence you to eternal damnation (infinite punishment). As a result, this would make believing in God a risk not worth taking. That is, Pascal’s Wager recommends a remedy (believing in God) that itself could lead to a catastrophic result (infinite punishment from a jealous god)."},"thoughts/Physical-Layer":{"title":"Physical Layer","links":["thoughts/Link-Layer"],"tags":["seed","CPSC317"],"content":"Layer 5, layer below the Link Layer\n\nHardware\nUnit: Bits\nResponsibilities: Encodes data appropriately for the physical medium\n"},"thoughts/Plato's-Ship-of-State":{"title":"Plato's Ship of State","links":[],"tags":["seed"],"content":"“Because large sailing vessels by their very nature need to be steered with a firm hand, sailors must yield to their captain’s commands; no reasonable person believes that ships can be run democratically”.\nCounterpoint: there should be democratic agreement from the crew about the final destination of the ship"},"thoughts/Post-It-Note-City":{"title":"Post-It Note City","links":["thoughts/urban-planning","thoughts/infrastructure","thoughts/move-fast-and-break-things","thoughts/feedback-loops","thoughts/catch-22","thoughts/From-Counterculture-to-Cyberculture","thoughts/quantization","thoughts/desire-paths"],"tags":["seed"],"content":"Source: Post-It Note City by Shannon Mattern\n\nAlphabet has the tools to design, build, fund, power, connect, monitor, and monetize a city, and that prospect scares people.\n\nSee also: urban planning\nOn Participatory Planning\n“Paper and ink, models and maps: these are the accessible tools of civic engagement — and corporate self-defense.”\nSidewalk lab’s choice to use the tools of civic design and participatory planning as a defense for their actions also strikes me as extremely cynical.\nBut, by mandating public participation as a requirement for new development, we run the risk of turning community relationship-building into a “checklist of codified practices.” Specifically, it makes participants wary when the invitation is from an org which has money to gain and data to harvest from the participants.\nMaps\nMapping as a way to “hold governments accountable … fill gaps when infrastructural and municipal services are fragmented … [and] make visible social and political processes and events that might be otherwise hidden or overlooked.” (from Crowdsourcing, Constructing, and Collaborating: Methods and Social Impacts of Mapping the World Today)\nMore on participatory mapping and politics: Mapping Politics in/of the Modern City: Cartography as Representation\n‘Participation’ as public performance to ‘signal’ democratic processes without providing the real thing.\nMapwashing: “a disingenuous use of maps, apps, and other tools of participatory planning”\nResponsiveness\n\nIf urban design can be automated, if cities can be made responsive to real-time data collected from environments and inhabitants without their explicit consent, how meaningful is our participation?\n\nSidewalk also proposes creating more responsive public infrastructure (e.g. benches, stormwater pipes, power grids, etc.) which collect data continuously and make ‘smart’ decisions. No opt-in or out process for this type of data collection, no consent.\nHow much of this can be attributed to the Silicon Value ethic of ”move fast and break things”? It feels like Sidewalk is extrapolating too much (trying to pull short feedback loops from naturally long timescales of city which operates on multi-year long scales).\nIs there a relationship between iteration cycle length and potential impact? Software people are used to iterating quickly without having consequence for each marginal build or project. Not the same case with cities where each change affects real people and there’s no real ‘staging’ or ‘development’ area (see Collingridge Dilemma)\nHumanistic Design\nIs creating people-first cities that are not driven by techno-solutionism possible? How do we legitimately involve people in the creation of these cities?\n“Do we need a commercial app to do what a robust, public democratic process should do?” one quote reads. Or is rather than these democratic processes are failing so we look to alternatives?\nSidewalk feels very much talking into the void and talking to the org, not much talking with each other, no discussion groups. Yes, process and documentation matter, but if there’s no input into this feedback loop, the ideas are just self-reinforcing in an echo chamber. The problem is that large corporations like Alphabet present already-fleshed-out ideas with little room for debate so that ‘conversations’ and ‘feedback’ are largely performative.\nThe use of “cheerful minimalism to mask the insidiousness of multinational tech corporations with friendliness and approachability,” feels very similar to the weird vibe of the signature corporate art style and why it feels so fake\nFrom Jasmine: With software, you can find product market fit, ignore edge cases and people who won’t make you money, etc. But with cities, you are democratically obligated to build for everyone (or at least the majority)\nLabels and Quantization\nSidewalk feels like a city trying to label and quantize everything\n\nParticipants are obliged to transform complex spatial phenomena into machine-readable points, lines, and areas, adopting a logic defined by Euro-American geopolitics, settler colonialism, and property relations.\n\n[These maps and markers], while useful and actionable, tended to “reduce complex life stories and neighborhood histories to ‘dots on a map.‘”\nData as digital desire paths? Not sure how I feel about ‘anonymized’ or ‘de-contextualized’ data, stripping down and de-contextualizing opinions (e.g. post-it notes) removes important context and nuance."},"thoughts/Postel's-Law":{"title":"Postel's Law","links":["thoughts/TCP","thoughts/Internet","thoughts/Protocol"],"tags":["seed","pattern"],"content":"\nBe conservative in what you send, be liberal in what you accept\n\nThe principle is also known as Postel’s law, after Jon Postel, who used the wording in an early specification of TCP.\n“It takes two to miscommunicate.” A great listener, or a skilled speaker, can resolve a lot of conflicts.\nPostel’s Law is the principle the Internet is based on. Not because Jon Postel was such a great salesperson and talked everyone into it, but because that is the only winning evolutionary strategy when internets are competing.\nStructure from Structurelessness\nAllow all, bless some is an important general principle for building evolvable systems. Allowing all means new use-cases can evolve from the bottom-up, while blessed fields provide a stable API which multiple clients can count on\nThis is how protocols evolve"},"thoughts/Precautionary-Principle":{"title":"Precautionary Principle (PP)","links":["thoughts/utility"],"tags":["seed","PHIL321A"],"content":"In ordinary scenarios, we maximize expected utility (EU Max). However in precautionary scenarios, we want to avoid catastrophic harm (Precautionary Principle)\nTripod Framework\nManson’s Framework for when to use the precautionary principle\n\nSerious damage (damage condition): some outcomes exceed a threshold of badness and are considered catastrophic\n\nThresholds for levels of harm: serious, catastrophic, irreversible, civilization collapse\nThresholds for time scales: present, pre-2100 (current population), post-2100 (future generations)\nScope: local, global\n\n\nUncertainty (knowledge condition): some outcomes meet a required threshold of evidence to count as a serious threat\n\nProbability: possible, non-negligible probability, predicted by a model, predicted by a well-understood scientific mechanism\n\n\nProportional remedy: response should correspond to the plausibility and severity of the threat\n\nProhibition/ban, further research, moratorium, mitigation/restriction\n\n\n\nHowever, could potentially lead to irrational outcomes or outcomes that are contradictory. e.g.\n\nGreenhouse gases (GHG) may cause catastrophic temperature rise ⇒ Ban GHG at once\nBanning GHG may lead to economic crisis, world war and nuclear holocaust ⇒ Don’t ban GHG\n\nVersions of PP\n\nIf the activity meets the damage condition, and\nIf the link between the e-activity and the e-effect meets the knowledge condition,\nThen apply the e-remedy.\n\nBut, PP versions must be internally consistent; it cannot ban its own remedies\nCatastrophe Principle\nIf an effect is catastrophic, and there is a possible link between the activity and the effect, then the remedy is to ban the activity completely.\nFor example, ban nukes because their possible use has a catastrophic effect"},"thoughts/Primacy-of-Consciousness":{"title":"Primacy of Consciousness","links":["thoughts/consciousness","thoughts/monism","thoughts/qualia","thoughts/emptiness","thoughts/philosophy-of-science","thoughts/ontology","thoughts/Hard-problem-of-consciousness"],"tags":["seed","PHIL451A"],"content":"Related: consciousness, neutral monism\nHorizon Metaphor\nHusserl had no word to denote what is not really an object, but a process of uncovering or displaying potentialities — thus, the horizon metaphor.\n\nThe existential primacy of consciousness: consciousness in the horizonal sense is not something we have; it’s something we live\n\nTwo conceptions\n\nThe horizon is real: it is the farthest point the eye can see before the Earth’s surface curves away beneath our view\nThe horizon is ideal: it is a structure of perception but not something that actually exists independent of perception\n\nThe horizon is a phenomenal structure of consciousness, not a particular phenomenal property (quale) or phenomenal content. Both qualia and other phenomenal contents always appear from within the horizon of consciousness.\nConsciousness then in the horizonal sense is not something we ‘have’, it is something we live in. There is no way to step outside consciousness and measure it against something else because inside the horizon is all we know\nThe horizon itself is empty.\nKant defines ‘transcendental knowledge’ as knowledge which is occupied not so much with objects as with the mode of our knowledge of objects in so far as this mode of knowledge is to be possible a priori. Consciousness then is not another object of knowledge, but that by which any object is knowable — consciousness is irreducible to the domain of objects.\nMerleau-Ponty on the world and consciousness\nTwo conceptions on the world and the universe\nDefine the life-world as the space of meaning within which anything is intelligible and thinkable\n\nNatural Science: the universe contains the life-world\nPhilosophy: the life-world contains the universe; the universe is always disclosed to us from within the life-world\n\nThe Blind Spot\nAdam Frank, Marcelo Gleiser, Evan Thompson in Aeon\nScientific materialists will argue that the scientific method enables us to get outside of experience and grasp the world as it is in itself. But experience is present at every step. Scientific models must be pulled out from observations, often mediated by our complex scientific equipment. They are idealisations, not actual things in the world.\n“In principle, it is absurd to think that we can explain consciousness by reducing it to certain objects of science, since these objects are abstract relational structures extracted from the life-world of lived experience” (Husserl)\nQuantum-Bayesianism (QBism) combines quantum information theory and Bayesian probability theory. It interprets the irreducible probabilities of a quantum state not as an element of reality, but as the degrees of belief an agent has about the outcome of a measurement. In other words, making a measurement is like making a bet on the world’s behaviour, and once the measurement is made, updating one’s knowledge.\nAdvocates of this interpretation sometimes describe it as ‘participatory realism’, because human agency is woven into the process of doing physics as a means of gaining knowledge about the world. From this viewpoint, the equations of quantum physics don’t refer just to the observed atom but also to the observer and the atom taken as a whole in a kind of ‘observer-participancy’\nThe upshot: there is no simple way to remove our experience as scientists from the characterization of the physical world. Observing it doesn’t only affect the measurement, observing it is the measurement\nScientific knowledge then is a self-correcting narrative made from the world and our experience of it evolving together (see Karl Popper’s Philosophy of Science)\nIs Consciousness Primary?\nMichel Bitbol in NeuroQuantology\nPosits that consciousness is methodological and existentially primary\n\nConsciousness is not something\n\nNouns refer to some manipulable or abstract object. But an object is an entity which supposedly exists independently of situations and subjective states.\nThis cannot possibly be the case as consciousness as experience is situated — it is what it feels like to be a subject/what it feels like to be\n\n\n\nArguing that consciousness is existentially primary and not ontologically secondary to matter. Sartre: “consciousness is never merely possible apart from existing; it is no possible instantiation of a definition apart from being actual”\n\nThey have forgotten that objective knowledge is ‘made possible’ by carving the lacuna of first person experience within it… scientists who believe that solving many such “easy problems” about consciousness will finally clear up the harder problem of its physical origin, look like somebody who believes one can finally reach the horizon by walking far enough\n\nIn the same way as the walker ignores the category gap between a line in space and an apparent line seen through space, these scientists ignore the category gap between the exclusively structural connections provided by science and the absolute and the absolute of experience analyzed through a structured framework\nTwo approaches to overcome the explanatory gap both fail\n\nAbsolutizing some properties of matter\n\nIf anything can be called “absolute”, it is conscious experience\n\nLived experience is immediately and completely given from a self-evidential standpoint\nFuture experience can by no means disconfirm its existence here and now, but only take its place — it is absolute in the sense that it is indubitable whenever it is present\n\n\nSearle: “Consciousness is the very fact that there is appearance; appearance is the reality of consciousness”\nSomething like “it appears red” instead of “it is red”. The subjective statement is admittedly indisputable, but the fact is not. This is a functional absolutization of the statement\n\n\nRelativizing/structuralizing experience\n\nKarl Mannheim: coordinating the variety of individual or collective perspectives entails an ever increasing formalization of knowledge\nCassirer: history of science as a whole tends towards relinquishment of substantial and toward research of “invariant relations” instead\n\nIf properties are referred to, it is only after the concept of property has been redefined in such a way that it includes in itself the concept of relation\n\n\nThe problem: accounting for the self-evidentially absolute conscious experience in terms of the relational concepts of objective science\n\nChalmers and Strawson both attempt to overcome this conflict in naturalist terms, seeking to explain it in terms of some hidden nonrelational property\nLeibnizian Monadology: “we can attribute to substances no other intrinsic state than that whereby we ourselves inwardly determine our sense”\n\n\n\n\n\nPotential solutions\n\nInstead of absorbing or reducing contents of experience or phenomenological reports into the structural network of objective science, strive towards embedding these experiences within a broader relational network of which the law-like relations of the objective domain is only a fraction\nVarela’s notion of “mutually generative constraints” towards reciprocal alteration and enrichment of experiential and objective concepts\n\nPhenomenological reports may help to pick out and ascribe meaning to previously unnoticed neural configurations\nNeurological findings may become an incentive for re-categorization and further development in phenomenological research\n\n\n\nThese are not ‘solutions’ to the hard problem per-se but rather dissolves it. It does not arise because the physical world is no longer the standard for being, and objectivity is no longer the ultimate standard of method."},"thoughts/Projects":{"title":"Projects","links":["thoughts/idea-list","posts/bft-json-crdt","thoughts/formality-considered-harmful","thoughts/game-design","thoughts/Raft-Consensus-Algorithm","thoughts/interaction-design","thoughts/digital-commons","thoughts/information-scales","thoughts/peer-to-peer","thoughts/Hypercore","thoughts/democracy","thoughts/NLP","res/gvrsf_report.pdf","thoughts/Internet"],"tags":["evergreen","technical"],"content":"This is a list of notable projects that I’ve finished and or currently maintaining. My (considerably longer) list of unfinished ideas can be found here.\nbft-json-crdt\nThe first public implementation of a JSON-like Byzantine Fault Tolerant CRDT. The project implements a simplified Automerge-like CRDT as well as the ideas in Martin Kleppmann’s 2022 paper on Making CRDTs Byzantine Fault Tolerant. The blog post also hit #3 on Hacker News the day it was released and has been featured in go-to resources for CRDTs.\nGitHub, blog post\nTabspace - a scratchspace for your new tab page\nA beautiful new tab replacement that gives you your very own scratch space to help you stay organized and focused. Wanted to experiment with low-friction note taking and integrating game design principles of ‘juiciness’ into UI/UX.\nGitHub, Chrome Webstore\nminiraft - &lt;1kloc Raft consensus algorithm implementation\nA minimal implementation of the raft Consensus Algorithm with a focus on readability/understandability. This project was created as an exercise in implementing and learning about distributed systems.\nGitHub, Documentation\nCursor Chat — open source library for digital presence\nGitHub, Demo\nA lightweight (31.8kB) cursor chat à la Figma for digital co-existing + presence. An experiment in spatial software, interaction design, and digital commons.\nBuilt on top of yjs and perfect-cursors.\nTelescopic Text — open source library for expandable text\nGitHub, Demo\nAn open-source library to help with creating expandable text, inspired by StretchText and TelescopicText.\nI’ve been thinking a lot about creating a browsable store of knowledge that provides something useful at all distance scales (e.g. viewing the entire library, just a subcategory, a single file, etc.) and concepts like Telescopic Text are a first step in creating more information scales than just a single document level.\nPortal — zero-config P2P encrypted folder syncing\nProducthunt, GitHub\nPortal is a command line tool that syncs folders between multiple devices. Perfect for syncing photos/videos/code between many devices without using a 3P tool like GitHub, Email, or Google Photos. Built on top of the Hypercore protocol with Ink for the CLI.\nBuilt with: Hypercore, React, Ink, and Typescript\nQuartz — create and publish digital gardens for free\nSite, GitHub\nQuartz is a tool and workflow to make maintaining and publishing a digital garden and second brain extremely easy. It involved creating a static site generator from scratch. See the architecture page for more information.\nBuild with: Typescript, esbuild, and Node\nLegist — a platform to summarize policy for democracy\nDevPost (Finalists at HTN 2020++), GitHub\nLegist is a web platform that allows users to digest policies in an efficient and accessible manner. Legist allows users view automagically summarize pieces of policy + legislation while still maintaining the key takeaways, view and filter policies by category, and subscribe to periodic rollups on updates. Frontend was built with React + Typescript + Chakra UI. Text summarization was done using DistilBART, Named-entity recognition with BERT, and zero-shot text categorization using BART. All models were served with BentoML. Built at Hack the North 2020++, winning the Founder Institute Fellowship Prize and finalist among over 3000+ participants\nBuilt with: React, Firebase, GraphQL, CockroachDB, Node.js, Flask, Docker, MailGun, BentoML, HuggingFace, TypeScript, Python, JavaScript\nctrl-v — a modern, open-source pastebin\nApp, GitHub\nctrl-v is a modern, open-source pastebin with LaTeX and Markdown rendering support. Any user can create a paste without an account, with the ability to protect it with a password and set an expiry date. Additionally, ctrl-v does code highlighting as well as LaTeX and Markdown rendering. Pastes are stored in a MongoDB Atlas instance. Backend is a containerized Go service deployed on Google Cloud Run. Frontend is a SSR Next.js app hosted on Vercel.\nBuilt with: React, Next.js, Vercel, styled-components, MongoDB, Cloud Run, GCR, JavaScript, Go, Docker\nreflect — a mindful website blocker for the productive\nSite, GitHub\nreflect is a browser extension with 800+ active users focused around asking users to reflect before visiting distracting sites, helping to reduce mindless scrolling while still being able to get work done. During closed-beta, we created a Go service that logged user intents to a Cloud SQL database and did intent classification by serving a basic Flask API. We then trained an LSTM network in Keras on the closed-beta data and augmented it using NLP data augmentation techniques, reaching ~86% classification accuracy. Finally, the model was ported to Tensorflow.js where it runs in-browser within the extension which is written in Typescript.\nBuilt with: Kubernetes, Docker, GKE, Cloud SQL, Keras, Tensorflow.js, Flask, CircleCI, TypeScript, Python, Go\nnanoDB — a simple, easy, and debuggable document database\nGoDoc, GitHub\nnanoDB arose out of many frustrations I’ve personally come across while prototyping, namely 1) difficulty of debugging data 2) faffing around with language specific drivers and 3) reference resolution. As a result, nanoDB stores everything on disk as a JSON document, has built-in reference resolution, and can be used fully through a REST API. Think of it like Redis but with MongoDB style documents — all of which is on-disk, human-readable, and through a REST API. The project is fully written in Go and is thoroughly unit-tested. It features a standalone server binary which creates a nanoDB server, as well as a shell which allows you to do some basic document inspection.\nBuilt with: Docker, Go\nreadAR — an AR app to help those with learning disabilities\nDevPost (TreeHacks 2020, Microsoft Azure Champ Prize - Hack for Good), GitHub\nreadAR is a mobile AR app re-renders text to be more dyslexic-friendly, and adds context-dependent word definitions and images. A custom BERT model was created and trained for word sense disambiguation (WSD), achieving a 76.6% F1% score on the test dataset which is only ~5% away from state-of-the-art. This model is served through Flask on an Oracle VM Instance. The API is also responsible for our image processing pipeline, which is a conglomerate of different Azure APIs (OCR, Text Analytics, Bing Search).\nBuilt with: PyTorch, Flask, Docker, Azure, Oracle Cloud, Python, Bash\nImpostor — group pomodoro timer with a twist\nDevpost (Finalist at DubHacks 2020, Disney Prize), GitHub\nImposter is a productivity timer designed to keep friends on task together even when working remotely. The Chrome extension monitors your browser tabs, checking against a blocklist of unproductive sites. If one of those websites is visited, the backend will be notified via Firebase and will notify all users in the room through a WebSocket connection. Chat and character customization is also supported.\nBuilt with: Firestore, WebSockets, Node.js, Heroku, JavaScript\nSpeech2Braille — a wearable device to transcribe speech\nPaper (Silver + 10k in awards at Canada Wide Science Fair), GitHub\nSpeech2Braille was created to help the over 360 million people in the world who have debilitating hearing loss. This project entailed creating an end-to-end speech recognition system using an Deep LSTM and a portable device to display braille. The device is able to recognize audio and transcribe it into Braille through the haptic feedback device via a novel neural network architecture. The feedback device is a self-made GPIO hat, consisting of 6 solenoids. The neural network itself is 2 layered LSTM-CTC network with 256 hidden cells in each layer, achieving 92% state-of-the-art word error rate on the TIMIT dataset.\nBuilt with: Tensorflow, numpy, Raspberry Pi, Python, Bash\nPacketBook — blockchain banking without Internet\nDevpost (nwHacks 2018 SAP Prize, Top 30), GitHub\nPacketBook is a financial accessibility chatbot, fully accessible through the SMS (text messaging) protocol. Users are able to issue simple commands to register, check their balance, deposit, withdraw, and send money. These commands are secured with two-factor authentication through a Flask server on Heroku with Twilio. The backend that handles transaction is written with Node and Express and deployed on stdlib. PacketBook is unique in that it leverages the Stellar blockchain and tokens (XLM) for its transactions which greatly reduces operating overhead with its minimal transaction costs (around 1/100 of a cent per transaction).\nBuilt with: MongoDB, Node.js, Express, Flask, Stellar, Heroku, JavaScript, Python"},"thoughts/Prolly-Trees":{"title":"Prolly Trees","links":["thoughts/peer-to-peer","thoughts/search","thoughts/content-addressed-storage"],"tags":["seed"],"content":"Everything that can apply to B+ Trees and database indexes, can now apply to Prolly Trees with the added ability of it being peer-to-peer, sparsely loaded, and mergeable.\nThis enables multi-tenant peer-to-peer search. With p2p databases we can begin to build up search indexes collaboratively with community members, where individuals or smaller groups can participate in generating indexed data, and larger indexes being formed from combining the smaller ones.\nWhy not just B-Trees?\nData structures with hysteresis have path dependency, in the case of B-trees the actual tree structure depends on the order of inserts and removes.\nMerkle Search Trees\nSource\nMerkle Search Trees are an incremental method for constructing determinstic B-trees. Their idea is to first hash all content which is inserted into the tree, and then to place items at a level proportional to the number of 0’s in the prefix of their hash written in base B.\nUnder the assumption that all hashes are uniformly distributed, a Merkle search tree has the following properties:\n\nEvery internal node has on average B children\nAll leaves are on average the same distance from the root\nThe tree is deterministic\n\nHowever, these are not Sybil resistant\nProlly Trees\nSource\nA Prolly Tree is a search tree where the number of values stored in each node is determined probabilistically, based on the data which is stored in the tree.\nWithin each hash, we look for a pattern that has a known probability of occuring. If the pattern is found, that position is a boundary. We slide the window forward to the end of the containing item, and write a new chunk containing the bytes between this boundary and the previous, if any. The resulting chunk is stored in a content addressed storage system\nIn Noms, the pattern we look for is the 12 high bits being 1. Since this has a probability of 1/2^12, the average chunk size in Noms is 4kb."},"thoughts/Protocol":{"title":"Protocols","links":["thoughts/HTTP"],"tags":["seed","CPSC317"],"content":"Defines:\n\nRoles of communicating entities\nFormat of messages\nOrder of messages\nActions taken on the transmission, receipt of a message, or other event\n\nProtocols as State Machines\nA fully-defined protocol must provide a proper action for any event in any state. Most protocols can be modelled in terms of state machines.\nEach interaction can have its own state machine.\nOpen vs Closed Protocols\n\nOpen Protocols\n\nExamples: HTTP, SMTP, SSH\nUsually defined in RFC documents (normally via some sort of working group or task force)\nDifferent implementations\nAllows a community, generally good!\n\n\nProprietary Protocols\n\nExamples: Skype, iCloud, Zoom\nOnly one implementation\n\n\n\nBuilding a Reliable Protocol\n\nGenerally includes a few states\n\nIdle - waiting for something to be initiated\nWaiting - waiting for a response\n\n\nEdges are actions (e.g. receives a response of a certain type) or timeouts\nEdge cases\n\nTimeout too soon! (may result in getting two ACKs)\n\n\nTimeout formulas\n\nAssuming measured round trip time (RTT) of t\nEstimated RTT\n\nERTTi​=(1−α)ERTTi−1​+αt\n\n\nDeviation of RTT (captures jitter):\n\nΔRTTi​=(1−β)ΔRTTi−1​+β∣t−ERTTi−1​∣\n\n\nTimeout:\n\nERTTi​+4ΔRTTi​\n\n\nSuggested: α=0.125, β=0.25\n\n\n"},"thoughts/Proxy-Objects":{"title":"Proxy Objects","links":[],"tags":["seed"],"content":"Source\nAllows us to create a ‘fake’ object that lets us intercept attribute accesses and calls with some funky stuff under the hood.\ninterface ProxyCallbackOptions {\n  path: string[];\n  args: unknown[];\n}\n \ntype ProxyCallback = (opts: ProxyCallbackOptions) =&gt; unknown;\n \nfunction createRecursiveProxy(callback: ProxyCallback, path: string[]) {\n  const proxy: unknown = new Proxy(\n    () =&gt; {\n      // dummy no-op function since we don&#039;t have any\n      // client-side target we want to remap to\n    },\n    {\n      get(_obj, key) {\n        if (typeof key !== &#039;string&#039;) return undefined;\n \n        // Recursively compose the full path until a function is invoked\n        return createRecursiveProxy(callback, [...path, key]);\n      },\n      apply(_1, _2, args) {\n        // Call the callback function with the entire path we\n        // recursively created and forward the arguments\n        return callback({\n          path,\n          args,\n        });\n      },\n    },\n  );\n \n  return proxy;\n}\n\nThe get method handles property accesses such as post.byId. The key is the property name we’re accessing, so when we type post our key will be post, and when we type post.byId our key will be byId. The recursive proxy combines all of these keys into a final path, e.g. [“post”, “byId”, “query”], that we can use to determine the URL we want to send a request to.\nThe apply method is called when we invoke a function on the proxy, such as .query(args). The args is the arguments we pass to the function, so when we call post.byId.query(args) our args will be our input, which we’ll provide as query parameters or request body depending on the type of procedure. The createRecursiveProxy takes in a callback function which we’ll map the apply to with the path and args.\n\nNote that we can cast the above unknown proxy to whatever shape we want. In tRPC, this is done by extracting the type from the server and then doing some transformations to get a client type and then casting to that."},"thoughts/Public-key-Infrastructure":{"title":"Public-key Infrastructure","links":["thoughts/Asymmetric-Key-Cryptography","thoughts/Key-Sharing-Problem","thoughts/trust"],"tags":["seed"],"content":"Public key infrastructure (PKI) is a catch-all term for everything used to establish and manage public key cryptography. Related is the key sharing problem.\nFor example, it can help ensure that all peers in the network know each other’s public keys (i.e. that public keys are… public knowledge)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrivate KeyPublic KeyStays secret on your deviceShared with all your peers on the networkActs like a password — only you have it, and it’s necessary for proving ownership of your public keyActs like a User ID — people publicly know about this to identity youCan create signatures — unforgeable, tamper-evident certifications that you created that dataAllows others to verify the integrity of your signatureA mailbox key — allows you to open messages that only you can seeA mail slot — allows others to send you data only you can see\nPGP\nPretty Good Privacy\nA sort of web of trust protocol where you determine whether to trust another party based on who else you know has trusted that party.\n\nAs time goes on, you will accumulate keys from other people that you may want to designate as trusted introducers. Everyone else will each choose their own trusted introducers. And everyone will gradually accumulate and distribute with their key a collection of certifying signatures from other people, with the expectation that anyone receiving it will trust at least one or two of the signatures. This will cause the emergence of a decentralized fault-tolerant web of confidence for all public keys.\n\nThe web of trust mechanism has advantages over a centrally managed public key infrastructure scheme"},"thoughts/Putnam's-Ant-Argument":{"title":"Putnam's Ant Argument","links":["thoughts/representation","thoughts/intentionality","thoughts/causality","thoughts/originality"],"tags":["seed"],"content":"The ant who traces a line that ends up looking like a caricature of Winston Churchill. Is it a picture or representation of him?\nResemblance is not necessary nor sufficient for representation\n\nnot sufficient → A can resemble B even if it fails to represent B\nnot necessary → A can represent B even if A fails to resemble B\n\nDo we need intentionality?\nWords and pictures aren’t inherently about something, that meaning is added via our own attitudes. What does matter for representation is some sort of causal link back to the original thing."},"thoughts/RDF":{"title":"RDF","links":["thoughts/LDP","thoughts/search"],"tags":["seed"],"content":"\nRDF is a standard model for data interchange on the Web\n\nRDF can be understood as a linking structure which forms a directed, labeled graph, where the edges represent the named link between two resources, represented by the graph nodes.\nSee also: LDP\nRDF Triple\nRDF extends the linking structure of the Web to use URIs to name the relationship between things as well as the two ends of the link (this is usually referred to as a “triple”). Much like a relational database, information in a triplestore is stored and retrieved via a query language.\nA store of RDF Triples is called a triplestore.\nSearch\nFrom Intertwingle\nFollowing a link only gives you one dimension of mobility. A search can be seen as following multiple links, and finding the intersection (or union) of the results of those links.\nAny link-relationship should be searchable. For example:\n\nAll messages from person between date and date that have pattern in the body.\nAll messages from person which contain a message from person.\nAll messages to mailing-list which refer to URL.\nAll messages containing text in the main body, but not in an attachment.\nAll messages with an attachment whose file name contains string.\n\nTurtle\nSource: W3\nA textual syntax for RDF called Turtle that allows an RDF graph to be completely written in a compact and natural text form, with shorthands for common usage patterns and datatypes\n@base &lt;http://example.org/&gt; .\n@prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; .\n@prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; .\n@prefix foaf: &lt;http://xmlns.com/foaf/0.1/&gt; .\n@prefix rel: &lt;http://www.perceive.net/schemas/relationship/&gt; .\n \n&lt;#green-goblin&gt;\n    rel:enemyOf &lt;#spiderman&gt; ;\n    a foaf:Person ;    # in the context of the Marvel universe\n    foaf:name &quot;Green Goblin&quot; .\n \n&lt;#spiderman&gt;\n    rel:enemyOf &lt;#green-goblin&gt; ;\n    a foaf:Person ;\n    foaf:name &quot;Spiderman&quot;, &quot;Человек-паук&quot;@ru .\nJSON-LD\nA JSON-LD document is both an RDF document and a JSON document and correspondingly represents an instance of an RDF data model\n{\n  &quot;@context&quot;: &quot;[https://json-ld.org/contexts/person.jsonld](https://json-ld.org/contexts/person.jsonld)&quot;,\n  &quot;@id&quot;: &quot;[http://dbpedia.org/resource/John_Lennon](http://dbpedia.org/resource/John_Lennon)&quot;,\n  &quot;name&quot;: &quot;John Lennon&quot;,\n  &quot;born&quot;: &quot;1940-10-09&quot;,\n  &quot;spouse&quot;: &quot;[http://dbpedia.org/resource/Cynthia_Lennon](http://dbpedia.org/resource/Cynthia_Lennon)&quot;\n}"},"thoughts/RPC":{"title":"RPC","links":["thoughts/Protocol","thoughts/HTTP"],"tags":["seed"],"content":"RPC: Remote Procedure Call. Sometimes called a remote function call, it is a call to a function whose implementation is on another node. Normally a part of a protocol specification\nRPCs needs an RPC client/server to mediate the call.\n\nClient marshals (encode) arguments and passes it to the server\nServer unmarshals (decodes) arguments and runs the implementation\n… same in reverse for response\n\nGenerally an HTTP request but other types of RPCs exists as well (e.g. gRPC)"},"thoughts/RSA":{"title":"RSA","links":["thoughts/Asymmetric-Key-Cryptography","thoughts/Key-Sharing-Problem","thoughts/Elliptic-curve-Cryptography-(ECC)"],"tags":["seed"],"content":"A form of asymmetric cryptography.\nFull name is the Rivest, Shamir, Adelson Algorithm\nIt relies on modular arithmetic which, unfortunately, is really slow :((. Encryption/decryption are computation-heavy. Ok for occasional communication but too slow for extensive data transfer. It’s good for establishing initial secure connection. Hard to crack because to determine d from (n,e) requires computing factors of n which is a hard problem\nSteps:\n\nChoose two large primes p and q (1024-bits each)\nCompute n=pq,z=(p−1)(q−1)\nChoose e&lt;n that has no common factors with z (commonly 3)\nChoose d&lt;n such that edmodz=1\nPublic key: KB+​=(n,e)\nPrivate key: KB−​=(n,d)\nEncrypting is then encrypt(m)=memodn\nDecrypting is then decrypt(c)=cdmodn\n\nKey exchange can also be performed using RSA\n\nIf Alice and Bob both know the other’s public key, how can they agree on a shared “session” key?\nAlice chooses key KS​ and encrypts it with Bob’s public key and Alice’s private key KA−​(KB+​(KS​))\nBob decrypt’s the message using his private key and Alice’s public key KB−​(KA+​(KA−​(KB+​(KS​))))=KS​\n\nSee also: elliptic curve cryptography"},"thoughts/Raft-Consensus-Algorithm":{"title":"Raft Consensus Algorithm","links":["thoughts/consensus","thoughts/Byzantine-Faults","thoughts/Tangaroa","thoughts/safety","thoughts/Transport-Layer","thoughts/RPC"],"tags":["seed","technical"],"content":"\nAn understandable consensus algorithm\n\nA distilled version of the Raft paper. For a more graphic version, see this visualization of Raft by The Secret Lives of Data.\nA really good video review of the algorithm by Martin Kleppmann\nFor a BFT-resilient version of Raft, see Tangaroa.\nDistributed Consensus\nWhen you only have one machine, it is easy to figure out what the state of that machine is in. But what happens when you have multiple machines that need to agree on some value or state?\nHow do we arrive at a shared set of state across multiple machines that can be as far as opposite sides of the world? How do we handle machines crashing and become unable to respond to incoming requests?\nThis is the problem of distributed consensus\nReplicated State Machines\nGenerally, this is done using a log of actions that are replicated across all machines. Keeping this replicated log consistent between all the machines is the job of the consensus algorithm. They allow a collection of machines to agree on some shared state which still make sense even when there is latency or unavailability.\nIn more formal language, consensus algorithms should typically have the following properties:\n\nSafety in the face of network delays, partitions, packet loss, duplication, and reordering (except under certain cases where there are no known solutions, e.g. Byzantine Fault Tolerance)\nFunctional (available) as long as the majority of servers are operational and can communicate\nLatency resilient and does not depend on timing of messages to ensure consistency\n\nRaft is one such consensus algorithm for managing a replicated log. It is an alternative to Paxos which is the main consensus algorithm in use over the last decade. The main aim is to make it understandable to builders and students alike.\nIt is important to note that Raft assumes that all messages have been authenticated/authourized. It hands this responsibility to the transport layer to deal with. As such, Raft does not have any protection against malicious actors. More discussion in this Google Groups conversation.\nConsensus\nRaft implements consensus by first electing a leader, then giving that leader temporary but complete responsibility for managing the replicated log. When a leader fails or becomes disconnected, a new leader is elected.\nGiven this approach, Raft decomposes this consensus into 3 independent subproblems\n\nLeader election: how do we choose a new leader when an existing leader fails?\nLog replication: how does the leader accept new log entries from clients and replicate them across all the other machines?\nSafety: when is it safe to consider log entries as ‘agreed upon’ and fully replicated across all machines?\n\nA server can only be in one of 3 states:\n\nLeader: handles all client requests\nFollower: issues no requests but respond to requests from leaders and candidates\nCandidate: used to elect a new leader\n\nState transitions follow the state diagram below:\n\nAll Raft servers communicate using remote procedure calls (RPCs) that happen over the network. The basis consensus algorithm only requires 2 types of RPCs, RequestVote and Append-Entries. These are retried if a request times out and are issued in parallel for best performance.\nLeader Election\nLeaders are active for terms of arbitrary length (this is randomly determined as we will see later). These are numbered with consecutive and monotonically increasing integers.\nEach term begins with an election in which one or more candidates attempt to become leader. If a candidate wins the election, then it serves as leader for the rest of the term.\n\nInitiate State\nServers start up in the follower state.\nA server remains in the follower state as long as it receives valid RPCs from a leader or candidate (this is usually in the form of a ‘heartbeat’ from a leader which is an empty AppendEntries RPC with no log entries).\nIf a follower receives no communication over a period of time called the election timeout (randomized between 150ms and 300ms), then it assumes there is no viable leader and begins an election to choose a new leader.\nBeginning an Election\nA follower increments its current term and transitions to candidate state. It then votes for itself and issues RequestVote RPCs in parallel to each of the other servers in the cluster.\nIt is important to note that a server can only vote once per election. It will give its vote to the first server that asks for it and meets the requirements for election. A server should only vote for a candidate if the candidate’s log is more up-to-date than its own. If the logs have different terms, the one with the larger term is more up to date. If the logs have the same term, the longer log is more up-to-date.\nA candidate remains a candidate until one of 3 events happens:\n\nIt wins the election. It received votes from a majority of servers in the cluster. Majority rule ensures that at most one candidate can win the election for a particular term. It then sends heartbeat messages to all other servers to establish authority and prevent new elections.\nAnother server establishes itself as leader. Received an AppendEntries RPC from another server claiming to be leader. This claim is legitimate if the leader’s term is at least as large as the candidate’s current term.\nA period of time goes by with no winner. Possible if many followers become candidates at the same time, votes can be split so no candidate wins majority. When this happens, each candidate times out and starts a new election by incrementing its term and initiating another election. Raft uses randomized election timeouts to ensure split votes are rare.\n\nAfter a leader has been elected, it beings servicing client requests.\nProperties\nGenerally, Raft will be able to elect and maintain a steady leader as long as the system roughly follows the timing requirement: broadcastTime &lt; 10 * electionTimeout &lt; 100 * MTBF where broadcastTime is amount of time for a server to send an RPC to every server in the cluster and MTBF is the mean time between failure for a server.\nBroadcast time should be roughly an order of magnitude less than the election timeout so that leaders can reliably send heartbeat messages required to keep followers from starting elections (similar to having RTT be roughly a magnitude smaller than request timeout).\nThe election timeout should be a few orders of magnitude less than MTBF so that the system makes steady progress. When a system crashes, it will be down for roughly the period of the election timeout.\nLog Replication\nEach client request is a command to be executed by the replicated state machines. The leader appends the command to its own log as a new entry, then issues AppendEntries RPCs in parallel to each of the other servers to replicate the entry.\nA single log entry contains the state machine command from the client request along with the term number when the entry was received by the leader. A log entry is considered ‘safely replicated’ or committed once it is replicated on a majority of servers.\nAfter an entry is committed, it applies the entry to its own state machine and returns the result of that execution to the client. When a follower learns a log entry is committed, it too applies the entry to its own state machine.\nThe Log Matching Property is maintained by Raft which guarantees\n\nLog entries with same index and term number store the same command\nLog entries with same index and term number mean that all preceding entries must identical (all log entries prior to that index are correctly replicated)\n\nWhen a leader comes to power, it just begins normal operation, and the logs automatically converge in response to failures of the AppendEntries consistency check.\nTo bring a follower’s log into consistency with its own, the leader must find the latest log entry where the two logs agree. To do this, the leader keeps a value nextIndex for each follower which is the number of the next log entry the leader will send to that follower. The leader pings each follower with a AppendEntries RPC call with that nextIndex value. If this call is successful, the leader knows that this follower is up to date. If it fails, then the leader decrements nextIndex again until it reaches a log entry that does succeed. At this point, the follower’s logs will be removed (as anything between nextIndex and what the follower currently has is conflicting) and the follower’s log is now consistent with the leader’s and will remain that way for the rest of the term.\nUnbounded Logs (Log Compaction)\nIn a practical system, a log cannot grow without bounds. The simplest solution is to use snapshotting where the entire current system state is written to a snapshot on stable storage, then the entire log up until that point is discarded.\nAll snapshots are taken independently by each server. Each snapshot contains data like last included index, last included term, and the state machine state.\nSometimes, snapshots need to be sent from leader to followers if the followers lag behind using the InstallSnapshot RPC. This can happen when the leader has discarded the next log entry that needs to be send to a follower (e.g. new server joining cluster).\nWhen a server receives a InstallSnapshot RPC call, it usually discard its entire log. In the odd case where the server receiving the RPC call has more entries in its log than the snapshot, it deletes all log entries covered by the snapshot but entries following the snapshot are still valid and must be kept.\nOther options like log cleaning and log-structured merge trees are also possible.\nRPCs\nAll Raft RPCs are idempotent so sending multiple RPCs causes no harms (e.g. telling a follower to AppendEntries it already has does nothing).\nLiveness Guarantees\nAdditionally, note that Raft (in its current specification) is not resilient to omission faults. This can be resolved with two additional RPCs however:\n\nPreVote: requires potential candidates to run a trial election to test if they can win an election before incrementing their term and running a normal election using RequestVote\nQuorumCheck: requiring leaders to actively step down if they do not receive AppendEntries responses from a majority of servers\n\nImplementation\nYou can find a reference implementation on GitHub\n\nMiniraft Github Repository\nMiniraft Crate Documentation\n"},"thoughts/Random-Forest":{"title":"Random Forest","links":["thoughts/Ensemble-method","thoughts/decision-tree"],"tags":["seed","CPSC340"],"content":"Example of an Ensemble method. They are non-parametric\nThey work by taking a vote from a set of deep decision trees. Two key ingredients to help ensure the deep decision trees make independent errors\n\nBootstrap sampling: generate different “versions” of your dataset\n\nUsually done by sampling with replacement n times, this creates a bootstrap sample\nOn average, this maintains roughly the same distribution as the original\n\n\nRandom Trees: grow decision trees that incorporates some randomness\n\nRandomly sample a small number of possible features (typically d​)\nOnly consider these random features when searching for the optimal rule so splits will tend to use different features in different trees\n\n\n"},"thoughts/Rawl's-Theory-of-Justice":{"title":"Rawl's Theory of Justice","links":["thoughts/fairness","thoughts/rights","thoughts/social-contracts","thoughts/Social-Contract-Theory"],"tags":["seed"],"content":"To be well ordered, a society must establish the rights and duties of its members and also determine a just way of distributing “the benefits and burdens of social cooperation”\nThe Veil of Ignorance\nRawls proposes a thought experiment: the principles are determined from an original position in which each person is hidden behind a veil of ignorance. People must agree to the principles before they know what place they will hold in society; they are ignorant of their sex, race, ethnicity, wealth, intellectual capacity, physical abilities or disabilities, and so on. Thus, Rawls claims that agreements reached from this initial condition would be fair because they could turn out to be in a disadvantaged position in society relative to others\nRawl proposes that rational people behind the veil of ignorance would decide on two principles of justice (Rawl’s Difference Principle)\n\nEach person has a fully adequate number of basic rights as long as these are consistent with everyone else having these same rights\nIf social and economic inequalities exist, it is for one of two reasons:\n\nAssociated with societal positions anyone has a fair opportunity to assume\nThey benefit the least-advantaged members of society the most\n\n\n\nSee also: social contracts, Social Contract Theory"},"thoughts/Reductionism":{"title":"Reductionism","links":["thoughts/emergent-behaviour","thoughts/downward-causation","thoughts/qualia"],"tags":["seed"],"content":"Reduction is a relationship between theories.\nMicroreduction\nMicroreduction is a specific subset of reduction that focuses on explaining the macro using the micro (emergent behaviour)\nThis is the opposite of downward causation\nMind-brain Reductionism\nA successful neuroscience would show how to reduce statements about mental properties (qualia) to statements about neural properties (actual neural activity).\nFunctionalism\nAn alternate to this is functionalism, wherein a successful neuroscience would show how to reduce statements about mental properties to statements about functional properties (an abstraction to apply this to other potentially non-human conscious minds — e.g. AI)"},"thoughts/Religious-Homeostasis":{"title":"Religious Homeostasis","links":[],"tags":["seed"],"content":"\n“If we do not go to church so much as did our fathers, we go to the woods much more, and are much more inclined to make a temple of them than they were” (John Burroughs in Dark Green Religion)\n\nThere appears to be a homeostasis in the human psyche for religion. As the western world sees the impacts of religion wane, there is a gravitation towards treating other things with an almost religious fervor (e.g. climate, the internet, etc.)"},"thoughts/Rhizome-Philosophy":{"title":"Rhizome Philosophy","links":["thoughts/Rhizome-Proposal","thoughts/digital-commons","thoughts/friction","thoughts/pace-layers","thoughts/cozy-software","thoughts/tragedy-of-the-commons","thoughts/evaporative-cooling","thoughts/NAT"],"tags":["seed","rhizome"],"content":"See the main Rhizome Proposal\nInteroperable\nData on the web today is mostly treated as second-class to applications. In this form, data is contextual — it only makes sense in the context of what you can do in an application. At its best, developers and companies expose this data in the form of an API, not allowing you to actually access the underlying data but rather only giving you a limited range of actions or verbs that you can perform on it.\nThis is known as verb-based interoperability and it creates a form of n-to-n problem where every app needs to know what the APIs of another app are to even begin to interoperate. Data exported in this form rarely makes sense on its own outside of the application and often requires a lot of glue code in order to make the data the right format so that you can pass it off to the next application. In fact, companies are incentivized to not provide an easily understandable format that users can easily export. Why would they make it easier for people to leave their product?\nUsers shouldn’t have to keep using platforms they don’t like just because it’s impossible to move all the data they have to another. Instead, users should want to use platforms because it fits the needs they have.\nThis might be possible if we treat data like a noun. How do we create sources of truth that are legible outside of the application, possibly in ways that the application developer never anticipated? Data-based interoperability can use the shared data directly. Once an app knows how to read a data format, it can read that data regardless of which app produced it.\nThis enables users to escape the iron-clad grasp of corporations which hold your data hostage in exchange for usage of their application. Users shouldn’t have to sign away all rights to their data when they agree to use app — they should control what applications have access to their data and how.\nModular\nWhen users choose to use an application, they generally have no choice but to use the whole stack that application uses.\nIn choosing an application, you are ceding agency to the application to decide how it stores the data, how it transforms that data, and how it displays that data. Don’t like the way an application looks and the app doesn’t give you an option to customize it? Better find a new app or deal with it.\nCan we give agency back to users by create modular layers so users can pick and choose what layers they want? Can we make it so these layers are available to others to reuse and adapt to their own needs?\nLocal-first\nThe cloud gives us collaboration, but old-fashioned apps give us ownership. Can’t we have the best of both worlds?\nWe should be able to use our applications without needing to be connected to an internet to load a 4MB web page. Many modern ‘desktop’ applications are simply Electron wrappers around a web application, providing no such thing as ‘offline-support’.\nYour apps shouldn’t break as soon as you reach an area with spotty internet connection or when some company’s server goes down. When you work with local-first software, your work should continue to be accessible indefinitely, even after the company that produces the software has died. You shouldn’t need to worry about companies selling your data to advertisers or using it to train large language models without your permission because you control exactly who has access and permission to your data.\nThe internet should be used to synchronize, update, and collaborate, not be a requirement for basic function. Your data should stay on your device unless you say otherwise.\nCollaborative\nOf course, local-first doesn’t mean local-only.\nIt was a dream of the early internet to be able to make spaces anyone can inhabit and enjoy, little pockets of digital commons that are tended to by many.\nApps should have the freedom to create new mediums of being digitally present with others. Apps should easily be able to have a digital indication that a space is lived in and occupied.\nAs of now, most platforms keep a primative chat log or history but thats it. What if there was a way to create digital gardens to foster and maintain existing relationships? A commonspace you could both take care of, share, and contribute to. Completely private common spaces often allow users to put whatever and allow people can construct their own digital nooks and cozy spaces. Places where people can consume the firehose of information through a feed or slowly and lazily through dialogue (see: friction, pace layers).\nA collaborative application doesn’t necessarily need completely open to the public either. It could just be among your family or close friends. It could just be permeable enough to be discovered by those curious enough to add their own drawings and words and details just hidden enough to be carefully unearthed by the intentional visitor. An app can be a home-cooked meal.\nSimple Developer Experience\nLast but not least, the experience should feel pleasant to both build and use.\nThere are clearly social problems when it comes to making collaborative apps (e.g. tragedy of the commons, evaporative cooling), but even enabling this collaboration on a technical level to happen in the first place is already really difficult.\nThe reason why client-server is so much easier to develop is because of ACID properties. Everything is atomic, things are easy to reason about. P2P and distributed technologies generally are much more difficult to reason about with things like eventual consistency making it hard to understand what is actually happening in the code. Networking quirks like NAT traversal make direct connections\nPeople tend to lean towards client-server models because nobody has made it easy to make P2P software. Almost everything today requires users to still setup their own signalling servers, STUN servers, ICE servers, and a bunch of other infrastructure that makes client-server applications seem like child play.\nBut what if developers didn’t have to worry about server hosting or complexities of eventual consistency? Can we make the tech really easy to build so even hobbyists can quickly spin up a chat app for their own friends that suits their own needs?\nThe libraries that enable this form of P2P software should be able to plug and play into existing applications. The experience should be simple enough that your average CS student should be able to write a simple P2P chat app just as easily (if not easier) as if they wrote it as a client-server application."},"thoughts/Rhizome-Proposal":{"title":"Rhizome Proposal","links":["thoughts/network-effect","thoughts/distributed-web","posts/towards-data-neutrality","thoughts/DID","thoughts/UCAN","posts/bft-json-crdt","thoughts/RDF","thoughts/Filecoin","thoughts/collaborative-software","thoughts/Self-sovereign-Identity-(SSI)","thoughts/web3","thoughts/Hypercore","thoughts/DHT","thoughts/Raft-Consensus-Algorithm","posts/digital-identity","posts/agentic-computing","thoughts/Rhizome-Research-Log"],"tags":["evergreen","rhizome"],"content":"\nCompanies of the future should derive value from the intelligence they provide on top of existing data rather than have the value be just the data.\n\nHow can we imagine more rhizomatic structures instead of arborescent systems?\nDISCLAIMER: To borrow words from Robin Sloan: While it is okay to share this link, I want to underscore that I am sending it specifically to you with the hope that you will really think about it! At such a primordial stage, a proposal like this doesn’t need diffuse, drive-by attention. It needs, instead, close consideration and generous imagination.\nThe competitive advantage of the vast majority of today’s centralized platforms are in their data moats and network effects. The major reason why these platforms remain so dominant is because of their data and users, not because of how good the service quality is.\nAs a result, apps have become inseparable from data. In an ideal world, there is data-neutrality. Much like how net neutrality strives to maintain separation of provider and content markets, data neutrality strives to maintain the separation of data and application markets.\nIn an ideal world, we focus on local-first software that works independently of large platforms – at the end of the day platforms should be used to support efficiency of collaboration at scale, not to gate users from moving their data for the sake of retention.\nI’ve spent a lot of time looking at the retrospectives of peer-to-peer protocols and distributed applications and there are 3 common themes I’ve found in all of them:\n\nRunning your own infrastructure is hard. We need to think about the average non-technical user.\nData availability and durability is largely unsolved. In most p2p systems, offline collaboration isn’t possible.\nLack of thought behind off-ramping off of existing systems. We have shiny new systems, how do we get people to switch to it?\n\nWhile blockchain can be used in creative ways to overcome most of these, it currently comes with a large set of downsides that make it hard to build on top of it (e.g. expensive to store things completely on chain, slow confirmation times).\nWhile I hope these will be mitigated in the future, I wanted to spend time exploring alternative and potentially more general-purpose means of addressing these main problems without blockchains.\nMy main research question is about how we can enable data-neutrality on a web dominated by data moats. A few consequences of this work:\n\nSingle purpose apps backed by general purpose data. If two apps are views on the same data, any change to the underlying data will instantly update both apps\nApplications ask for access rather than store their own data. You give apps permission to read or write specific parts of your data\nAs there are separate markets for data and applications, it creates competition based on service quality rather than on data ownership\nWe can get the convenience of a single centralized platform without the lack of agency that typically comes with it.\n\nI’m tentatively calling this project Rhizome. It aims to be a data-persistence and identity layer for the distributed web.\n\nA personal data pod that you own. Think iCloud or Dropbox but you have agency over how much storage you want, who has access to it, and what you want to do with it.\nA framework for easily developing cohesive peer-to-peer applications on top of data from the prev layer\n\nAs a whole, it forms the basis for a new model of the internet where first and foremost, people own their own data.\nThis is a summarized version of the full vision of Rhizome. Read the full essay on data neutrality.\nTechnical Details\nRhizome is a set of abstractions on top of DIDs, UCANs, BFT CRDTs, and a tuple store. It is a local-first data replication and synchronization service much like iCloud/Dropbox.\n\nAll application data is stored in the form of an EAV Tuple store. This is fully replicated between devices.\n\nData availability is achieved with an always-available cloud peer, a companion add-on to the sometimes-available personal devices we have. A cloud peer is not a hosting provider, it is rather a different type of a personal device. It does not have a screen, but it is capable in a different way, it complements our personal devices with its high availability, storage, and compute.\nA public marketplace where people can buy and sell compute/storage. Reliability of service is ensured using a modified version of FileCoin’s Proof of Replicationand providers can advertise storage/compute specification so purchasers can choose whether to optimize for space or performance.\n\n\nUsers can collaborate with others by creating a UCAN which gives another user permission to read/write a portion of their data\n\nFor example, if I were to share a photo album with a friend, I might create a UCAN that allows read/write access to any tuple that matches the following: is-photo: true &amp;&amp; photo-album: garibaldi-camping-trip\nThis allows live collaboration both in asynchronous and synchronous modes (more notes on this in collaborative software)\n\n\nAll of this will be exposed in the form an SDK in general programming languages like JavaScript, Python, and Rust so that developers can easily build collaborative and interoperable apps without needing to relearn everything from scratch\n\nRhizome’s properties handily solve or avert the three problems listed above:\n\nData replication is considered solved as devices under a single DID sync with each other. Data availability is solved with a cloud peer which can be bought from a distributed and decentralized network of providers.\nUsers no longer need to run their own server infrastructure as compute happens natively on a users device rather than on some remote sever. When a user needs more compute, they can utilize a cloud peer which is like renting compute from a neutral provider.\nAs all apps have a public schema which describe what types of tuple attributes they use (e.g. friend and chat-message attributes for a social media application). To interoperate with outside apps, anyone can publish a schema file for the output of a data export of API call for example.\n\n\nRough architecture diagram as of Dec 1st\nDifferentiation from existing work\n\nUrbit\n\nClaims to be an overlay OS and networking layer\nA bad case of NIH, pretty much reinvented everything from scratch in a language that nobody really understands. Very vaporwavy, not much of their tech lives up to their promises. Good summary here but TLDR; good in principle, didn’t work out in practice.\nNo real applications built on top of it.\nModular collaborative yes, no on everything else.\n\n\nCeramic\n\nProvides a universal document graph (Ceramic Documents) which by default are interoperable, scalable, and permissionless.\nSeems to require a blockchain to anchor storage and provide strict ordering which in turn makes real-time data read/write infeasible (e.g. games, chat).\nUse of DIDs is incredibly smart, potentially enabling self-sovereign identity down the line.\nDoesn’t seem to support multi-writer documents right out of the box, seems to be an ongoing area of work/research.\nGreat principles and solid work already. Seems to have gained some adoption from people in web3 already.\n\n\nDat Protocol:\n\nExtremely values aligned! Streaming based append-only log that aims to be the lego-block of distributed applications.\nGreat developer experience.\nUse of DHT means that it doesn’t need a signalling server for peer discovery.\nNot amazing availability, no incentive system for people to run nodes (though Dat is working on this using a blockchain-based reward system).\nNot exactly great local first support. Continues working locally without an internet connection but new users cannot connect or get an up-to-date version of your data. If the user wants to send data to someone else, both devices need to be online simultaneously.\nHypercore also does not guarantee long-term write-once storage.\nMulti-writer support is still being worked on.\n\n\n\nOutput\nResearch artifacts\nBlog posts explaining distributed systems concepts as I learn and become more familiar with them\n\nExplainer on the Raft Consensus Algorithm\nExplainer on BFT CRDTs\nModelling distributed systems\n\n&lt;1kloc Raft Implementation\n\n\nFrom legibility to identity\nLAN the Internet Again\n…more to come\n\n\nYou can find the ongoing research log here.\nAcknowledgements\nThank you to Anson Yu, Spencer Chang, Sebastien Zany, Jamie Wang, Raymond Zhong, Vincent Huang, Justin Glibert, Morgan Gallant, Ryan Johnson, David Zhou, Aadil Ali, JZ, Nishant Medicharla, Anh Pham, Farzaa Majeed, Amir Bolous, Aaron Pham, Rishi Kothari, Jasmine Sun, and Athena Leong for your continued support. This project wouldn’t be possible without all of you."},"thoughts/Rhizome-Research-Log":{"title":"Rhizome Research Log","links":["thoughts/Operational-Transform","thoughts/bitemporal","thoughts/incremental-view-maintenance","thoughts/Three-Legged-Stool","thoughts/file-system","thoughts/CRDT","thoughts/privacy","thoughts/composable","thoughts/programming-models","thoughts/time","thoughts/Datalog","thoughts/local-first-software","posts/agentic-computing","thoughts/DHT","thoughts/Overlay-Network","thoughts/clocks","thoughts/Antimatter","thoughts/web3","thoughts/distributed-systems","thoughts/NAT","thoughts/LLMs","thoughts/information-scales","thoughts/access-control","thoughts/Upwelling","thoughts/RDF","thoughts/Prolly-Trees","thoughts/system-model","thoughts/Raft-Consensus-Algorithm","thoughts/Arrow's-Impossibility-Theorem","thoughts/Network-Theory","posts/bft-json-crdt","thoughts/Rhizome-Proposal","thoughts/UCAN","thoughts/CALM-Theorem","thoughts/Asymmetric-Key-Cryptography","thoughts/Kademlia-DHT","thoughts/Public-key-Infrastructure","thoughts/Seeing-like-a-State","posts/digital-identity","posts/context-collapse","thoughts/Byzantine-Faults","thoughts/CID","thoughts/CAP-Theorem","thoughts/A-Certain-Tendency-Of-The-Database-Community","thoughts/IPFS","thoughts/WebAssembly","posts/the-fools-who-dream","thoughts/optionality","thoughts/exploit-explore","posts/play","thoughts/State-Machine-Replication-(SMR)","thoughts/emergent-behaviour","thoughts/computer-networking","thoughts/Merkle-DAG","thoughts/content-addressed-storage","thoughts/CRON-Theorem","thoughts/Braid-HTTP","thoughts/Yjs","thoughts/Secure-Scuttlebutt","thoughts/OrbitDB","thoughts/HotStuff","thoughts/HoneyBadgerBFT","thoughts/LR-Permissionless-Result","thoughts/Weaving-the-Web","thoughts/Casper-FFG","thoughts/SBFT","thoughts/PBFT","thoughts/cryptography","thoughts/hypertext","thoughts/liveness","thoughts/digital-commons","thoughts/Holochain","thoughts/cascading-failures","posts/2021","thoughts/inevitability-of-centralization","thoughts/credible-exit","thoughts/Verifiable-Credential","thoughts/identity","thoughts/Self-sovereign-Identity-(SSI)","thoughts/digital-permanence","thoughts/causal-tree","thoughts/CRDT-Implementations","thoughts/encryption","thoughts/GDPR","thoughts/consensus","thoughts/causality","thoughts/Order-theory","thoughts/neutrality","thoughts/Tendermint","thoughts/Byzantine-Broadcast","thoughts/blockchain","thoughts/CouchDB","thoughts/Hyper-Hyper-Space","thoughts/Solid","thoughts/decentralized-marketplace","thoughts/DID","thoughts/Sidetree","thoughts/WebID","thoughts/FOAF","thoughts/LDP","thoughts/decentralization","thoughts/authorization","thoughts/federation","thoughts/petname","thoughts/independent-research","thoughts/idea-list","thoughts/Solana","thoughts/internet-computing","thoughts/mechanism-design"],"tags":["evergreen","rhizome"],"content":"I think research logs tend to generally focus too much on what one did rather than what one felt. This log aspires to have a healthy mix of both.\nApril\nApril 29th\n\nSpent some time doing the Jepsen Maelstrom distributed systems challenges in Rust! Feel like I am slowly getting better at going from idea to working code in the language\nGoing through the Causal Islands talks\n\nApril 11th\n\nVarious notes on Operational Transform, bitemporal, incremental view maintenance, Three Legged Stool\nFile system CRDTs\n\nMarch\nMarch 19th\n\nMore notes on privacy and what a pluralistic interpretation of ‘public’ means\nNoticing some common themes between a lot of papers I’m reading lately\n\nAntimatter/inverse operations for undo and/or time-travel mechanisms\nSnapshotting to bound the time complexity of rewind\n\n\n\nMarch 18th\n\nElm time-travel debugging\n\nGives me very similar vibes to how Braid does their rewind mechanism\nElm programs may have state, even though all functions are pure. The runtime stores this state, not your program.\nThe Elm runtime combines the previous state and new inputs to make the current state.\nTo avoid replaying the universe from the start, Elm uses snapshotting\n\n\nLive, Rich, and Composable: Qualities for Programming Beyond Static Text\n\nWe hypothesize that by combining liveness, richness, and composability, programming systems can meaningfully extend the capabilities of static text without losing its characteristic expressivity.\nLiveness: providing programmers with in-depth feedback about a program’s dynamic behaviour as the program is edited.\n\nNormally accomplished through some sort of live/hot reload that preserves the state of the system, however this only reflects the final output of the program without revealing any information on the internal model of the program that led to that output\n\n\nRichness: allowing programmers to work with domain-specific visualizations and interactions\n\n[W]riting code means articulating thoughts as precisely as possible… Often these thoughts involve geometrical relationships: tables, nests of objects, graphs, etc. Furthermore, the geometry differs from problem domain to problem domain. To this day, though, programmers articulate their thoughts as linear text.\n\n\nComposability: the ability to freely combine smaller programmed artifacts into larger ones, to accomplish larger goals\n\nUnlike liveness and richness, this is not a quality static text lacks, which interactive programming systems strive to add to it. Rather, it is a familiar quality of static text which new programming systems must work hard to maintain.\n\n\n\n\nMore notes on programming models\nCRDTs and Relational Databases (RDBs)\n\nDefines multisynchronous access which is composed of two modes of accessing data on edge devices:\n\nasynchronous mode—the user can always access (read and write) the data on the device, even when the device is off-line, and\nsynchronous mode—the data on the device is kept synchronous with the data stored in the cloud, as long as the device is online\n\n\nSynchronizes tuples using causal-length set CRDT (CLSet CRDT)\nCreates a two-layer relational database system where the top layer is the Application Relation (AR) Layer and the bottom is the Conflict-free Replication Relation (CRR) Layer.\n\nAny violation of integrity constraint is caught at the AR layer. A local update of r~i​ and refresh of ri​ are wrapped in an atomic transaction: a violation would cause the rollback of the transaction. A merge at r~i​ and a refresh at ri​ is also wrapped in a transaction: a failed merge would cause some compensation updates.\n\n\n\n\n\nMarch 13th\n\nTools for development feel important to focus on for a good DX, LiveBlocks does a great job at this\n\nMarch 12th\n\nOn time\nIncremental Maintenance of Externally Materialized Views\n\nWide-area access to database servers by autonomous clients (which may or may not have local databases) is becoming more and more popular\nDefine monitoring service: not only request the initial answer to a certain query but also notifications about changes in this answer over an extended period of time\nExpressed in Datalog , 3-step consensus procedure that computes view differentials\n\n\n\nMarch 9th\n\nThick vs Thin clients\n\nthin clients are terminals for a centralized computer (cloud-first, internet-first)\nthick clients are computers in their own right, that sync (local-first software)\n\n\nReadings on time\n\nJohn McCarthy, inventor of LISP on logical time\n\n“John started thinking about modal logics, but then realized that simply keeping histories of changes and indexing them with a “pseudo-time” when a “fact” was asserted to hold, could allow functional and logical reasoning and processing. He termed “situations” all the “facts” that held at a particular time — a kind of a “layer” that cuts through the world lines of the histories.”\n\n\nNAMOS by David Patrick Reed: versions have two component names consisting of the name of an object and a pseudo-time, the name of the system state to which the version belongs\n\nSynchronization is then treated as a mechanism for naming version to be read and for defining where in the sequence of versions the version resulting from some update should be placed\n\n\n\n\nHow is schema evolution addressed in tuple stores?\n\nTODO\n\n\n\nFebruary\nFebruary 24th\n\nAn economically sustainable model for local-first software from the lo-fi manifesto\n\nUsers can use the product for free on their own device, and free users don’t cost anything (or next to nothing) to “host.”\nOnly paying subscribers utilize paid server infrastructure. Meaning, every server request and megabyte of database storage has associated revenue.\n\n\nI like having technical principles like “local-first,” but I’m even more excited by what these concepts can enable in terms of building more authentic, sustainable, and personable software products online.\nThe hard problem of schema migration:\n\n“In a traditional cloud-hosted model, you’d do some diagramming, tweak your database schema to support the new system, and then schedule some time to deploy a migration. Users wake up the next day, update the app, and find their data has already been massaged into a new shape, ready for 2.0”\n“We don’t get that in local-first world. There’s no magic spell you can utter overnight to change your app’s data across everyone’s devices. You can update your own server, but the clients who connect will still be running 1.0 when they do.”\n\n\nOasis Builders Founding Essay\n\nThe physical frontiers of our world are largely mapped and tamed. But the digital frontier is endless, and it’s always open to settlers seeking better ways forward. We are all free to venture forth, follow our curiosity and aliveness, direct our attention in new ways, and find the others. We are free to trust our intuition when it says ”things can be so much better than this.”\nLocal spaces of abundance: “Oases give us the breathing room to experience the fullness and richness of our humanity. In an oasis, we have the safety, space, and resources to begin blossoming into more fully realized versions of ourselves.”\n\n\n\nFebruary 15th\n\nGave a talk about communal computing at the DWeb YVR node that went really well\n\nAt first, I was really nervous that people would just dismiss what I had to say because I was so young with statements like “oh he’s so naive” or “what does he know about how the internet works”\nBut really, people seemed to really resonate with the talk\nIf anything, it felt like it renewed their confidence that there was still young people worried about and thinking about problems like these\n\n\n\nFebruary 11th\n\nI really like how both Matrix and Email allow for people to host separately and still interop with each other\nDe-perimeterizing the walled gardens of the web\nLots of reading about networks today…\nI feel like each of the components of this project on their own could be full-fledged companies on their own. I can easily see how I could sink my entire life into this line of work\n\nFebruary 10th\n\nHow do we make a DHT that works in a sparsely connected world?\nThings to tie together (how do these pieces fit together?)\n\nOverlay Network Layer\n\nMake it easy to address each other on the open web by creating a virtual private network\n\n\nIdentity and Permissions Layer\n\nBasically key management software\nOAuth / wallet / did:key\nUser friendly interface on top of persistence and data layer which embeds the persistence and data layer\nManage which applications have access to your data/indices\nLike most other databases, includes role/access information in the underlying persistence layer\n\n\nPersistence and Data Layer\n\nMake it easy to spin up personal databases\nIngestion endpoint? Converting objects to tuples\n\nNamespaced subscription to remote databases\nEfficient set reconciliation\nLogically monotonic\n\n\nMerge semantics and CRDTs\n\nTrivial merge for fact tuples is the set union operator (maybe use Hybrid Logical Clocks here too)\n\n\nBlob storage like git LFS\nUser-managed garbage collection, mark fact as retracted to update dependent indices\nOnce all index nodes have ack’d the deletion, we can actually delete it (similar to Antimatter)\n\n\nIndex Layer\n\nQuery over fact store (data layer) with materialized view maintenance\nProlly Tree data structure\nPeers are incentivized to ‘pin’ and help index data for indices they are interested in\n\n\nApplication\n\nFolk programming\nEnables programming ‘agents’ and crawlers (like geists)\nReads from index layer through subscriptions\nWrite to persistence layer easily\n\n\nHosting\n\nNetwork: hosted Wireguard or Headscale\nData: On providers of the user’s choice (also managed option)\nIdentity and Permissions: self-hosted application run on a user’s devices\n\nEach of these maintains a shallow clone of the indexes it’s interested in\n\n\nApplications: run on user’s devices\nCommunal Clusters: an organizational unit, co-owning data and compute (think : a single Mastodon server)\n\nIndex: On providers of the user’s choice (also managed option)\n\n\n\n\n\n\nI feel like there is a large common ground between git, CRDTs and room for cross-pollination across both\n\nBoth basically focus on version control and collaboration\nHowever, both aren’t perfect\n\nThe railroad metaphor for git is powerful but the affordances of how to manipulate it aren’t made clear to the end-user. CRDTs don’t explicitly expose version control to the end-user\nCRDTs have great conflict-free merge semantics that git (relatively) sucks at\n\n\nBoth are also pretty bad in terms of operation efficiency\n\n\nFound out about Ditto, I guess local-first/offline-first applications make a lot of sense for aviation LOL\n\nFebruary 8th\n\nTypes, not tables: I agree! What if we had a similar format for types as a sequence of map, filter, and reduce statements?\nFact stores with schemas as queries\nProduct : Product Market Fit :: Protocol : Protocol Platform Fit\n\nProducts derive value from benefiting end-users directly\nProtocols derive value by expanding the horizons for what can be build\n\nThat is, the value is not reified into people build on top of it\nThus, part of the work of the protocol is figuring out the right platforms/applications that can be built on top of it and providing the right incentives for those to exist\nI suspect this is partially why so much of web3 seems like vaporware: you necessarily need to promise things to attract people to build\n\nBut anyone who treats a protocol like a product is bound to find it suspect\n\n\n\n\n\n\nHad a chat with Spencer and Raymond about the future of data / folk forums which was really energizing\n\nEverything is pub-sub\n\nWhat does a protocol level inbox/outbox system for the web look like?\n\n\nNow, directly addressing people or applications is super hard because of distributed systems, NAT, and a bunch of other nasty things\nSee also: communal computing\nWhat if we had Docker-namespace- or val.town-style application network addressing?\n\n\nNotes on Braid research\n\nAntimatter\nRhizomeDB (Fission renamed WebnativeDB)\n\n\n\nFebruary 5th\n\nWhy continually doing matters\n\nPeople talk about ‘momentum’ when it comes to projects a lot\nI get told by people looking to do more projects/research that they think about it a lot but rarely spend much time actually doing it\nRobin Sloan captures this well:\n\n“When you start a creative project but don’t finish, the experience drags you down. Worst of all is when you never decisively abandon a project, instead allowing it to fade into forgetfulness. The fades add up; they become a gloomy haze that whispers, you’re not the kind of person who DOES things.”\n“When you start and finish, by contrast — and it can be a project of any scope: a 24-hour comic, a one-page short story, truly anything — it is powerful fuel that goes straight back into the tank.”\n“Unfinished work drags and depresses; finished work redoubles and accelerates.”\n\n\nTruly doing something that is creative and agentic gives you energy. It doesn’t drain it\n\n\nA* path search for DHTs? Locality mapping?\n\nThis video explainer of Meridian is really cool too! Taking account earth curvature when working with distance estimations\n\nEssentially, the idea is for each node-to-node query hand-off to “zoom in” to the solution space, reducing the necessary state requirement on each node to only logarithmic of the system size\n\n\nOptimizing geographical coverage through hypervolumes\n\nSay you have a set of K servers in your cluster\nAnd a set of L servers that are candidates\nIteratively sub out servers in K for servers in L to find the set that maximizes the hypervolume to maximize the geographical coverage\n\nThe ring members are diverse within the enclosed space by looking at the three dimensional case, which selects the three nodes from each ring that form the largest triangle\n\n\n\n\nThis of course assumes laying wires that follow the curvature of the earth. Does this assumption hold for interplanetary systems? Probably not but eh we’ll think about it when we get there\n\n\nRewind in Braid, a talk by Jonathan Blow\n\nImplementation options\n\nDeterministic Simulation; record and play back user input. However, things break and it’s not robust across revisions\nReversible sim. tick() and also reversetick(). However, this makes gameplay code really complex\nRecord full world state; drop frames and interpolate. However, this isn’t exact and can lead to exploits.\n\n\n40MB rewind data gives 30-60m of recording time\n\n\n\nFebruary 4th\n\nDelightful apps\n\nWhat makes an app feel delightful? Optimistic Updates, Multiplayer, and Offline-Mode\nOptimistic Updates\n\n“Interaction time changes how you use an application. Get fast enough, and your fingertips become your primary constraint. I think this is the key to unlocking flow”\nTo do this well, we need to support undo. We need to maintain order, and we need to be able to cancel dependent mutations.\n\n\nOffline-mode\n\n“When we know that an app will work no matter what, we use it differently.”\n\n\nGood permissions models\n\nUser-defined functions for whether a user can modify a resource\n\n\nWhy not SQL? “The frontend’s common case is SQL’s advanced case. We shouldn’t need advanced features for common cases.”\n\nFacebook runs on a graph database! So does Bytedance (so all of TikTok)!\n\n\n\n\n\nFebruary 3rd\n\nIn response to Spencer’s creative seeing and provocation “what does the internet look like in 5 years?”\n\nOn developing taste\nAgentic software and how current software makes it hard for people to make their own things\n\n\n\n\nJanuary\nJanuary 30th\n\nAlan Kay on “Should web browsers have stuck to being document viewers”\n\nThis led to a “sad realization” that sending a data structure to a server is a terrible idea if the degrees of freedom needed on the sending side are large.\nAnd eventually, this led to a “happy realization”, that sending a program to a server is a very good idea if the degrees of freedom needed on the sending side are large.\n\n\n\nJanuary 29th\n\nI hosted a session in Andy Matuschak’s unconference! It turns out a lot of people are thinking about collaborative software. It was super causal and we just chatted about ideas for 30 minutes but wow I felt so energized afterwards.\n\nFull FigJam file\n\n\nSome ideas I found particularly insightful:\n\n“What is the handwriting for digital spaces? Something that passively conveys ownership for a particular unit of work” (Gus Rasch)\n\n“Someone just surfaced the idea of personal fonts and some other avenues are 1. color, and 2. hand-drawn image” (Spencer)\n“Imbuing digital spaces with personality feels more powerful than you might expect. Makes it easier to remember where things live because they’re “emotionally” different.” (Amelia Wattenberger)\n\n\nEffective diff interfaces: “one of the most important factors here is being able to quickly understand the difference between versions.” (Amelia Wattenberger)\n\nDomain-specific merge flows: “How do you enable people to manage many copies and forks of the same document and recombine? Is this going to be the same for prose and digital gardening as it will be for coding? Could we use LLMs to describe differences semantically?” (Rob Haisfield)\n\n\n\n\n\n\n\nJanuary 28th\n\nStarted thinking about this Git for writing idea which I think will be a good vehicle for thinking about version control systems in general\nPrototype Sketch\n\nQuestion: what feels good about a collaborative medium?\nObjects that have affordances we can interact with and manipulate in intuitive ways\n\nDocuments are made up of paragraphs which are made up of characters\n\nBlock-based editing allows us to manipulate at the paragraph level in addition to the character level editing we get in normal text editors\nNetworked note taking apps allow us to transclude text which is a very primitive form of document level editing\n\nBut what about manipulating documents through time?\nTime as an object we can interact with and control? Similar to what Bret Victor says in Up and Down the Ladder of Abstraction: A designer needs direct, interactive control over the independent variables of the system.\nSee notes below on version control\n\n\n\n\n\n\nSupports a spectrum of collaboration\n\nWhen we’re collaborating with others, there’s a natural human tendency to desire some privacy while working through something, the freedom to take a piece of the creative work and play out different ideas, move things around, edit and refactor, without fear of judgement or the burden of having to explain or communicate our thinking or concern for overhauling sections where another is actively reading or working. (Jess Martin on Collaboration)\n\nAlso sometimes called the fish-bowl effect. Some studies have found that real-time collaboration like in Google Docs creates stress as writers feel watched by their co-authors\n\n\nIt’s not just a binary of either working on your own or working with others\n\nNormally binded together on a single document (fully sync editing)\n\nThis is the typical Google Docs or Figma editing experience\n\n\nObvious button to “make a new version of this document” (async branching)\n\nWhat if we did ‘implicit’ branching where any change makes a new branch?\n\nOpen question: Could we utilize LLMs to bundle these into “commits”?\n\n\nEach copy has an obvious flow of “merge back into original document”\n\n\n\n\nPresence (peripheral awareness of others)\n\nImportant so people don’t step on each others toes when working across versions\nUseful information at every scale\n\nCoin this the Engelbart Zoom which Engelbart explored in HyperScope\nTKTK: what type of peripheral information is actually useful?\n\ne.g. see what paragraphs other people are editing with a scroll bar minimap?\n\n\n\n\n\n\n\n\nVersion control\n\nNot only across time (edits to the same document) but also managing parallel versions by different authors\nVersion control implies agency over how merge and conflict resolution processes occur\n\n‘Best-effort’ merge using traditional merge techniques (borrowing from CRDTs or git merge strategies)\n\n\nWhen working with physical objects, holding areas are very normal\n\ngit stash is a really handy tool, but using it through a CLI makes it confusing\nImagine the railroad track metaphor that git uses but each individual commit is an object you can manipulate (iirc GitKraken does this in their GUI but haven’t had the chance to play with it yet)\n\nDrag the tips of branches together to merge\nSelect multiple commits and drag them off of a branch\n\nThis ‘detaches’ them into free space\nYou can then ‘re-attach’ these commits anywhere\n\nThis is effectively what git stash does\n\n\n\n\n\n\n\n\n\n\nFlexible Access Control\n\nIt feels important to know who has access to your work (and what kind of access?)\n\nUsers should feel empowered to have agency over whether their work is public- or private-facing\nThese should also be flexible and dynamic\nOpen questions: what are good ways to deal with derivative works where the permissions over the source material changes\n\ne.g. Say I make a private document public by accident\nIf someone makes a copy of the public document before I make it private it again, what should happen? Should the user no longer be able to access the document because the source document is no longer public? Should the user still be able to access the document because the fact that it was public at the time it was copied mean they should still have access to it?\nIs this something that should even be reinforced at a technical level? Or social level (e.g. bump the responsibility up to users)?\n\n\n\n\n\n\nModular and customizable workspaces\n\nIt is important for people to customize the spaces they work in\n\nWhy force everyone to use the same interface and tools?\nWe all have differing needs for knowledge and collaborative workspaces\n\n\nEnd-user scripting like Smalltalk for HyperCard\nSubconscious calls this concept ‘geist’: programs that may work in concert with users\n\nLittle agents that crawl over your notes and help suggest things\nEach one specialized for its own thing, e.g. a geist that fixes grammar, a geist to suggest common themes between notes that you may have missed\n\n\n\n\n\n\n\nJanuary 15th\n\nDeclarative Programming over Eventually Consistent Data Stores\n\n“geo-distribution does not come for free; application developers have to contend with weak consistency behaviors, and the lack of abstractions to composably construct high-level replicated data types, necessitating the need for complex application logic and invariably exposing inconsistencies to the user”\n\n\n\nJanuary 14th\n\nLadder of Abstraction\n\nA designer needs direct, interactive control over the independent variables of the system. We must not be slaves to real time.\n\n\nQuery Guarantees in Keep CALM and CRDT On\n\n“The soundness of state convergence does not translate to predictable guarantees for computations that examine them. One might say that CRDTs provide Schrodinger consistency guarantees: they are guaranteed to be consistent only if they are not viewed”\n“Can we develop a query model that makes it possible to precisely define when execution on a single replica yields consistent results?”\nQuerying over something monotonic would be nice but computer time is non-monotonic? What about entropy?\n“The space of monotone queries is quite large; for example, four of the five operators of relational algebra are monotone: selection, projection, union, and intersection. Only set difference is non-monotone.”\n\n“A pipeline composing monotone functions will always give a monotone function end-to-end, but if the pipeline contains any non-monotone function then the end-to-end-computation will be non-monotone”\n^ this is quite similar to earlier observations about RedBlue consistency\n\n\n“With apologies, potentially-inconsistent observations are accompanied by compensating actions, which are intended to clean up any negative effects of weak consistency. By leveraging lineage tracing, a CRDT-enabled database could automatically determine when such apologies are necessary, prompting the application accordingly”\n\nHow does this compare with netcode rollback techniques?\n\n\n\n\nUpwelling pre-print\nPresence affordance that is not intrusive? (“I’m working on the introduction today, please don’t touch that section”)\nMaking cherry picking easy\nI want to prototype a collaborative writing tool as a testing ground for the version control stuff\n\nAlso had the change to give feedback to a few friends on writing and one big pain point is just finding the best way to comment/give feedback. We always just paste in Google Docs and pepper the document with comments and suggestions but there needs to be a better way than this\n\n\n\nJanuary 13th\n\nHad a lovely chat with Quinn from Fission and wow that was amazing\n\nFor more context, Quinn has been working on Dialog which has recently been renamed to Rhizome (!!!)\nFeel like its a case of convergent evolution that we are separately coming to roughly the same conclusions about what a ‘post-modern’ database should look like\nWe talked about pvh’s thoughts on RDFs and realized that actually, we don’t necessarily need to expose this complexity to the user! We can have Datalog as a compilation target\n\n\nAlso caught up with Kleppmann for the first time in a while. He seemed really excited by the interface stuff I’m thinking about for CRDTs and version control in a collaborative setting!\n\nJanuary 12th\n\nMore on Datomic!\n\nWhy immutability actually makes sense when representing real-world things:\n“Facts don’t go away. If the princess’s tastes change so that she prefers sriracha, it’s still useful to know that in the past she preferred mustard. More importantly, new facts don’t obliterate old facts.”\nThat’s because time only works in one direction in the universe (that we know of): forward\n\nSo by encoding causal dependencies, we get this for free\n\n\n\n\nMerging reallly old changes (localfirst/auth discussion)\n\nBig usability problem for distributed apps. If a long-dormant device can come online and introduce a single operation that overturns months’ worth of activity, people will perceive the app as unstable — even if there’s no malice and no security issues involved.\nCan we set limit L on how far out of date a device can be before we require it to catch up before submitting changes? The idea would be that you couldn’t base a change on a head that’s older than that. Instead you’d have to catch up with the latest information, and then rebase your change onto the current head.\n\nL is a wall-clock timestamp\nL is a logical timestamp\n\n\n\n\n\nJanuary 11th\n\nDeconstructing the Database, talk by Rich Hickey, author of Clojure, and designer of Datomic\n\n“I think one of the questions we have in revisiting the architecture of a database is, what’s possible? How much of the value propositions of databases can we retain while tapping into some of the new value propositions of distributed systems, in particular, their arbitrary scalability and elasticity?”\n“Other problems we have in general when we talk about traditional databases are flexibility problems. Everyone knows the rigidity of relational databases and the big rectangles. We also have the artifice of having to form intersection record tables and things like that”\nI love (and strongly agree with) Datomics approach to thinking about databases\n\n\nHad a brief chat with pvh about this same topic and he interestingly disagreed. Speaking from empirical evidence, RDF and tuples have never really worked. It’s hard for people to wrap their heads around\nDisk-locality considered irrelevant\n\nReading from local disk is only about 8% faster than reading from the disk of another node in the same rack\n\n\n\nJanuary 3rd\n\nRollback-based mode more thoughts\n\nAttaching an epoch to each non-commutative operation (this is effectively making the implicit causal dependency explicit)\n\nSeparates non-commutative and commutative sequence numbers\n\n\nOrdered so epoch numbers are treated as earlier (opposite to sequence numbers)\n\nThen, if we add rules to deal with conflicting actions add a query/view level\ne.g. how to deal with an insert op by author A if author A’s access to the document is revoked\n\n\n\n\nI think the research direction I want to explore further is expressing CRDTs as queries over an ever-growing fact-base (Represented as a )\n\nCommutativity is trivial using the set union operator\nFact-base is a 6-tuple\n\nEntity ID (E)\nAttribute (A)\n\nCardinality of one or many\n\n\nValue (V)\nOpID (Id)\nCausalOrigin (Origin)\nRetracted (Del)\n\nThis is equivalent to a delete operation\n\n\n\n\nIncremental View Maintenance\n\nDRed\n\n\nBuilding indexes using Prolly Trees for optimized lookups\nQuestions\n\nHow might capabilities be modelled? And private data?\nAutocodec for translating attributes between applications?\n\n\n\n\nAnother possible route… exploring UI/UX of CRDTs as time travel\n\nMore suited as a short, term-long project (and potentially a project I can do with Ink &amp; Switch)\nVisual drag-and-drop interface\n\n\n\nJanuary 2nd\n\nAn idea: Hashgraphs + safety-certificates for non-commutative operations\nEager mode (similar to Delta-CRDTs)\n\nFlood communication\nAssumes unique ID for each operation\nWe track a list of all peers who we have received messages from (a list of AuthorID for each neighbour)\nEach time we receive an operation with ID OpId we haven’t seen before and successfully apply it, we broadcast an ACK(OpId) message to all neighbouring peers\nFor each op OpId, we locally track the set of all peers who have acknowledged OpId\nOnce we have received an acknowledgement from each of our neighbours for a single operation (call this the safety-certificate), it is safe to apply a non-commutative operation on it (we can now delete it or deliver any causal dependents)\n\nThis works for applications with explicit causal dependencies (e.g. text editing)\nHowever, it is a bit more difficult to reason about for implicit causal dependencies (e.g. access control)\nConsider two admins A and B who are accessing the same document.\n\nConcurrently:\n\nop1: Admin A types the letter ‘a’ in the document\nop2: Admin B revokes A’s access to the document\n\n\nThe problem is that the algorithm treats these as operations that commute when the clearly don’t! Some peers may see ‘a’ (if they receive op1 ahead of op2) and others will not (op2 ahead of op1 so op1 becomes invalidated)\n\n\nThis is solved with the epoch-based approach below\n\n\nHowever this requires fixed membership as it seems to completely mishandle cases where members leave the group (can no longer get a whole safety-certificate)\n\nThis is potentially solved by signalling departure and setting an inactivity timeout for nodes\n\nThough using heartbeats to refresh inactivity timeout feels counter to the whole CRDT ethos of offline support\n\n\n\n\n\n\nRollback-based mode\n\nAttaching an epoch to each non-commutative operation (this is effectively making the implicit causal dependency explicit)\nThis implies that each CRDT requires the ability to undo and redo operations between non-commutative operations\n\nNot all computation is reversible though (e.g. entropy increasing operations like blurring), how do we reconcile this?\nCan we utilize the hashgraph to do git-like rebasing to avoid having to implement a redo?\n\n\n\n\n\nJanuary 1st\n\nMore thoughts on access control\nDistributed Access Control for Collaborative Applications using CRDTs uses a total ordering of roles in order to resolve access conflicts\n\nHowever, in the case of two top-level administrators revoking access, the same problem occurs\nAdditionally, it is not not always possible to totally order a set of permissions. Consider one person with access to file 1 but not file 2 and another person with access to file 2 but not file 1.\n“Combining CRDTs for data with CRDTs for policies raises several challenges. Conflicts between two concurrent operations based on diverging policies cannot be safely resolved.”\n\nThey resolve this by attaching an epoch to each policy change\n\nThe epoch doesn’t grow in size, but merely refers to a parent operation that last changed the policy.\n\n\nThis implies that each CRDT requires the ability to undo and redo operations between epochs\nUndo-redo may be expensive if it happens a lot! The assumption here is that policy changes are rare so this doesn’t happen very often\n\n\n\n\nGarbage Collection\n\nTwo part series (pt1 and pt2)\nWhy GC is hard:\n\nFirst, establishing the stability of an update as described in the paper assumes that the set of all replicas is known and that they do not crash permanently.\n\n\nInspiration from Delta-CRDTs\n\nIn the causal-consistency-ensuring anti-entropy algorithm. When a node sends a delta-interval to another, the receiving node replies with an acknowledgment after merging the interval into its local state. A delta that has been acknowledged by all of a node’s neighbours is then garbage-collected\n\n\nSynchronized GC\n\nUnder two-phase commit, each replica will vote on whether each tombstone is still necessary.\n\n\nQCs for GC?\n\n\nType-level consistency guarantees?\n\nSource\nJust as ‘function colouring’ exists as a way of distinguishing async and non-async functions, what if we could colour other sorts of system models?\n\nIPA does this\nRedBlue consistency?\n\n\n\n\nCockroachDB Layers\n\nSQL Layer: translates high-level SQL statements to low-level read and write requests to the underlying key-value store\nTransactional KV: Requests from the SQL layer are passed to the Transactional KV layer that ensures atomicity of changes spanning multiple KV pairs\nDistribution: monolithic key space. Range-partitioning on the keys to divide the data into contiguous ordered chunks of size ~64 MiB, that are stored across the cluster. We call these chunks Ranges\nReplication: consensus replication using Raft across replicas\nStorage: KV-store, use RocksDB\n\n\n\nDecember\nDecember 30th\n\nReading about Datalog as a way of expressing CRDTs… some promising work in this direction\n\nI think there’s a pretty clear articulation of this over SQL as this allows us to separate data representation from data views in a more clear way\nThis also means we can do away (?) with SQL migrations\n\n\n\nDecember 28th\n\nFinally finished up my blog post on Communal Computing! It turns out, sharp feedback leads to better writing, who would have ever thought :‘)\nDid a bit more thinking about Kleppmann’s Recovering from key compromise in decentralised access control systems\n\nI feel like there is a close connection to be made with Arrow’s Impossibility Theorem but haven’t been able to formally show it\nPerhaps using Quorum Certificates for group membership voting?\n\nThese can be built offline (and even allows for receiving updates offline) and sent when a user is back online.\nA membership change is then considered stable when it receives 2 unique supporting QCs. Of course, this only works with f&lt;3n​.\nVoting members: all members of the group which were members prior to the membership change\nMutual removal: going with something that is intent-preserving feels important here. One potential way to resolve this is to restrict membership changes to at most one removal per round. Requiring 2 QCs would mean that we have consensus on which group member to remove and removing a single member cannot possibly cause a conflict. (this may not be ideal for situations where a large number of group members are removed but I suspect these cases are very rare)\n\n\n\n\nFound out that Beaker Browser is now archived :(( A really interesting retro that has a lot of good reflections and learnings for anyone working on p2p tech\n\nMajor challenges:\n\nWithout some logically centralized repository of data or router of messages, you struggle with discovery and delivery.\nUsers don’t stay consistently online and connections will randomly fail, so you stuggle with availability and performance.\nInitial connections and thus time-to-first-paints are slow, which is very bad news for web browsing.\nDebugging is quite hard.\nManaging resource usage on the device is hard.\nScaling a user’s view of the network past (say) 100k users is pretty much out the window because you’re not sharing indexes; rather, you’re having each device build the indexes locally.\n\nIs this fixable with Prolly Trees?\n\n\n\n\n“As decentralizers we may be pursuing a mission, but our work only wins in the market, and to win in the market we need to think like entrepreneurs. Ultimately, my lesson learned is that mission needs PMF.”\n\n\nCoordination in CRDTs? Inspiration from the TreeDoc paper\n\nTreeDoc is occasionally flattened from a tree into an array to cleanup tombstones and balancing issues\nHowever, this is not commutative. TreeDoc solves this by using an update-wins approach in a two-phase commit protocol\n\nThe site that initiates a flatten acts as the coordinator and collects the votes of all other sites. Any site that detects a concurrent update to the flatten votes “no”. The coordinator aborts the flatten if any site voted “no” or it never received a response\n\n\nSupporting large groups of replicas\n\nUses hubs to help scale — see Network Theory\nCore: well-known nodes that are well-connected\nNebula: all other nodes\n\n\nEpoch-based flattens\n\nEach flatten – each change of epoch – changes the frame of reference for TID\nA core site maintains a buffer of updates messages it needs to send to the nebula, some in old epoch some in the new one\nA nebula site maintains a buffer of update messages to be sent to the core; these are all in the old epoch\nThe nebula must first bring the out of date messages into the new epoch to replay them\n\n\n\n\n\nDecember 14th\n\nHad a wonderful chat with Brooklyn from Fission. We nerded out a lot about capabilities, lot’s more reading for me to go through:\n\nCapability Myths Demolished\nRecovering from key compromise in decentralised access control systems by Kleppmann and Bieniusa\nRobust Composition: Towards a Unified Approach to Access Control and Concurrency Control\n\n\nMore research direction refinement!!\nReleased some unfinished work and asked people close to me to read it\n\nA lot of feelings stewing around feeling not confident in my own work after the average sentiment was lukewarm at best\nI think it stems around sharing inherently incomplete/vague/in-progress work. It’s a very weird frustration that comes from mismatches between my mental model, the work itself, and the readers mental model\nBut this is the great part of feedback!! Even though emotional brain says that “oh no, harsh feedback scary”, it is actually really constructive and helps me articulate my thoughts better. I’m going to try to make a piece of writing I’m fully proud of when I finish exams (soon!)\n\n\n\nDecember 1st\n\nWell would you look at that… it’s December\nIt’s been a week but the hype from my BFT JSON CRDT project has finally started to cool\n\nOver 10k people viewed the blog post for meaningful length of time and HN was surprisingly nice about it :‘))\nA bunch of people reached out asking to chat more about CRDTs (some even asked if I was open to contracting!) Super cool to find more people working in this space and who are as equally excited about it as I am\nMost importantly, Kleppmann reached out! I had messaged him to schedule a call at some point but we never found the time. But he saw (and even retweeted!) my blog post and really thought it was solid work.\n\nHe asked if I would be interested in doing a PhD with him at Munich University. Unfortunately, visa problems combined with the requirement that I get a Masters first mean that I probably won’t be taking him up on this offer.\nHowever, we are still going to be formally collaborating on some papers regardless which I am still kind of in shock over. This felt so full circle for me! I quite literally started this summer with zero distributed systems knowledge and now I get the chance to collaborate with one of the people on the bleeding edge of distributed systems knowledge and research. Bonkers!!\n\n\n\n\nToday, I spent a lot of time thinking about the technical architecture of Rhizome now that I have the experience of the project behind me. Updated some diagrams in Rhizome Proposal but TLDR;\n\nEAV tuple store not append-only log\nUCAN good\nCRDTs instead of Raft for most things, CALM Theorem may be useful to figure out when coordination is necessary\n\n\nOpen questions\n\nHow will we mark state as requiring coordination?\nHow do we efficiently reconcile big tuple stores?\n\n\n\nNovember\nNovember 18th\n\nI know I’ve been neglecting this research log a little bit…\nI’ve been squeezing time wherever I can to work on this silly little project. As of 11:59pm tonight, I have finished the project and fully written out a 6.2k word blog post (read it here) on it to pair\n\nThis project realllyyy pushed me to my limits in terms of my engineering ability\nSo many times I doubted if something was even possible or not and many late nights of pushing off other responsibilities to get more hours on this silly little thing\nI always knew I’d finish but to be honest, I can’t really believe its over\nThere’s still more projects I want to work on but in the meanwhile… time to take a small break : )\n\n\n\nOctober\nOctober 27th\n\nAdding JSON support is harder than I expected!\n\nMostly taking inspiration from yet another Kleppmann paper\nInsert\n\nIgnore if we have it already\nCreate new entry in table with hashed OpID with is_deleted = false\n\n\nUpdate\n\nAll the steps of delete and insert\n\n\nDelete\n\nLookup prev OpID and mark it as deleted\n\n\n\n\nProbabilistic decay mechanism for CRDTs\n\n‘Remind me…’ mechanism\n\n\n\nOctober 24th\n\nPrivacy preserving CRDTs??\nTurns out Automerge is actually fast now\n\nThey’ve refactored their codebase significantly and use a b-tree similar to Diamond Types\nEd25519 should be able to sign + verify upwards of 100k signatures/s on a 1 GHz processor so I need to make some improvements here\n\n\n\nOctober 23rd\n\nAnother great talk on WNFS by Brooklyn Zelenka\n\nThe whole ‘ask for permission’ thing isn’t actually new!\nOur phones already do this: “Google Photos is asking permission to access your camera roll”\n\n\nNew Directions in Cloud Programming\n\nThe way we write distributed systems today is like writing assembly by hand — incredibly error prone\n\nCreative programmers are held back by the need to account for these complexities using legacy sequential programming models originally designed for single-processor machines.\n\n\nWe need projects like Bloom/Hydro that help with ‘compiling away’ those concurrency semantics\n\n\n\nOctober 22nd\n\nHad a random question about a paper that Kleppmann wrote and just straight up messaged him on Twitter LOL totally not expecting him to respond\n\nHe did! Within just a few hours and helped to confirm that I did in fact need to sign messages using asymmetric cryptography to prevent forgery\n\n\nAlso, Nalin helped to clarify a lot of my understanding for cryptography which was super nice of him :))\nFinally finished implementing tests for BFT and… it seems to work?? Kinda bonkers that I’ve been working on this project for almost 2 months now. Probably the most technically involved project I’ve done that integrates so much stuff I’ve learned in the past few months in systems design, networking, cryptography, and information theory\n\nJust need to finish up hashgraph reconciliation and the JSON aspect of the CRDT and should be good to go\n\n\nThinking about a potential sharded/partitioned design for a triple store DB\n\nUsing distance metrics like Kademlia DHT does?\n\n\n\nOctober 20th\n\nStarted writing post on a BFT JSON CRDT\nRan into a potential problem with message forgery…\n\nSeems like Kleppmanns’s Paper doesn’t address cases where, say a Byzantine node tries to send a message on behalf of another node (as it knows the unique IDs of other nodes) and forges an update.\nThis is possible as the unique ID doesn’t have any other properties that guarantee that only that the node with the ID can send that message.\nWe would potentially need some sort of PKI assumption where the unique ID of a node is its public key and the ID is the signed digest of the message\n\n\nThis is (sort of) confirmed in Kleppmann’s 2020 paper\n\n“We assume that each replica has a distinct private key that can be used for digital signatures, and that the corresponding public key is known to all replicas. We assume that no replica knows the private key of another replica, and thus signatures cannot be forged”\n\n\n\nOctober 19th\n\nPicked up Seeing Like A State again, it feels a lot more relevant to my research now for some reason\n\nWe can think of a triple store as a distributed and fragmented SQL database, where instead of tables with rows and value, we have entities with attributes and values.\n\nAny application can declare new attributes or alias an attribute to a more common one\nThe most important part is that applications that share attributes can automatically interoperate their data\n\nThe harder question is how to build good indices so that when the number of triples grows really large, we still get fast queries\nI suspect there’s a lot to learn from decades of SQL index/query optimizations\nWould like the syntax to borrow from GraphQL\n\n\nThis type of ‘decentralized’ database means there is no canonical schema. You can’t mistake the map for the territory because everyone has their own map and can’t force others to view the ‘truth’ of the world through your map\n\n\nForcing ourselves into schemas make it hard to innovate\n\nTo make new things requires us to provide migration paths forward or just accept stagnation\nIt inadvertently shapes what people build — leads to easily legible/classifiable applications (see: post on digital identity and legibility)\nIt is treading outside the map that gives us innovation\n\n\nThis would give us contextual data for app specific data\n\nWe can see this as analogous to context dependent personalities (again, context collapse bad)\n\n\n\n\nSpent a lot of time trying to optimize bft-json-crdt to squeeze more performance out of the base list CRDT but to no avail\n\nRealizing this was kind of a waste of time as I was just using this is a proof-of-concept\nEspecially if I want to focus on something that’s more like a triple store, a list is kind of useless lol\nGoing to focus more on the BFT and JSON-aspects of this project\n\n\n\nOctober 12th\n\nOk well… it’s been 3 weeks since I last wrote an update. School has been busy!\nI got really stuck with Rhizome work so I took a week and a bit off to work on Tabspace and launched it. It felt good to launch something and ‘unstick myself’\nGot more motivation to work on bft-json-crdt and started a more methodical approach to debugging (rather than just changing index offsets and rerunning LOL).\n\nEventually pinpointed two bugs:\n\nNot accounting for repeated delete elements\nNot properly updating the internal sequence number\n\n\nOnce these were fixed, it kind of just worked! Of course, the performance isn’t great but it still happens to be ~4x faster than the base Automerge implementation B))\n\n\n\nSeptember\nSeptember 30th\n\nI think decision is that going down splay tree route is not worth and I’ll just do this using a simple vector LOL\nBeen slowly but surely working away at this BFT CRDT implementation in Rust\n\nFiguring out some tradeoffs, I already rewrote the crate from using doubly-linked lists to using a splay tree but maybe this isn’t the right data structure either\nDesired attributes\n\nFast insert at arbitrary location\n\nA decent chunk of edits happen in places that are not the start or end of edits!\nIdeally less than O(n)\n\n\nOrdering in list is a local property\n\nIt should be easy to figure out location of a node given its ID\n\n\nInsert time for integrate\n\nFind right position to insert\n\nComparison involves looking up position of parent\n\n\nInsert\n\n\nUpdate should be considerably faster than render (which realistically doesn’t need to happen that often)\n\n\nCandidates\nB-Tree (Diamond Types uses this)\n\nNode location is not local (worst case O(logn) indirections)\nInsert time for integrate\n\nFind right position: O(logn) amortized\n\nFinding parent is O(logn) amortized\nOverall is O(log(nlogn)) amortized\n\n\nInsert: O(logn) (need to recount up the tree)\n\n\nNote: has pretty good cache locality because you can read entire lines of nodes into memory\nRequires indexing by character position which is not ideal\n\n\nSplayTree\n\nNode location is not local (average case O(logn) levels of indirection and potentially O(n) worst case)\nInsert time for integrate\n\nFind right position: O(logn) amortized\n\nFind parent is O(logn) amortized (we can use a binary encoding of the search path as an index)\nOverall is O(log(nlogn)) amortized\n\n\nInsert: O(logn) (need to rebalance up the tree)\n\n\nNote: rebalancing may not be bad in terms of time complexity but sucks because of memory locality\n\nSplayTrees are binary search trees which can lead to some deep trees which require many pointer dereferences\nAre there m-ary SplayTrees??\n\n\n\n\nDoubly Linked List (Yjs uses this)\n\nNode location is not local\nInsert time for integrate\n\nFind right position: O(n)\n\nFind parent is O(n)\nOverall is O(n2) which is slow on many concurrent inserts\n\n\nInsert: O(1)\n\n\n\n\nVector\n\nNode location is local\nInsert time for integrate\n\nFind right position: O(n)\n\nFind parent is O(n)\nOverall is O(n2) which is slow on many concurrent inserts\n\n\nInsert: O(n)\n\n\n\n\n\n\n\n\nCatching up today on a bunch of talks + reading\n\nWonderful talk by Brooklyn Zelenka (CTO of Fission)\n\n“The limitation of local knowledge is the fundamental fact about the setting in which we work, and it is a very powerful limitation” — Nancy Lynch, A Hundred Impossibility Proofs for Distributed Computing\nCIDs give us global pointers that we can all agree on (these are hard links, unbreakable)\n\nCompared to URLs (soft links, kind of like symlinks, can break). Point to a latest something\n\n\n\n\n\n\n\nSeptember 3rd\n\nBunch of weird Rust things today\n\nGenerally, use .take() on Option&lt;Box&lt;T&gt;&gt; and .clone() on Option&lt;Rc&lt;T&gt;&gt;\n.as_ref() is like &amp; but generally acts on the internal reference (i.e. on an Option&lt;T&gt;, &amp; gives you &amp;Option&lt;T&gt; whereas .as_ref() gives you Option&lt;&amp;T&gt;)\n\nAdditionally, .as_deref() basically is just .as_ref() with an additional .deref() on the unboxed value (effectively performing deref coercion)\n&lt;option&gt;.map(|node| &amp;**node) is equivalent to &lt;option&gt;.as_deref():\n\n\nRc::try_unwrap which moves out the contents of an Rc if its ref count is 1\n\n\n\nSeptember 1st\n\nCAP Theorem Tradeoffs and A Certain Tendency Of The Database Community\nRhizome Architecture: now with triple-stores :))\n\nRoot: identity + persistence layer\n\nStandalone app to manage identity + storage\nSupport multiple identities\nessentially a managed DID:key that controls a set of IPFS nodes to pin certain things\n‘open [app] on your computer’ type authorization for web applications\n\n\nTrunk: application layer\n\nData framework layer: distributed triple store\n\nrust → compiled to WASM for web\neach node has its own triple store that is created from an append-only data log\neach triple contains ID, relation, and value\n\nhow do we do realllyyy fast triple search? on multiple relations?\nhow do we pack memory efficiently for this?\nwe can ‘subdomain’ relations (e.g. it belongs to a certain set of schemas or application) using a trie\n\n\noptional author_id field to link to Root\n\n\nQuery layer: turns the triple store into live views that are interpolated\nDisplay layer: uses the views to perform calculations and display things\nBring your own data: an application has a specific fingerprint\n\nDefines exactly which types of triples it reads/writes\nEnables you to invite another user to ‘bind’ to your current application state (similar to ‘invite’ to collaborate on a document or something)\n\n\n\n\nDitto: publicly contributable schema and API definitions\n\n\n\nAugust\nAugust 31st\n\nWhat would it be like to build in interpolation into the state replication level?\n\ne.g. similar to Quake 3’s Networking or perfect-cursors both do 3 types of smoothing:\n\n\nInterpolation: If it knows the state of the world at time t and at time t+50 ms and it needs to render additional frames between those points in time, it interpolates the positions of all visible objects between their known two states.\n\nThat means that when the client is rendering the frame at t+16 ms, it already needs to have received the information about the server frame from t+50 ms!\nThe only way that is possible is if the client intentionally delays its view of the world in relation to what it’s receiving from the server.\n\n\nExtrapolation: What happens when the network packet containing the next snapshot is delayed or lost and the client runs out of states to interpolate between? Then it’s forced to do what it normally tries to avoid: extrapolate or guess where the objects will be if they keep moving the same way they’re currently moving.\nPrediction: The only exception here is player input. Instead of waiting for the server to do that and send back a snapshot containing that information, the client also immediately performs the same player movement locally. In this case there’s no interpolation - the move commands are applied onto the latest snapshot received from the server and the results can be seen on the screen immediately. In a way this means that each player lives in the future on their own machine, when compared to the rest of the world, which is behind because of the network latency and the delay needed for interpolation.\n\n\nSee also: GGPO which is heavily used in real-time fighting games\n\nAugust 30th\n\nA Graph-Based Firebase\n\nTurns out most modern real-time applications look something like this:\nSQL seems to be too complex. The common request for data in front end is a complex case to express to SQL. “We shouldn’t need advanced features for common cases.”\n\nThe most common query is our “fetch nested relations”. This should be supported first class\n\n\nWe can potentially emulate this using triple stores built on top of an append-only CRDT. Datalog and triple stores have been around for decades. This also means that people have built reactive implementations.\nUnsure if we can leverage CALM as its Datalog but not monotonic (facts can be retracted)\n\n\nDAG instead of append-only log\n\n“Both of these abilities follow directly from the explicit embedding of causality into a DAG, with time travel being analogous to a traversal over that graph” Dialog\n\n\n\nAugust 23rd\n\nPhillip Wang’s talk — related to reflection post\n\ntldr; We should leave space in our lives for finding conviction in things we work on\nHow do we enable the “other” path for high achievers? Not the one where they can just work at a big company, get paid lots of money, life a cozy life, but the harder path in which they truly question the why and ask themselves what they find truly fulfilling\nFully commit to one thing — deep beauty in choosing, against optionality\n\nHow does this fit into the exploit-explore tradeoff?\nFeels like there’s a necessary balance between\n\nbeing deeply invested in something to be able to have a level of almost unwavering conviction\nnot being so deeply invested that you are oblivious to exploring better potential options\n\n\n\n\nLeaving space to have conviction is inherently a privilege, how might we enable local spaces of abundance (places for play) so that more people have that privilege?\n\nThis also feels like a generational thing. My parents first immigrated to Canada when I was around 3. They’ve spent a big chunk of their lives worrying about how to meet basic survival needs\nI grew up with enough resources around me that I’ve begun looking for what I have conviction in; what I feel invigorated by\n\n\n\n\n\nAugust 22nd\n\nJustin Glibert’s talk\n\ntldr; composability + permissionlessness enable novel affordances we haven’t seen before in digital systems\nWhat if… Kademlia-like XOR distance metric but for SMR\nPermissionlessness is an important characteristic to enable innovation and emergent behaviour\n\nSee also: Simon Harris on the same topic\n“Many tales exist of [the origin of the bistro]. Some say it was working-class landlords opening their kitchens for extra income. Others say it was the Auvergnats, immigrating to Paris from what is today central-south France, who first worked as rag-pickers, then wood and coal sellers, then metalworkers, who created small working-class restaurants to supplement their income. Either way, it was not planned or engineered, but simply not-disallowed. There were no rules in place to stop this invention.”\nSame with YouTube which started out as a dating app but then let people upload anything. The users, not the website creators, found its real uses\nDoes this conflict with the fact that good DX/UX comes with strongly opinionated use cases?\n\n\n\n\n\nAugust 19th\n\nPacking and flying back to NY for Hack Lodge! Will be posting an ongoing thought stream :))\n\nAugust 18th\n\nPreparing workshop notes to talk about computer networking + P2P\nFeelings rant — I feel an odd and unusually heavy sense of impostor syndrome today. Going to write out more stuff in this blog post I’m going to flesh out\nFrustrated by this video by one of the founders of the Browser Company\n\nTheir vision is that the ‘next generation’ of computers — after the mainframes and personal computers — is the internet computer, where everything we do happens in the cloud and our machines are just dumb portals to access these\n\nWe can’t be going back to time-sharing! Time-sharing was only a thing because we didn’t have access to powerful enough consumer hardware — this is no longer the case\nNot only do you need to always be connected to the internet to use it, it is also incredibly Orwellian except with all-powerful companies instead of states which have detailed metrics into how you conduct every moment of your digital lives\n\n\nJosh seems to be conflating local-first software with software that is not connected to the internet\n\nJust because our data lives locally on our device, does not mean your work is trapped on one device\nI think the future is a happy middle between completely offline and completely online — we’ve pendulum-ed to both sides of the spectrum and are perhaps settling on the reasonable option\n\nServers have a role to play in the local-first world — not as central authorities, but as “cloud peers” that support client applications without being on the critical path. For example, a cloud peer that stores a copy of the document, and forwards it to other peers when they come online, could solve the closed-laptop problem.\n\n\n\n\n“Nobody makes native apps anymore”\n\nPeople want the performance of native apps without having to maintain many codebases across them.\nAs more and more apps become ‘internet-first’, libraries for storing things locally and reconciling them with remote copies of that data have not made nearly enough progress.\nAs a result, many ‘native apps’ are just wrappers for a single source of truth that lives on a remote server. This is not ideal in terms of many things but mostly performance and data ownership.\n\n\nIn a million years time when they dig back down in the archive history of our digital footprint, they won’t see vibrant replicas of the web but rather a digital dark age.\n\nThe documents created in cloud apps are destined to disappear when the creators of those services cease to maintain them.\nCloud services defy long-term preservation.\nNo Wayback Machine can restore a sunsetted web application.\nThe Internet Archive cannot preserve your Google Docs.\n\n\n\n\n\nAugust 17th\n\nReally diving into whether a dual optimistic replication (CRDT) + transactional replication (Raft SMR) approach is needed or if one will do\n\nOptimistic replication\n\nBest for global collaboration. Local nodes can still be speedy even with collaborators from across the world\nCan lead to inconsistent states if not careful (again, can use a DSL to help catch these types of errors but it just becomes difficult to write and will require extra research time)\n\nAlternatively, have no global invariants. JSON-style data structure\n\n\nStrong eventual consistency data stores (e.g. CRDTs) will hit a few million TPS per second locally for sticky writes with actual TPS being roughly RTT1​ (where RTT is ~500ms at worst, ~150ms usually)\nBandwidth use is n (just send to all nodes)\nLatency is 21​RTT (don’t need to wait for reply)\n\n\nTransactional replication\n\nEasier to reason about for application developers\nAtomic commit-type data stores (e.g. SQL, CockroachDB) still achieve upwards of 28k TPS in a single-region zone. In a global environment, TPS will be roughly 2RTT1​. This means that if you have a very global team working on something, synchronously collaborating something will still be quite laggy (~1TPS). Doesn’t work on an ‘inter-planetary scale’!\nBandwidth use is rn2+1 where r is number of rounds of the consensus mechanism\n\n1 for initial request and r rounds of n2 communication between all nodes (overhead can be reduced to rn+1 if normal state is O(n))\n\n\nLatency is\n\n\nHybrid\n\nBest of both worlds, but the most complex to reason about and write programs for\nAlternatively… what if we expose a simple KV store using CRDTs to exchange routing info? This would open it to easily layering real-time applications on top (e.g. video calls, WebRTC). This eliminates the need for a signalling server\n\nThis can technically be done already by the user in transactional replication model if they want\n\n\n\n\n\n\nAddendum: the CALM Theorem conjectures that if program state can be expressed in monotonic Datalog, it can safely use optimistic replication. If we can always express something using an immutable Merkle-DAG with occasional consensus for GC, shouldn’t this work?\nHave one SMR instantiation of the SMR algorithm per application\nHow do we do live reconfiguration of cluster quorum size?\n\nhttps://users.ece.cmu.edu/~reiter/papers/2000/DSN.pdf\nhttps://www.alibabacloud.com/blog/raft-engineering-practices-and-the-cluster-membership-change_597742\n\n\n\nAugust 16th\n\nFinally made my way through all my research papers. There’s a weird peace to have no open browser tabs, down from around ~75 open\nThinking about access control and revocation. Especially for add-only data structures, how can we prove data has been deleted or removed?\nWhat is the base metaphor we should use when building applications?\n\nA chat except the base unit is not text but structured data. Call this the ‘event history’\nThis implies a certain causal history and a partial ordering\n\n\nTrunk\n\nUser defines\n\ndata Op = ...: All possible operations of the app\ndata State = ...: Application state\nr :: State -&gt; Op -&gt; State : The reducer function\ns0 :: State: The initial state\n\n\nHow is state persisted?\n\n\nRoot\n\nIdentity\n\nA did:key is generated for every history\nOne root IPFS document tracks all active did:keys associated with a root DID\n\n\nThe Merkle-DAG will be anchored using IPLD, this means that hopping cloud providers is easy as everything is content-addressed\nStorage Providers\n\nProviders should pass a suite of unit tests for correctness in terms of satisfying certain behaviour.\nWith this model, all a storage provider needs to do is pin a few CIDs\n\n\nThis takes care of data availability… but what about liveness? This is where SMR comes in\n\n\nIdeal SMR algorithm properties\n\nFavour liveness over consistency when potentially majority replicas are offline (i.e. handle all cases f&lt;n in asynchronous crash-stop model)\nShould scale well with number of participants\nSynchronization should not be on the critical path (read: CRDTs where possible, consensus otherwise)\nCollaboration over consensus (i.e. try to preserve user intent where possible)\nThings to figure out\n\nWhen is it safe to GC?\nIs it worth writing a DSL that compiles down to different host languages? This could be really useful to provide helpful compile-time checks\n\nBasically to adhere to CALM, we want to make it easy to write synchronization free code (similar to Rust and how it makes it easy to write GC-free code)\nGenerate the appropriate boilerplate for code that requires synchronization\nHave a good standard library of data structures that are primarily synchronization-free\n\n\n\n\n\n\nPotential demo apps\n\nBasic chat app\nGoogle drive/Dropbox clone (testing large op/diff sizes)\n\nTool for thought, Google Docs-like writing primitive (testing permissioned access and collaboration)\n\n\nSemantic diffing, live git\nMinecraft or other real-time game (testing latency)\nEVM (testing expressiveness)\nSynced file system.. with editing and hosting of local-files baked in\n\nco-creating w ebsites live, similar to Beaker Browser\n\n\n\n\n\nAugust 14th - 15th\n\nOrganized The SF Commons: Hack Day #0 with Athena! A non-zero number of people were like “Hey! I’ve read your blog before” or “I love the work you do” and it was a little surreal\n\nAugust 13th\n\nVisited the Computer History Museum today! Lots of interesting tidbits on how we got to where we are today\n\nIf economies of scale favoured large consolidated computer systems how did the personal computing revolution happen?\n\nOne reason is that the main bottleneck to adoption back then was the price. But as Moore’s Law continued to hold, hardware became exponentially cheaper due to innovations in chip design, manufacturing, storage, etc.\nPeople started buying it because companies like Apple started branding personal computing devices not as something reserved for only programmers and geeks:\n\n“Since computers are so smart, wouldn’t it make sense to teach computers about people, instead of teaching people about computers?”\n\n\nThere were magnitude level improvements over existing technology. The census for example, took 10x-100x less time using computers\n\n\nHow can this be applied to the moving away from large, consolidated, monolithic applications to the personal application era?\n\n“Why is it so hard to own my own hardware?” roughly translates today to “Why is it so hard to own my own data?”\n\n\nExplaining data availability like the differences between calling versus texting someone\n\nCalling means that the other person needs to pickup\nTexting means that you can still communicate without both being on a call\n\n\n\n\n\nAugust 12th\n\nCALM Theorem and CRON Theorem: Basically, avoid coordination where possible, it makes things slow. When we can avoid or reduce the need for coordination things tend to get simpler and faster. This theorem tell us when it is safe to avoid coordination.\n\nI wonder if there’s possibility here to write a DSL (perhaps similar to BLOOM) that compiles to JS/Rust/etc. but also checks for monotonicity properties.\nSimilar to that Quilt piece on why hiding network complexity in APIs is bad, perhaps baking in these inefficiency warnings (i.e. warning on ‘accidental’ coordination, is there a way to refactor this program to use a different set of data structures which don’t require coordination) into the language\n\n\n\nAugust 11th\n\nNotes on Braid HTTP, Yjs, SSB, OrbitDB\nQuilt has a great piece arguing for more CRDTs and why APIs are lacking and what the next logical step is\n\nPut more simply, going back to picking on APIs, what will complete this analogy? assembly/C : Java/Python/Clojure :: APIs : ???\nTo quote Leslie Lamport: “Most people view concurrency as a programming problem or a language problem. I regard it as a physics problem.”\nSadly, looks like the project is no longer maintained\n\n\n\nAugust 10th\n\nNotes on HotStuff, HoneyBadgerBFT. HotStuff seems to be a really useful lens to analyze future protocols as it is a general framework for expressing byzantine fault-tolerant SMR.\nLots of paper reading… I feel a little burnt out. I’ve been spending almost 15h days just trying to mental sponge as much as this as I can.\n\nI think I’m getting enough sleep and my eating habits aren’t terrible but my body seems to disagree. My eye sometimes just twitches randomly and my stomach has a certain tightness to it that I can’t really describe well.\n\n\nA bit of a slump day. Really spent the last month just reading about consensus protocols in partially synchronous system models only to discovery that what I was really looking for was consensus protocols in completely asynchronous system models (which handle cases where potentially majority replicas are offline).\n\nI realized this as I was digging into the very last PDF I had in my browser tab on consensus algorithms — the last of 50 or 60 odd papers I made my way through.\nIn this last paper, I found out about the LR Permissionless Result which was derived earlier this year in February. It rules out the possibility for deterministic consensus in a Byzantine, permissionless model, which voids my current assumptions about the right type of consensus model for Rhizome.\nI don’t think the research and learning went to waste per-se, I feel like I really learned a lot, but it sure feels like that whole month went to waste — none of the protocols are of any direct use to the project. I just feel incredibly frustrated.\n\n\nA summer retrospective. Anson encouraged me to write a more in-depth reflection on my research processes. I mentioned on a call that I felt unhappy with my progress this summer. I think the bulk of it comes down to doing way too much reading and not enough building and producing things.\n\nA large part of this I think comes down to underestimating just how much I didn’t know about the space to begin with\n\nEvery paper I read opened 2-4 new ones. An unknown concept or definition meant another day or two to get familiar with the literature surrounding it. It wasn’t until a month ago that the number of tabs I had open started to go down.\n\n\nIt feels like the attitude I’m taking towards research is one of bumping around in the dark. For the most part its enjoyable and exhilarating, finding things out for the first time.\n\nThere’s a certain joy to putting yourself in an environment where you can discover things for yourself. I can ask for help when I need it, but most of the time I’m puttering along at my own pace.\nThis is roughly what my self-satisfaction curve looks like for self-motivated exploration: \nThis is usually fine, but when I look at it instrumentally, just from a perspective of how much I’ve actually got done, I’m a little disappointed in myself.\nI’ve decided that I’m okay with it. I’m not trying to any% speedrun my work. I want to be able to enjoy research for what it is, to visit unexpected results and learn what I find intriguing about it.\n\n\n\n\n\nAugust 9th\n\nFinished reading Weaving the Web! Probably my favourite non-fiction read so far this year. Was supposed to just write up quotes but instead wrote a 1.2k word History of the Web piece instead :‘)\n\nIt gives me hope!! Trying to change deeply intrenched habits is hard. Getting people to see the potential is hard. But there are so many people working on this and putting their whole hearts and souls into the projects they believe in that I can’t help but believe it’ll work out.\nTo quote from Tim Berners-Lee: “When I try to explain the architecture now, I get the same distant look in people’s eyes as I did in 1989, when I tried to explain how global hypertext would work. But I’ve found a few individuals who share the vision; I can see it from the way they gesticulate and talk rapidly.”\n\n\nRough notes on Casper FFG, SBFT. Revising notes on PBFT\n\nAugust 8th\n\nI want to target 60 updates per second (~16ms budget) for local and 10 updates per second (~100ms budget) for global updates\n\nThis is a good target to aim for but also wary of premature optimization\nWill likely need to just build stuff out first and experiment to see if it is usable\n\n\nSpent some time restructuring all my notes around cryptography to have better note and concept separation\nReading more about IPFS, their BitSwap protocol for block exchange is a super cool case study on how to do incentive design.\n\nAugust 5th - 7th\n\nWent to Hackclub Assemble and was just inspired by the magnitude of talent of the next generation of hackers and builders. Zach (+ Sam and rest of the HC team) really blew it out of the park this time. The theme was to build something completely useless and the kids went wild with it. I still strongly believe that one of the best signals for someone who deeply and intrinsically cares about technology is one who can still play and tinker for the hell of it.\n\nIn a similar vain, I’m organizing a Hack Day at The SF Commons on August 14th! A little callback to my hackathon organizing days :)) Really hoping to bring this new space to life with this event\n\n\nReading Weaving the Web by Tim Berners-Lee. More thoughts on this coming soon, but tldr; it is reassuring to hear that it took almost 13 years to combine the Internet and hypertext together to conceptually create the Web. Even then, it took a lot of trying over many years to bring adoption for something that many didn’t really see as potentially revolutionary\nReading through PBFT paper, really trying to understand the correctness and liveness proofs\n\nAugust 4th\n\nRandom thoughts:\n\nWhat if messages were doubly-signed with the hash of application source? This would mean that all events are specific to application version.\n\nHolochain cites that this may be a problem: “unfortunately anyone can modify their own source chain, regenerate the hashes and signatures, and create a perfectly valid, but wrong, alternate history for themselves.”\nHowever, this is actually a non-issue, given we split this into two cases:\n\nSingle-player App: whatever the user does is ‘correct’ behaviour anyways, what does it mean to have a wrong alternate history when you are the only person dictating it? All actions will still need to be in the domain of valid actions as dictated by the app (otherwise, message signature would not add up as we sign messages with the hash of application source).\nMulti-player App: peers will have a hash of the last known action of a user. If the action history is completely rewritten, the probability of arriving at the same hash is negligible, meaning that the peers will reject any further actions as invalid.\n\n\nThis brings up a new question of what migration paths look like between old and new versions of applications. If we go by hash of application source, then each update to the source code will seem like a completely new application!\n\nEach application perhaps can be signed by an author. If a newer application by the same author claims to be an update for the existing application, it can propose an upgrade path to interpret the older data in a usable format for a new one, essentially ‘importing’ the data in\n\n\n\n\nGood furniture and architectural choices respect user agency, allowing those in the space the ability to move around at will. How might we analogize this to software? Digital commons?\n\n\nRead through Holochain docs which are actually quite similar to what I have in mind for Rhizome.\n\nReally liked\n\nUsing RDF triples in a DHT to create a distributed graph database is a smart way to network the data — feels like what semweb was supposed to be\nEverything is self-owned and consistency of application state is maintained by storing hashes of actions to a global DHT which allows for peer accountability\n\n\nThings that I think are unaddressed\n\nDocumentation was well-written but the terminology was confusing at times. Was not immediately obvious what part each piece played\nHow important is global data-witnessing? Why do we need social pressures for this when we can do this using cryptography?\n\nThis also means that progress cannot be made until a node is back online (otherwise, actions remain unvalidated)\n\n\nProblem of getting people to migrate off of existing platforms remains unsolved\nDeveloper experience is difficult to set up and get started (see HApp setup docs) — heavy use of technical terminology\n\n\n\n\n\nAugust 2nd - August 3rd\n\nLearned a lot about Network theory\n\nExpanded more on thoughts about the inevitability of centralization with more insights from advantages and disadvantages of scale-free networks compared with random ones\nAlso notes on cascading failures, interesting to note that sometimes the most effective way to stop a failure is to prematurely kill edges and nodes (e.g. burning parts of the forest ahead of a forest fire to clear debris in a controlled manner)\n\n\nStressed about what to do for the upcoming gap term/year!!\n\nThings that are weighing on my mind:\n\nI want to graduate. Most visas other than the O-1 visa require at minimum a Bachelor’s degree so this is definitely something I’m thinking about. Also, a lot of highly technical jobs are (unfortunately) still gated by degrees so having at least a Bachelors is useful in that regard.\nI want to be able to spend the at least 3-6 hours per day thinking about this research project. I really think that given I have the financial means to pursue research full-time I should. A younger version of me once said that if they found an idea that excites them when they wake up every morning, they would pursue it without fail.\n\n\nSchool Situation: I need 2 more 4xx+ Computer Science Courses and 5 more 3xx+ Electives to graduate. Problem is that it is wayyy past course registration time to jig things around so either:\n\nI keep my current course schedule (stay on track to graduate next May)\nUnregister from all my courses (push back graduation date to next fall or 2024 May)\n\n\nResearch Situation: Although the research grant is no-strings attached, I really want to be able to output good work that I am proud of. Plus, the people at Protocol Labs are super cool and I want to be on good working terms with them for potential future collaboration.\n\nI think summer research has been literature review era and once next semester starts, it will mostly be building things out.\n\n\nOther thoughts: in my Letter to my Future Self, I mentioned I wanted to reach deep focus in whatever work I do and have the resources to be able to choose the work I find enjoyable. I’m not going to half-ass do whatever, so it will either be full-send research or full-send finishing school.\nOptions\n\nKeep current class schedule and try to move research to after graduation\n\nNot nearly as interesting as working on research\nWill graduate on-time in May!!\nLess long-term stress about returning to school to finish things\nI think I’m going to choose this option!!\n\n\nGap year to purely focus on research\n\nOutput research work I am proud of, keep good relations with Protocol Labs\nKeeps research momentum going, will be more effective than picking up the project a year from now\nWill need to go back to school which will feel like I’m set back a year\n\n\n\n\n\n\n\nAugust 1st\n\nGordon Brander deep-dive today… more thoughts on the inevitability of centralization and credible exit\nIt seems we are reaching that ‘recentralization’ step of the decentralization-recentralization cycle, with power concentrating in the infrastructure and application level.\n\nGordon proposes abolishing the same-origin policy. His thesis is that this forces resources on the web to be centralized around the ownership of domains. Everything — security, privacy, identity, data, and scripting — needs to be provided by the same origin, unless explicitly set otherwise. The ‘hub’ here that everything goes through is the domain. We’ve arrived back at the original centralized hub model of the internet.\nHow we can learn from the leap that Baran made going from circuit switching to packet switching and apply it to this new layer of the web? In the words of Gordon: “Can we imagine a new weblike thing that is to the web as packet switching is to circuit switching?”\n\n\nContent-addressing feels like a viable alternative to how loading resources works on the internet today.\n\nAddress-based addressing relies on a central registry to figure out where things are. Content addressed storage (e.g. CIDs) decouples the data from the origin. If you know what the hash is, you can request the original file, irrespective of where the file actually lives.\n\nThere is no single domain hosting your file.\nMany copies of your file exist across the network. This redundancy keeps things safe in case of failure.\n\n\n\n\n\nJuly\nJuly 30th\n\nI think I’m going to take a gap semester from September to December to give myself the time to finish the research to a level I’d be happy presenting to Protocol Labs at the end of the year.\n\nNever thought I’d actually take a gap semester but here I am… looks like I might graduate late now LOL\n\n\nI want to spend some time thinking about how to create effective learning and research communities. I know that since I started working in the public hackerspace at Incepto, my productivity has gone up something like 5x. How do I geographically surround myself with people that constantly inspire me to work on ambitious things?\nOn a whim, read this awesome Patreon post by Andy Matuschak\n\nBen Shneiderman, a pioneering human-computer interaction researcher, offers this charming schematic for research project design in The New ABCs of Research. He calls it the “two parents, three children” pattern.\n\n\nI’ve been thinking more about how research seems to come in two layers\n\nThe ideas\nThe language in which it is expressed\n\n\nMajority of my time has been on refining 1. so that 2. may come easier, but perhaps both should be worked on in concert.\n\n\n\n\n\nJuly 24th - 29th\n\nRoadtrip! Had a really fun roadtrip with Anson, Joss, and Jaclyn from SF to LA. Stopped at a million beaches, observed some stunning sunsets, surfed for the first time, ate great food, and sang lots of songs. Learned about ‘Surfboard’ by Cody Simpson which was the trip themesong.\nOn July 25th, I heard back from Protocol Labs and got my first large grant for 20k for the next 4 months!!!\n\nI think this support will mean a lot for the project. Protocol Labs is incredibly values aligned and they have some of the brightest minds thinking about similar problems. Even just being in an environment where I know I can expect feedback from these people (not to mention financial support) feels like a major milestone\nI really loved how they have an RFP-000 which is an ‘open-call’ for research that may not fall any other current category\nThey are strongly encourage I have more ‘traditional’ research outputs, which I think makes a lot of sense! This will give me more exposure to the more ‘academic’ side of research as\n\nAn open-access paper or brief technical report (e.g. submitted to arXiv)\n\nAn open-source code library with good documentation\nA recorded, shareable presentation of the work, preferably as part of our research talk series (!! this is really exciting).\nA blog post describing the impact of your work to be featured it their research blog.\n\n\n\n\nI especially love how the grant is ‘no-strings attached’. I think this really incentivizes honest behaviour and reporting of true outcomes rather than encouraging fabricated results or demos to get funding.\n\n”Unsuccessful projects. Our interest is in accelerating science for the benefit of all. Naturally, over time we will be more likely to fund proposals from active and effective members of our community.  However, we understand the complexities of research and do not revoke payment if the work changes course, is unsuccessful, or reaches a dead end. We value great results but also understand the value of exploration and impossibility results.”\n\n\n\n\nMy passport is about to expire so I will need to be in Canada for fall :(\n\nJuly 22nd - 23rd\n\nFinished up the identity piece! Cent and B from Metagov gave some very good advice and clarifying feedback on the piece.\n\nI think the essay was trying to do too much so I’m going to split out the content and keep all the stuff about agency and legibility in this one.\nI want to write another piece eventually about different ways of being online (specifically, collective inter-being vs individualism)\n\nThe new atom of identity is not a single entity but a set of relationships. A group chat. A chat that isn’t just a text messaging history but can embed applications and rich worlds on top of it.\nElaborate more on groupchat as an entity\n\n\n\n\nI have a bunch of reading piled up this week since I’ve been doing a lot of writing so I’ll focus on getting through some more stuff today and tomorrow.\n\nJuly 21st\n\nPolishing up identity piece. I’ve worked on it enough that I’m starting to feel ick just touching it but I’m happy that I’ve thought about this deeply. Implications for Rhizome as a whole:\n\nSelf-sovereignty seems useful for agency if implemented in ways that don’t force legibility\n\ne.g. be careful about VCs without zkSNARKS\n\n\nProbably will need to think more heavily about how to model a relation history on the dev side of things as people are used to modelling individual users. Perhaps phrasing the basic item as a group chat makes sense?\n\n\n\nJuly 20th\n\nFinally got the piece to a place where it is ready for feedback. B, Shrey, and Saffron took a look and left a bunch of comments which identified plenty of spots where either my reasoning was flawed or just wasn’t good.\n\nRevision time !! 🙃\n\n\nFeel like I haven’t been very proactive in thinking about funder relations.\n\nFound out today that GitHub has a very handy feature to email all of your sponsors!\nWill probably draft up a short update email and include a link to the identity piece as soon as it’s done.\n\n\n\nJuly 19th\n\nSlowly reaching a place where I’m happy with the direction of this piece on identity, framing it more around 3 modes of thinking about identity.\nGot a comment from Zoë Ruha Bell on my essay in Reboot that asked about “the complexities of how moving data between contexts changes its meaning and that individual control over data may not match up well with the relational information encoded in data”\n\nIncidentally, this is exactly what I’ve been thinking more about! My response:\nThis is a great question and I’m still grappling with (and in the midst of writing a whole other piece about!). Data in context is incredibly important. Like identity, when taken out of context, it can be incredibly harmful and misused. Pursuing interoperability without considering the intention behind the actions that data encodes can easily turn dangerous very quickly.\nOne way I’m thinking about this is analogizing the multiple facets and contexts of data as people. Just as people behave differently in different contexts, so can data. The same reason we have so many ‘alts’ or ‘finstas’ is that this multifaceted-ness isn’t accommodated by existing media platforms. Data platforms similarly treat data as single faceted. What does a multi-faceted encoding of data look like? What does communal ownership of data look like?\n\n\n\nJuly 18th\n\nI’ve been trying to write down some more cohesive thoughts around identity for the past two days and running into a block where I’m struggling to articulate why and how real world identity and digital identity differ\nJust read this blog post on Going Doorless that really resonated and gave new language to ideas and concepts that have been floating around in my head for a while\n\nPublic commons like parks and libraries feel public because moving around in them is effortless\n\nWe don’t have commons because the space between digital spaces have the viscosity of honey. Movement becomes heavily disincentivized.\nTo be clear: I mean that people should be able to move freely. Data should have access control switches exposed to users. They should decide whether it moves freely or not. “after voluntary communication to others, free as the air to common use”\n\n\nYou shouldn’t need to pay or set up and account to walk through the gates, commons just let you show up and start using it\nSoftware is the principles of an experience, your data is just the details\n\n\n\nJuly 16th - 17th\n\nCurrently on a two-day writing retreat with Belinda, Athena, and Vincent. It’s been such a good mix of sight-seeing and focused writing.\n\nThe other choice of spending this weekend was to go to an Art Book Fair and assemble furniture with friends. In all honesty, I’m very glad I chose to focus and write over just socializing.\nSome good time away from purely technical reading meant I had time to think more about identity. More thoughts around verb based identity\n\n\n\nJuly 15th\n\nPart of the nail of my left pinkie ripped off today argH it is now painful to type :((\nRealized that when doing site redesign, I lost a commit’s worth of notes (sad) but also Obsidian Sync which I normally use for backup also expired today (double sad). Not as bad as it could have been though! Thankfully I commit often :)\nSpent a lot of time just reading today, a lot of different scattered blogs that I’ve been meaning to get to. One link that was sent in the Metagov Slack particularly stood out to me though. It was on human identity and, more specifically, a critique of specifically SSI\n\nLong read but I think it captures a lot of my thinking around why I think digital permanence is scary (and why I’ve been thinking about relational notions of identity!) TLDR;\n\nThe concept of identity is very noun-like (i.e. tied to physical traits and current state) in Computer Science + software systems\nContrasts with identity as verb-like (i.e. incredibly contextual, based on who you are with, how you are feeling, what experiences you have)\n\n“The joins are the pathways for information exchange and transformation, for organising, and the expansion of organisational identity. Joins give the dots their meaning, their contextual relevance, their identity, just as dots give the information exchange direction and potency.”\n\n\n\n\n\n\n\nJuly 14th\n\nReboot published my research proposal / manifesto / essay on Rhizome and data-neutrality today!\n\nCheck it out on Substack\nBen Tarnoff, the guy who cofounded Logic, actually read and tweeted about it and readers seemed to resonate a lot with the post!\nTwo general sentiments:\n\nThis project seems really exciting and I appreciate it recognizes existing work. What will get people to use this though? The social difficulty with “apps as a view over data” is that it requires users to understand data models and this consensus over data models has proven difficult\n\nI agree with this evaluation and so far, feels unsolved. I think a promising solution is to think about it like how community-sourced types for TypeScript work. There’s a whole open-source project that wildly popular that provides TypeScript definitions for plain JS libraries called DefinitelyTyped (with over 12.6 million usages!). I think there’s a potential for this to work with data schemas as well\n\n\nWell.. crypto/existing-thing actually solves this! Blockchain scaling techniques are getting pretty good, don’t see why this work is necessary.\n\nAgain, true. I think these new technologies all seem really promising and I definitely try to keep up with all the developments in the space but my main concern comes from how convoluted these solutions are slowly getting.\nA lott of smart people in crypto have been working on these problems for a while, I’m starting to think it might be easier to tackle it from the other side. Plurality of approaches y’know :))\n\n\n\n\n\n\nnobody: …\nyoutube: https://www.youtube.com/watch?v=g7MSfHEdxXs\nGoing to read more about causal trees as a way of understanding more basic forms of CRDTs that value readability over correctness\n\nJuly 13th\n\nFeeling a little tired of just reading papers and coding\nGoing to do a mental reset and just play piano for a while and then doing that person website redesign that I’ve been thinking about for a while now…\n\nUpdate: this is so fun gah\nRealized that the typography on the old site was kind of garbage and hard to read. Spent a bit of time reading up on good typography practices and it looks soooo much better\n\n\n\nJuly 12th\n\nOk, a bit of a wrench in the system : ’ )))) CRDTs are incredibly hard to reason with for the average dev and cannot guarantee global invariants without requiring consensus.\nFinished up CRDT implementation collection over at CRDT Implementations.. I feel like I’m getting a better grasp at how to write op-based CRDTs but less so for state-based\n\nJuly 11th\n\nSeems like there are a lot of open research questions in CRDTs that I could plausibly spend years working on (e.g. undo operations in CRDTs, encrypted CRDTs using homomorphic encryption)\nI need to read more about this but it seems like most traditional consensus algorithms require synchronicity from all nodes for them to be considered honest. I wonder how we can reconcile this methods like CRDTs that allow for more asynchronous forms of consistency\n\nIs it possible to take advantage of the partially synchronous system model and having CRDT-like behaviour in async modes and Raft/Paxos-like behaviour during synchronous periods for compaction\nThis is especially important as users will rarely have all (or even supermajority) of their nodes online at any given time. Will need to look into variations on Raft that tolerate live membership changes\n\n“The network can partition and recover, and nodes can operate in disconnected mode for some time.”\n\n\n\n\n\nJuly 10th\n\nA lot of good meditations on adoption of tech that gives agency to users at a Hack Night that Rishi hosted :))\nCurrently at another session of the writing circle. Good to probably zoom out from a lot of the technical in-the-weeds work and re-orient about what this means for the average consumer\nDo people care about data ownership and data agency?\n\nThe average user probably doesn’t. They want convenience and are comfortable with current options.\nBut as a counterpoint, if you ask anyone on the street whether they would be comfortable sharing their entire browsing history right there on the spot, my bet is that ~95% of people will say no\n\nThis could be a really fun social experiment: incrementally increase the amount you offer strangers to look at their browsing history\nHow much does the average person value their privacy?\n\nYes, companies and the government have a lot of data/info on us\nBut what has come out of it? For the average consumer, nothing! There is a definitely anxiety of but what if with no real bad cases (that we know of)\n\n\n\n\nAnother question is why having a real person snoop on your data feels so different than large companies snooping and profiting off of your data\n\nI suspect a large part of this is due to learned helplessness\nWe haven’t ever really known what it is like for companies not to be doing that\nIt feels abstract! A company remotely snooping on your data is something that a user could remain fully blissfully ignorant from\n\n\nI think people don’t care because they haven’t known what a possible future could look like\n\nPeople don’t ask for cars because all they’ve known in their lives are horses. They can only think of faster horses\n\n\n\n\nHow do we convince the average consumer that this is something worth caring about?\n\nNear zero-friction doesn’t necessarily people will want to switch to this new paradigm. It is still a non-neglible activation energy to move platforms\nThis probably won’t happen unless there is both 1) a radical push away from existing centralized platforms and 2) a strong and convincing pull towards new decentralized platforms like Rhizome\n\nThis is where I think regulation, anti-trust, and legal requirements for data usage transparency are incredibly important! There are institutions just as powerful as these large tech companies that can serve as a counterbalancing force too\n\nThis still feels incredibly difficult as these tech companies have started invading these regulatory bodies and holding immense lobbying power\n\n\nWe do so by providing tangible and real improvements over existing products (that matter to the average consumer)\n\nNever need to manage a million different accounts again\nLocal-first feels lighter and faster\nEasily understandable ToS (people know what access are giving away)\nEnd-user programming should be trivial and non-technical (i.e. making integrations like Zapier useless)\n\n\n\n\n\n\nWhy would companies care about this model of computing?\n\ntldr; building and maintaining a data moat is hard\nComputation happens almost entirely on end-user devices, need to host massive infrastructure goes way down (unless you are doing heavy ML and info processing, which most companies are not)\nNew markets for lending compute to the masses rather than just to programmers and tech companies\nAlmost all the grunt work of data transformations is eliminated so companies can focus on business logic\nGDPR compliance built-in, users have freedom to manage their own data\n\n\nMeta-meditations: this was incredibly helpful to iron out philosophy a bit more. I think this is starting to make more sense from both a user and company perspective, but only for people who care about these sorts of things. I think end-game is getting my Mom to understand why this is important.\n\nJuly 9th\n\nVarious notes on CRDTs\n\nSeem to require consensus for state compaction\nLearning about HLCs and maintaining causal order in CRDTs\n\n\n\nJuly 8th\n\nLearning more about CRDTs, Order Theory\nI got my first email from a mutual which was along the lines of “help, I’m stuck in leetcode hell, how do I escape and do other things?”\n\nI feel like I barely know what I’m doing, let alone ready to help another person along on their journey! I sent along a few questions that were really helpful for me when convincing myself to do this project and I hope it’s useful.\nHere was the email response I sent: \n\n\nHad a great chat with Saffron today about some of the really cool research she’s thinking about doing re: online identity and data agency.\n\nThis really would not have happened if Spencer hadn’t pushed me to write a thread briefly summarizing what I was working on this summer. I’ve had a lot of super cool folks reach out and say that they are really excited by the work I’m doing! This is both reassuring but also extremely nerve-wracking. Expectations!!!! A concept\nOne point we talked about that I’m still ruminating on is the idea that selling data requires a baseline level of interoperability between two parties\n\nHow do current data markets work? If Facebook for example sells their data to another company, is it literally a raw export? How does that handoff happen and how do they ensure the format of the data they are using is understandable by both parties?\nI think inspiring more thoughts on what potential business models could exist on a platform like this\n\n\n\n\n\nJuly 7th\n\nFinished up Tim Roughgarden’s lectures on the foundations of blockchain which had some really useful theoretical details on traditional consensus mechanisms which definitely solidified my understanding\nCurrently at ~$900/mo in terms of sponsors and I’m just blown away by the volume of support people have expressed. This is more than halfway to monthly living expenses and it feels so close?\n\nAt first I was a little nervous because, y’know, that’s a lot of expectation of producing something meaningful\nBut I think this is exactly the forcing function I need to be active in doing as much as I feasibly can and share as much as possible. Even if someone steals my idea and runs with it, so what? The future is pluralistic. If it’s interoperable, it doesn’t matter how many implementations there are!\n\n\nRead a really good paper on neutrality and learned about the Data Transfer Project\n\nJuly 6th\n\nHoly shit Morgan, Aadil, David, and JZ just sponsored me for ~$400/mo for this research project and … I am genuinely just speechless??\n\nIt’s just so wild to me that this little project that I’ve just felt so strongly about because of reasons that still seem to evade words is something that other people are interested in seeing come to life too.\nIt feels like this project is a mountain I’ve set my sights on hiking for the longest time. And for a while I was hiking it alone, appreciating the scenery and the path but the path was lonely. But now I can hear the singing and laughing of my friends as people cheer and join along for the journey and it feels just a bit more manageable.\nThe generosity of these friends (shoutout to MFC and Anson) means that paying the next two months of rent and food isn’t something constantly nagging at the back of my mind :‘)\n\n\nThere were many points this summer I was fully ready to give up (see: plenty of mental breakdowns below) and stop trying because I questioned whether this was worth doing — if I should just stop and get an actual job\n\nI felt really silly for asking people to help financially on a project that I sometimes had trouble believing in too.\nSo thank you for your trust, thank you for dreaming with me\nToday’s soundtrack is from La La Land — Audition (The Fools Who Dream)\n\n\n\n\nJuly 5th\n\nFinalizing notes on Tendermint and wondering if I should switch out Raft for it. How valuable is BFT anyways? Do we assume nodes are prone to potentially malicious takeover?\n\nJuly 2-4th\n\nAn ‘aha’ moment caught in 4k… watch me try to figure out why asynchronous and partially synchronous system models aren’t the same thing (s/o Sebastien for being so kind and patient). This was super satisfying!\n\n\nJuly 1st\n\nInternet went out today halfway through watching lectures :((\n\nSpent a bunch of time just reading books + thinking\n\n\nMore notes from Tim Roughgarden’s foundation course on PKI, BB, impossibility theorems, etc.\n\nJune\nJune 30th\n\nSettling into a better work rhythm I think.\n\nFood here is surprisingly expensive but groceries is still miles cheaper than just getting Uber Eats everyday.\n\n\nHave a sudden urge to work on my personal site but I will ignore that for the time being…\nSebastien sent a YouTube playlist on the foundations of blockchains that have some sections which seem highly relevant. Slowly making my way through these\n\nJune 29th\n\nFinally wrapped up school! Anson is headed back to Arizona today too :((\n\nLiving together has been a fun dance of trying to balance our energy levels, but felt very much like a team throughout. I’m really glad I chose to prioritize relationship, truly some moments over the past month where I was like “wow, is this real.” It feels like I’m selectively giving deep attention in-turn to the things I care most about.\nNow is the era to just fully focus my attention on research and this project though\n\n\nI think this finally means that the vast majority of my waking hours will be on research. Uninstalled a game I was spending way too much time playing :’)’ it is grind time\n\nJune 27-28th\n\nNearing the end of my literature review era. Still need to go through Braid/Redwood, SSB, Yjs, and Hypercore inner-workings.\nThinking it might be good to do a general overview of CRDTs before delving any further\n\nJune 26th\n\nBelinda and Athena from Incepto told me about an SF writer event which happens every week and I’m currently at it right now. So many people here are just working on such really cool things and I’m excited to potentially have this space as incredibly condensed resesarch + thinking time. I think this is a great forcing function every Sunday to just… orient myself for the week and get shit done.\nTalked with some really really cool people at a birthday party in SF which were surprisingly receptive and interested in my work. Will definitely follow up on these conversations.\nMore research on CouchDB and other database replication mechanisms to see what I can learn from it\n\nJune 25th\n\nHackLodge meetup today, also met up with Spencer and Liam. Talked lots about the project then realized I haven’t spent much time just… sitting down and grinding out work.\nA decent chunk of it is 1) summer courses taking up much more time than I expected them to and 2) wanting to meet people in SF and spend time with Anson while she is still in SF… priorities priorities\nTo borrow words from Anson, it’s “hermit time”. I feel like I am definitely behind schedule in terms of what I wanted to get done by this point of summer and I need to put in some serious work and thinking into this project.\n\nJune 24th\n\nReading about Hyper Hyper Space, doesn’t seem to place a big deal of emphasis on finality which seems important for a large chunk of applications.\nOpen questions:\n\nAppend-only log or append-only Merkle-DAG? Leaning more towards log still for easy understandability + debug even though Merkle-DAGs are more expressive (and battletested in blockchains and git)\n\n\n\nJune 20th - 23rd\n\nReading about VDFS’s (specifically Alluxio) and\nOpen Questions\n\nHandling cases where data &gt; storage availability\nCheckpoint heuristics: when to checkpoint? especially important if Rhizome is to run indefinitely\n\n“Lineage chains can grow very long in a long-running system like Alluxio, therefore the checkpointing algorithm should provide a bound on how long it takes to recompute data in the case of failures”\n\n\n\n\nSettling into new place, we cleaned out the garage (which is where I am staying) and made it somewhat liveable?? Took a lot of work, the previous tenant didn’t even properly move out which was a stressor for a little while\n\nBecause there is no proper heating/cooling, sometimes I literally work with the garage door open for good circulation which gets me weird looks from the neighbours but it’s fun\nIncepto people have all been super nice and they are all working on/exploring cool things. I get a little distracted sometimes just working in the garage so it’s really nice I can just hop over to the hackerspace in the house to get some more focused work done.\n\n\n\nJune 16 - 19th\n\nInteract Retreat! Lots of good conversations about the work I’m doing which has been super clarifying for what type of explanation gets through to certain types of people\nGenerally find framing it in terms of net neutrality but applied to data gets a lot of people excited about it, as well as meaningfully explaining + differentiating from Tim Berners-Lee’s Solid project and how Rhizome focuses on addressing main retro points from major p2p protocols.\n\nJune 14 - 15th\n\nMostly trying to answer questions around how decentralized marketplaces for demand work, looking at Golem and Orchid\nLots of moving around (moved from Tempe to SF, about to head to Interact retreat!)\n\nJune 13th\n\nRough research notes and open questions on DWNs\nDID document needs to specify the service\n\nResolve a DID to web node URI\ndid:example:123 → resolve to Decentralized Web Node endpoint(s) → https://dwn.example.com\n\n\nRaw vs Signed Data\n\nRaw → only data + descriptor\nSigned → data + descriptor + attestation (JSON web signature/JWS)\nmore details: https://identity.foundation/decentralized-web-node/spec/#message-descriptors\n\n\nStoring data relative to a schema\n\nhttps://identity.foundation/decentralized-web-node/spec/#query\nschema field in descriptor\nJSON-LD + https://schema.org ?\nor… openzepellin style, vetted schemas\ndata lensing should fit into this\n\n\nPermissions request\n\nhttps://identity.foundation/decentralized-web-node/spec/#request\nsigned message\ndefine scope\nbased on DAG commit range perhaps?\nPotentially using UCANs\n\n\nOpen questions\n\nHow does DID ownership work? what is it pinned to? is IPFS sufficient?\n\nTLDR; DID needs to be generally anchored to something. Notes on Sidetree, a backend agnostic DID persistence mechanism\n\n\nHow do we make ownership/data management easy for non-technical people?\n\n\n\nJune 11-12th\n\nRoadtrip with Anson! Much needed break to get a mental break and reset\n\nJune 10th\n\nSpicy day today… Jack Dorsey just announced TBD working on Web5, supposedly an extra decentralized web platform (https://twitter.com/jack/status/1535314738078486533)\n\nweb5 seems to focus on the philosophy side a lot more than actual usability\nVery similar to WebID except anchored on bitcoin (lots of interesting stuff using Sidetree)\n\n\nFeel like a little boat in a big ocean where huge battleships drift by every now and then\n\nMakes me doubt what I can really do as this small little boat\nBut reminded that steering my own little boat gives me agency as to what I can explore and do\nThe little boat that could\n\n\n\nJune 9th\n\nLots of research, mostly around FOAF, LDP, RDF\nLooked more into decentralized marketplaces like Raiden and Orchid to see how they handle payments\nMostly just reading articles and specifications, your average day of research\n\nJune 8th\n\nGot my first grant rejection from Emergent Ventures today :((\n\nFeeling.. kinda numb? I feel like grand scheme of things it doesn’t matter but this is the first hard no that I’ve gotten\nSpent some time looking for some other grants but my conclusion is that I should spend more time getting shit done before asking for more funding.\nI have enough in savings to last me until end of summer but it means I’ll have to start contracting during the school year which isn’t ideal, but gives me pure focused time this summer to just do research.\nOnwards!\n\n\nLots of really great bits from Browser Co’s piece on Optimizing for Feelings\n\n“Anything new is by nature without precedent — meaning, without data to know whether it will work or not. So when we approach building new things, we don’t optimize for metrics. We optimize for feelings”\n“How do you feel when you finally step foot in your own living room, after weeks away from home? When you plop down on your own bed, or whip up a meal in your own kitchen? It conjures up a specific feeling, doesn’t it? That’s because these spaces are a reflection of you — created by you, for you. Software can feel the same way if individuals have agency and sovereignty over what is on their screens.”\n\n\n\nJune 4th - June 7th\n\nGetting back into a working groove after moving again, Arizona is ridiculously hot. Made the dumb mistake of walking to the grocery instead of taking transit lol\nLearned more about underlying datastructures of IPFS including CIDs\n\nPotential for interop between IPFS and DID Documents?\n\n\nMore notes on DHTs and Kademlia in particular\n\nJune 1st - June 3rd\n\nHad a call with a few others folks working adjacent to decentralized infrastructure and people seemed pretty excited about the proposal! It was the first time in the past month that I felt pretty confident about the project when talking about this with others, definitely a personal milestone :)\n\nMay\nMay 28th - May 29th\n\nAttending friends’ graduation for the past few days, crazy to think that this will be the last time I see some of these friends for a long time.\nWorked on thinking about and polishing my grant proposal, finally getting to some phrasings that resonate and sound good\n\nMay 27th\n\nFinishing up miniraft, added tests for voting and fixed up some workflow stuff to auto-test and publish documentation!!! It’s published now on GitHub :))\nNotes on DID which seems particularly applicable to the notion of identity + identity documents\nOnce again had a breakdown :)) Constantly feel like I’m not doing enough and that time is slipping between my fingertips…\n\nMay 23rd - May 26th\n\nCatching up on school work\nMore reading + notes in decentralization, authorization, and federation. Notable readings:\n\nIETF Draft on Centralization and Internet Standards\nGordon Brander on Modularity, weblike things, and feudal metaphors for the web\nFission on UCAN for serverless authorization\n\n\n\nMay 21st - May 22nd\n\nPacking + flights! I am now in Vancouver for the next week :))\nHectic flying experience… didn’t get much done\n\nMay 20th\n\nChatted with Justin Glibert who gave some very piercing advice\n\nWhat is the most you can cut from your current proposal and have it still be meaningful?\n\nvia negativa: essentially the study of what not to do\n\nIn action, it is a recipe for what to avoid, what not to do—subtraction, not addition.\n\n\nYou can’t know what is going to work but also you know there are things that are obviously not.\nDon’t try to think you are a god and reinvent everything from scratch. Don’t catch NIH syndrome.\n\n\nYou only have 10 beautiful idea tokens in your life you want to do it so you should just do it\n\nDon’t just do the plumbing and make stuff you already are good at if you’re trying to learn\nIf this is something you just want to work on (true in this case) then work on it with your full heart\n\n\nNot being harsh because it’s a bad idea\n\nBut rather I don’t want you to waste your time. This is your last summer without ‘real-world’ responsibilities. I would trade so much to be in your position right now.\nI am being harsh so that you spend your time wisely and don’t do something stupid.\n\n\n\n\nTechnical thoughts\n\nIs Rhizome actually a generalized form of state channels?\n\nEVM + Solidity on top of little chains between people\nMinecraft on top of this to build engines like https://www.worldql.com/\n\n\n\n\n\nMay 19th\n\nProposal re-writes + more research today, got a lot done in office today and still had time to head to Central Park to read… a great day all things considered.\nOpen questions from today’s reading + writing:\n\nHow do identity ‘clusters’ or organizations/groups of people work? How are they represented?\n\nPerhaps instead of having separate instantiation of your identity on fixed set of apps, we can have the same identity with separate instantiations of the app?\n\n\nWho runs cloud peers?\n\nHave a global marketplace where people can list/sell spare compute and storage\n\n\nWho does the compute?\n\nMost apps are lightweight to run on people’s own devices\nThe main reason we’ve needed massive datastores and compute centers in the first place is because large companies have centralized billions of people’s data into their own servers\nCloud peers can offload and perform heavy lifting if necessary\n\n\n\n\nMore meditations on identity and data\n\nThinking about how data exists only as relations between things… how do we preserve this?\n‘Data’ is data in the context of that user (or group of users) using that specific application\nLearned about the concept of petnames in more depth today and there’s a really cool way of thinking about identity here perhaps\n\nAlmost all of the contexts in which we collaborate are not global. The you I know is likely different from the you your family knows. Identity should be relational rather than standalone?\n\n\n\n\n\nMay 18th\n\nGrant writing + Verses proposal wrangling\nHad Anson tear apart my proposal today\n\nIt was so incredibly helpful to get that level of honest feedback but I just feel in the dumps right now LOL I need to figure out how to untie my own self-worth from my work\nI expect something similar will happen when I meet with Justin.. and many more times this summer\n\n\nGood feedback is equal parts bitter and sweet\n\nBitter in that it tells you the harsh truths that few have the courage to\nSweet in that they truly care enough and have enough faith to point harsh truths out\n“When you’re screwing up and nobody says anything to you anymore that means they’ve given up on you…you may not want to hear it but your critics are often the ones telling you they still love you and care about you and want to make you better.” ― Randy Pausch, The Last Lecture\n\n\n\nMay 17th\n\nWent to NYC to work at the Thrive Capital office with some Interact folks and wow… the difference being outside and in a good working environment makes is ridiculous.\nMigrated all the tracing stuff out of server.rs and log.rs into its own file. Makes the code a lot cleaner to work with.\nDeleted transport.rs (and moved the contents into tests/common.rs) now that it is no longer a part of the server. Realizing now I’ll probably need to do another refactor of the transport layer to support simulating network partitions, dropping packets, etc. so I have more surface area to test with.\nTalked with Sebastien who has been doing independent research for almost a decade now. Mentioned that I was really feeling like I was in the depths of the Valley of Despair and he just laughed and said “that was me 10 years ago and I still feel that way.” Horrifying but also weirdly comforting? He gave me some advice and thoughts (mostly with regards to independent research but honestly a lot of sage life advice too) which I wrote down in this page on independent research\nTo be honest, I don’t really understand all of this advice yet and I don’t pretend to but at the moment, it gives me comfort that even if there isn’t light at the end of the tunnel, the darkness will still be enjoyable\n\nMay 16th\n\nGrant writing again… Finished rough draft for Protocol Labs RFP 000 and writing EV grant proposal + getting feedback\nHad a mini-breakdown today after realizing I am just not enjoying this as much as I thought I would be. I’m often spending 12+ hour days writing code or grants and I just feel so behind. And I don’t get why!!!! I’ve been looking forward to this summer for so long.\n\nI think financial uncertainty is becoming more real day after day… really hoping that one of these goes through and is successful\nIt’s too early to quit. There’s still so much more to build/learn/do/write and I’m not ready to throw in the towel just yet.\n\n\n\nMay 15th\n\nFamily roadtrip, no work today :)\n\nMay 14th\n\nFinish testing harness - it looks so pretty!\nFinally updating research proposals after putting it off for 3 days. I suspect I’m using miniraft as an excuse to avoid the grant writing because making things legible is hard!! I’d much rather write code and look at pretty command line outputs instead but this is important work that needs to be done.\n\nMay 12th - May 13th\n\nReaffirming myself that a lot of this is necessary learning and this is a worthwhile project\n\nNot sure if this is actually true\nBut more so convincing myself of it so that I have the energy/motivation to go through with it\n\n\nA lot of technical refactoring going on to accommodate unit testing\n\nRemoved a lot of unnecessary lifetimes while changing RaftServer functions to return a vector of sendable messages rather than directly having each server hold a mutable reference to the transport layer (Rust doesn’t allow multiple mutable references without a RefCell!)\n\n\n\n\nLet’s say you want to become good at [x].\nIt’s almost impossible to do it because every day on Twitter you have friends who’ve raised 6 million to do crazy stuff. And so every single day, you open your books, and you take your notes and you start writing stuff, and you have to solve those equations.\nAnd every single day you tell yourself, why am I doing this?\nI could just go out and bullshit investors and build a company. And I think too many people actually do that. Myself included. I managed to resist for a while and I spent a lot of time learning different, difficult things, but it’s very hard not to have ADD in this world. It’s very hard to stay focused on important things that take a while to be learned.\n\nJustin Glibert on doing hard things\n\n\nMay 11th\n\nFinished the first pass of implementation of miniraft! In the midst of adding test infrastructure and verifying correctness of the implementation.\n\nProbably spent tooo long making it look nice but hey, if I’m going to be spending hours looking at this it might as well be good to look at\n\n\nAlso spent an hour trying to debug a test only before realizing cargo test runs in parallel so debug messages were out of whack\nFeeling quite demotivated regarding overall self-belief in the project even though I’m only 11 days in! Been trying to explain Rhizome to a few folks who have experience in the space and it is often so intimidating.\n\nLike yeah, I know this probably isn’t the best way to go about it. Maybe they’ll tell me what I’m working on is a long solved problem and I’m wasting my time. Or “couldn’t you just use x and y to achieve the same effect”? I can’t help but sometimes feel like I’m wasting my time — there are so many smart people working on the same problem, what makes me feel like I can be the one to make a meaningful contribution to it?\nI know that regardless of whether this project succeeds a lot, I’m already learning a lot in terms of technical skills and also about myself in the face of uncertainty and more independent work so I will take that as a win regardless.\n\n\n\nMay 10th\n\nDiscussing grant proposals with Verses folks, doing a lot more grant/proposal writing than I’d like these days\nFinished most of miniraft logic up until commit_log_entries. Still need to add tests though :‘)\nTech bear market isn’t promising for raising funding, esp for more experimental/greenfield work like this :((\n\nMay 9th\n\nLiterally just wrestling with Rust’s borrow checker because dyn traits are funky :((\n\nRan into a really weird design problem where I wasn’t sure how to order the lifetimes of the log or the state machine (should the app own the log which owns the state machine? should the log hold a reference to the app)?\nI opted to construct the application first then pass a pointer to the log so that when appending entries to the log, it can just call self.entries.iter().for_each(|entry| self.app.transition_fn(entry))\n\n\nFinally caved and watched an hour long video on closures, Fn, FnOnce, FnMut, boxed closures, and function pointers (Jon Gjengset, I owe you my life)\n\nFeels really stupid but it was literally a change from &amp;&#039;s mut dyn App&lt;&#039;s, T, S&gt; to Box&lt;dyn App&lt;T, S&gt;\nWhen lifetimes get as messy as they did, there’s probably a cleaner way to do it with a heap allocated value :)) Use Box more often!\n\n\n\nMay 8th\n\nSketching out grant proposals to Emergent Ventures + Protocol Labs\nHad a chat with Sebastien about research institutes and what long-term support for work like this could look like in the context of Verses\nMore implementation work for miniraft, about halfway done I think?\n\nMay 7th\n\nDoes not seem promising that my research work will be support by Verses this summer…\nLooking for other places to apply for funding but ugh this is unfortunate\nLots of coding today for miniraft! Finally feeling like I’m becoming more fluent in Rust. Figured out some nasty named lifetime stuff today by drawing a few diagrams and kinda feel like a wizard!!! Small wins\n\nMay 6th\n\nMostly writing up recent learnings and incorporating them into the research proposal… lots of words today\n\nSometimes I feel like I’m doing research to be able to do more research…\n\n\nI think I am finally getting to a point where Rhizome is making more and more sense and obvious why it is necessary\n\nI started this project/research very much like “oh wow, this is a cool set of technologies and here are some vague words and feelings about what I think is inadequate in the space” and it has sort of refined itself into a clear use case!\nCame across the concept of a “cloud peer” today in Hypercore documentation and it was like “WOW I had this exact same idea and they already have a name for it” and it was so cool\nReally excited about this future of ‘personal cloud computing’\nI think this summer will be mostly focused on the data replication / identity aspect of Rhizome, realizing that I think I was way too ambitious with my first proposal\n\n\nMore implementation on miniraft. Rust feels so slow to get back into a ‘de-rusted state’ (hah) where code just ‘flows’. It feels fun though! Type system reminds me a lot of Haskell.\n\n\nMay 5th\n\nFinishing up Martin Kleppmann’s Course on Distributed Systems\n\nCleaning up notes into atomic concepts that I can reference\n\n\nContinuing implementation of miniraft\nWhat if… Rhizome had built in mechanisms for managing ‘branches’\n\nDefault branches are single stream\nTo make a collaborative doc you can ‘fuse’ or ‘join’ branches together temporarily to sync them with each other\n\nWhat if we made something on top of git like this that actually functions on a syntax level rather than a character level… one for the idea list\n\n\nPace layers for collaboration\n\nReal-time (keystroke-by-keystroke)\nOn-click (manually click refresh)\nSuggest changes (like Google Docs, accept/reject)\n\n\n\n\nAgreeing on what operations a CRDT can perform still seems to be difficult (see 1hr into this talk)\n\nPossible room for data lensing on public schemas to be useful here\n\n\n\nMay 4th\n\nSkimming Martin Kleppmann’s Course on Distributed Systems\n\nReally good foundation to work off of\nLearned about differences between physical and logical clocks and realizing that miniraft should probably use some sort of logical clock rate\n\n\n\nMay 3rd\n\nRead about more NAT traversal and holepunching efficacy, turns out hole punching is just not as reliable as I thought it was\nCompared more traditional consensus algorithms like Raft to Solana.\nFirst formal architecture sketch?\n\nNeed to read more about DID and IPFS but this seems like a promising start?\nEach user is essentially a DID that is associated with an IPFS document that references a bunch of other things\n\nEach device in the devices array runs a Rhizome Node which is essentially a wrapped IPFS node that pins the user IPFS object and can edit it\nRight now, this means that if all a user’s devices are offline, those files are unreachable. For people who still want their stuff to be replicated online, perhaps can integrate FileCoin to incentivize other nodes to pin their document?\n\n\nThe devices array is also used by Raft to coordinate what devices should be included in the cluster\n\nModifications in the devices array leads to a Raft configuration update\nAll devices that are reachable sync via Raft to keep an appState object up to date for the user\nWhen any appState log gets too long, it is snapshotted by the leader and persisted in IPFS.\n\n\nAll the questions that are unanswered right now are in red. Lots of unanswered questions :))\n\nHow does auth work for applications?\nHow will schemas be published? Is there an app store?\nWho runs the web host? Is it self-hosted?\n\nWhat about non-technical people?\n\n\nHow is a user created?\n\n\n\n\n\n\nMay 2nd\n\nMostly reading about Raft consensus algorithm today and understanding how it works\n\nAlways wondered how these consensus algorithms deal with bad actors — turn out they don’t! That’s where BFT comes in\nSeems to be promising for replicating between trusted peers (potentially applicable)\n\n\nStarting a very minimal stripped down implementation of Raft in Rust I am nicknaming miniraft. Code here (but will most likely be private until it is done).\n\nMay 1st\n\nSettling back into home, general research reading + writing the proposal\nRead various papers\n\nChanging an entrenched Internet\nMechanism Design\nRaft\n\n\nLearned about the basic premise of SSI\n"},"thoughts/Rhizome-Specification":{"title":"Rhizome Specification","links":["thoughts/Rhizome-Research-Log"],"tags":["sapling","rhizome"],"content":"\nSee also: Research Log\n\n… tbd\nscraping the page for now as my idea for what Rhizome even is is still evolving rapidly"},"thoughts/Rice's-Theorem":{"title":"Rice's Theorem","links":[],"tags":["seed"],"content":"We cannot decide anything “interesting” about any Turing-complete program. “Interesting” is defined as any property that is not either true for every program, or false for every program. “Decide” means write a program that returns either true or false on all programs. The halting program is an instance of Rice’s Theorem.\nFormally\nAll program analysis problems that are non-trivial are undecidable. There is no program analysis that achieves all of the following for all input problems:\n\nIs fully automatic (no user input/interaction other than the program)\nAlways terminates (the analysis itself, not program being analysed)\nOn termination: always says “yes” when the answer should be “yes”\nOn termination: always says “no” when the answer should be “no”\n"},"thoughts/Risk-Homeostasis":{"title":"Risk Homeostasis","links":["thoughts/Religious-Homeostasis"],"tags":["seed","pattern"],"content":"\nWhen seatbelts were made mandatory, the number of deaths on the road actually didn’t decrease, they stayed roughly the same! Similarly, it has been observed that motorists drove closer to the vehicle in front when the vehicles were fitted with anti-lock brakes.\n\nRisk compensation is a theory which suggests that people typically adjust their behavior in response to perceived levels of risk, becoming more careful where they sense greater risk and less careful if they feel more protected\nThe most common form of this is called risk compensation, also called the Peltzman Effect, after Sam Peltzman, a University of Chicago economist who studied the phenomenon in the 1970s. His research found that as you introduced noticeable safety features, risky behaviour shifted accordingly.\nSee also: Religious Homeostasis\nStop gapping\nIn the 1920s, when the big uptake in automobiles was getting underway, most people had a family member or a friend on a farm or in a factory who was familiar with machinery. Having a knowledgeable person accessible meant that you wouldn’t simply believe scare stories. They acted as a ripstop thread in the fabric of society."},"thoughts/Roko's-Basilisk":{"title":"Roko's Basilisk","links":["thoughts/Utilitarianism","thoughts/effective-altruism","thoughts/utility","thoughts/probability","thoughts/The-ones-who-walk-away-from-Omelas"],"tags":["seed"],"content":"A form of arithmetical utilitarianism (see: effective altruism), assuming that one can meaningfully calculate the utility of actions as a numerical value.\nYou could then “shut up and multiply” utterly negligible probabilities by hypothetical huge outcomes, and take the resulting number seriously — there exists scenarios in which you should torture one person for 50 years if it would prevent dust specks in the eyes of a sufficiently large number of people — resulting in claims like eight lives being saved per dollar donated (a claim made using a calculation of this sort, very popular arguments in EA circles).\nSee also: The ones who walk away from Omelas"},"thoughts/Root-Verses-on-the-Middle-Way-(MMK)":{"title":"Root Verses on the Middle Way or the MMK","links":["thoughts/Buddhism","thoughts/emptiness","thoughts/phenomenology"],"tags":["seed","PHIL240A"],"content":"by Nāgārjuna\nA key text in Buddhist religious texts. Use of reductio ad absurdum (proof by contradiction) to show how all phenomena exhibit emptiness.\nNāgārjuna is a phenomenologist.\nWithin it, heavy use of the tetralemma form of argument that is composed of 4 forks\n\nAffirmation: X\nNegation: ¬X\nBoth: X∧¬X\nNeither: ∅\n\nObjection posed that emptiness is nihilistic and would lead to the non-existend of the Four Noble Truths, Three Jewels, and Buddha. Yet, Nāgārjuna shows this not to be the case.\n\n“Like a poisonous snake when held incorrectly or a magic spell when improperly used, emptiness, when incorrectly understood, devastates the simple minded”\n"},"thoughts/SBFT":{"title":"SBFT","links":["thoughts/Byzantine-Faults","thoughts/State-Machine-Replication-(SMR)","thoughts/PBFT","thoughts/digital-signatures"],"tags":["seed"],"content":"A Byzantine fault-tolerant state machine replication system that improves upon the scalability of PBFT.\nTo achieve this performance improvement, SBFT uses a combination of four ingredients: using collectors and threshold signatures to reduce communication to linear, using an optimistic fast path, reducing client communication and utilizing redundant servers for the fast path"},"thoughts/SQL":{"title":"SQL","links":["thoughts/Database"],"tags":["seed"],"content":"Based on the Relational model proposed by Edgar Codd in 1970"},"thoughts/SSL":{"title":"SSL","links":["thoughts/Internet","thoughts/TLS"],"tags":["seed"],"content":"Protocol for encrypting, securing, and authenticating communications that take place on the Internet.\nReplaced by TLS"},"thoughts/SVM":{"title":"Support Vector Machine","links":[],"tags":["seed","CPSC340"],"content":"An SVM is just Hinge loss with L2-regularization\nf(w)=∑i=1n​max{0,1−yi​wTxi​}\nThey can also be viewed as ‘maximizing the margin’:\n"},"thoughts/Sandglass":{"title":"Sandglass","links":["thoughts/consensus","thoughts/system-model"],"tags":["seed"],"content":"A permissionless consensus algorithm that guarantees deterministic agreement and termination under a crash-stop system model.\nSource Writeup\nSandglass proceeds in virtual rounds.\nEvery round, nodes propose a value v by broadcasting it. Values come with an associated priority.\n\nIn the first round, each node proposes its initial value v0​ with priority initialized to 0.\nIn subsequent rounds, nodes propose a value vi​ chosen among those received in the previous round.\n\nThe priority of vi​ depends on the number of consecutive rounds during which v was the only value received by the node proposing vi​.\nWhenever a node receives a value other than v, it resets v’s priority back to 0. When proposing a value in a given round, node p selects the highest priority value received in the previous round; if multiple values have the same priority, then it selects randomly among them.\n\n\nA node can safely decide a value v after v’s priority is sufficiently high. Termination follows from the non-zero probability that the necessary sequence of unanimous, consecutive rounds will actually eventually occur.\n"},"thoughts/Scientific-Freedom":{"title":"Scientific Freedom: The Elixir of Civilization","links":["thoughts/research-institutions","thoughts/independent-research","thoughts/scientific-progress","thoughts/research-debt","thoughts/agency","thoughts/iconic-space","aesthetics-and-taste"],"tags":["seed","book"],"content":"Book by Donald W. Braben on scientific progress.\n\n In the 1970s, the nature and pace of scientific discovery began to stagnate due to a combination of peer review, mandated justification of spending, and the push for short-term miracles. In Scientific Freedom, first published in 2008, Donald W. Braben presents a framework to find and support transformative scientific innovation.\n\nTransformative research: research that sets out to radically change the way we think about an important subject\nSee also: research institutions, independent research, scientific progress\nCritiques on current institutions for science\n\nResearch proposals are not treated as if they were completed works, and the so-called best, as selected by peer review, determine the search that gets done. This inhibits the scope of scientific progress\n\nThe limit is what you imagine it to do — this presumes the person doing the selecting has the best overall grasp as to what is worthwhile to presume. Often this is wrong — these researchers are the top of their field. Trust them to pick for themselves what is worthwhile to work on.\n\n\nNature does not respect consensus. We cannot expect to actually make progress simply because we have agreed among ourselves that progress lies in a particular direction. Unless, of course, that direction is a simple extrapolation from what has gone before, but such objectives usually serve only to consolidate — unless researchers are free to follow interesting observations wherever they might lead.\n\nCommittees can rarely, if ever, be creative. The fact that the best scientists might be involved is irrelevant. It can, of course, provide a defense against criticism as a committee’s membership can cloak its conclusions with respectability, but the collective responses of any group, however distinguished they may be as individuals, are usually uninspiring and based on compromise\n\n\nOn March 3, 2007 The Economist published a well-documented briefing on the shift from research to development, particularly in the big companies. Industrialists presiding over R&amp;D’s loss of the ampersand, as The Economist succinctly puts it, might therefore have a rude awakening when, as they have long been accustomed to doing, they turn to academia for new scientific insights. It may simply not be available in the quality or breadth they look for.\nScientists back then could afford the investments of time and energy to make their work more accessible because they had much more security in those days. Those taking on these duties today would be penalized as they would have less time to spend on their endless searches for new funding. See also: research debt\n\nCase study of the rise and fall of the UGC in the UK\nIn the early 20th century the UK government “acknowledge and assured the existence of a large area of free action in which the universities did as they thought right, according to the lights that were interior to them” (Daalder and Shils 1982, 439).\nIn 1919, it created the University Grants Committee, a body that at least until 1963 rigorously and jealously protected that autonomy. Importantly, it was an instrument of the Treasury. The Treasury, not being a spending department, would probably be content if the UGC kept to its spending limits and generally did not rock the boat. The UGC then could resist all attempts by most politicians and run-of-the-mill officialdom to interfere, or to closely examine academic expenditures or impose such strictures as “earmarked grants”. Thus, universities were free to deal with such labeled offerings as they thought fit. Furthermore, in those heady days funds came to them through rolling five-year grants — the quinquenniun — and each university was more or less free to spend their allocation “according to their own rights”\nIn 1963, however, the tanks of a harsher regime began to roll when responsibility for the UGC was transferred to the Department of Education and Science, the same body that had responsibility for the research councils. Zealous civil servants long denied access to university affairs now became increasingly free. The quinquennium was one of the first casualties, and in the early 1970s the university had to limit their planning horizons to a single year. In 1989, the government replaced the Olympian UGC with the University Funding Council, and in 1993 with the regional Funding Councils, one for each member of the UK. The bombardment continued with Research Assessment Exercises, Foresight, and Quality Assessment., and other bureaucratic impositions that thankfully for scientific enterprise have largely been confined to the UK.\nThus, university autonomy became severely compromised, and the climate of fear these bombardments induced was such that the universities offered barely a whimper of protest. Subsequently, the average number of Nobel Prizes per year in the sciences dropped from ~1.1/yr to ~0.38/yr (1 every 2.6 years). The primary agent of that autonomy — the UGC — was eliminated solely on misplaced idealogical grounds.\nResearch Taste\nSee also: Agency and finding ones own iconic space, aesthetics-and-taste\n\n“The state of mind which furnishes the driving power here resembles that of the devotee or the lover. The long-sustained effort is not inspired by any set plan or purpose. Its inspiration arises from a hunger of the soul.” (Albert Einstein, in his Preface to Max Planck’s book, Where is Science Going?)\nScientific progress on a broad front results from the free play of free intellects, working on subjects of their own choice, in the manner dictated by their curiosity for exploration of the unknown (Vannevar Bush, 1945)\nCreativity is the essence of the human spirit, and flowers best when it’s unconstrained. If you try to control it for your own ends you must learn that you can get only what you ask for. The unexpected will not arise. Unfortunately your world leaders have now decided that wonder is inefficient because it cannot be controlled, quantified, or targeted.\nResearchers today are rarely free to be full-fledged scientists — that is, to survey Nature’s vast unexplored domains and to plan their forays accordingly to their personal inclinations. Instead, they are subject to financial and intellectual pressures to deliver the results that consensus deems would be the best value for money. Thus a generation of researchers has been encouraged to think parochially rather than globally, and not surprisingly, this has led to a death of major new discoveries\n\nIn the consultancy field, the old adage was that successful consultant merely delivered the advice their customers wanted to hear. Do we really want that of our researchers? If we oblige them to provide precisely what our various proxies think that we need, according to timetable and balance sheet, they may have little energy left to do the other things.\nManaged creativity can, at best, produce only what its managers specify.\n\n\nWhy do people doing independently guided research tend to attempt riskier ventures?\n\nTake skydivers for example. They, while acknowledging and even enjoying risks, do not expect that their next jump will be other than successful. They would not jump otherwise. As their own necks are on the line, they take every reasonable step to reduce, eliminate, or control every identifiable source of risk. As a result, they are confident that their jump will be exhilarating and enjoyable, and that they will land in one piece more or less where they thought they would.\nOn the other hand, if it were decreed that the risks of skydiving had to be managed by those paying to enjoy the spectacle — that is, the spectators had to take full responsibility for anything that might happen — it is most unlikely that any jumps would take place. Responsibility shared is responsibility declined. In these circumstances, consensus opinion would opt for safety first as it usually does, and yet another expression of joie de vivre would vanish\nIn any event, when funding agencies foster high-risk research, they promote the idea that science is intrinsically risky. It is not. It is difficult, but scientists are fully aware of that fact when they begin careers in research\n\n\n\nOn advancing knowledge\n\nPlanck once said that science advances one funeral at a time: “A new scientific truth does not triumph by convincing its opponents and making them see the light, but rather because its opponents eventually die, and a new generation grows up that is familiar with it.” The role of young researchers — their exuberance, confidence, and tendency to ignore tradition — is as vitally important today as it always has been.\nEdward Denison’s study of American growth between 1929 and 1982 concluded that education per worker accounts for 30% of the increase in output per worker and the advance of knowledge accounts for 64%. Technology remains the dominant engine of growth with human capital investment in second.\nThere are few defining names or living legends that might resonate in the imaginations of future commentators or inspire the young. we seem to have turned research into a faceless industry. Is it any wonder that young people are turning away from science?\n\nOn starting venture research initiatives\n\nIn 1960s, IBM’s chairman, Thomas Watson Sr., began the Fellows program, in which he appointed Fellows for five years to be “dreamers, heretics, mavericks, gadflies, and geniuses”\n\nTheir remit was simply to “shake up the system”. Of course, this Fellows program was extremely successful by any metric, even without explicitly trying to. Only some 165 scientists were appointed, but 5 of these won Nobel Prizes. General Electric and Bell Labs ran similarly distinguished programs\n\n\n\nCase study of the BP Venture Research Program\nThis program was run by the author.\nApplications\nWritten applications could be made on one page or preferably less, and if researchers needed a quicker reply, they could apply by telephone.\nAny researchers who did not agree with our apparent rejection could return at any time with a rebuttal. Thus, however long (or short) our dialog might be, we tried to arrange that we always returned the ball to the applicant’s court. The onus would always be on them to respond the line we had taken\nIn effect, therefore, we were creating an environment in which researchers could select themselves.\nScreening\nThe second part of the application sat them down near a whiteboard and talked about whatever science interested them. Almost invariably, the first thing we had to do was to discourage them from describing the possible benefits that might flow from their proposed work (applications, technology, etc.). In this, they merely seemed to be following normal practice by telling us what they thought we as a funding agency wanted to hear. The level of possible funding was another of their priorities. Our response was to ask them to assume that we had an infinite amount of money and could offer freedom to match! with these and other extraneous issues so beloved of conventional funding agencies out of the way, we could finally ask them to tell us what they would like to do that they were not doing now.\nThe final step was to satisfy ourselves that the researchers were capable of doing what they set out to do. This is not the same as peer review, we had effectively made up our minds, but wanted to be reasonably sure that we had not missed anything.\nOne of us would then visit their home environment “to kick the tires”. They would show us what they were doing and something of what was going on elsewhere in their department. Almost invariably, the fields would be new to us, but we found that it is remarkably easy to judge levels of competence from the fluency with which presentations are made, or questions answered (or avoided). As we would often tour their labs, it was interesting to note other scientists’ response to what had been proposed and the degrees of respect (or otherwise!) in which the applicants were held.\nSymposiums\nWe also arranged annual two-day meetings at our headquarters in London to which all Venture Researchers were invited. It took a few years to work out a viable format for these meetings, covering the entire spectrum of research as we did. we knew that we had got it about right when the attendance, which was not compulsory, of the more than 100 participating scientists reached close to 100%. indeed, many researchers told us that despite the plethora of conferences nowadays, ours was a “must go”\nOne of their attractions was that they were a festival of science — specific disciplines were rarely mentioned, and we hope that Pasteur would have been at home\nResults\nOut of the 26 groups that were running at the initiative’s close in 1990, perhaps 14 made transformative discoveries: that is, they did radically change the way we think, and several succeeded in achieving important scientific objectives that their peers had thought were impossible or irrelevant."},"thoughts/ScrapScript":{"title":"ScrapScript","links":["thoughts/content-addressed-storage","thoughts/CID","thoughts/IPFS"],"tags":["seed"],"content":"Source\nTo make software safe and sharable, scrapscript combines existing wisdom in new ways:\n\nall expressions are content-addressible “scraps”\nall programs are data\nall programs are “platformed”\n\nContent-Addressible Everything\n(see: content addressed storage)\nAny chunk of the language can be replaced with a hash. Scraps are stored/cached/named/indexed in global distributed “scrapyards”. This is like an npm but at the expression level\nExpression level-versioning, every expression in the ecosystem can be independently spliced and “time-travelled”.\n(spaceq/is-planet@2005 &quot;pluto&quot;) ; true\n(spaceq/is-planet@2006 &quot;pluto&quot;) ; false\nMagic compression: Instead of sharing large dumps of data, you can send references to any data anywhere. By sending references, other machines can opt to pull the data from cache or high-speed CDNs (see: CID)\nFirst-class Network Requests\nScrapyards enable new compile-time primitives for verifying type-safety across network boundaries. “Contracts” are automatically inferred and enforced between clients, servers, and external APIs. (similar to Racket Contracts)\n$ echo &quot;@rebbit/users 42&quot; | scrap eval\nerror: @rebbit/users expects type rebbit/users-request\nPublish\nScrapyards store scraps in an IPFS-like system with name and versioning information.\n## publish a scrap\n@yard/publish my-key &quot;greet&quot; &quot;| _ -&gt; \\&quot;hello\\&quot;&quot;\n# task:success ()\n \n## execute a scrap\nconnie2036/greet &quot;hi&quot;\n# &quot;hello&quot;\n \n## get the code of a scrap\n@yard/get &quot;connie2036/greet&quot;\n# task:success &quot;greet&quot; &quot;| _ -&gt; \\&quot;hello\\&quot;&quot;"},"thoughts/Secure-Scuttlebutt":{"title":"Secure Scuttlebutt (SSB)","links":["thoughts/digital-signatures","thoughts/Public-key-Infrastructure","thoughts/petname","thoughts/DHT"],"tags":["seed"],"content":"SSB\nSecure Scuttlebutt is a database protocol for unforgeable (read: digitally signed) append-only message feeds.\nScuttlebot\nScuttlebot forms a global cryptographic social network with its peers. Each user is identified by a public key, and publishes a log of signed messages, which other users follow socially.\nIdentity\nWeb-of-Trust style (see: PGP). There is no global registry of usernames. Instead, users name themselves, and share petnames for each other.\nIdentities are Ed25519 key pairs.\nPub Servers\nTo get over the data availability problem and because Scuttlebot has no DHT or NAT-traversal utilities, users must “join” a Pub to distribute their messages on the WAN.\nPubs are bots that follow users and rehost the messages to other peers, ensuring good uptime and no firewall blockage.\nSecret Handshake\nAn encrypted channel protocol based on a mutually authenticating key agreement handshake, with forward secure identity metadata. It’s used by Scuttlebot to authenticate and encrypt peer connections."},"thoughts/Seeing-like-a-State":{"title":"Seeing like a State","links":["thoughts/Collingridge-dilemma","thoughts/Chesterton's-Fence","thoughts/housing","thoughts/software-principles","thoughts/traditional-knowledge"],"tags":["seed","book"],"content":"Book by James C. Scott\nAn account of the logic behind the failure of some of the great utopian social engineering schemes of the twentieth century.\nLessons:\n\nTake small steps: In an experimental approach to social change, presume that we cannot know the consequences of our interventions in advance. Given this postulate of ignorance, prefer wherever possible to take a small step back, observe, and then plan the next small move (see also: Collingridge dilemma)\nFavour reversibility: prefer interventions that can easily be undone if they turn out to be mistakes. Irreversible interventions have irreversible consequences. Interventions into ecosystems require particular care in this respect, given our great ignorance about how they interact. Aldo Leopold captured the spirit of caution required: “The first rule of intelligent tinkering is to keep all the parts” (see also: Chesterton’s Fence)\nPlan on surprises: in agricultural schemes, this may mean choosing and preparing land so that it can grow any of several crops. In planning housing, it would mean “designing in” flexibility for accommodating changes in family structures or living styles.\nPlan on human inventiveness: always plan under the assumption that those who become involved in the project later will have or will develop the experience and insight to improve on the design\n\nThese, unsurprisingly, have heavy overlap with the design of software\nSimplification\n\nLegibility: certain forms of knowledge and control require a narrowing of vision.\n\n“Legibility is a condition of manipulation. Any substantial state intervention in society — to vaccinate a population, produce goods, mobilize labour, tax people and their property, conduct literacy campaigns, conscript soldiers, enforce sanitation standards, catch criminals start universal schooling — requires the invention of units that are visible”\n“The great advantage of such tunnel vision is that it brings into sharp focus certain limited aspects of an otherwise far more complex and unwieldy reality.”\n“This very simplification, in turn, makes the phenomenon at the centre of the field of vision more legible and hence more susceptible to careful measurement and calculation.”\n\n\nStandards\n\n“Large-scale commercial exchange and long-distance trade tend to promote common standards of measurement.”\nBut yet, most complexities of human life fail to be marshalled into a single regulatory code.\n“Even in a particular locality, practices varied greatly from farm to farm and over time; any codification would be partly arbitrary and artificially static. To codify local practices was thus a profoundly political act.”\n“At the limit, there would be at least as many legal codes as there were communities”\n\n\nCities\n\n“For Jacobs, the city as a social organism is a living structure that is constantly changing and springing surprises. Its connections are so complex and dimly understood that planning always risks unknowingly cutting into its living tissue, thereby damaging or killing vital social processes.”\nSee also: Chesterton’s Fence\n\n\nWhy schemes have failed, a tldr;\n\n“I would say that the progenitors of such plans regarded themselves as far smarter and farseeing than they really were and, at the same time, regarded their subjects as far more stupid and incompetent than they really were.”\n\n\n\nMonocultures and centralization are fragile\n\nScientific, “fiscal forestry”, and monocultures\n\nIn which the actual tree with its vast number of uses was replaced by an abstract tree representing a volume of lumber or firewood.\nPurely, the state wanted to optimize the greatest possible constant volume of wood\n“In the short run, this experiment in the radical simplification of the forest to a single commodity was a resounding success.”\n\n“It is apparent that centralized high-modernist solutions can be the most efficient, equitable, and satisfactory for many tasks. Space exploration, the planning of transportation networks, flood control, airplane manufacturing , and other endeavours may require huge organizations minutely coordinated by a few experts.”\n\n\n“But it was the whole world that lied ‘outside the brackets’ which returned to haunt this technical vision.”\n\n“The narrowness in turn means that production agronomy is occasionally blindsided by factors outside its analytical focus and is forced, by the resulting crisis, to take a broader perspective”\n\n\n“The monoculture meant that the whole nutrient cycle got out of order and eventually was nearly stopped, representing a production loss of 20 to 30 percent. A new term, Waldsterben (forest death), entered the German vocabulary. An exceptionally complex process involving soil building, nutrient uptake, and symbiotic relations among fungi, insects, mammals, and flora was apparently disrupted with serious consequence.”\n“Monocultures are, as a rule, more fragile and hence more vulnerable to the stress of disease and weather than polycultures are”\n\n\nLeo Tolstoy, War and Peace\n\n“While the sea of history remains calm the ruler-administrator in his frail bark, holding it with a boat hook to the ship of the people and himself moving, naturally imagines that his efforts move the ship he is holding on to. But as soon as a storm arises and the sea begins to heave and the ship begins to move, such a delusion is no longer possible. The ship moves independently with its own enormous motion, the boat hook no longer reaches the moving vessel, and suddenly the administrator, instead of appearing a rule and a source of power, becomes an insignificant, useless, feeble man.”\n\n\n“Perspective makes the single eye the centre of the visible world. Everything converges on the eye as to the vanishing point of infinity, The visible world is arranged for the spectator as the universe was once thought to be arrange for God.” (see also: The Unflattening)\n\nDiversity is Good, actually\n\nExplaining the Western appeal for ‘order’\n\n“The diversity of species naturally occurring in a tropical setting is, other things being equal, consistently greater than the diversity of species in a temperate setting. An acre of tropical forest will have far more species of plants, although fewer individuals of each species, than will an acre of temperate woodland. Thus unmanaged nature in temperate climates looks more orderly because it is less diverse, and this may play a role in the visual culture of Westerners.”\n\n\nJacobs and parallels with cities\n\n“A highly specialized neighborhood, by contrast, is like a gambler placing all his bets on one turn of the roulette wheel. If he wins, he wins big; if he loses, he may lose everything. For Jacobs, of course, a key point about the diversity of a neighborhood is the human ecology it fosters. The variety of locally available goods and services and the complex human networks that it makes possible, the foot traffic that promotes safety, the visual interest than animated and convenient neighborhood provides — all interact to make such a location’s advantages cumulative. The diversity and complexity that cause systems of flora to become more durable and resilient work, at another level apparently, to cause human communities to become more nimble and satisfactory.”\n\n\n\nFlexibility for the unexpected\n\nCities and Design\n\nWhereas Le Corbusier’s planner is concerned with the overall form of the cityscape and its efficiency in moving people from point to point, Jacob’ planner consciously makes room for the unexpected, small, informal, and even nonproductive human activities that constitute the vitality of the “lived city”\n\n\n“[imperial pretensions of agronomic science’s] inability to recognize or incorporate knowledge created outside its paradigm sharply limited its utility to many cultivators. Whereas farmers, as we shall see, seem pragmatically alert to knowledge coming from any quarter should it serve their purposes”\n\n“Farmers, being polytheists when it comes to agricultural practice, are quick to seize whatever seems useful from the epistemic work of formal science”\n\n\nErosion Control\n\n“Erosion control in Japan is like a game of chess. The forest engineer, after studying his eroding valley, makes his first move, locating and building one or more check dams. He waits to see what Nature’s response is. This determines the forest engineer’s next move, which may be another dam or two, an increase in the former dam, or the construction of side retaining walls. Another pause for observation, the next move is made, and so on, until the erosion is checkmated.”\n\n\n\nInformal Order\n\nJane Jacobs’ Sidewalk Terms\n\n“Jacobs explains that when a friend used their apartment while she and her husband were away or when they didn’t want to wait up for a late-arriving visitor, they would leave the key to their apartment with the deli owner, who had a special drawer for such keys and who held them for friends. She noted that every nearby mixed-used street had someone who played the same role: a grocer, candy-store owner, barber, butcher, dry cleaner, or bookshop owner/ This is one of the many public functions of private business. These services, Jacobs notes, are not the outgrowth of any deep friendship; they are the result of people being on what she calls ‘sidewalk terms’ with others… The city relies on the density of people who are on sidewalk terms with one another to maintain a modicum of public order… A person didn’t think twice about asking someone to hold one’s seat at the theatre, to watch a child while one goes to the restroom, or to keep an eye on a bike while one ducks into a deli to buy a sandwich”\n\n\n“The planned city, the planned village, and the planned language are, we have emphasized, likely to be thin cities, villages, and languages. They are thin in the sense that they cannot reasonably plan for anything more than a few schematic aspects of the inexhaustibly complex activities that characterize ‘thick’ cities and villages. One all-but-guaranteed consequence of such thin planning is that the planned institution generates an unofficial reality — a ‘dark twin’ that arises to perform many of the needs that the planned institutions fails to fulfil… Nearly every new, exemplary capital city has, as the inevitable accompaniment of its official structures, given rise to another, far more ‘disorderly’ and complex city that makes the official city work — that is virtually a condition of its existence. That is, the dark twin is not just an anomaly, an ‘outlaw reality’; it represents the activity and life without which the official city would cease to function.”\n“It is helpful to imagine two different maps of activity [in a city]. In the case of a planned urban neighbourhood, the first map consists of a representation of the streets and buildings, tracing the routes that the planners have provided for the movements between work places and residences, the delivery of goods, access to shopping, and so on. The second map consists of tracings, as in time-lapse photograph, of all the unplanned movements — pushing a baby carriage, window shopping, strolling, going to see a friend, playing hopscotch on the sidewalk, walking the dog, watching the passing scene, taking shortcuts between work and home, and so on. This second map, far more complex than the first, reveals very different patterns of circulation. The older the neighbourhood, the more likely that the second map will have nearly supeseded the first, in roughly the same way that planned suburban Levittowns have, after fifty years, become thoroughly different settings from what their designers envisioned.”\n\n“If our inquiry has taught us anything, it is that the first map, taken alone, is misrepresentative and indeed nonsustainable… As with industrial agriculture and its dependency on landraces, the first map is possible only because of processes lying outside its parameters, which is ignores at its peril.”\n\n\n\nMetis and Local Knowledge\n\nSee also: traditional knowledge\n“One powerful indication that [a skill requires] metis is that they are exceptionally difficult to teach apart from engaging in the activity itself. One might imagine trying to write down explicit instructions on how to ride a bicycle, but one can scarcely imagine that such instructions would enable a novice to ride a bicycle on the first try”\n“We might reasonably think of situated, local knowledge as being partisan knowledge as opposed to generic knowledge… An insurer of commercial shipping for a large, highly capitalized maritime firm can afford to rely on probability distributions for accidents. But for a sailor or captain hoping for a safe voyage, it is the outcome of the single event, a single trip, that matters. Metis is the ability and experience necessary to influence the outcome — to improve the odds — in a particular instance.”\n“The big mistake of the rationalist — though it is not inherent in the method — is to assume that ‘tradition’ or what is better called ‘practical knowledge’ is rigid fixed and unchanging — in fact it is ‘preeminently fluid’. Tradition, in part because of its local variation, is pliable and dynamic. No traditional way of behaviour. no traditional skill ever remains fixed. Its history is one of continual change.”\n"},"thoughts/Self-sovereign-Identity-(SSI)":{"title":"Self-sovereign Identity (SSI)","links":["thoughts/agency","thoughts/security","thoughts/encryption","thoughts/blockchain","thoughts/soulbound","thoughts/Verifiable-Credential","thoughts/introductions"],"tags":["seed"],"content":"Self-sovereign identity (SSI) is an approach to digital identity that gives individuals agency over their digital identities\n\n“Identity is a uniquely human concept; however modern society view this concept of identity as state-issued credentials as driver’s license and social security cards, which suggests a person can lose his very identity if a state revokes his credentials or even if he just crosses state borders.” (Christopher Allen)\n\nIdentities are required for trust to be established (Party A needs to ensure Party B is actually who they claim to be and vice versa).\nIn centralized identity paradigms, this is usually done through authorities (e.g. Certification Authorities) who are trusted by both parties.\nIn SSI systems, holders have control over unique identifiers (decentralized identifiers). These can be verified using encryption and anchored on some sort of distributed ledger (e.g. blockchain)\nWhy SSI matters to the average citizen\n\nAvoid a million accounts to log into. SSI federates logins to various services and applications\nGranular access permissions for content\nData provenance through signatures, can request takedowns as proving ownership is trivial\n\nCritiques of SSI\nSource: Molly White\nMostly critiques about certain implementations of SBTs or VCs (which she refers to as Verifiable attestations)\nPeople are able to send soulbound tokens without the consent of the recipient—given that it is unlikely people would consent to police departments recording their crimes for others to later use against them if they had the choice\nShe is against a world where relationships are front-run by a deluge of data rather than formed more organically between individuals: “An acquaintance now quits those ‘old-fashioned’ relationship-building niceties and gets straight to the SSI point. Where do you work? Which college did you go to? Which college did your parents go to? Republican or Democrat? What’s your gender? Your ethnic origins? Do you have this gene or the other one? If you fail to offer up the requisite verifiable claims then you fail to get to ‘trust building’ first base in the SSI century.” (see also: introductions). Similar to Black Mirror’s Nosedive Episode (S3E1)\nPeople suck at security: the average person is shit at securing their data. Although, I wonder if this is just bad UI/UX or is it just fundamentally hard (tm) to make it easy for people to be secure? I feel like VPNs for example have made ‘good security’ practice pretty easy for the average consumer"},"thoughts/Sidetree":{"title":"Sidetree","links":["thoughts/DID","thoughts/IPFS"],"tags":["seed"],"content":"Source Spec\nA ‘meta’/Layer 2 protocol that can be applied to any target trust layer to create a scalable DID method (batteries almost included)\nA bunch of DID methods work fine at lab scales (~100 DIDs) but how do we scale to billions?\n\nDoesn’t require any additional consensus, relies on the consensus of the underlying trust layer\nStrict deterministic ruleset means no conflicting states are allowed\nIDs are not transferable\n\nBatches a bunch of operations as content-addressable storage references (read: IPFS) and anchors them to underlying trust layer.\nION takes roughly 20 minutes for commitment finality"},"thoughts/Simone-Weil":{"title":"Simone Weil","links":["thoughts/attention","thoughts/epistemology","thoughts/Jestermaxxing","thoughts/self-effacing-ends"],"tags":["sapling"],"content":"French philosopher and activist who took a very pragmatic and grounded yet radical approach to her philosophy.\nAttention and epistemology\nThe way which you direct your attention (i.e. your way of attending to the world) drastically influences how you experience the world. Weil went a step further and believed that your attention dictates which experiences are even possible to experience.\nSpecifically, Weil called for a sort of ‘negative effort’ toward attention: one which removes the I from the experience and is receptive to all the possible experiences of the world.\n\n“Our thought should be in relation to all particular and already formulated thoughts, as a man on a mountain, who, as he looks forward, sees also below him, without actually looking at them, a great many forests and plains. Above all our thought should be empty, waiting, not seeking anything, but ready to receive in its naked truth the object that is to penetrate it.”\n\nShe held an intersubjective epistemology, believing that knowing the truth requires not extending one’s own limited perspective, but suspending or abandoning it such that reality—including the reality of the existence of others—could appear on its own terms.\nIt is an attitude that is more detached from expectations, more open, more receptive but at the same time, it doesn’t ignore the knowledge that you’ve already gained in a particular area, it just doesn’t makes sure you’re not chained to that knowledge unable to move. In a way, it’s a method of intellectual playfulness that avoids epistemic traps.\nJestermaxxing#^81b0a0\nTo Simone Weil, having this form of neutral attention allows us to have much more lucidity in our thought, something that she saw as one of our most important responsibilities.\n\n“[Lucidity] does away with insatiable desire and vain fears; from this and not from anything else proceed moderation and courage, virtues without which life is nothing but a disgraceful frenzy.”\n\nAfter finishing her time in school, she had realized that our education systems were at least in part responsible for how students were being taught about how to pay attention around them in a narrow way and became a teacher.\nShe wanted to teach in a way that didn’t limit the way students oriented themselves towards problems, noting that most education systems fixate students to an agenda where they are always searching for a singular right answer to the problem.\nThe hardest job of teaching, Simone Weil thought, was to shake people out of this collectivist way of thinking where, they’ve already decided what their positions are, and now it’s not about thinking so much anymore as it is about defending their answer, their viewpoint, again all the obviously wrong answers and perspectives.\n\n“The great human error is to reason in place of finding out”\n\nWill vs Attention\nSource with slight paraphrasing\nThe will is mostly good at performing physical stuff. It’s a great tool to use if your goal in life is that want to increase your max deadlift weight. It’s a great tool if you want to sit in an ice bath or any uncomfortable situation and endure things for as long as you can. If you’re a writer… the will is great at getting you to sit down and put pen to paper and to start doing the work.\nBut there are certain things that strength of the will can’t get you closer to. For example, if you’re a writer…you can’t will yourself to something like creativity. You can force yourself to move the pencil up and down on the piece of paper but if every word you write is completely uninspired, writing another 2000 words isn’t going to change it. See also: self-effacing ends\nSimone Weil thought by the end of her life that instead of cultivating pure willpower, cultivating attention was usually a much better way of transforming yourself.\nThe will is an amazing tool at doing some stuff, but the point she’s making is that if all you ever worked on when developing yourself in this life was will and discipline and you didn’t practice this cultivation of attention at all, then when you’re in a situation where you need something like inspiration or moral clarity or an openness to the truth what can you do in that spot? Pure willpower is the wrong tool for the job.\nWhat if instead of getting really good at forcing yourself to do things you don’t like, you got super good at reimagining the things you get no joy out of to be things, that you do get joy out of? Your life is driven instead, to Simone Weil, by a true desire, to do the thing. Your life is driven by joy, rather than self-denial."},"thoughts/Sloppy-Hashing-DHT":{"title":"Coral: Sloppy Hashing DHT","links":["thoughts/Kademlia-DHT","thoughts/IP-Address","thoughts/Network-Theory","thoughts/Byzantine-Faults"],"tags":["seed"],"content":"Source\nMain problems with Kademlia DHT is that it has poor locality. A peer could make requests that hop all the way around the globe when the information they are looking for is in their local network!\n\nThough some DHTs make an effort to route requests through nodes with low network latency, the last few hops in any lookup request are essentially random. Thus, a node might need to send a query half way around the world to learn that its neighbor is caching a particular web page.\n\nChoral achieves locality through clustering! It creates self-organizing clusters of nodes that fetch information from each other to avoid communicating with more distant or heavily-loaded servers.\nNotes\n\n‘Sloppiness’ comes from the fact that a set(key, nodeaddr) operation doesn’t just store the pointer nodeaddr on one node\n\nIt stores pointers along the lookup path for popular keys (this is called “spilling-over”)\nHelps to balance load while inserting pointers, retrieving pointers, and downloading data\n\n\nGenerally set a TTL for records to expire quickly enough to keep the fraction of stale pointers below 50%\n\nNetwork Layers\nIn order to restrict queries to nearby machines, each Coral node is a member of several DSHTs, which we call clusters, of increasing network diameter.\nThe diameter of a cluster is the maximum desired round-trip time between any two nodes it contains.\nFor example, a node can be a part of 3 clusters, and L0, L1, and L2.\n\nL0 is the ‘lowest’ level and widest network diameter, having a maximum desired round-trip of ∞ so the network spans every node in the DHST\nL2 is ‘highest’ level and narrowest network diameter, having a small maximum desired round-trip time to restrict it to local nodes\n\nSimilar concept to isochrone maps\n\nDownsides\n\nThe privacy sucks sucks: nodes publish not only their IP Address but the path to get there too!\nRequires network size estimation which is hard to do if the number of nodes are small (i.e. requires a large deployment to be effective)\n\nCan be done using Network Theory as lookups are on average O(logn) hops\n\n\nNot BFT: a malicious actor could pollute the DHT and cause really poor routing\n"},"thoughts/Social-Bias-in-Information-Retrieval":{"title":"Social Bias in Information Retrieval","links":["thoughts/explainability","thoughts/transparency","thoughts/privacy","thoughts/bias","thoughts/To-Live-in-their-Utopia","thoughts/data-distributions","thoughts/Algorithms-of-Oppression"],"tags":["seed"],"content":"Social Bias in Information Retrieval\nSource: Addressing Social Bias in Information Retrieval in In Experimental IR Meets Multilinguality, Multimodality, and Interaction\n“Many algorithmic processes are opaque and that the reasons for this may vary. For instance, it is more often than not difficult to interpret results from models induced by new machine learning techniques such as deep learning” (especially why we need to work on explainability)\nAs a counter argument, there are social and economic challenges for achieving algorithmic transparency, such as the need for developers/owners of such processes to protect trade secrets, or even the privacy concerns of users.\nFriedman and Nissenbaum’s definition of bias:\n\nits results are slanted in unfair discrimination against particular persons or groups\nthe observed discrimination is systematic within the system\n\n\nIndeed, over the past years, many researchers have found that search engines, through the result sets they present to users, tend to reinforce a view of the social world that aligns with the status quo.\n\nRelated: To Live in their Utopia, Data Distributions, Algorithms of Oppression"},"thoughts/Social-Choice-Problem":{"title":"Social Choice Problem","links":["thoughts/probability","thoughts/Arrow's-Impossibility-Theorem"],"tags":["seed","PHIL321A"],"content":"\nHow do we aggregate many individual preference orderings into a single group or social preference ordering; or,\nHow can rational individuals make rational choices as a group\n\nWe can define a Social welfare function (SWF) combining individual preference orderings (over social states) into a social preference ordering (over those same states).\nInitial Attempts\n\nUse majority rule to aggregate individual preferences into group preferences.\n\nProblem: The voting paradox. Individual preferences may be transitive but the group preference can be cyclic when we do a majority vote. The violates a Klmogorov Axiom about ordering\n\n\nUse maximum total utility to determine group preference\n\nDifferent scales (equivalent vNM scales) yield a different social preference ordering\n\n\nJust use ordinal rankings\n\nArrow’s Impossibility Theorem\n\n\n\nDefinitions:\n\nA group D∈G is decisive with respect to some pair of social states (a,b) iff a≻b by the whole group G whenever everyone in D prefers a≻b\nA group is decisive if it is decisive over all pairs of social states\n\nSee: Arrow’s Impossibility Theorem"},"thoughts/Social-Contract-Theory":{"title":"Social Contract Theory","links":["thoughts/rights","thoughts/social-contracts","thoughts/Rawl's-Theory-of-Justice"],"tags":["seed","CPSC430"],"content":"Social contract theory is the view that persons’ moral and/or political obligations are dependent upon a contract or agreement among them to form the society in which they live. It focuses on the individual and collective benefits of protecting certain human rights, such as the right to life, liberty, and property\nSee: social contracts, Rawl’s Theory of Justice\nWhat prevents the community from enacting bad rules is that no one is above the rules. Since everyone is in the same situation, no community members will want to put unfair burdens on others because that would mean putting unfair burdens on themselves.\nCounterarguments\n\nSocial contract implies agreement. None of us agreed to it when we were born!\nSocial contract theory does not explain how to solve a moral problem when the analysis reveals conflicting rights\nClassifying those who deliberately break moral rules and those who cannot understand a rule can be difficult\n"},"thoughts/Solana":{"title":"Solana","links":["thoughts/blockchain","thoughts/censorship","thoughts/bitcoin","thoughts/proof-of-work","thoughts/PBFT","thoughts/peer-to-peer","thoughts/Filecoin"],"tags":["seed"],"content":"Solana is a blockchain that claims to be fast, secure, scalable, affordable, and resistant to censorship\nMain technical innovations\nProof of History\nFor a blockchain to work, participant nodes need to reach an agreement on time. Traditional blockchains like Bitcoin function by proof of work.\nThe whole philosophy behind it is:\n\nRunning the function takes some time\nRunning the function is the only way to produce the output\nWith the known input and output of the function, the only way of evaluating the output is to re-execute the function with the provided input\n\nThis guarantees that when an output is valid for an input, some time has passed for producing that output.\nTower BFT\nPBFT which uses proof of history as a reliable source of time.\nFrom Solana Documentation\n\nThe basic idea to this approach is to stack consensus votes and double lockouts. Each vote in the stack is a confirmation of a fork. Each confirmed fork is an ancestor of the fork above it. Each vote has a lockout in units of slots before the validator can submit a vote that does not contain the confirmed fork as an ancestor.\nWhen a vote is added to the stack, the lockouts of all the previous votes in the stack are doubled (more on this in Rollback). With each new vote, a validator commits the previous votes to an ever-increasing lockout. At 32 votes we can consider the vote to be at max lockout any votes with a lockout equal to or above 1&lt;&lt;32 are dequeued (FIFO). Dequeuing a vote is the trigger for a reward. If a vote expires before it is dequeued, it and all the votes above it are popped (LIFO) from the vote stack. The validator needs to start rebuilding the stack from that point.\n\nTurbine\nSimilar to data sharing approaches in p2p, seed chunks to peers that can then share amongst themselves.\nGulf Stream\n\nEach Validator knows the order of upcoming Leaders due to Solana’s architecture. So clients and Validators forward transactions to upcoming Leaders before they act as a Leader in the network. This allows Validators to start processing transactions ahead of time. This results in fewer transactions cached in Validators’ memory and faster confirmations.\n\nIsn’t this potentially problematic? If all nodes know what the upcoming leader is, couldn’t they just DDoS the next leader?\nArchiver Nodes\nIf each node in the network was required to store that much data, a limited group of participants who could afford and manage that kind of storage, could join the network and this makes the network centralized.\nPoRep stands for proof of replication and it’s a system introduced by Filecoin initially in which a prover defends a publicly verifiable claim that it is dedicating unique resources to storing one or more retrievable replicas of a data file.\nOccasionally, the network will ask/challenge the archivers to prove they’re doing their job of storing data and at this point, archivers should complete PoRep."},"thoughts/Solid":{"title":"Solid","links":["thoughts/GDPR","thoughts/HTTP","thoughts/LDP","thoughts/RDF","thoughts/CORS","thoughts/access-control","thoughts/WebID"],"tags":["seed"],"content":"\nSolid is a specification that lets people store their data securely in decentralized data stores called Pods. Pods are like secure personal web servers for data. When data is stored in someone’s Pod, they control which people and applications can access it.\n\nPushing for universalization at the data level: just as any website can be viewed on any browser and any internet provider, any data should be accessible by any application under people’s control.\nGDPR technically does all of this, Solid is just technology that ensure it actually happens.\nSolid is essentially a glue between HTTP, LDP, and LDN\nPods\n\nPods are like secure personal web servers for your data. You can think of it like a website with data.\nYou can get a Pod from a Pod Provider, or you may choose to self-host your Pod.\nUsers can own multiple pods\nLinked Data means that different applications can work with the same data\n\n\nDetails\nSummarized from specs\n\nExchanges data between clients using HTTP + TLS\nA storage (pim:Storage) is a space of URIs in which data can be accessed; it is the root container for all of its contained resources\nSeems to just be a fancy HTTP file server (operated on RDF Documents)\n\nApplications can ‘patch’ pods with new data, given that they have the correct access to it\n\n\nReal-time collaborative communication between pod and application uses WebSockets\nCORS by default prevents apps that run on one origin from accessing data on other origins\n\nGet around this by having servers waive the cross-origin protection as Solid handles this access control themselves\n\n\nIdentity is done through WebID\nServers are strongly discouraged from exposing information beyond the minimum amount necessary to enable a feature.\n\nOpinions\n\nFeels completely unopinionated, see this as a negative. Should shepherd and guide the average user down the happy path but still make it easy to customize for those who wish to.\nStill seems to try to emulate client-server interactions heavily\nProviders are not distributed\nDX seems pretty poor, comment section on this video which has a lot of laypeople seem to dislike\n"},"thoughts/Speaking":{"title":"Speaking","links":["thoughts/DWeb-Camp-2023"],"tags":["seed"],"content":"2023\n\nLet us imagine a communal-owned internet! at DWeb Camp 2023. We live on rented land, play according to immutable rules, and created in limited spaces. How do we create an internet of communal agency &amp; what does it look like now? See the Internet Archive blog post  and accompanying blogpost reflection.\nCommunal Computing Networks at DWeb YVR. What might networks that enable experimentation and agency look like?\n\n2022\n\nIntro to Computer Networking &amp; P2P at Hack the North 2022. A story about how the internet was made resilient to nuclear attacks and an introduction to peer-to-peer networks.\n\n2021\n\nReact in an hour or your money back at Hack the North 2021﻿\nIntro to Docker at Hack the North 2020++\n"},"thoughts/State-Machine-Replication-(SMR)":{"title":"State Machine Replication (SMR)","links":["thoughts/consensus","thoughts/consistency","thoughts/liveness","thoughts/Byzantine-Broadcast"],"tags":["seed"],"content":"A subset of the algorithmic consensus problem about agreeing on the same state\n\nConsistency: all notes agree on the same history\nLiveness: every transaction submitted eventually added to all node’s histories\n\nSMR can be reduced to Byzantine Broadcast"},"thoughts/Strangler-Fig-Pattern":{"title":"Strangler Fig Pattern","links":[],"tags":["seed"],"content":"Incrementally migrate a legacy system by gradually replacing specific pieces of functionality with new applications and services. As features from the legacy system are replaced, the new system eventually replaces all of the old system’s features, strangling the old system and allowing you to decommission it.\nIn this pattern, a “legacy system” (the current implementation) and a new system that have the same interface but different implementations. They are called behind an abstraction layer that calls both systems in parallel and throws one response away\nNamed after the strangler fig which suck up the nutrients from its victims, causing them to die eventually. An original support tree can sometimes die, so that the strangler fig becomes a “columnar tree” with a hollow central core.\n"},"thoughts/Stream-of-Consciousness":{"title":"Stream of Consciousness","links":["thoughts/pace-layers"],"tags":["seed","PHIL451A"],"content":"Quotes from The Principles of Psychology by William James\n\n“Consciousness, then, does not appear to itself chopped up in bits. Such words as ‘chain’ or ‘train’ do not describe it fitly as it presents itself in the first instance. It is nothing jointed; it flows. A ‘river’ or a ‘stream’ are the metaphors by which it is most naturally described. In talking of it hereafter, let us call it the stream of thought, of consciousness, or of subjective life.” – William James, Principles of Psychology\n\n\nGaps and changes in quality that we do notice don’t undermine the feeling that our consciousness is continuous (we bridge the gap between pre-gap consciousness easily)\nGaps and changes in quality that we don’t notice aren’t felt as interruptions because we’re not aware of them\nThe stream of consciousness is really both a stream of perception and a stream of thought and attention shifts dynamically back and forth between them\n\n\n“When we take a general view of the wonderful stream of our consciousness, what strikes us first is the different pace of its parts. Like a bird’s life, it seems to be an alternation of flights and perchings… Let us call the resting places the ‘substantive parts’ and the places of flight the ‘transitive parts’ of the stream of thought”\n"},"thoughts/Sybil-Attack":{"title":"Sybil Attack","links":["thoughts/consensus","thoughts/fault-tolerance"],"tags":["seed"],"content":"Source: Sybil attack on Wikipedia\nSybil attacks are also called sock puppetry\nCreating a large number of pseudonymous identities and uses them to gain a disproportionately large influence (e.g. control of &gt;50% nodes allows you to ‘override’ the consensus)\n3E’s of Preventing Sybil Attacks\n\nEntry Cost\nExistence Cost\nExit Penalty\n\nSee also: fault tolerance"},"thoughts/Symmetric-Key-Cryptography":{"title":"Symmetric Key Cryptography","links":[],"tags":["seed"],"content":"\nBob and Alice share same key KS​\nMethod/algorithm maybe be different (opposite) for decryption but same key is used\n"},"thoughts/Syndication":{"title":"Syndication","links":["thoughts/distributed-web"],"tags":["seed"],"content":"Important distributed web concept\nPOSSE\nSource\nPOSSE is an abbreviation for Publish (on your) Own Site, Syndicate Elsewhere, the practice of posting content on your own site first, then publishing copies or sharing links to third parties (like social media silos) with original post links to provide viewers a path to directly interacting with your content."},"thoughts/TCP":{"title":"TCP","links":["thoughts/Transport-Layer"],"tags":["seed","CPSC317"],"content":"Transport layer protocol\nTCP provides a reliable, in-order, port to port, byte-stream service to applications. The application byte-stream is conveyed over the network via TCP segments, with each TCP segment sent as an Internet Protocol (IP) datagram.\nOverview\n\nPoint-to-point: one sender, one receiver\nReliable, in-order byte stream: no message boundaries\nPipelined: TCP congestion and flow control set window size\nSend and receive buffers (similar to GBN and SR)\nFull duplex data: bi-directional data flow in same connection\n\nMSS: maximum segment size\n\n\nConnection-oriented: handshaking initializes both sender and receiver state before data exchange\nFlow controlled: sender will not overwhelm receiver\n\nFlags\nHeader format:\n0                   1                   2                   3\n0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|          Source Port          |       Destination Port        |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|                        Sequence Number                        |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|                    Acknowledgment Number                      |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|  Data |       |C|E|U|A|P|R|S|F|                               |\n| Offset| Rsrvd |W|C|R|C|S|S|Y|I|            Window             |\n|       |       |R|E|G|K|H|T|N|N|                               |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|           Checksum            |         Urgent Pointer        |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|                           [Options]                           |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|                                                               :\n:                             Data                              :\n:                                                               |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\nThe control bits are also known as “flags”\n\nSYN (sychronize): packets used to initiate a connection\nACK (acknowledgement): packets that are used to confirm that the data packets have been received, also used to confirm the initiation request and tear down requests\nRST (reset): signify the connection is down or maybe the service is not accepting the requests\nFIN (finish): indicate that the connection is being torn down. Both sender and receiver send the FIN packets to gracefully terminate the connection\n\nConnection Establishment\nThree-way handshake. We must prevent segments from one incarnation of a connection from being used while the same sequence numbers may still be present in the network from an earlier incarnation. To solve single initial sequence number problem, we randomly choose the initial sequence number\n1) A --&gt; B  SYN my sequence number is X\n2) A &lt;-- B  ACK your sequence number is X\n3) A &lt;-- B  SYN my sequence number is Y\n4) A --&gt; B  ACK your sequence number is Y\n\n(we can combine 2 and 3 into a single message)\n\nClient sends initial SYN message\n\nSequence number for client to server is specified\n\n\nServer responds with a SYN/ACK (flip both bits) message\n\nClient to server sequence number is confirmed in ACK\nServer to client initial sequence number is specified\n\n\nClient sends an ACK message\n\nServer to client sequence number is confirmed in ACK\n\n\n\nWindow Management\n\nSize is selected by the application (if not the default)\nBoth sender and receiver have congestion windows\n\nMeasured with segments of maximum segment size (MSS)\nSize is determined by the presence of absence of congestion\nActual send window is the min of the flow control window (receiver) and the scaled congestion window (computed by sender)\n\n\nRetransmission strategy\n\nACKs correspond to first sequence number not yet received (similar to GBN)\nReceiver stores packets in its own window (like SR)\nFour or more ACKs with same number triggers a retransmission without a timeout\nRetransmit just one segment instead of whole window (like SR)\n\n\n\nCongestion Management\n\nVery conservative, at first sign of congestion, cuts congestion window in half\nWhen it appears that congestion has eased, it increases slowly (1 segment to congestion window each time)\nTCP uses bandwidth in a fair way\nSlow start: always start with a congestion window of 1 segment\n\nIncrease by 1 each time a segment is ACKed (this is exponential, equivalent to doubling each time we send a window full of data)\nStop doubling when we detect congestion\n\n\n\nFlow Control\nDifference between a sender’s sequence number and the remote host’s acknowledgement number represents any outstanding, unacknowledged data\nACK flag is offset 107\nSequence and acknowledgement number are both 32 bit fields so the range is from 0 to 232−1. After all the 232 sequence numbers are used up and more data is to be sent, the sequence numbers can be wrapped around and used again from the starting.\nSequence Number\nOffset 32\nTracks number of bytes sent outward by a host. If a TCP packet contains 1400 bytes of data, then the sequence number will be increased by 1400 after the packet is transmitted.\nAcknowledgement Number\nOffset 64\nTracks number of bytes received. If 1000 bytes are received by a host, it increases the acknowledgement number by 1000 when it sends out a packet in response.\nThe flag is set if the acknowledgement number field contains a valid acknowledgement number."},"thoughts/TLFS":{"title":"TLFS","links":["thoughts/local-first-software","thoughts/Yjs","thoughts/Hypercore"],"tags":["seed"],"content":"From Cloudpeers\nThe local-first SDK offers a stack to write applications as productively as when using state-of-the-art cloud-based architectures. It enables building serverless apps that traditionally require backend engineers to build, scale and maintain.\nReally great SDK but unsure how this differs from existing platforms like Yjs or Hypercore (aside from being non-JS native). Could see this being useful for cross-platform live collaboration.\nActually uses a Cambria-like system for data lensing which is cool."},"thoughts/TLS":{"title":"TLS","links":["thoughts/SSL","thoughts/Asymmetric-Key-Cryptography","thoughts/MAC"],"tags":["seed"],"content":"TLS evolved from the previous encryption protocol SSL\nCertificate\nThe certificate contains important information about who owns the domain, along with the server’s public key, both of which are important for validating the server’s identity.\nHandshake\nA TLS connection is initiated using a sequence known as the TLS handshake. When a user navigates to a website that uses TLS, the TLS handshake begins between the user’s device (also known as the client device) and the web server.\nOnce data is encrypted and authenticated, it is then signed with a MAC. The recipient can then verify the MAC to ensure the integrity of the data."},"thoughts/Tangaroa":{"title":"Tangaroa","links":["thoughts/Byzantine-Faults","thoughts/Raft-Consensus-Algorithm","thoughts/PBFT","thoughts/liveness","thoughts/digital-signatures","thoughts/hash-function"],"tags":["seed"],"content":"\nA Byzantine fault-tolerant Raft algorithm inspired by PBFT\n\nSource Paper\nByzantine nodes are problematic for Raft:\n\nNode can keep calling for elections to terminate the current term. As Raft cannot progress until a leader is elected, this makes Raft unavailable (breaking liveness)\nByzantine leader could modify a client’s request and violate correctness\n\nDifferences from Raft\n\nMessage Signatures: Uses digital signatures to authenticate messages and verify integrity. This prevents a Byzantine leader from modifying the message contents or forging messages\nClient Intervention: Clients can force interrupt leadership if the cluster fails to make progress. This prevents a Byzantine leader from continuously calling elections\nIncremental Hashing: each entry has a hash that is computed over the previous hash and the newly appended log entry, preventing Byzantine nodes from reorganizing the event log\nElection Certificates: on a successful leader election, its first heartbeat will contain a quorum certificate with all the RequestVoteResponse RPCs it received to become leader. Nodes can individually verify this is the case (all public keys are known ahead of time)\nCommit Verification: rather than having leader be responsible for incrementing the commit index, it keeps track of this itself by broadcasting AppendEntriesResponse RPC to every node (not just the leader). That way, when a node receives quorum on the number of AppendEntriesResponses, it will increment the commit index (similar to the prepare phase in PBFT)\nLazy Voters: a node does not grant a vote to a candidate unless it believes the current leader is faulty. This prevents unnecessary elections from being started in attempts to starve the system\n"},"thoughts/Technosolutionism":{"title":"Technosolutionism","links":["thoughts/Seeing-like-a-State","thoughts/quantization","thoughts/Design-Justice"],"tags":["seed"],"content":"\nNew technology will save us!!\n\nTechnosolutionism in governing from just a quantized dashboard where the people responsible for making the decisions are so abstracted away from the actual problems that they need to solve\n\n…the tendency of jobs to be adapted to tools, rather than adapting tools to jobs. If one has a hammer one tends to look for nails, and if one has a computer with a storage capacity, but no feelings, one is more likely to concern oneself with remembering and with problem solving than with loving and hating. (Silvan Tomkins, founder of affect theory)\n\nSee also: Seeing like a State, quantization\nDesign Justice\nFrom Design Justice\nAn exclusionary and elitist understanding of what technology is and where it comes from and a lack of interest in preexisting, community-based design practices.\n”… where we are wholly dependent on a handful of extraordinarily gifted entrepreneurs to lead us out of the dark ages. This is a myth.”\nFirst, it contains a somewhat masked normative assumption that “technology adoption” is always a good thing."},"thoughts/Tendermint":{"title":"Tendermint","links":["thoughts/Raft-Consensus-Algorithm","thoughts/consensus","thoughts/system-model","thoughts/consistency","thoughts/liveness","thoughts/Public-key-Infrastructure","thoughts/clocks","thoughts/State-Machine-Replication-(SMR)","thoughts/PBFT"],"tags":["seed"],"content":"\nTendermint is most useful as an analog of Paxos/Raft but in a multi-stakeholder, or otherwise more adversarial, setting. However, the performance may not be as high due to the overhead of cryptographic operations\n\nSource Paper, authored by Buchman, Kwon, Milosevic in 2018, stabilized in 2019.\nA state machine replication protocol with a partially synchronous system model that, when f&lt;3n​, satisfies always consistency and eventually satisfies liveness (under the presence of an attack). However, the time to obtain a supermajority increases linearly with the number of nodes in the network.1\nHigh-level ideas:\n\nIterated single-shot consensus (something that looks like Byzantine Agreement) where the output of each single-shot consensus instance outputs a block (ordered list of transactions)\nFor a fixed height, keep proposing + voting until agreement is reached\nTwo stages of voting as different nodes may see different voting schemes\n\nWe assume PKI and a shared global clock. A round is 4Δ timesteps, leaders are rotated once per round.\nProperties\nQuorum Certificate (QC) Lemma\nA collection of a supermajority (≥32​) of votes for a block B in a particular round at some height h and some stage s. Any two QCs overlap in at least one honest node as overlap≥n−31​n−31​n&gt;f and thus any two QCs must support the same block B.\n\nState\n\nEach node maintains a (Bi​, QCi​) and periodically updates these variables block-QC pair it’s heard about\nEach node also keeps a local append-only data structure for blocks considered ‘delivered’\nEach node maintains it’s own height (which block it is currently working on) and ignores all messages about other heights\n\nPseudocode\nAssume a specific height h and round r with leader l. We split each round into 4 phases (t=4Δr).\n\nt=4Δr:\n\nl updates (Bl​,QCl​) to most recent QC known\nbroadcast (Bl​,Ql​) signed by l to all other nodes\n\n\nt=4Δr+Δ:\n\nhonest node i will ignore the proposal if it seems out of date (QCl​ seems behind QCi​)\nif node i receives (Bl​,QCl​) from l and it is up to date\n\nbroadcast first-stage vote for vote1​(Bl​)\nupdate (Bi​,Qi​):=(Bl​,Ql​)\nbroadcast (Bl​,Ql​) signed by i\n\n\nelse, do nothing\n\n\nt=4Δr+2Δ:\n\nif node i receives ≥32​n round-r stage-1 votes (supermajority) for block B,\n\nif this occurs, all possible QCs must all support the same block (by QC overlap property)\nassemble QC from supermajority of votes\nset QCi​:=QCassembed​, Bi​:=B\nafter witnessing a conclusive winner to the first stage, we broadcast second stage vote for vote2​(Bi​)\nbroadcast (Bi​,QCi​) signed by i\n\n\nelse, do nothing\n\n\nt=4Δr+3Δ:\n\nif node i receives ≥32​n round-r stage-2 votes for block B,\n\nset QCi​:=QCassembed​, Bi​:=B\ncommit B to local history\nbroadcast (Bi​,QCi​) signed by i\nincrement hi​, re-initialize Bi​ and QCi​ to null\n\n\nelse, do nothing\n\n\nt=4Δr+4Δ (just before round r+1):\n\nIf we have heard of a stage-2 QC for block hi​ supporting block B\n\ncommit B to local history\n\n\nelse, do nothing\n\n\n\nIn the background,\n\nAll honest nodes store all QCs received for future blocks hi​+1,hi​+2,…\n\nProof of consistency\nDefinition of consistency: For a given block number, all honest nodes commit the same block B∗.\nThis seems pretty obvious from the QC lemma but we can formalize this through proof by induction:\nAssumptions\n\nFix a height h.\nLet r be the first round in which &gt;3n​ honest nodes (set S) cast stage-2 votes for some block B∗. r is the first round in which a stage-2 QC could have been created.\n\nInduction: at the end of round r\n\nwe know Bi​=B∗, ∀i∈S\ncurrent QCi​ is from round-r stage-1 or later\nall QCs for other blocks are from round r−1 or earlier\n\nThese properties remain to be held in round r+1 given they hold in round r as no nodes of S change their mind.\nProof of liveness\nDefinition of liveness: if a transaction T is known by all honest nodes, then it will get added to all of their local histories.\nNote: this is a weaker definition of liveness than usual for SMR which states that if a single honest node knows about a transaction, then all honest nodes will eventually add that transaction to their local histories.\nWe define a clean round when\n\nwe are post-GST\nthere is an honest leader\nall honest nodes are working on the same block number\n\nProofs:\n\nFast forward to pair of r1​, r2​ consecutive rounds after GST+Δ with honest leaders l1​, l2​ (this must be true for f&lt;3n​)\nLemma: at the start of round r1​, every honest node is working on either block h or h+1\n\nTrue because of the broadcast of a stage-2 QC at the end of t=4Δ(r−1)+3Δ, and all nodes should pick this up by t=4Δr+3Δ and be working on at least h\nNodes could possibly be split between working on h and h+1 if a Byzantine node keeps secret a stage-2 QC for h and selectively forward it to honest nodes.\n\n\nLemma: if there is a clean round, all honest nodes commit the block proposed by the leader\n\nBy part 2 in assumption of clean round, after the update in the first phase, the leader’s QC is at least as recent as any other honest nodes.\nAs we are post-GST, vote request will arrive at each node for 4Δr+Δ where they all broadcast stage-1 votes for Bl​ and have their local variable updated.\nNodes assemble super majority for Bl​ and can create a QC… same argument for stage-2 votes\nAll nodes then commit Bl​ to their local history\n\n\n\nFootnotes\n\n\n“There is a practical limit to how decentralized a blockchain with PBFT-based consensus can be. For instance, most Tendermint based blockchains only have 100-150 validators; this is done to strike a balance between time to finality and decentralization” (from Scott’s Guide to Finality) ↩\n\n\n"},"thoughts/The-Anthropocene-Reviewed":{"title":"The Anthropocene Reviewed","links":[],"tags":["seed","book"],"content":"\nTo fall in love with the world isn’t to ignore or overlook suffering, both human and otherwise. For me anyway, to fall in love with the world is to look up at the night sky and feel you mind swim before the beauty and the distance of the stars. It is to hold your children while they cry, to watch as the sycamore trees leaf out in June… We all know how loving ends. But I want to fall in love with the world anyway, to let it crack me open. I want to feel what there is to feel while I am here.\nSendak ended that interview with the last words he ever said in public: “Live your life. Live your life. Live your life.”\nHere is my attempt to do so.\n\n“Aesthetic beauty is as much about how and whether you look as what you see. From the quark to the supernova, the wonders do not cease. It is our attentiveness that is in short supply, our ability and willingness to do the work that awe requires.”\n“It is fortunate that each generation does not comprehend its own ignorance.” (Charles Dudley Warner)\n“What’s news isn’t primarily what is noteworthy or important, but what is new. So much of what actually changes in human life isn’t driven by events, but instead by processes, which often aren’t considered news. We don’t see much about climate change on CNN, unless a new report is published, nor do we see regular coverage of other ongoing crises, like child mortality or poverty.”\n“In the alley, there is a bright pink flower peeking out through the asphalt. A) it looks like futility B) it looks like hope”\n“Third things are essential to marriages, objects or practices or habits or arts or institutions or games or human beings that provide a site of joint rapture or contentment. Each member of a couple is separate; the two come together in double attention”\n“The pleasure isn’t owning the person. The pleasure is this. Having another contender in the room with you”\n“Our obsessive desire to make and have and do and say and go and get — six of the seven most common verbs in English — may ultimately steal away our ability to be, the most common verb in English.”"},"thoughts/The-Dragon-and-the-bridge":{"title":"The Dragon and the Bridge","links":["thoughts/epistemology","thoughts/Nyāya","thoughts/philosophical-realism","thoughts/trust","thoughts/language"],"tags":["fruit","PHIL240A"],"content":"Paper #1 for PHIL240A at UBC\n\nIt’s been three weeks now since you woke up deep in a forest. You’ve managed to survive; the forest itself isn’t that bad. But, there’s a pack of wolves that keeps trying to hunt you down, and every few days you have to run away from where you’ve set up camp. This is one of those days. You hear the howling of the wolves and run run run for it… until you break through the tree line and find yourself facing a massive ravine. There’s clearly no way to jump, go around, or scale down it. Scanning frantically, you see a rope bridge about half a kilometer away, and you dash toward it. There seems to be some large boulder blocking the entrance, but you’ll deal with that when you get there. As you approach, a long neck raises an elegant reptilian head, and two golden eyes turn to regard you. The boulder was a sleeping dragon! You slow down and approach with caution. Dragons are known to be intelligent and mostly benevolent; maybe you can just talk to them and ask them to move. “Great dragon!,” you call out. “Please let me past you onto the bridge. There’s a group of wolves coming after me, and I don’t want to be eaten.” The dragon looks askance at you, turns its long neck to look across the ravine, and turns back to you again. “Bridge? What bridge?,” they ask. “The one right behind you,” you reply, surprised. “There’s a bridge here?,” the dragon queries, feigning (or not?) confusion. You’re stunned. “Yes, the bridge, it’s right there!,” you say. “Aha!,” says the dragon. “But how do you know there’s a bridge here? How can you be certain of what you see, that it’s not an illusion, or that all of this isn’t a dream? Death approaches you from behind, death looms before you if you fall unsupported into this ravine. Why would you trust that the world has provided you with a bridge?”\nRespond from the perspective of Uddyotakara, assuming that he endorses Gautama’s verses and Vātsyāyana’s comments (see: epistemology)\n\n\n\nD: the dragon guarding the bridge\nU: Uddyotakara, a philosopher in the Nyāya school of philosophy, running for their life from wolves. A proponent of philosophical realism\n\nU: Great dragon! Please let me pass you so I can cross the bridge.\nD: But how do you know there’s a bridge here? What if your senses deceive you?\nU: Turn around and see for yourself! Can you not see clearly? There is clearly a rope bridge right behind you.\n[The Dragon turns around.]\nD: Hm.. there does not appear to be a bridge. How can you be sure you are not hallucinating a bridge?\nU: I have trust in my vision — it is a reliable epistemic instrument. Why doubt my sight now when it has served me faithfully so many times before? I trust that my eyes see I bridge so I believe there is a bridge for me to cross the ravine.\nRational inquiry requires purpose. We do not doubt everything, lest we not trust the ground beneath us. (Nyāyasūtra 4.2.33).\nD: You seem to rely heavily on inductive principles in your reasoning. One cannot infer that “my vision will always be faithful” given that it has been faithful in the past. How do you convince yourself that this bridge is not a black swan? Or that bridges exist at all? Perhaps nothing exists at all.\nU: Vision is a subset of perception, which is a pramāna. It is a means of knowing that is gained through “close examination of objects through cognition” (Nyāyasūtra 4.2.29). With the existence of pramānas the thesis “Nothing exists” cannot possibly be true.\nLet us suppose the claim were true and it was supported by a pramāna. In which that case, that very pramāna would contradict that claim. If there were no pramāna to support this claim, then the thesis could not be proved (Nyāyasūtra 4.2.30). Thus, the claim is false. Pramānas must exist and some things must be real.\nD: Yet, you cannot be sure that the bridge is real. Your vision can deceive you. Have you not seen a simple magic trick? How do you know the bridge is not a visual illusion or dream?\nU: Well, first I must get close enough to examine the bridge closely. If it was a visual illusion, I can consult another sense like touch to reinforce my trust in it. But even if it was, say a dream, then I could wake up from it! Both dream wolf and dream bridge would cease to matter.\nThe concept of being chased by wolves and the concept of a rope bridge may have been real, but the physical risk associated with them would be dissolved. In a dream, things are still real in the conceptual sense. For something to have been in a dream, I must have had the essence of the object as a prior. There is no concept ‘dream wolf’ if there is no concept of ‘wolf’ (Nyāyasūtra 4.2.34-35).\nD: I concede then that conceptually the bridge must exist. Yet, how do you reconcile that with the real and the physical? What if that bridge is actually broken? What if you mistook the bridge for a fallen log?\nU: That is to say that my cognition of the bridge is erroneous? Do you believe there to be nothing in its place? I cannot make a mistake about what it is unless there is something there that I could be wrong about. What do you see instead?\n[The Dragon takes another glance at the ravine.]\nD: I see a log. There is no concept of ‘bridge’ behind me.\nU: This is quite the anti-realist argument — that because you and I see two different things when referring to the same object (due to language understanding, presupposed knowledge, etc.) then the object must therefore have no ‘true nature’ (Nyāyasūtra 4.2.37).\nYet, to even be wrong about something, one needs to mistake something ¬F for something F (to see a post as a person). This, of course, happens when the differences between ¬F and F are ignored while their similarities are grasped.\nWhile either one of us could be wrong about the true nature of the thing in the ravine, it is undoubtable that there is something there which has the essence of ‘bridge’ and ‘log’ and should be usable to cross the ravine.\nD: Well, seeing as there is something for you to cross the ravine with, you must get going now. The wolves are getting close, I shall let you cross.\n[The Dragon steps aside.]"},"thoughts/The-Grasshopper,-Games,-Life-and-Utopia":{"title":"The Grasshopper: Games, Life and Utopia","links":["thoughts/games","thoughts/utopia","thoughts/play","thoughts/art","thoughts/positive-sum","thoughts/telic-action","thoughts/zero-sum","thoughts/paratelic-action"],"tags":["seed","book"],"content":"“It is the attempt to discover and formulate a definition, and to follow the implications of that discovery even when they lead in surprisingly, and sometimes disconcerting, directions.”\nThe Grasshopper\nA fun Platonic dialogue poised as a tale between insects. The grasshopper, about to die for winter, leaves his disciples with a few riddles. Within these tales, the grasshopper details his philosophy of “refusing to work and insisting upon devoting himself exclusively to play”. Of course, his disciples challenge him about this, leading to a journey of defining what games are, attitudes toward playing games, and the role of games in utopias.\n(When talking about why the author chose to write the book as a Platonic dialogue) “His refusal to express himself in a plain expository style is perhaps no different in principle from someone’s setting out to write an entire book without using the letter e”\nUtopia and Scarcity\n“For one cannot help reflecting that if there were no winters to guard against, then the Grasshopper would not get his come-uppance nor the ant his shabby victory. The life of the grasshopper would be vindicated and that of the ant absurd”\n“Thus, although time is a finite quantity for everyone, it is not a limited resource for everyone. For a bored person time is a burden; for a person on the rack it is agony. And when time is a resource for someone it is not always a limited resource. For a person with very few goals there is always enough time to accomplish all of them.”\n”Play is necessary but not sufficient adequately to account for the ideal of existence”\nA post-work society\nLet us imagine that all of the instrumental activities of human beings have been eliminated. All of the things ordinarily called work are now done by wholly automated machines which are activated solely by mental telepathy, so that not even a minimum staff is necessary for the housekeeping chores of society.\n“You talk as though there were but two possible alternatives: either a life devoted exclusively to play or a life devoted exclusively to work. But most of us realize that our labour is valuable because it permits us to play, and we are presumably seeking to achieve some kind of balance between work input and play output. People are not, and do not want to be, wholly grasshoppers or wholly ants, but a combination of the two; people are and want to be (if you will forgive a regrettably vulgar but spooneristically inevitable construction) asshoppers or grants. We can, of course, all cease to work, but if we do then we cannot play for long either, for we will shortly die.”\nArt and Pursuit of Knowledge\nArt has a subject matter which consists in the actions and passions of humanity: with human aspirations and frustrations, hopes and fears, triumphs and tragedies, with flaws of character, moral dilemmas, joy and sorrow. But it would seem that none of these necessary ingredients of art could exist in Utopia, thus there are no artists of any sort in Utopia.\nThe acquisition of knowledge, just like the acquisition of anything else, is an instrumental process; that is acquisition is instrumental to possession, no matter what it is. We must therefore assume that all Utopians have acquired all the knowledge there is. Thus, there are no scientists, philosophers, or any other intellectual investigators.\nWe can call this state of affairs the Alexandrian Condition of Man, after Alexander the Great. When there are no more worlds to conquer, we are not filled with satisfaction but with despair.\nThus in Utopia, we need therefore is some activity in which what is instrumental is inseparably combined with what is intrinsically valuable, and where the activity is not itself is not itself an instrument for some further end: games. Games have obstacles which we can strive to overcome just so that we can possess the activity as a whole (playing the game).\nThe counter-argument though, is the existence of individuals value the means as much as the ends themselves, if not more. Once a scientist or philosopher after great effort solves a major problem he is very let down, and far from rejoicing in the possession of his solution or discovery, he cannot wait to be engaged once more in the quest. Success is something to shoot at, not to live with. This seems to dismantle the previous argument dismissing art and pursuit of knowledge as important in a post-work society.\nThe resolution to this appears to be the fact that activities which from one point of view be seen as instrumentally valuable can, from another point of view, be intrinsically valuable. For example, we can agree that carpentry is an instrumental activity; that is, instrumental to the existence of houses. But to a person who enjoys building for its own sake, that otherwise instrumental activity has intrinsic value as well. The same could be true of anyone who really enjoys their work, whatever that work might be. I then posit this type of work as game playing so thus can exist in Utopia.\nEven in a purely abundant world, one can create scarcity for themselves through imposing constraints [constitutive rules]. The dedicated puzzle solver will say, “Don’t tell me the answer; let me work it out for myself.” Even if other means for coming to know the answer are readily available, he voluntarily rejects these means so that he will have something to do. This is definition of game playing.\nWhereas our own culture is based on various kinds of scarcity — economic, moral, scientific, erotic — the culture of Utopia will be based on plentitude and abundance. A utopic society would not study economics but rather agalmics.\nGames\nDefinitions:\n\n“Let us say that games are goal-directed activities in which inefficient means are intentionally chosen.”\n“A game is an activity in which observance of rules is part of the end of the activity, and where such rules are non-ultimate; that is, where other rules can always supersede. the game rules; that is, where the player can always stop playing the game”\n\nThen, to play a game is to attempt to achieve a specific state of affairs [prelusory goals], using only means permitted by rules [lusory means], where the rules prohibit the use of more efficient in favour of less efficient means [constitutive rules], and where the rules are accepted just because they make possible such activity possible [lusory attitude] (this is to say that rules are sufficient to make possible a game but not required). Simplified, playing a game is the voluntary attempt to overcome unnecessary obstacles.\n“Rules in games thus seem to be in some sense inseparable from ends, for to break a game rule (a means) is to render impossible the attainment of an end.”\n“In anything but a game the gratuitous introduction of unnecessary obstacles to the achievement of an end is regarded as a decidedly irrational thing to do, whereas in games it appears to be an absolutely essential thing to do.”\nGames which are competitive (read: telic) involve winning and losing — that is the aim or end of the game in the first place. The win of one implies the loss of another, a zero sum world. Playing, though, are the means which are ends themselves; the act of playing is enough and paratelic in nature. Thus, someone who plays games as an end is autotelic.\n\nDo you like the act of climbing the mountain or just being at the top? Because the latter doesn’t require the first.\n\nGames, simply put, reverse the ends and means of other activities. In Kant’s Critique of Aesthetic Judgment, he likens aesthetic experience to play as a kind of ‘purposiveness without purpose’\nOpen and Closed Games\n\nOpen games: system of reciprocally enabling moves whose purpose is the continued operation of the system.\nClosed games: games which have an inherent goal whose achievement ends the game. e.g. crossing a finish line, mating a king, etc.\n\nDisagreement in games then, usually comes from the disagreement over whether a certain game is open or closed.\n“We might expect societies which place a high value on success through co-operation to be more inclined to emphasize open games”\nDistinction between rules and goals\nIt is possible to follow the rules of chess [the institution] without playing chess [the game]. On the contrary, one cannot play chess without following the rules (to do so is to be a cheat).\nTerminology\n\nPrelusory goals: specific achievable state of affairs. They can be described before, or independently of, any game of which it may be, or come to be, a part.\nLusory goals: winning the game. Game specific.\nLusory means: means which are permitted (legal or legitimate) in the attempt to achieve prelusory goals.\nConstitutive rules: rules which prohibit use of the most efficient means for reaching a prelusory goal.\nLusory attitude: the acceptance of constitutive rules just so the activity made possible by such acceptance can occur.\n\nLusory Attitude — the game attitude\nThe element which unifies the other elements into a single formula which successfully states the necessary and sufficient conditions for any activity to be an instance of game playing. The elements of game are\n\nThe goal\nThe means of achieving the goal\nThe rules\nThe lusory attitude\n"},"thoughts/The-Machiavelli-Effect":{"title":"The Machiavelli Effect","links":["thoughts/Where-is-My-Flying-Car","thoughts/Overton-Window"],"tags":["seed"],"content":"From Where is My Flying Car. See also: Overton Window\nNiccolo Machiavelli in his 1532 masterpiece The Prince:\n\n“It ought to be remembered that there is nothing more difficult to take in hand, more perilous to conduct, or more uncertain in its success, than to take the lead in the introduction of a new order of things. Because the innovator has for enemies all those who have done well under the old conditions, and lukewarm defenders in those who may do well under the new. This coolness arises partly from fear of the opponents, who have the laws on their side, and partly from the incredulity of men, who do not readily believe in new things until they have had a long experience of them. Thus it happens that whenever those who are hostile have the opportunity to attack they do it like partisans, whilst the others defend lukewarmly, in such wise that the prince is endangered along with them.”\n\nIt can be considered a sort of immune respond of the existing social and economic system as a means to keep power.\nAlso observed by Isaac Asimov:\n\n“I discovered, to my amazement,  that all through history there had been resistance — and bitter, exaggerated, last-ditch resistance — to ever significant technological change that had taken place on earth. Usually the resistance came from those groups who stood to lose influence, status, money, as a result of the change. Although they never advanced this as their reason for resisting it. It was always the good of humanity that rested upon their hearts.”\n"},"thoughts/The-Midnight-Library":{"title":"The Midnight Library","links":["thoughts/optionality","thoughts/life"],"tags":["seed","book"],"content":"by Matthew Haig\nA really good fiction read on optionality, the meaning of life and what it means to be a truly happy with your life.\n\n“I think it is easy to imagine there are easier paths,” she said, realising something for the first time. “But maybe there are no easy paths. There are just paths. In one life, I might be married. In another, I might be working in a shop. I might have said yes to this cute guy who asked me out for a coffee. In another I might be researching glaciers in the Arctic Circle… Who knows? Every second of every day we are entering a new universes. And we spend so much time wishing our lives were different, comparing ourselves to other people and to other versions of ourselves, when really most lives contain degrees of good and degrees of bad.”\n\n\n“There are patterns to life… Rhythms. It is so easy, while trapped in just one life, to imagine that times of sadness or tragedy or failure or fear are a result of that particular existence. That it is a by-product of living a certain way, rather than simply living… sadness is intrinsically part of the fabric of happiness.”\n\n\n“She realized that you could be as honest as possible in life, but people only see the truth if it is close enough to their reality.” As Thoreau wrote, “It’s not what you look at that matters, it’s what you see.”\n"},"thoughts/The-Mythical-Man-Month":{"title":"The Mythical Man-Month","links":["thoughts/desktop-metaphor"],"tags":["seed","book"],"content":"Quotes\n\nMen and months are interchangeable commodities only when a task can be partitioned among many workers with no communication among them. The bearing of a child takes 9 months, no matter how many women are assigned.\nThe second system an engineer ever builds is the most dangerous one they ever design; the general tendency is to over-design it.\nBrook’s law: Adding manpower to a late software project makes it later.\n“Never go to sea with two chronometers; take one or three.” The same thing clearly applies to prose and formal definitions. If one has both, one must be the standard and the other must be a derivative description, clearly labeled as such.\nThe programmer at wit’s end can often do best by disentangling himself from his code, rearing back, and contemplating his data. Representation is the essence of programming.\nThe reluctance to document designs is not due merely to laziness or time pressure. Instead it comes from the designer’s reluctance to commit himself to the defense of decisions which he knows to be tentative. “By documenting a design, the designer exposes himself to the criticisms of everyone, and he must be able to defend everything he writes. If the organizational structure is threatening in any way, nothing is going to be documented until it is completely defensible.”\n\nThe corollary being that nothing meaningful or forward-leaning is completely defensible so will never be documented.\n\n\nNiklaus Wirth’s refinement steps:\n\nOne sketches a rough task definition and a rough solution method that achieves the principal result.\nThen, one examines the definition more closely to see how the result differs from what it wanted, and one takes the large steps of the solution and breaks them down into smaller steps.\nEach refinement in the definition of the tasks becomes a refinement in the algorithm for the solution, and each may be accompanied by a refinement in the data representation.\nFrom this process one identifies modules of solution or of data whose further refinement can proceed independently of other work.\n\n\n“Coding is ‘90 percent finished’ for half of the total coding time. Debugging is ‘99 percent complete’ most of the time. ‘Planning complete’ is an event one can proclaim almost at will.”\nTo write a useful prose description, stand way back and come in slowly:\n\nPurpose: what is the main function, the reason for the program?\nEnvironment: on what machines, hardware configurations, and operating system configurations will it run?\nDomain and range: what domain of input is valid? What range of output can legitimately appear?\nFunctions realized and algorithms used: precisely what does it do?\nInput-output formats: precise and complete.\nOperating instructions: including normal and abnormal ending behaviour, as seen at the console and on the outputs.\nOptions: what choices does the user have about functions? Exactly how are those choices specified?\nRunning time: how long does it take to do a problem of specified size on a specified configuration?\nAccuracy and checking: how precise are the answers expected to be? What means of checking accuracy are incorporated?\n\n\nAutomatic programming always has been a euphemism for programming with a higher-level language than was presently available to the programmer.\nThe screens of today are too small, in pixels, to show both the scope and the resolution of any serious detailed software diagram. The so-called ”desktop metaphor” of today’s workstation is instead an “airplane-seat” metaphor. Anyone who has shuffled a lapful of papers while seated in coach between two portly passengers will recognize the difference — one can see only a very few things at once… Moreover, when fits of creativity run strong, more than one programmer or writer has been known to abandon the desktop for the more spacious floor.\nEven perfect program verification can only establish that a program meets its specification. The hardest part of the software task is arriving at a complete and consistent specification, and much of the essence of building a program is in fact the debugging of the specification.\nGrowing complex software systems via incremental development.\n\nThe system should first be made to run, even though it does nothing useful except call the proper set of dummy subprograms.\nThen, bit by bit it is fleshed out, with the subprograms in turn being developed into actions or calls to empty stubs in the level below.\nThe morale effects especially are startling. Enthusiasm jumps when there is a running system, even a simple one. Efforts redouble when the first picture from a new graphics software system appears on the screen, even if it is only a rectangle. One always has, at every stage in the process, a working system.\n\n\nThe principle of subsidiary function teaches us that the center will gain in authority and effectiveness if the freedom and responsibility of the lower formations are carefully preserved, with the result that the organization as a whole will be “happier and more prosperous.” How cans such a structure be achieved?… The large organization will consist of many semi-autonomous units, which we may call quasi-firms. Each of them will have a large amount of freedom, to give the greatest possible chance to creativity and entrepreneurship… each quasi-form must have both a profit and loss account, and a balance sheet.\n"},"thoughts/The-Psychopathology-of-Everyday-Things":{"title":"The Psychopathology of Everyday Things","links":["thoughts/human-computer-interaction","thoughts/interaction-design"],"tags":["seed"],"content":"\n“The design of the door should indicate how to work it without any need for signs, certainly without any need for trial and error.”\n\nTwo of the most important characteristics of good design:\n\nDiscoverability: is it possible to even figure out what actions are possible and where and how to perform them?\nUnderstanding: what does it all mean? How is the end product supposed to be used?\n\nCovers three major areas of design\n\nIndustrial Design: creating/developing concepts and specs that optimize function, value, and appearance of products and systems for the mutual benefit of both user and manufacturer\nInteraction Design: enhance people’s understanding of what can be done, what is happening, and what has just occurred\nExperience Design: practice of designing products, processes, services, events, and environments with a focus placed on the quality and enjoyment of the total experience\n\nThe machine does what it is told, no matter how insensible and illogical. Humans, on the other hand, are imaginative and creative, filled with common sense. Yet, to interact with machines, they require us to be precise and accurate, things we are not very good at.\nThe problem with the designs of most engineers is that they are too logical. We have to accept human behaviour the way it is, not the way we would wish it to be.\nTL;DR\n\nMost failures of human-machine systems are due to poor designs rather than human error itself.\nGood design accounts of human limitations.\n\nRelevant for human computer interaction and interaction design"},"thoughts/The-Sword-of-Damocles":{"title":"The Sword of Damocles","links":["thoughts/Scientific-Freedom"],"tags":["seed"],"content":"Excerpted from Scientific Freedom, originally written by Marcus Tullius Cicero in Tusculan Disputations, Book 5 and translated by C. D. Yonge in 1877:\n[King Dionysius II], however, showed himself how happy he really was [among his endless riches and spoils]; for once when Damocles, one of his flatterers, was dilating in conversation on his forces, his wealth, the greatness of his power, the plenty he enjoyed, the grandeur of his royal palaces, and maintaining that no one was ever happier.\n“Have you an inclination”, said [Dionysius], “Damocles, as this kind of life pleases you, to have a taste of it yourself, and to make a trial of the good fortune that attends me?” And when [Damocles] said that he should like it extremely, Dionysius ordered him to be laid on a bed of gold with the most exquisite work, and he dressed out a great many sideboards with silver and embossed gold. He then ordered some youths, distinguished for their handsome persons, to wait at his table, and to observe his nod, in order to serve him with what he wanted. There were ointments and garlands; perfumes were burned; tables provided with the most exquisite meats.\nDamocles thought himself very happy. In the midst of this apparatus, Dionysius ordered a bright sword to be let down from the ceiling suspended by a single horse-hair, so as to hang over the head of that happy man. After which he neither cast his eye on those handsome waiters, nor on the well-wrought plate; nor touched any of the provisions: presently the garlands fell to pieces. At last he entreated [Dionysius] to give him leave to go, for that now he had no desire to be happy.\nDoes not Dionysius, then, seem to have declared there can be no happiness for one who is under constant apprehensions?"},"thoughts/The-Upanisads":{"title":"The Upanisads","links":["thoughts/the-Self","thoughts/Descartes'-Meditations","thoughts/consciousness","thoughts/Dreams","thoughts/death"],"tags":["seed","PHIL240A"],"content":"A collection of texts composed over several centuries (the oldest from ca. 700-300 BCE) and in various regions (northern India, ranging from the upper Indus valley to the lower Ganges).\nTwo interpretations\n\nUpa(near)+ ni(down) + sad(sit): “to sit close beside” The secret teaching passed orally from teacher to disciple\n“Connection” or “equivalence” secret knowledge or hidden connections (the Upanisads being the texts containing those doctrines)\n\nBṛhadāraṇyaka Upaniṣad 4.3-4.4\nA dialogue between\n\n(Y) Yājñavalkya: a great teacher of secret doctrines. Learned, sarcastic, and irreverent\n(K) King Janaka: a great and learned King from Videha\n\nK asks Y “What light does a person have?” and Y responds. K keeps asking “What light does a person have when that is gone?”\nThe order of Y’s responses is as follows:\n\nThe sun\nThe moon\nFire\nSpeech\nThe Self (see: Cartesian Realism)\n\nInteresting to note that this moves from far away to closeby in terms of spatial distance (astronomically far to the intimate self)\nThen K asks, “What is the Self?“. Y answers “the inner light that is the person” (puruṣa) or consciousness as it travels through 3 states. (Map of consciousness, four aspects/“quarters” of the self)\n\nWaking: person in its physical nature, awareness of external objects, perception and thought, identification with the body\nDreaming: person in its mental nature, awareness of dream images (mental impressions, memories), identification with the dream ego\nDeep and dreamless sleep: person beyond desire, peace and bliss, absence of identification\nPure Awareness (Māṇḍūkya Upaniṣad) (added in a latter Upaniṣad): ground state of consciousness\n\nThis is where the OM/AUM sound comes from. A from waking, U from dreaming, M from deep sleep, and pure awareness as a combination of all 3\nDeath is described as the dissolution of vital functions (prāna) of the mind and body which then culminates in pure awareness and transitioning to rebirth\nLife, then, is impelled by desire which leads to rebirth according to karma. Death is liberation for those who have freed themselves from desire (and achieve nothing but brahman or pure light)"},"thoughts/The-Writing-Life":{"title":"The Writing Life","links":[],"tags":["seed","book"],"content":"By Annie Dillard\nQuotes\nOn throwing away early work:\n\nA painting covers its tracks. Painters work from the ground up. The latest version of a painting overlays earlier versions, and obliterates them. Writers, on the other hand, work from left to right. The discardable chapters are on the left. The latest version of a literary work begins somewhere in the work’s middle, and hardens toward the end. The earlier version remains lumpishly on the left; the work’s beginning greets the reader with the wrong hand. In those early pages and chapters anyone may find bold leaps to nowhere, read the brave beginnings of dropped themes, hear a tone since abandoned, discover blind alleys, track red herrings, and laboriously learn a setting now false.\n\n\nThe work is not the vision itself, certainly. It is not the vision filled in, as if it had been a colouring book. It is not the vision reproduced in time; that were impossible. It is rather a simulacrum and a replacement. It is a golem.\n\nOn rest:\n\nOctavio Paz cites the example of “Saint-Pol Roux, who used to hang the inscription ‘The poet is working’ from his door while he slept”\n\nOn spending time wisely:\n\nHow we spend our days is, of course, how we spend our lives. What we do with this hour, and that one, is what we are doing. A schedule defends from chaos and whim.\n\nOn learning to write:\n\nWho will teach me to write? a reader wanted to know.\nThe page, the page, that eternal blankness, the blankness of eternity which you cover slowly, affirming time’s scrawl as a right and your daring as necessity; the page, which you cover woodenly, ruining it, but asserting your freedom and power to act, acknowledging that you ruin everything you touch but touching it nevertheless, because acting is better than being here in mere opacity; the page, which you cover slowly with the crabbed thread of your gut; the page in the purity of its possibilities; the page of your death, against which you pit such flawed excellences as you can muster with all your life’s strength: that page will teach you to write.\n\n\nA well-known writer got collared by a university student who asked, “Do you think I could be a writer?”\n“Well,” the writer said, “I don’t know… Do you like sentences?”\nThe writer could see the student’s amazement. Sentences? Do I liked sentences? I am twenty years old and do I like sentences? If he had liked sentences, of course, he could begin, like a joyful painter I knew. I asked him how he came to be a painter. He said, “I liked the smell of paint”\n\nOn being in the rut:\n\n“You asked how my work is going.” he said. “That’s how it’s going. The current’s got me. Feels like I’m about in the middle of the channel now. I just keep at it. I just keep hoping the tide will turn and bring me in.”\n\nJust beautiful:\n\nEach sentence hung over an abyssal ocean of sky which held all possibilities, as well as the possibility of nothing.\n"},"thoughts/The-ones-who-walk-away-from-Omelas":{"title":"The Ones Who Walk Away From Omelas","links":["thoughts/Utilitarianism","thoughts/utopia","thoughts/crutch-and-shoe-metaphor","thoughts/counterculture","thoughts/From-Counterculture-to-Cyberculture","thoughts/virtual-worlds","thoughts/pain"],"tags":["seed"],"content":"Source: The Ones Who Walk Away From Omelas by Ursula Le Guin\nThere is no happiness without suffering. The story presents a classic utilitarian problem: is it morally justifiable to inflict suffering on one person in the service of others’ happiness (and a potential utopia)?\nThinking about it in terms of technology as a multiplicative tool. Is it then morally just to develop technology to benefit others knowing that it will exacerbate the suffering of marginalized groups?\nFollow up: is there any way we can use tech as a running shoe instead of a crutch?\nWhat does it mean to walk away from Omelas?\nJoining the counterculture rather than feeding into the status quo: From Counterculture to Cyberculture. Rejecting the capitalist society and ‘returning to the land’\nImmersion in the virtual worlds rather than reality?\nQuotes\n“The trouble is that we have a bad habit, encouraged by pedants and sophisticates, of considering happiness as something rather stupid. Only pain is intellectual, only evil interesting.”\nSome of them understand why, and some do not, but they all understand that their happiness, the beauty of their city, the tenderness of their friendships, the health of their children, the wisdom of their scholars, the skill of their makers, even the abundance of their harvest and the kindly weathers of their skies, depend wholly on this child’s abominable misery.\n“To exchange all the goodness and grace of every life in Omelas for that single, small improvement: to throw away the happiness of thousands for the chance of the happiness of one: that would be to let guilt within the walls indeed.”"},"thoughts/The-purpose-of-a-system-is-what-it-does":{"title":"The purpose of a system is what it does","links":["thoughts/systems-design"],"tags":["seed","pattern"],"content":"The purpose of a system is what it does (POSIWID) is a systems thinking heuristic coined by Stafford Beer, who observed that there is “no point in claiming that the purpose of a system is to do what it constantly fails to do.”\nThe term is widely used by systems design theorists, and is generally invoked to counter the notion that the purpose of a system can be read from the intentions of those who design, operate, or promote it."},"thoughts/Theory-of-Niche-Construction":{"title":"Theory of Niche Construction","links":["thoughts/agency","thoughts/terminology","thoughts/generational-learning","thoughts/collective-intelligence"],"tags":["sapling"],"content":"Theory of Niche Construction\nMany animals intervene in their environment, shaping it in ways that improve the adaptive fit between the agent and its world\nSuch animals in part adapt to their niche, in part construct their own\nThe niche construction perspective focuses our attention on the common features of this whole range of cases whereas the extended mind model does not\nHuman capacities, cognitive and non-cognitive alike, turn out to depend on the fact that humans engineer their environment to support their activities (see: agency)\nExtended digestion example: some animals do the hard digestion stuff on-board, powerful jaws, large mouths, lots of time chewing. Others (like humans), just cook and selectively breed livestock which improves the food value of domestic stock\nExtended Phenotypes Concept\nThings animals build are part of their phenotype (physical exhibited traits that are determined genetically)\nThey are developmentally stable and as heritable and predictable in their ecological effects as other traits (e.g. wasp nests, beaver dams, spider webs)\nArguably, language and terminology are extended phenotypes\nLike beaver dams, these technologies have evolved by cumulative trail and error but the mechanism of inheritance is cultural rather than genetic\nInheritance is not strictly vertical, it can be oblique and many-to-one (information flow from many members of the parental generation — and from each other) (see: intergenerational learning)\nThe cognitive competence of generation N+1 individually and collectively depends on cognitive provisioning by generation N\nSee also: collective intelligence"},"thoughts/Theseus-DHT":{"title":"Theseus DHT","links":["thoughts/Kademlia-DHT","thoughts/TCP"],"tags":["seed"],"content":"Anyone can store data in the DHT and receive an estimate of how long that data will be stored. Once stored, data is very hard to remove or modify. Small data is stored longer; this makes the DHT well-suited for exchanging things like lists of peers, signed cryptographic hashes, compressed text, and so on.\nRouting is based on Kademlia DHT but with better security properties. It runs over TCP\nSecurity\n\nAll protocol traffic is indistinguishable from random noise. Length-prefixing schemes are used on both protocol ciphertexts and plaintexts, and messages may be padded to any degree. This allows arbitrary message chunking, which is essential for traffic obfuscation. All this is meant to make the protocol very hard to fingerprint\n"},"thoughts/Three-Legged-Stool":{"title":"Three Legged Stool","links":["thoughts/plurality","thoughts/Syndication","thoughts/interoperability","thoughts/LLMs","thoughts/composable","thoughts/explainability"],"tags":["seed"],"content":"Source\nA Manifesto for a Smaller, Denser Internet\n\nA truly sustainable and resilient digital public sphere consists of many different platforms with a wide variety of sizes and purposes, that users can navigate with a loyal client that aggregates, cross-posts, and curates, all supported by cross-cutting services rooted in interoperable data\n\nInspired by an example from ecology: the Miyawaki method. A small movement is underway in ecological restoration to bring biodiversity back to environs either devastated by urban development or monoculture farming\nLoyal Client\nThis is an old idea which seeks to solve a common problem on the internet: what you want to do as a user, and what a web server may want to do, often come into conflict.\nWeb browsers block pop-ups and ad trackers because the server wants a user’s attention and data, and the user would rather not hand them over. Email clients block spam, allow you to customize your inbox, and give you new tools like schedulers and auto-complete because users want control over how they interact with email servers.\nAdverserial Interoperability\nSee: Adverserial Interoperability\nRegulations that support interoperability are another way to address platforms that refuse to interoperate voluntarily. The EU recently passed a regulation, the Digital Markets Act, that mandates large platforms make their messaging interoperable. Other legislation could similarly require that social media platforms make their content available to third-party clients, or offer protections for adversarial interoperability (similar to right to repair laws).\nAlgorithm Store\nPlugin marketplace\nOn order for a loyal client or a VSOP to serve an individual’s preferences, they need to be able to select from a wide variety of algorithms that reflect those preferences (perhaps will eventually be taken over by LLMs that can create these on-demand).\nShould satisfy 4 properties:\n\nTuneable: you should have meaningful settings that allow you to use the algorithm in different ways.\nAuditable: you should be able to throw arbitrary content at the algorithm and see how it responds. You should be able to investigate the data underlying the algorithm and the decisions it made.\nCombinable: you should be able to use more than one algorithm and have them work together. Algorithms should be able to work across different source platforms.\nUnderstandable: you should have a sense of what the algorithm is trying to do and how you can control it.\n\nSimilar to geists in Subconscious"},"thoughts/To-Live-in-their-Utopia":{"title":"To Live in their Utopia","links":["thoughts/creation-vs-maintenance","thoughts/Design-Justice","thoughts/utopia","thoughts/The-ones-who-walk-away-from-Omelas","thoughts/truth","thoughts/feedback-loops","thoughts/quantization","thoughts/potemkin-village","thoughts/data-distributions","thoughts/map-as-territory","thoughts/Seeing-like-a-State","thoughts/frame-problem","thoughts/traditional-knowledge"],"tags":["seed"],"content":"Paper and Video Essay\nDesign and Development of Systems\nCreation vs Maintenance view of developing AI:\n\nCreation: “focus on finding new places and ways to use technologies and new insights that AI might yield when ML is applied to massive datasets to find relationships in the data”\nMaintenance: “surfaces problems with existing systems and attempts to mitigate those harms (for instance, by making them more fair, accountable, and transparent)”\n\n“When designers of these algorithmic systems train computational models that ignore transgender identity, these systems demand that trans people somehow shed an identity they can’t; identities that cisgender people hardly ever bother to regard.”\n“Designers of sociotechnical systems have repeatedly built computational systems and models rendering decisions that exacerbate and reinforce historical prejudices, oppression, and marginalization”\nFor those of us who can just not deal with race, or gender, or sexuality, we get to pass through these systems relatively unscathed. But for those of us who can’t ignore those dimensions of who we are, those aspects of ourselves make us stick out. More examples in Design Justice.\nUtopia\nA utopia implies perfection and thus no feedback. ML models think they live in a perfect world unless told otherwise.\nRelated: The ones who walk away from Omelas\n’Truth’ and Feedback loops\n“Absurdity follows when algorithmic systems deny the people they mistreat the status to lodge complaints, let alone the power to repair, resist, or escape the world that these systems create.” How do feedback loops play into these systems? Is it possible to create good human-in-the-loop ML?\n“Absurdity and tragedy tend to manifest when bureaucratic imaginations diverge from reality and when people can’t override the delusions baked into those imaginations” It’s dangerous when a single source dictates the truth.\nBut when the institution does wield power and people can’t just leave anymore, these institutions can (and do) get more and more detached from the lives and needs of people. Those bureaucracies construct their own worlds where everything gets “rationalized” in simplified, reductive language.\n“People talk about “debiasing” data and reviewing code before a model is trained and deployed. What I’m saying is that even if you’ve done everything right, if you don’t pay attention to the power dynamics as they unfold and play out, the system out in the world is going to drift further and further away from reality.”\nSystemized classification and quantification of the world acts as an interpretive and transformational force. In other words, quantization changes the world.\nWhy monopolies (over data and power) are bad: bureaucracies with no power self-correct (or be corrected) → they have no place in a world where people can freely walk away or reject the bureaucracy’s nonsense (give feedback)\nAbridged Maps\nAbridged maps as potemkin villages, producing a simplified yet inaccurate view of the world. It’s not necessarily wrong to create ‘abridged maps’, the problem comes when projecting the map onto the world to try and create change.\n“When modelers and designers of influential systems use these maps as guides to substantially transform the world, the abridgements and the omissions they make become targets of erasure.”\n“In the process of training a model, the algorithm creates its own world — it generates its own sort of utopia where things are clear and calculable. That system imposes its model upon the world, judging and punishing people who don’t fit the model that the algorithm produced in the interest of some ostensibily objective goal that designers insist is better than decisions humans make in some or many ways.”\nThese systems become more actively dangerous when they go from “making sense of the world” to “making the world make sense”\nThere’s no dataset in the world that adequately conveys white supremacy, or slavery, or colonialism. (see: data distributions)\nSo at best these systems generate a facsimile of a world with the shadows of history cast on the ground skewed, flattened, and always lacking depth that only living these experiences can bring. Once again, creating a potemkin village of what the true problem is: an incredibly reductionist view on complex problems.\nSee also: map as territory\nMetis\nJames C. Scott in Seeing like a State desribes metis, which he translates substantively as the intelligence required to adapt to new and changing circumstances.\nMetis is more than constructing any number of “rules of thumb”. Rather, knowing how and when to apply those rules in a concrete situtation is the essense of metis. Isn’t metis then just the frame problem?\n“A person without the lived experience of disabilities can never truly understand what it means to be ‘like’ someone who experiences it.” Disability simulation doesn’t work; why do we let ML systems do it then, let alone systems without metis?\nImportant in the context of traditional knowledge (TK)"},"thoughts/Tomorrow,-and-Tomorrow,-and-Tomorrow":{"title":"Tomorrow, and Tomorrow, and Tomorrow","links":["thoughts/games","thoughts/friendship"],"tags":["seed","book"],"content":"On games, friendship, grief, and the creative process.\nby Gabrielle Zevin\n\n“You’re incredibly gifted, Sam. But it is worth noting that to be good at something is not quite the same as loving it.”\n“‘Life is very long, unless it is not.’ Sadie knew this to be a tautology, but it also happened to be true.”\n“One of Sam’s eventual strengths as an artist and as a businessman was that he knew the importance of drama, of setting the scene. He wanted to ask her to work with him at a special place — the occasion of their prospective creative union should be memorable.”\n“There is a time for any fledgling artist where one’s taste exceeds one’s abilities. The only way to get through this period is to make things anyway.”\n“Sometimes, I would be in so much pain. The only thing that kept me from wanting to die was the fact that I could leave my body and be in a body that worked perfectly for a while — better than perfectly, actually — with a set of problems that were not my own.”\n\n“Sam’s grandfather had two core beliefs: (1) all things were knowable by anyone, and (2) anything was fixable if you took the time to figure out what was broken”\n“She was intelligent, but her intelligence didn’t get in her way of her enthusiasm”\n“Computers are great for experimentation, but they’re bad for deep thinking”\n“Though you cannot see him, you become aware of the fact that your father is sitting on the floor. He is folding cranes so that your mother can string them. This is marriage.”\n“What is a game? It’s tomorrow, and tomorrow, and tomorrow. It’s the possibility of infinite rebirth, infinite redemption. The idea that if you keep playing, you could win, No loss is permanent, because nothing is permanent, even.”\n\nOn Unfair Games almost being called Tomorrow Games\n\n\n“To make a game is to imagine the person playing it”\n“She had once read in a book about consciousness that over the years, the human brain makes an AI version of your loved ones. The brain collects data, and within your brain, you host a virtual version of that person. Upon the person’s death, your brain still believes the virtual person exists, because, in a sense, the person still does. After a while, though, the memory fades, and each year, you are left with an increasingly diminished version of the AI you had made when the person was alive.”\n“Maybe it was the willingness to play that hinted at a tender, eternally newborn part in all humans. Maybe it was the willingness to play that kept one from despair.”\n\n\n\nParts that I loved that are too long to directly quote\n\nThe chapter on Marx as a bird being shot in a game juxtaposed with him actually being shot in real life and how it tied back to an earlier flashback where there is a man-sized thrush, stealing a strawberry.\nHow real all the games felt. It must have taken so long to not only write the book, but to write the games in a way that made them feel natural and not potemkin constructions\n"},"thoughts/Tools-for-Conviviality":{"title":"Tools for Conviviality","links":["thoughts/agency","thoughts/interdependence","thoughts/housing","thoughts/degrowth"],"tags":["seed","book"],"content":"\nconviviality means ‘alive with’\n\nIvan Illich on the proper use of technology to reclaim agency and practical knowledge for the average citizen.\n\nThe main TLDR; science and technology with their panoply of elite controlled knowledge and procedures have brought us so far, but cannot take us much further. They are, in fact, damaging as we approach crisis.\nThese tools suppress other ideas and systems of knowledge and concentrate control of knowledge and power in the few and the elite. As such, these tools can only provide a limited and very unsatisfactory set of answers to how we may live meaningful lives. Illich argues that these tools alienate humans from each other, moving away from communal notions of interdependence and towards increasing dependence on systems of production, killing our ability to work together and towards a better world.\nThis dependence on production creates a treadmill, encouraging the average citizen to consume more and forever be unsatisfied. The only way out, Illich posits, is through giving people convivial tools. ‘Convivial’ is used as by Illich as a technical term to designate a modern society of responsibly limited tools. Often times, this is used interchangeably with a notion of agency or autonomous discourse.\nIllich seems to advocate for a world where we consume less, depend less on systems which depend on us consuming more to survive, and depend more on each other.\n\nPeople will suddenly find obvious what is now evident to only a few: that the organization of the entire economy toward the “better” life has become the major enemy of the good life.\n\nQuotes\nAcceleration and law\n“Speed is one of the means by which an efficiency-oriented society is stratified… Fostered addiction to speed is also a means of social control.”\n“Society can be destroyed when … cancerous acceleration enforces social change at a rate that rules out legal, cultural, and political precedents as formal guidelines to pres**ent behaviour.”\n“Convivial reconstruction requires limits on the rate of compulsory change. An unlimited rate of change makes lawful community meaningless. Law is based on the retrospective judgement of peers about circumstances that occur ordinarily and are likely to occur again. If the rate of change which affects all circumstances accelerates beyond some point, such judgements cease to be valid. Lawful society breaks down. Social control does not accommodate community participation and becomes the function of experts and the elite.”\n“Judges, governments, and voters abdicate their own evidence about the necessity of resolving conflicts in a situation of defined and permanent scarcity and opt for further growth on the basis of data which they admittedly cannot fully understand”\nWatershed Moments\nOn the two watershed moments of institutions: “At first, new knowledge is applied to the solution of a clearly stated problem and scientific measuring sticks are applied to account for the new efficiency. But at the second point, the progress demonstrated in a previous achievement is used as a rationale for the exploitation of society as a whole in the service of a value which is determined and constantly revised by an element of society, by one of its self-certifying professional elites.”\nTransportation\nIt has taken almost a century to pass from an era served by motorized vehicles to the era in which society has been reduced to virtual enslavement to the car. Cars have ceased to be effective tools for mass transportation.\n\nDuring the American Civil War steam power on wheels became effective. The new economy in transportation enabled many people to travel by rail at the speed of a royal coach, and to do so with a comfort kings had no dared dream of.\nWhen transportation had passed through its second watershed, vehicles had created more distances than they helped to bridge; more time was used by the entire society for the sake of traffic than was “saved”\n\nMedicine\n\n1913, we reached the point in Western medicine where a patient had a better than 50-50 chance that trained doctors would provide better treatment than anyone else. Medicine and our expanding knowledge grew in leaps and bounds, and improvements resulted in corresponding improvements in health.\nThe point at which we shifted to keeping people alive longer, without worrying about quality. Treatment has become further and further professionalised, removed from the control of patients and their families and communities. Multiple studies in health argue this exact point — that medical knowledge can solve only a portion of health issues, the others are interconnected with society, environment, employment, housing, inequality, etc.\n\nAgency and Conviviality\n“I choose the term ‘conviviality’ to designate the opposite of industrial productivity … I consider conviviality to be individual freedom realized in personal interdependence and, as such, an intrinsic ethical value. I believe that, in any society, as conviviality is reduced below a certain level, no amount of industrial productivity can effectively satisfy the needs it creates among society’s members” (Illich seems to be very against of a ‘utopia’ like the one imagine in B.F. Skinner’s Walden Two)\n“A convivial society would be the result of social arrangements that guarantee for each member the most ample and free access to the tools of the community and limit this freedom only in favour of another member’s equal freedom”\n“What is fundamental to a convivial society is not the total absence of manipulative institutions and addictive goods and services, but the balance between those tools which create the specific demands they are specialized to satisfy and those complementary, enabling tools which foster self-realization”\nTools foster conviviality to the extent that\n\nthey can be easily used, by anybody, as often or as seldom as desired\nthey allow the user to express their meaning into action (i.e. for a purpose chosen by the user)\nthe use of such tools by one person does not restrain another from using them equally (non-rivalrous)\ntheir existence does not impose any obligation to use them\n\nSee also: agency\nRadical monopoly\n“By ‘radical monopoly’ I mean the dominance of one type of product rather than the dominance of one brand… Cars can thus monopolize traffic. They can shape a city into their image—practically ruling out locomotion on foot or by bicycle in Los Angeles. That motor traffic curtails the right to walk… constitutes radical monopoly.”\n“People will face a danger that threatens their own self-interest but not one that threatens society as a whole. Many more people are against cars than are against driving them. They are against cars because they pollute and because they monopolize traffic. They drive cars because they consider the pollution created by one car insignificant, and because they do not feel personally deprived of freedom when they drive. It is also difficult to be protected against monopoly when a society is already littered with roads, schools, or hospitals, when independent action has been paralyzed for so long that the ability for it seems to have atrophied, and when simple alternatives seem beyond the reach of the imagination. Monopoly is hard to get rid of when it has frozen not only the shape of the physical world but also the range of behaviour and of imagination.”\n“The attempt to make a better environment has turned out to be a presumptuous as the attempt to create better health, education, or communication. As a result there are now more people, most of them less at home in the world. This large population can survive because of new tools. In turn, it spurs the search for even more powerful tools, and thereby demands more radical monopoly; this monopoly, in its turn, calls for more and more education.”\nDegrowth\nSee also: thoughts on degrowth\n“Most of the present laws and present legislators, most of the present courts and their decisions, most of the claimants and their demands are deeply corrupted by an overarching industrial consensus: that more is better, and that corporations serve the public interest better than men”\n“If within the very near future man cannot set limits to the interference of his tools with the environment and practice effective birth control, the next generations will experience the gruesome apocalypse predicted by many ecologists.”\n“This expansion is maintained by the illusion that careful systems engineering can stabilize and harmonize present growth, while in fact it pushes all institutions simultaneously toward their second watershed”"},"thoughts/Transformative-Technology-Trilemma":{"title":"Transformative Technology Trilemma","links":["thoughts/progress","thoughts/Seeing-like-a-State","thoughts/degrowth"],"tags":["seed"],"content":"Source\nTransformative technologies (TTs) refer to technological advances with a high likelihood of significantly altering society. Specifically, among TTs, there are significant trade-offs between\n\nprogress: advancing technological capabilities\nparticipation: enabling public input and self-determination\nsafety: avoiding disproportionate risks\n\nThis reliably leads to a set of three failure modes.\n\nCapitalist Acceleration: sacrificing safety for progress while maintaining basic participation.\n\nParticipation comes in the form of consumer choice and investor agency\nThe downsides include proliferating risk and lack of public oversight (minimal regulation, auditing, or provision of public goods)\n\n\nAuthoritarian Technocracy: sacrificing participation for safety while maintaining basic progress.\n\nBuilt on the belief that ensuring safety requires entrusting only a few entities (individuals, companies, nation-states) with the ability to develop advanced technologies\nThe downsides include the risks of illegitimacy, the well-documented failures (see: Seeing like a State) of central planning (e.g. the economic calculation problem and the challenges of gathering representative information for centralized decision-making), and the basic injustice of autocracy\n\n\nShared Stagnation: sacrificing progress for participation while maintaining basic safety.\n\nCombines anti-technology inclinations with concerns about worsening global conditions (such as climate change, inequality, bias and discrimination) due to current trajectories of progress (see: degrowth)\nThe downsides include a lack of investment in necessary economic or technological development, and undervaluing the need for large-scale coordination, e.g. via international bodies or large-scale production.\n\n\n"},"thoughts/Transport-Layer":{"title":"Transport Layer","links":["thoughts/Application-Layer","thoughts/Network-Layer"],"tags":["seed","CPSC317"],"content":"Layer 2, the layer below the Application Layer and layer above the Network Layer\n\nUnit: Segment\nResponsibilities: Ensures data arrives in order (if required), Recovers lost data (if required), Identifies process on machine, flow control\nAdds an additional addressing space at the port level (historically a 16 bit uint from 0 to 65535)\nCan be either be either packet or stream based\n\nPacket - best effort, no established connection, no transport level delay/waiting (e.g. video, games, etc.)\nStream - pipe model, established connection, flow/congestion control, possible delays (e.g. HTTP, email, etc.)\n\n\n"},"thoughts/Turing-Test":{"title":"Turing Test","links":["thoughts/intelligence","thoughts/LLMs","thoughts/semantics"],"tags":["seed"],"content":"Dialogue-based test of intelligence\nTuring proposed that a human evaluator would judge natural language conversations between a human and a machine. The evaluator would be aware that one of the two partners in conversation was a machine. The conversation would be limited to a text-only channel, such as a computer keyboard and screen, so the result would not depend on the machine’s ability to render words as speech. If the evaluator could not reliably tell the machine from the human, the machine would be said to have passed the test\nLLMs are one of the first breakthrough technologies to have passed this test by the traditional definition\nSemantics\nAre the things the machine refers to the same things we refer to? e.g. when talking about steak, are we talking about the same steak?\nThe machine doesn’t have any real input into the world, no sense organs. It is true that the machine can discourse beautifully about, say, the scenery in New England. But it could not recognize an apple tree or an apple, a mountain or a cow, a field or a steeple, if it were in front of one"},"thoughts/Twin-Earth-Argument":{"title":"Twin Earth Argument","links":["thoughts/semantics","thoughts/terminology"],"tags":["seed"],"content":"Related to semantics and terminology\nThere exists a twin Earth that is in every respect the same as Earth but except but we call water or H2O, they call XYZ.\nMy twin on this planet is thinking of XYZ and I am thinking of H2O.\nLine of argumentation:\n\nContent determines object\nIf thought 1 and thought 2 have different objects, then they must also have different content\nTwin and I have thoughts with different content\nThoughts are individuated by their content\nTwin and I have different thoughts even though we are molecule-for-molecule duplicates\n\nThus, even though water is H2​O in all worlds, but ‘water’ doesn’t mean the same thing in W1​ and W2​"},"thoughts/Type-Theory":{"title":"Type Theory","links":["thoughts/functional-programming","thoughts/semantics"],"tags":["seed"],"content":"See also: functional programming\nSource\nChurch Types\nAlso called intrinsic types.\nIn a Church-style system, types are an intrinsic part of the semantics of the language. The language would be different without the types---it may even be meaningless.\nIntrinsic types are great because you are guaranteed to have a mathematically-sound safety net at all times.\ne.g. Haskell\nThe Church typist argues that untyped programs are a subset of typed programs. They are programs that have a single type any and all values are of that one type. So instead of being liberating, dynamic languages restrict you to one type.\nCurry Types\nAlso called extrinsic types.\nCurry-style types is when a system of types is applied that is not part of the semantics of the language.\nExtrinsic types are great because you can apply multiple type systems to your code and also write things that you don’t know how to prove are sound.\ne.g. TypeScript / Python type annotations\nThe Curry typist argues that well-typed programs are a subset of dynamically typed programs. In other words, well-typed programs are just dynamic programs that are checked for type errors."},"thoughts/UCAN":{"title":"UCAN","links":["thoughts/authorization","thoughts/Bluesky","thoughts/DID"],"tags":["seed"],"content":"At a high level, User Controlled Authorization Networks (UCANs) are a way of doing authorization (“what you can do”) where users are fully in control. There’s no all-powerful authorization server, or server of any kind required. Everything that a users is allowed to do is captured directly in a key or token, and can be sent to anyone that knows how to interpret this format.\n\nUCANs work more like movie tickets or a festival pass between multiple venues. No one needs to check your ID; who you are is irrelevant. For example, if you have a ticket to see Citizen Kane, you are admitted to Theater 3. If you cannot attend an event, you can hand this ticket to a friend who wants to see the film instead, and there is no coordination required with the theater ahead of time.\nfrom the UCAN working group spec\n\n\n\nThe master keypair requires strong security and should not be duplicated to multiple locations or enter low-security environments such as the browser. This makes it difficult to access every time a new repository commit needs to be produced. Therefore we issue child keypairs from the master keypair in the form of UCANs, a JWT-style token that contains a permission description. UCANs can prove the authority of some key to undertake a given action, or produce new UCANs with a subset of their authority. Through this mechanism, a user is actually associated with many (likely hundreds) of keys, each belonging to a given context (a device or an application). These keys are granted only the authority they require from the root signing key.\nfrom Bluesky’s ADX\n\nSince anything with a private-public keypair can generate a DID, UCANs can be an auth layer that naturally plugs into existing identity primitives and services\nSee source 1 and source 2"},"thoughts/UDP":{"title":"UDP","links":["thoughts/Protocol","thoughts/TCP"],"tags":["seed","CPSC317"],"content":"User Datagram Protocol (UDP)\n\nSource Port\nDestination Port\nLength\nChecksum\nPayload\n\n68 is usually client, 67 is usually server\nFor reliable networks (like local) where out-of-order protections of TCP are unnecessary, or for time sensitive applications (e.g. streams or calls) where lossy transmission at high speed is better than quality transmission at choppy speed.\nSegment Format\n\nSource Port (16 bits)\nDestination Port (16 bits)\nLength in bytes, including header (16 bits)\nChecksum (16 bits)\nApplication Data\n"},"thoughts/Unicode":{"title":"Unicode","links":[],"tags":["seed"],"content":"o god the horror\nA letter maps to some bits which you can store on disk or in memory. How do we know how to convert from bit to letters and vice versa?\nUnicode FAQ\nASCII Era\nBack in the semi-olden days, the only characters that people cared about on the internet were good old unaccented English letters, and we had a code for them called ASCII which was able to represent every character using a number between 32 and 127.\nAnd all was good, assuming you were an English speaker.\nIn Asia, even more crazy things were going on to take into account the fact that Asian alphabets have thousands of letters, which were never going to fit into 8 bits. This was usually solved by the messy system called DBCS, the “double byte character set” in which some letters were stored in one byte and others took two. It was easy to move forward in a string, but dang near impossible to move backwards.\nUnicode\nThe commonly recited ‘fact’ that Unicode is simply a 16-bit code where each character takes 16 bits and therefore 65,536 possible characters is in fact a myth!!\nA letter maps to something called a code point. Every platonic letter in every alphabet is assigned a magic number by the Unicode consortium which is written as a code point in the form U+0639.\nHow these are stored depends on encodings.\nUTF-8\nUTF-8 is the byte-oriented encoding form of Unicode. Code points are represented as a sequence of one to four 8-bit code units.\n\nEvery code point from 0-127 is stored in a single byte.\nOnly code points 128 and above are stored using 2, 3, in fact, up to 6 bytes.\n\nThis has the neat side effect that English text looks exactly the same in UTF-8 as it did in ASCII,\nUTF-16\nUTF-16 uses a single 16-bit code unit to encode the most common 63K characters, and a pair of 16-bit code units, called surrogates, to encode the 1M less commonly used characters in Unicode.\nConverting to UTF-8, by definition requires that supplementary characters (those using surrogate pairs in UTF-16) be encoded with a single 4-byte sequence.\nUTF-32\nUTF-32 uses a single 32-bit code unit.\nQuirks\nUTF-16 and UTF-32 use code units that are two and four bytes long respectively. For these UTFs, there are three sub-flavors: BE, LE and unmarked. The BE form uses big-endian byte serialization (most significant byte first), the LE form uses little-endian byte serialization (least significant byte first) and the unmarked form uses big-endian byte serialization by default, but may include a byte order mark at the beginning to indicate the actual byte serialization used.\nSurrogate Code Points\nA Unicode code point in the range U+D800..U+DFFF. Reserved for use by UTF-16, where a pair of surrogate code units (a high surrogate followed by a low surrogate) “stand in” for a supplementary code point.\nIllegal sequences\nNone of the UTFs can generate every arbitrary byte sequence, thus there are some illegal byte sequences.\nWhen faced with this illegal byte sequence while transforming or interpreting, a UTF-8 conformant process must treat the first byte as an illegal termination error: for example, either signaling an error, filtering the byte out, or representing the byte with a marker such as FFFD (REPLACEMENT CHARACTER)\nFFFD is normally displayed as a black diamond with a white question mark in it\nUsages\nUTF-8 is most common on the web. UTF-16 is used by Java and Windows (.Net). UTF-8 and UTF-32 are used by Linux and various Unix systems."},"thoughts/Universal-Scaling-Law":{"title":"Universal Scaling Law","links":["thoughts/authenticator-complexity"],"tags":["seed"],"content":"Original paper\nWhen we can avoid or reduce the need for coordination things tend to get simpler and faster.\nWhere C(N) is relative capacity\nC(N)=1+α(N−1)+βN(N−1)N​\n\nN is the number of users/load generators\nContention (∝α): effect of waiting or queueing for shared resources. When α=0, we get linear scalability behaviour (e.g. lock-free computing)\nCoherency (∝β): cost of getting agreement on what the right thing to do is (see also: authenticator complexity)\n\nShare Nothing\nAlso called horizontal scaling or scaling out. Coordinate by communicating rather than sharing resources.\nIn effective leadership\nSource\nUSL, couched in terms of ‘advice to leaders of fast-growing organisations.’\n\nAs a leader of a fast-growing company, in a fast-growing sector, you probably care about how much work your company can get done in a given unit of time (aka throughput), and also how long any one piece of work takes to get through the system (aka latency, or lead time).\n\n\nI always fall back to software engineering principles – particularly those around modularity – when it comes to thinking about organisation structure\n\nYou want strong cohesion within a group, and weak coupling between groups, you want to keep a handle on fan-out etc..\n\n\nThe more operational decisions you need to be involved in, and the deeper you get involved, the more tasks you handle yourself, the higher your α coefficient and the more you limit the overall scalability of your organisation. So the first lesson is that it’s really important you learn to delegate effectively and to choose carefully the things that you do get involved in.\n"},"thoughts/Unix":{"title":"Unix","links":["thoughts/computer-architecture"],"tags":["seed"],"content":"Unix philosophy: expect the output of every program to become the input to another, as yet unknown, program.\nSee also: computer architecture\nProcesses\nPID1 Zombies\nSource\nZombie processes: processes that have terminated but have not (yet) been waited for by their parent processes.\nUnix processes are ordered in a tree. Each process can spawn child processes, and each process has a parent except for the top-most process.\nThis top-most process is the init process. It is started by the kernel when you boot your system. This init process is responsible for starting the rest of the system, such as starting the SSH daemon, starting the Docker daemon, starting Apache/Nginx, starting your GUI desktop environment, etc. Each of them may in turn spawn further child processes.\n\nIf a process terminates, it turns into a “zombie process” which Unix still keeps some minimal set of information about (PID, termination status, resource usage information). In Unix, parent processes must explicitly ‘wait’ for child processes to terminate.\nThe action of calling waitpid() on a child process in order to eliminate its zombie, is called “reaping”.\nIf a parent process is killed, the children are ‘orphaned’ (have no parent process). PID1’s job has a special task to adopt these orphaned processes and becomes the parent.  Thus, the operating system expects the init process to reap adopted children too.\nSignals\n\nSIGTERM can also be referred as a soft kill because the process that receives the SIGTERM signal may choose to ignore it. Allows the process to cleanup, etc.\nSIGKILL is used for immediate termination of a process. This signal cannot be ignored or blocked. SIGKILL cannot be trapped, so there is no way for processes to terminate cleanly. Suppose that the app you’re running is busy writing a file; the file could get corrupted if the app is terminated uncleanly in the middle of a write. Unclean terminations are bad. It’s almost like pulling the power plug from your server.\n\nNamespaces\nSource\nCurrently, Linux implements six different types of namespaces. The purpose of each namespace is to wrap a particular global system resource in an abstraction that makes it appear to the processes within the namespace that they have their own isolated instance of the global resource.\n\ncgroup\npid: process ID, allows a PID 1 per container (each process then has a within namespace PID and host PID)\nuser: user and group ID name spaces\nuts: hostname and NIS domain names\nipc: interprocess communication\nmnt: filesystem hierarchy\nnet: network devices, IP addresses, routing tables, port numbers, etc.\n\nOne of the overall goals of namespaces is to support the implementation of containers, a tool for lightweight virtualization (as well as other purposes) that provides a group of processes with the illusion that they are the only processes on the system.\nThe default namespace is the “host” namespace."},"thoughts/Unrepeatable-Miracle-of-Silicon-Valley":{"title":"Silicon Valley: An unrepeatable miracle?","links":["thoughts/From-Counterculture-to-Cyberculture","thoughts/skyhooks","thoughts/feedback-loops","thoughts/Design-Justice"],"tags":["seed"],"content":"Source: Silicon Valley: An unrepeatable miracle? with Margaret O’Mara\nTouches on a lot of similar topics to From Counterculture to Cyberculture\nWhat caused the big influx of extraordinary opportunity in Northern California, despite mostly being another sleepy fruit-growing region? Mostly the result of the Cold War and the military-industrial complex and the power of Stanford — these two were not completely separate.\nA lot of military funding went to the west (and to California and thus Stanford). Big result of Fred Terman (Dean of Engineering at the time): “let’s remake Stanford so that it’s a perfect receptacle for this new money. We are a true Cold War university.”\nThe military-industrial complex rose as a result of the Cold War, showing how a capitalist democracy can be ‘triumphant’ over more socialist policies. As a result, a lot of government money went to private defense contractors (think Lockheed). When the Space Race came along, a lot of that money went to places specializing in small, light, powerful devices (microchips, integrated circuirts), and this happened to be Silicon Valley.\nGovernment wasn’t the sole reason this explosion happened, rather more of “a customer, as a catalyst, as a kind of de facto venture capitalist at an early stage, when there was no commercial market for this stuff.”\n\nTo have such mobilization around this massive effort, you do probably need some sort of geopolitical catalyst. It’s hard for it to be purely commercial… That’s the thing that can catalyze people, government, and resources into doing these big things.\n\nProblem is that this ‘urgency’ needs to be bipartisan — this was very difficult when the Soviet Union was widely feared. As a result, anything even mildly centrist is perceived as socialist → https://www.youtube.com/watch?v=ULYWIDcUOY4\nAntipathy that the Valley has had towards the government is kind of like, “Well it doesn’t have anything to do with us, if they could stay as far away as possible it would be good.”\nDemocratic Capitalism\nDemocratic capitalism and the ability for people to protest what their government is doing.\nHistorically, white-collar tech workers in industry have been very OK with whatever their companies are doing as long is it gives them a comfy lifestyle — yet now people are starting to question it.\nSo much of the technical expertise that used to be distributed across industry and government is now just in industry. This inbalance means that government agencies like the Pentagon and CIA need to go to these big tech companies to build good tech.\nBlue-sky Operations\n“Spending money on moonshots, whether metaphorical or real, is kind of something only the government can do.”\nExamples like DARPA, investing in technology that is at least a few decades away from being commercializable or being something the military could possibly use.\nTo tell a company that is accountable to shareholders and quarterly earnings calls, “Okay, the moonshot’s on you guys,” is not so great.\nSee: skyhooks\nSandbox Model\n“Throw a lot of money in its direction and get out of the way” Create the sandbox, an incredible container with lots of resources in it, and allow creative people to play around in the sand and see what they develop. Naturally very incompatible with political traditions in other places.\nMoonshot People and Immigration\nPeople like Andy Grove who later becomes the CEO of Intel, come into the country as teenage refugees from places like Hungary where the immigration officers probably thought he would be a ‘drain on the system’.\nBut then you have winners of one generation picking the winners of the next, creating a very self-reinforcing feedback loop. This as a result causes huge diversity problems where it works really well for elite college students in the US but less so for the housewives in Myanmar."},"thoughts/Upwelling":{"title":"Upwelling","links":["thoughts/Fishbowl-effect","thoughts/collaborative-software"],"tags":["seed"],"content":"Source\n\nFishbowl effect of real-time collaborative software\nFile-based collaboration creates problems of versioning and merging edits from different co-authors.\nVersioning in file-based collaboration does not suffer from the fishbowl effect. They work well for people writing alone, but have the downside of requiring manual version management when collaborating with others.\nReviewing changes\n\nCarefully reviewing a document is important in many professional contexts, but existing software makes it difficult to visualize and review the changes that have been made to a document.\n\n\nLayers and drafts\n\nAn Upwelling document is always edited within a layer. We call unmerged layers “drafts”.\nAn Upwelling document can have multiple drafts at the same time, all of which are separate from each other: edits made in one draft do not appear in other drafts. As such, a writer who needs a private space can create a new draft and work there in solitude.\nWhen the draft is ready, it is merged onto the stack: \nThe stack captures the full editing history of the document, starting from the moment it was created as an empty document, and each merged layer represents a batch of changes relative to the previous (lower) layer.\nAll drafts float on top of the stack of merged layers; when one draft is merged, all other drafts are updated to include the changes in the merged layer. Using a Git analogy, every time a branch is merged into main, all other branches are automatically rebased on top of the latest main: \n\n\n"},"thoughts/Urbit":{"title":"Urbit","links":["thoughts/internet-computing","thoughts/digital-permanence"],"tags":["seed"],"content":"Terminology\n\nArvo: OS\nHoon: language\nTlon: company\nAzimuth: ID\nAmes: Network\n\n\nimo, falls into the category of vaporwave tech that never manifested itself into anything useful\n\nOS\nIn 1974 a computer was a mainframe the size of a room and was shared by hundreds of people. By 1984 a computer was the size of a desk and everyone had their own PC. The PC was more flexible and more fun, so it won by a wide margin. Then, with the rise of the internet, the PC’s flexibility slowly became irrelevant — we’re more or less back to the timesharing model of the 1970s.\nUrbit OS is the PC to MEGACORP’s mainframe.\n\nID\nUrbit ID is a decentralized addressing and public key infrastructure designed for Urbit OS.\nThe Urbit ID registry is live and deployed to the Ethereum blockchain. Urbit ID isn’t specifically wedded to Ethereum – someday we’d like it to be hosted by Urbit OS itself.\nRelated: internet computing\n\n256 (2^8) Galaxies\n\nWeb analogue: DNS Root Servers\nThe senate that can upgrade the logic of the Urbit ID system by majority vote\n\n\n65,280 (2^16) Stars\n\nWeb analogue: ISP, for routing packets\n\n\n4,294,901,760 (2^32) Planets\n\nIndividual ID\n\n\n2^64 Moons\n\nIn a way, upgraded version of ENS?\nUses Kelvin Versioning which is an interesting exercise in digital permanence\nLibraries\n\nProfile picture generation from Urbit: https://github.com/urbit/sigil-js\nPhonetically pronounceable data: https://github.com/urbit/urbit-ob\n"},"thoughts/Utilitarianism":{"title":"Utilitarianism","links":["thoughts/utility","thoughts/Decisions-under-risk","thoughts/Kant","thoughts/quantization","thoughts/The-ones-who-walk-away-from-Omelas"],"tags":["seed","CPSC430"],"content":"Act Utilitarianism\nAlso called the Greatest Happiness Principle.\nAn action is good if its benefits exceed its harms, and an action is bad if its harms exceed its benefits. This is based on calculations of utility.\n\nAn important decision an act utilitarian must make is determining which beings are considered to be morally significant.\n\nIt is a consequentialist theory because it focuses on the consequences of an action rather than the intention.\nCalculating the utility to make a decision falls under DUR (specifically, EU Max)\nRule Utilitarianism\nWe ought to adopt those moral rules that, if followed by everyone, lead to the greatest increase in total happiness over all affected parties\nDifference between rule utilitarianism and Kantianism:\n\nA rule utilitarian looks at the consequences of the action\nA Kantian looks at the will motivating the action\n\nCritique\n\nUtilitarianism forces us to use a single scale or measure to evaluate completely different kinds of consequences.\n\nSee also: quantization\n\n\nUtilitarianism ignores the problem of an unjust distribution of good consequences.\n\nOne person receiving 100 units of good while 99 people receive nothing is treated the same as 100 people receiving 1 unit of good.\nThis doesn’t sit right with most people.\nSee also, The ones who walk away from Omelas\n\n\n"},"thoughts/VPN":{"title":"VPN","links":["thoughts/TCP"],"tags":["seed","CPSC317"],"content":"Virtual Private Networks\nMotivation:\n\nCompany with multiple locations wants everything to appear as one big network\nWorkers want access to resources restricted to company internal network\nUsers want to bypass regional blocks\n\n\nVPN Encapsulation\n\nVirtual end points establish software association between them (e.g. TCP connection) usually referred to as a tunnel\nRouting rules on local machine send traffic to virtual interface\nVirtual card encapsulates IP message and sends it through tunnel\nReceiver receives message and sends it through its own network\n"},"thoughts/Valve-Handbook":{"title":"Valve Handbook","links":["thoughts/metalabel","thoughts/friction"],"tags":["seed"],"content":"Source\nValve is a company is one that I respect a lot for the agency they give their employees. Feels very much like a metalabel where you have a bunch of highly agentic people working on things they wanna work on rather than a company.\nTLDR; hire well\n\nThis book is an abbreviated encapsulation of our guiding principles. As Valve continues to grow, we hope that these principles will serve each new person joining our ranks.\n\nSome highlights from their handbook:\nFacts that matter\n\nValve is self-funded: gives agency to the org to work on work they care about and that customers care about\nValve owns all of their IP: agency to make their own decisions about what they do with their products\n\nHierarchy\nFlatness necessarily implies high responsibility for the individual\nAdding individuals then (hiring), is one of the most important things people have control over at the company:\n\n“If you’re thinking to yourself, “Wow, that sounds like a lot of responsibility,” you’re right. And that’s why hiring is the single most important thing you will ever do at Valve”\n\nFlatness also means more mobility. Why does your desk have wheels? Think of those wheels as a symbolic reminder that you should always be considering where you could move yourself to be more valuable.\nPeople move frequently; there is no organizational structure keeping you from being in close proximity to the people who you’d help or be helped by most\nProjects\nFlatness implies anyone has the power to green-light projects. So how does Valve choose what projects to prioritize?\n\nEmployees vote on projects with their feet (or desk wheels)\nStrong projects are ones in which people can see demonstrated value; they staff up easily.\n\nHow should I as an individual choose what to work on? A few good guiding questions:\n\nOf all the projects currently under way, what’s the most valuable thing I can be working on?\nWhich project will have the highest direct impact on our customers? How much will the work I ship benefit them?\nIs Valve not doing something that it should be doing?\nWhat’s interesting? What’s rewarding? What leverages my individual strengths the most?\n\nHow do people discover new projects? There is no central ‘board’, but the best way to find out about projects is to just ask people.\n\nLots of people at Valve want and need to know what you care about, what you’re good at, what you’re worried about, what you’ve got experience with, and so on. And the way to get the word out is to start telling people all of those things.\n\nPeople first frictionful onboarding!\nStructure\n\nTeam leads\n\nThis person’s role is not a traditional managerial one. Most often, they’re primarily a clearinghouse of information.\nThey’re keeping the whole project in their head at once so that people can use them as a resource to check decisions against. The leads serve the team, while acting as centers for the teams.\n\n\nThere is still structure\n\nProject teams often have an internal structure that forms temporarily to suit the group’s needs.\nThis is dynamic on scales of months to years, but gives some semblance of expectation and stability on the day-to-day.\n\n\nHours\n\nFor the most part working overtime for extended periods indicates a fundamental failure in planning or communication. If this happens at Valve, it’s a sign that something needs to be reevaluated and corrected (of course, there are exceptions like when a project nears ship date)\n\n\n\nLong-term thinking\n\nIf we’re not careful, these traits can cause us to race back and forth between short-term opportunities and threats, being responsive rather than proactive. So our lack of a traditional structure comes with an important responsibility. It’s up to all of us to spend effort focusing on what we think the long-term goals of the com- pany should be.\n\n\nSomeone told me to (or not to) work on X. And they’ve been here a long time!\n\nThey aren’t always right! Hold on to your goals if you’re convinced they’re correct. Check your assumptions. Pull more people in. Listen. Don’t believe that anyone holds authority over the decision you’re trying to make.\n\n\nI constantly feel behind with everything going on! How do I make my work feel sustainable?\n\nTrust us, this is normal. Nobody expects you to devote time to every opportunity that comes your way. Instead, we want you to learn how to choose the most important work to do.\n\n\nHow does Valve as an organization decide what to work on?\n\nWe believe in each other to make these decisions, and this faith has proven to be well-founded over and over again.\nWe have learned that when we take nearly any action, it’s best to do so in a way that we can measure, predict outcomes, and analyze results.\n\n\nCan I be involved with X?\n\nYes. You either\n\nStart working on it\nStart talking to all the people who you think might be working on it already and find out how to best be valuable\n\n\n\n\n\nRisk\nProviding the freedom to fail is an important trait of the company— we couldn’t expect so much of individuals if we also penalized people for errors.\nThere are still some bad ways to fail.\n\nRepeating the same mistake over and over is one.\nNot listening to customers or peers before or after a failure is another.\nNever ignore the evidence; particularly when it says you’re wrong.\n\nCollective Risk\nWhen everyone is sharing the steering wheel, it seems natural to fear that one of us is going to veer Valve’s car off the road.\n\nConcepts discussed in this book sound like they might work well at a tiny start-up, but not at a hundreds-of-people-plus- billions-in-revenue company. The big question is: Does all this stuff scale?\n\nWell, so far, yes. And we believe that if we’re careful, it will work better and better the larger we get. This might seem counterintuitive, but it’s a direct consequence of hiring great, accomplished, capable people.\nHiring\nIn the mean- time, here are some questions we always ask ourselves when evaluating candidates:\n\nWould I want this person to be my boss?\nWould I learn a significant amount from them?\nWhat if this person went to work for our competition?\n\nWe want people who are integral to high-bandwidth collaboration. People who can\n\ndeconstruct problems on the fly\ntalk to others as they do so\nsimultaneously being inventive, iterative, creative, talkative, and reactive\n\nT-Shaped People\nWe care about T-shaped people: people who are both generalists (highly skilled at a broad set of valuable things—the top of the T) and also experts (among the best in their field within a narrow discipline—the vertical leg of the T).\nAn expert who is too narrow has difficulty collaborating. A generalist who doesn’t go deep enough in a single area ends up on the margins, not really contributing as an individual.\nThings to improve\nThings Valve wishes they were better at\n\nHelping new people find their way\nMentoring people\nDisseminating information internally\nFinding and hiring people in completely new disciplines\nMaking predictions longer than a few months out\nWe miss out on hiring talented people who prefer to work within a more traditional structure (isn’t something we should change, but it’s worth recognizing as a self-imposed limitation)\n"},"thoughts/Vanilla-Ice-Cream-effect":{"title":"Vanilla Ice Cream effect","links":["thoughts/consensus","thoughts/funding"],"tags":["seed","pattern"],"content":"Ask everyone to vote on their favourite ice cream, you’ll most likely end up with vanilla.\nSimilar to design by committee: a project that has many designers involved but no unifying plan or vision\nCoordination Headwind\nSource: How Organizations are like Slime Molds by Alex Komoroske\nIn small orgs, individuals matter a lot, but when the org gets larger the system (structure + dynamics) dominate. Consensus is especially difficult in larger groups.\nBottom-up orgs (slime mold model)\n\nunpredictable\nuncontrollable (can influence via incentives)\nindividual autonomy\nfluid\nmessy\nresilient\nexploratory\n\nTop-down orgs (military model)\n\npredictable\ncontrollable\ncog in the machine\nstructured\nefficient\nfragile\nexploitative\n\nOn avoiding Vanilla Ice Cream\nRelated to how to better fund moonshot things\nSetting an appropriate council size so that if one person proposes an idea and the others don’t agree, they can still drive that decision. obviously based on high trust and should not dictate entire budget → have a consensus (more publicly governed) budget and an aconsensus budget (core council budget)"},"thoughts/Verifiable-Credential":{"title":"Verifiable Credential","links":[],"tags":["seed"],"content":"Verifiable credentials can be issued by anyone, about anything, and can be presented to and verified by everyone\n\nIssuer: the entity that generates the credential\nHolder: entity that receives the credential\nVerifier: entity that wants to check the credentials of the Holder\n\nFor this to work requires a triangle of trust:\n\nThe issuer trusts the holder\nThe holder trusts the verifier\nThe verifier trusts the issuer\n\nVerifiable Data Registry (VDR): can be used to maintain identifiers and schemas\nTo make a VC:\n\nIssuer registers a DID and its associated verification key (verkey) to the VDR\nIssuer writes a credential definition (a template) to the VDR\n(Optional) Issuer offers a credential to the holder\nHolder requests a credential from the Issuer\nIssuer creates a credential based on the definition for the holder\nIssuer signs the credential with their private part of the verification key, and gives it to the holder (offer)\nVerifier can then check the credential against the issuer’s verkey\n\n\nExample VC\n{\n  &quot;@context&quot;: [\n    &quot;https://www.w3.org/2018/credentials/v1&quot;,\n    &quot;https://www.w3.org/2018/credentials/examples/v1&quot;\n  ],\n  &quot;id&quot;: &quot;0892f680-6aeb-11eb-9bcf-f10d8993fde7&quot;,\n  &quot;type&quot;: [&quot;VerifiableCredential&quot;, &quot;UniversityDegreeCredential&quot;],\n  &quot;issuer&quot;: {\n    &quot;id&quot;: &quot;did:example:76e12ec712ebc6f1c221ebfeb1f&quot;,\n    &quot;name&quot;: &quot;Acme University&quot;\n  },\n  &quot;issuanceDate&quot;: &quot;2021-05-11T23:09:06.803Z&quot;,\n  &quot;credentialSubject&quot;: {\n    &quot;id&quot;: &quot;did:example:ebfeb1f712ebc6f1c276e12ec21&quot;,\n    &quot;degree&quot;: {\n      &quot;type&quot;: &quot;BachelorDegree&quot;,\n      &quot;name&quot;: &quot;Bachelor of Science&quot;\n    }\n  },\n  &quot;proof&quot;: {\n    &quot;type&quot;: &quot;Ed25519Signature2018&quot;,\n    &quot;created&quot;: &quot;2021-05-17T15:25:26Z&quot;,\n    &quot;jws&quot;: &quot;eyJhbGciOiJFZERTQYjY0Il19..nlcAA&quot;,\n    &quot;proofPurpose&quot;: &quot;assertionMethod&quot;,\n    &quot;verificationMethod&quot;: &quot;https://pathToIssuerPublicKey&quot;\n  }\n}"},"thoughts/Weaving-the-Web":{"title":"Weaving the Web","links":["thoughts/hypertext","thoughts/Internet","thoughts/inevitability-of-centralization","thoughts/Solid","thoughts/Rhizome-Proposal","thoughts/RDF","posts/networked-thought","thoughts/privacy","thoughts/software-and-politics","thoughts/DNS","thoughts/notation","thoughts/teaching"],"tags":["sapling","book"],"content":"\nThe original design and ultimate destiny of the world wide web.\n\nBook, written by Tim Berners-Lee\nA Brief History of the Web\nHypertext was invented by Ted Nelson in 1965. The Internet, as worked on by Donald Davis, Paul Barran, Vint Cerf, and Bob Kahn, were already becoming pervasive by the 1970s. Tim Berners-Lee came at the right time to marry them together into the Web.\nThe Web was a slow process — there was no “Eureka!” moment. He described the process like getting a bobsled down the hill — something you needed to put a lot of upfront effort into getting moving, but once you did, you needed to get in and steer.\nHe first wrote a proposal for the Web at CERN in March of 1989 and began work on it with Robert Cailliau. On Christmas Day 1990, the first WorldWideWeb browser/editor was working, communicating with the info.cern.ch server, a full 20 years after the existence of both hypertext and the Internet. Not only was it able to view web pages, it could edit them collaboratively with others.\nEven still, people at CERN “didn’t seem to see how it would be useful.” This created a lot of tension between Robert and Tim about how to deploy their resources effectively.\n\nShould we develop it further on the NeXT? Should we reprogram it for the Mac or the PC or Unix, because even though the NeXT was an efficient machine, few other people had them? After all, what good was a “worldwide” web if there were only a few users? Should we tailor the Web to the high-energy physics community, so they’d have a tool that was theirs and would support it, since CERN was paying our salaries? Or should we generalize the Web and really address the global community, at the risk of being personally disenfranchised by CERN? … My gut told me I had to pursue my larger vision of creating a global system.\n\nAt the same time, several other Internet-based information systems were surfacing too, namely WAIS and Gopher. Both systems took off much more quickly and he was concerned that they would suffocate the Web. Nonetheless, they put their heads down and kept working at it.\nAs browsers started to spread, no one working on them really focused on writing or editing functions for the web. Browsers were just that — things to browse the web. This was one of the first major divergences between the vision that Tim had for the web and what actually happened. He wanted the web to be collaborative, citing that “without a hypertext editor, people would not have the tools to really use [it] as an intimate collaborative medium. Browsers would let them find and share information, but they could not work together intuitively.” This was a concern that would come back to continuously haunt him.\nBut by the summer of 1992, the bobsled had started to move. The logs showed a dramatic exponential curve over the past twelve months, doubling every three to four. After a single year, the load had grown by a factor of ten.\nNow, companies and individuals were starting to eye the Web as a way to make profit. With the rise of Mosaic (which would later become Netscape), they tried to make the web ‘theirs’. It wasn’t “on the Web”, it was “on Mosaic.” The media started to portray Mosaic as if it were equivalent to the Web.\nAs technologists and entrepreneurs were launching or merging companies to exploit the Web, they seemed fixated on one question: “How can I make the Web mine?” Tim was asking, “How can I make the Web yours?” Gopher, one of two most popular Internet information systems at the time, decided to charge an annual fee for access and just as quickly as people flocked to it, they dropped it like a hot potato. Many asked if the Web would suffer the same fate. His response? On April 30th, 1993, Tim and Robert signed a declaration saying that the Web protocol and code was free to use and anyone could create a server or a browser to give away or sell without royalty or other constraint.\nMore than ever, this experience convinced Tim that it was time to stop pushing the bobsled and instead to jump in and steer it. With yet another factor of 10 growth since the previous year, he decided to start the World Wide Web Consortium, the W3C. It wasn’t a standards body, but rather an international organization to help developers of servers and browsers alike reach consensus on how the Web should operate. With everyone trying to pick up the Web and run with it in different directions, Tim thought the W3C would help unify their pursuits in creating a single, universal, accessible, hypertext medium for sharing information. It would develop open technical specifications and have a small full-time staff1 to design and develop the code wherever necessary. With members open to any organization, regardless of commercial, educational or governmental, for-profit or non-profit, W3C would represent the power and authority of millions of developers, researchers, and users.\nOf course, the W3C was not meant to be a point of control. Tim made it clear that he had designed the Web so there should be no centralized place where someone would have to “register” a new server, or get approval of its contents. Anyone could build a server and put anything on it. Philosophically, if the Web was the be a universal resource, it had to be able to grow in an unlimited way.\nThe international telephone system offers a great analogy. It defines what it has to (i.e. the voltages and signals), but then leaves how it is used up to the devices. That’s what we needed for computers on the web. Universality.\nQuietly, under the noise of all the companies trying to make a fortune off the web, the W3C has been steering the web and leading the web to its full potential. Today, Tim currently sits as Director of the W3C. He was knighted by Queen Elizabeth in 2007. Now, Sir Tim promotes open government data globally and spends time fighting for rights such as net neutrality, privacy and the openness of the Web.\nBut there is still work left to be done. A slow recentralization of the Web is happening, and users lack agency over their own privacy. Of course, there have been many proposals to address this, including Tim’s own Solid project as well as my research on Rhizhome. We are still early.\n\nWhen I try to explain the architecture now, I get the same distant look in people’s eyes as I did in 1989, when I tried to explain how global hypertext would work. But I’ve found a few individuals who share the vision; I can see it from the way they gesticulate and talk rapidly.\n\nQuotes\nRelational Theories of the World\nEnquire was a very early triple-store like networked thought note-taking tool developed by Tim Berners-Lee. He made it to stored information without using structures like matrices or trees. After all, the human mind uses the organizing structures all the time, but can also break out of them and make intuitive leaps across the boundaries — those coveted random associations.\nIn an extreme view, the world can be seen as only connections, nothing else. We think of a dictionary as the repository of meaning, but it defines words only in terms of other words.\nSeparation of layers of the Web\nThe web’s infrastructure can be thought of as composed of four horizontal layers; from bottom to top they are the\n\ntransmission medium: connecting computers together\ncomputer hardware\nsoftware: runs web access\ncontent: the Web itself\n\nThe independence of these layers is important. From the software engineering point of view, this is the basic principle of modularity. From the point of view of economics, it is the separation of horizontal competitive markets from anticompetitive vertical integration. From the information point of view, think of editorial independence, the neutrality of the medium.\nI am more concerned about companies trying to take a vertical slice through the layers than creating a monopoly in any one layer. Keeping the medium and the content separate is a good rule in most media. When I turn on the television, I don’t expect it to deliberately jump to a particular channel, I expect my television to be an impartial box.\nI also expect the same neutrality of software. When I ask a search engine to find the information it can on a topic, I don’t expect it to return just the sites of companies that happen to advertise with or make payments to the search company.\nIf a company claims to give access to the world of information, then presents a filtered view, the Web loses its credibility. That is why hardware, software, and transmission companies must remain unbiased toward content. I would like to keep the conduit separate from the content. I would like there always to be a choice of the unbiased way, combined carefully with the freedom to make commercial partnerships. And when other people are making a choice for me, I would like this to be made absolutely clear to me.\nSee also: inevitability of centralization\nPrivacy\nPeople should be able to surf the Web anonymously, or as a well-defined entity, and should be able to control the difference between the two. I would like to be able to decide who I will allow to use my personal information and for what.\nThe W3C is creating a technology that will allow automatic negotiation between a user’s browser and store’s server, leading to an agreement about privacy. The Platform for Privacy Preferences Project (P3P) will give a computer a way of describing its owner’s privacy preferences and demands and give servers a way of describing their privacy policies.\nSee also: privacy\nAnnotations\nI would like annotation servers to exist where groups could add links to documents they want to comment on. Annotation servers are third-party services allowing a group to share each others’ comments on documents anywhere else on the Web. The browser gets the original page and then separately checks annotation servers for comments which are then superimposed on the page.\nImagine having servers for comments in different forums, perhaps family, school, and company. Again, the theme is human beings doing the thinking and machines helping it work on a larger scale, but nothing replacing wisdom in the end.\nTechnology and Policy\n[The W3C] defines mechanism, not policy. That said, it is essential that policy and technology be designed with a good understanding of each other. As I noted in closing the first International World Wide Web Conference at CERN in May 1994, technologists cannot simply leave the social and ethical questions to other people, because technology directly affects these matters.\nSee: software and politics\nThe Web’s Achilles’ Heel: DNS\nAt the top of the DNS hierarchy sits 13 root servers. An operator error at this level can black out huge portions of the web. However, that technical weakness is itself less of a concern than the social centralization that parallels it.\nAll domain names are given out in a delegated fashion. To set up the name www.lcs.mit.edu, one registers it with the Lab for Computer Science, which is owner of the lcs.mit.edu domain, LCS got its domain in turn from MIT, which is the registered owner of mit.edu. MIT got its domain from the owner of edu. Control over these ‘top-level’ domains gives control over all domain names and so is something of great power. Who should exercise that power?\nSemantic Web and Data Lensing\nTo build understanding, we need to be able to link terms. This will be made possible by inference languages, which work one level above the schema languages. Inference languages allow computers to explain to each other that two terms that may seem different are in some way the same — a little like an English-French dictionary. Inference languages will allow computers to convert data from one format to another.\nSocial\nA person who’s completely turned inward, who spends all his or her time alone, is someone who has trouble making balanced decisions and is unhappy. Someone who is completely turned outward, who’s worried about the environment and international diplomacy and spends no time sitting at home or in their local community, also has trouble making balanced decisions and is also unhappy. It seems a person’s happiness depends on having a balance of connections at different levels. We seem to have built into us what it takes in a person to be part of a fractal society.\nTeaching\nHaving to work with someone else’s definitions is difficult. An awe-inspiring talent of my physics tutor, Professor John Moffat, was that when I brought him a problem I had worked out incorrectly, using a strange technique and symbols different from the well-established ones, he not only would follow my weird reasoning to find out where it went wrong, but would then use my own strange notation to explain the right answer.\nThis great feat involved looking at the world using my definitions, comparing them with his, and translating his knowledge and experience into my language. It was a mathematical version of the art of listening.\nSee: teaching\nFootnotes\n\n\nThis full time staff team was paid through the membership fee for the W3C, which was $50k for full-membership and 1/10 the price for non-profit or governmental entities. Members had to commit to a three-year term of membership, after which they could renew annually. Members were free to attend any meeting, and sit on any working group. They would also get exclusive access to in-depth information on all activities under way, much like how the Xerox Parc membership worked. ↩\n\n\n"},"thoughts/Web-Tiles":{"title":"Web Tiles","links":["thoughts/CORS","thoughts/access-control"],"tags":["seed"],"content":"Source\nCORS#abolish-the-same-origin-policy\nWeb Tiles asks the question about how we might enable composable software on the web, focussing on applications rather than documents.\nWhy permissions don’t work\n\n“Asking people to approve access that they know they don’t fully understand and that they couldn’t monitor even if they did understand it does not empower them. On the contrary, it trains them to be despondent, helpless at the hands of the High Priesthood of Computer Whisperers. And our job as technologists building a better world is to eradicate the High Priesthood.”\n\nSee: access control\nWishes and intents\nA wish is a verb applied to a type of thing. A tile’s metadata describes which wishes it can grant. This is similar to the existing technology matching this approach: Web Intents. Web Intents were developed (and abandoned) by the W3C’s Device APIs Working Group\nWhereas hyperlinks are nouns — they name things — wishes are verbs.\nwishes: [\n  // this can pick images and return them\n  {\n    &quot;can&quot;: &quot;pick&quot;,\n    &quot;what&quot;: [&quot;image/*&quot;],\n    &quot;name&quot;: &quot;Select an image from our cat memes collection&quot;\n  },\n  // this can create a social post which the user can post\n  {\n    &quot;can&quot;: &quot;post&quot;,\n    &quot;what&quot;: &quot;com.atproto.repo.create&quot;,\n    &quot;name&quot;: &quot;Post a cat meme&quot;\n  }\n]"},"thoughts/WebAssembly":{"title":"WebAssembly","links":["thoughts/content-addressed-storage","thoughts/Universal-Scaling-Law","thoughts/BitTorrent"],"tags":["seed"],"content":"\nA binary instruction format for a stack-based virtual machine\n\nDistributed Invocation\nSource and talk on Homestar\nWe can imagine WASM modules as functions that execute some behaviour. If we content address it, we now have a consistent way of referring to the same computation. We can imagine a particular WASM module call with a set of parameters as a suspended closure that is deterministic1.\nThen, if we know we’ve ran that module with a specific set of parameters and have a receipt that it produced a certain result, we can be sure that we can just use the result instead of doing the computation again. Memoization at global scale!\nOnce we have a way for compute to be orchestrated at a global scale (e.g. IPVM through Homestar), this means we pretty much have a global mapping from source code + arguments to result.\nThis actually gives us superlinear results as we increase concurrency (opposite of what the Universal Scaling Law says! BitTorrent does this too)\nNondeterminism\nNondeterministic execution can only occur in a small number of well-defined cases (described below) and, in those cases, the implementation may select from a limited set of possible behaviors.\nSee a list of nondeterministic behaviours in WASM\nFootnotes\n\n\nSee point on nondeterminism ↩\n\n\n"},"thoughts/WebID":{"title":"WebID","links":["thoughts/DID","thoughts/FOAF"],"tags":["seed"],"content":"Content summarized from W3\nSimilar to DIDs.\n\nA WebID is a way to uniquely identify a person, company, organisation, or other agent using a URI.\n\nPeople often publish data about themselves on the Web, such as:\n\nWho they know\nWhat they are interested in\nPhotos they have taken\nProjects they work on\nTheir curriculum vitae or employment history\nTheir publications\n\nMost importantly of all, having a Web ID allows people to reference you and declare social relations on the web (such as that you are their friend, colleague, parent, etc.) even when their profile is hosted on a different web server than yours (emphasis added).\nCommonly uses FOAFs for profiles.\nProof of Ownership\nWebID protocol is just a technology that leverages X.509 Certificates, already in common (and largely invisible) use. By placing that same URL in the certificate that identifies the user, and then placing information at that URL about the public key of the certificate, a web server that receives a user request can verify that the user has write access to that URL\nSeems to be read-only (i.e. only user can write, can’t tell apps to write to your own WebID)?"},"thoughts/Where-is-My-Flying-Car":{"title":"Where Is My Flying Car?","links":["thoughts/Overton-Window","thoughts/The-Machiavelli-Effect","thoughts/Energy-Maximalism","thoughts/telerobotics"],"tags":["seed","book"],"content":"\n\nThe question “Where is my flying car?,” Wikipedia tells us, “is emblematic of the supposed failure of modern technology to match futuristic visions that were promoted in earlier decades”. Flying cars have become a symbol of mismatch: the future as imagined in the first half the 20th century seemed a lot brighter than the present we’re living in now.\n\nAn exploration of the technical limitations of building flying cars evolves into an examination of the global economic and scientific stagnation that started in the 1970s.\nSee: Overton Window, The Machiavelli Effect, Energy Maximalism\nOn nanotechnology\n\nThe original concept of a telemanipulator, often termed a “waldo”, is a robot arm but instead of being programmed, it is operated via remote control by a human operator. The goal was to produce a series of ever smaller waldoes to operate on individual nerve cells:\n\n“Neither eletromagnetic instruments nor neural surgery was refined enough to do accurate work on the levels he wished to investigate. But he had waldoes. The smallest waldoes he had used up to this time were approximately half an inch across their palms — with micro scanners to match, of course. They were much too gross for his purpose… He used the waldoes to create tinier ones… his final team of waldoes used for nerve and brain surgery varied in succeeding stages from mechanical hands nearly life size down to these fairy digits which could manipulate things much too small for the eye to see. They were mounted in bank to work in the same locus. Waldo controlled them all from the same primaries; he could switch from one size to another without removing his gauntlets. The same change in circuits which brought another size of waldoes under control automatically accomplished the change in sweep of scanning to increase or decrease the magnification so that Waldo always saw before him in his stereo receiver a ‘life size’ image of his other hands”\n\n\n\nOn policy\n\nOn public vs private development\n\nIt was not just the opinion of a few futurists such as Robert Prehoda, but the firm consensus of the entire economic and scientific establishment, that more federal money for scientific research could only help economic growth. Yet the evidence simply does not support the conclusion.\nThe great innovations that made the major quality-of-life improvements came largely before 1960: refrigerators, freezers, vacuum cleaners, gas and electric stoves, and washing machines; indoor plumbing, detergent and deodorants; electric lights; cars, trucks, and buses; tractors and combined; fertilizer; air travel and containerized freight; the vacuum tube and the transistor; the telegraph, telephone, phonograph, movies, radio, and television — and they were all developed privately\nA survey and analysis performed by the Organization for Economic Cooperation and Development in 2005 found, to the researchers’ surprise, that although private R&amp;D had a positive 0.26 correlation with economic growth, government-funded R&amp;D had a negative 0.37 correlation\n\n\n“There is a pattern that we see recurring throughout history, when a successful empire expands its borders so far that it becomes the biggest kid on the block. When survival is no longer at stake, selfish elites and other special interest groups capture the politic agenda, The spirit that ‘we are all the in the same boat’ disappears and is replaced by a ‘winner take all’ mentality”\nRegulation\n\n“Over the long run, unchecked regulation destroyed the learning curve, prevents innovation, protects and preserved inefficiency, and makes progress run backward.”\n\n\n\nOn failure modes\n\nFailure of Nerve:\n\nApplies when the facts are known: The science is there, the engineering understood, the pathway clear, and only the details remain to be worked out.\n\n\nFailure of the Imagination\n\nOccurs when the result is far enough out of the common-sense experience, outside the Overton Window, the mind balks and fails to see it for what it could be\n“One obvious reason is that science fiction needs to provide characters with whom the reader can identify, and this gets harder to do as characters become less human.”\n\n\n"},"thoughts/Willow-Protocol":{"title":"Willow Protocol","links":["thoughts/deletion","thoughts/access-control","thoughts/CRDT"],"tags":["seed"],"content":"Source\nA protocol for peer-to-peer data stores.\n\nWe made Willow to make running a network together a sustainable practice.\nWe made Willow to reconcile peer-to-peer networks with social realities. Wrangling the complexity of distributed systems shouldn’t mean we trade away basic features like deletion, or accept data structures which can only grow without limit.\n\nProperties\n\nAs many of these stores as you want, keyed to different namespaces. When stores from different devices belong to the same namespace, they deterministically sync with each other.\nPrivate and end-to-end encrypted. Other users can’t find out what you’re interested in unless they already know about it themselves.\nTotal delete via prefix pruning (essentially cutting a tree of causal dependencies by trimming down to root and marking that with a single tombstone). Destructive edits. When you update a value, the old values and associated metadata are overwritten.\nFine grained access control. Restrict read and write access by semantically meaningful ranges of data, or time range.\nPeers can communicate resource budgets, so devices with very limited memory can sync too.\n\nData Model\nSource\nWillow is a system for giving meaningful, hierarchical names to arbitrary sequences of bytes (called payloads).\nFor any given subspace, you can address payloads via paths (e.g. blog/idea/1 and blog/idea/2).\n\nEntries can be overwritten by more recent entries (Willow tracks timestamps and deletes actually delete everything except the metadata)\nDeletes are hierarchical (e.g. deleting blog will delete all of tis subpaths).\n\nThis is called prefix pruning\n\n\n\nEntries live in separate subspace owned by different users (intuitively, each user writes to their own, separate universe of data. Willow allows for various ways of controlling who gets to write to which subspace)\n\nInterestingly, namespaces can also be aggregated into namespaces.\nSome types\n\nPayload: at most 264−1 bytes\nEntry: metadata for a payload\n\nNamespaceId: keys namespaces\nSubspaceID: keys subspaces\nPath: parametrized by\n\nmax_component_length: max length for a path segment\nmax_component_count: max path segments in a path\nmax_path_length: overall limit for size of path\n\n\nTimestamp: time in microseconds since Unix epoch time\npayload_length\nPayloadDigest: content addressing for a payload\n\ncalculated from hash_payload(payload) -&gt; hash: maps Payload to PayloadDigest\n\n\n\n\n\nAn entry e1 is newer than an entry e2 if:\n\ne2.timestamp &lt; e1.timestamp or\ne2.timestamp == e1.timestamp &amp;&amp; e2.payload_digest &lt; e1.payload_digest or\ne2.timestamp == e1.timestamp &amp;&amp; e2.payload_digest == e1.payload_digest &amp;&amp; e2.payload_length &lt; e1.payload_length\n\nAuth:\n\nAuthorisationToken: proving write permission\nis_authorized_write(entry, token) -&gt; bool: maps a path and token to whether that token proves a valid permission to write to entry\nPossiblyAuthorisedEntry is a pair of an Entry and an AuthorisationToken\nAuthorisedEntry is a PossiblyAuthorisedEntry for which is_authorised_write returns true\n\nStores are collection of AuthorisedEntry:\n\nAll Entry have the same NamespaceId\nAn invariant such that an Entry a cannot both a prefix of another Entry b and a be newer than b\n\nThat is, updating blog will invalidate blog/abc (is this true?)\n\n\n\nA join of two stores is obtained by:\n\nStart with the union of the two stores\nRemove all Entry e1 where there is some Entry e2 such that\n\ne2.path is a parent of  e1.path and\ne2 is newer than e1\n\n\nFor all Entry with the same SubspaceID, Path, and Timestamp, remove them all except for the one with the greatest PayloadDigest\nFor all Entry with the same SubspaceID, Path, Timestamp, and PayloadDigest, remove them all except for the one with the greatest payload_length\n\nStores form a state-based CRDT under the join operation.\nGrouping Entries\nSource\nAn application might want to access all chess games that a certain author played in the past week. This kind of query corresponds to a box in the three-dimensional Willow space.\nThere are one-dimensional queries called ranges which work along SubspaceId, Path, or Timestamp\nA 3dRange is a 3-tuple of ranges across all three dimensions:\nstruct 3dRange\n  subspaces: SubspaceRange\n  paths: PathRange\n  times: TimeRange\n\nSync Protocol\nSource\nRequirements:\n\nIncremental sync: peers can detect regions of shared data with relatively sparse communication to avoid redundant data transfer\nPartial sync: peers synchronise only those regions of data they both care about\nPrivate area intersection: peers can discover common interests without disclosing any non-shared information to each other\nResource control: peers communicate (and enforce) their computational resource limits so as not to overload each other\nTransport independence\nGeneral efficiency: peers can make use of efficient implementation techniques, and the overall bandwidth consumption stays low\n"},"thoughts/World-Building":{"title":"World Building","links":["thoughts/games","thoughts/fiction","thoughts/friendship","thoughts/Hackers","thoughts/play","thoughts/pace-layers"],"tags":["fruit"],"content":"Emissaries Guide To Worlding\nJames Stuber’s Blog\nWell-built worlds enable new stories. Games and fiction are forms of worldbuilding (or ‘worlding’ as Cheng says), but not the only ones\nInstitutions, religions, and life itself are Worlds unto their own–real Worlds where we can choose to play or shape the rules ourselves.\n\n“A World is a reality you can believe in: one that promises to bring about habitable structure from the potential of chaos, and aim toward a future transformative enough to metabolize the pain and pleasure of its dysfunction.”\n\nRelationships, friendships, and love are all forms of Worlding\nFour different kinds of people (masks) required to World\n\nThe Director: shapes the World’s container and narrative, giving birth to an idea\n\nThe container must be bound enough to enable creation but surprising enough to encourage narrative\nTo be the Director requires good taste: knowing what you like and what you dislike gives rise to an opinionated view on what your World should look like\n\n\nThe Cartoonist: reduces the complexity of the world by creating understandable characters and structures\n\nCharacters help make the Director’s ideas comprehensible.\n“A character is good when a child can pick out its features and draw their version of it.”\n\n\nThe Hacker: pushes the boundaries of the World, using exploration and ‘meaningless’ play\n\nThe Hacker’s job is to experiment, to play, and to create “New” art. Why? Because they can.\n\n\nThe Emissary: serves the World by ensuring its continuation\n\n“A world wants to emerge as an infinite game: one that keeps on going, invites new agents to keep it in play, is fertile with surprises, and continues to generate unexpected meanings”\n\n\n\nCrucially, each of these masks can be worn by a single person: the Artist\n\n“It requires the Artist to abandon any sense of a coherent self and take on a different kind of psychology. One that moves between the artistic masks that already exist unevenly inside us, letting each contribute to the creation of a World.”\n\nWorld Seed\n\nthe world seed\n\n“It is made so that the created new worlds do not require one large server, but can run on multiple smaller servers.”\n“All games using The Seed as their game engine allow conversion of one account in a Seed-based world to another.\n\n\ninfrastructure that operates at different pace layers\n"},"thoughts/XGBoost":{"title":"XGBoost","links":["thoughts/regularization","thoughts/decision-tree"],"tags":["seed","CPSC340"],"content":"Uses regularized regression trees. These are like decision trees where each split is\n\nbased on a single feature\neach leaf gives a real-valued prediction\n\nFitting a regression tree\n\nTrain: set each weight wL​ at leaf L by minimizing squared error ∑i=1n​(wLi​​−yi​)2\n\nWe use using greedy recursive splitting for growing the tree\n\n\nPrediction: At each leaf, the prediction y^​i​ is the mean of all yi​ that fall under that leaf node\n\nEnsemble and Boosting\nWe create a series of trees that are trained on the residual of the previous tree. That is, the first tree is trained on the actual dataset. The second tree is trained on the residuals of the prediction of the first tree, and so on.\n\ntree[0] = fit(X,y)\ny_hat = tree[0].predict(X)\ntree[1] = fit(X,y - y_hat)\n`y_hat = y_hat + tree[1].predict(X)\ntree[2] = fit(X,y - yhat)\n`y_hat = y_hat + tree[2].predict(X)\n…\n\nRegularization\nAs long as not all wL​=0, each tree decreases training error. However, it may overfit if trees are too deep or you have too many trees\nWe can apply L0-regularization (stop splitting if wL​=0) and L2-regularization to discourage this."},"thoughts/Yjs":{"title":"Yjs","links":["thoughts/CRDT","thoughts/Hypercore","thoughts/Matrix","thoughts/CRDT-Implementations"],"tags":["seed"],"content":"\nYjs is a linked-list-based, network-agnostic CRDT implementation in Javascript.\n\nFrom GitHub Documentation\nYjs supports many different transport layers:\n\nWebRTC\nWebsockets\nLibp2p + GossipSub\nDat\nMatrix\n\nAt its heart, Yjs is a list-based CRDT:\n\nArrays are easy - they’re lists of arbitrary items\nText is a list of characters, optionally punctuated by formatting markers and embeds for rich text support\nMaps are lists of entries. The last inserted entry for each key is used, and all other duplicates for each key are flagged as deleted\n\nSyncing\nThe client can ask a remote client for missing document updates by sending their state vector (often referred to as sync step 1). The remote peer can compute the missing Item objects using the clocks of the respective clients and compute a minimal update message that reflects all missing updates (sync step 2).\nYATA\nThe underlying conceptual framework that Yjs builds on top of.\nOriginal paper\n\nYATA, an approach for peer-to-peer shared editing applications that ensures convergence, preserves user intentions, allows offline editing and can be utilized for arbitrary data types in the Web browser\n\nOne frustration is that applications based on complex models must therefore map the underlying data to the data structure that is supported by the used collaboration framework.\nYATA works by defining all data structures in terms of a doubly-linked list. Insertions take the form of insert(id, origin, left, right, isDeleted, content). origin is set at time of insertion and can’t be changed, but left and right are references that can change.\nThey use &lt;c​ to define a total ordering which depends only on the origin, where\no1​&lt;o2​⟺o1​ is a predecessor of o2​\nThis is quite similar to RGA"},"thoughts/Zooko's-Triangle":{"title":"Zooko's Triangle","links":["thoughts/petname"],"tags":["seed"],"content":"Postulates that names of participants can have at most 2 of these 3 properties\n\nHuman-meaningful: Meaningful and memorable (low-entropy) names are provided to the users.\nUnique: statistically impossible collisions\nGlobal: names are globally scoped, correspondence between name and the entity behind the name does not depend on context\n\nSee also: petname system"},"thoughts/academia":{"title":"Academia","links":["thoughts/play","thoughts/constructionist","thoughts/incentives","thoughts/independent-research","thoughts/Hackers","thoughts/generational-learning","thoughts/mimetic"],"tags":["seed"],"content":"Academia feels more pure, more playful, than industry? More of a ’constructionist’ approach, freedom to ask your own questions. But it also has its fair share of perverse/dated incentives\nSee also: independent research\nResearch Incentives\n\nResearch must be original — this drives people to stake out a piece of ground no one wants\nResearch must be substantial — awkward problems mean more substance to write about and solve, discouraging elegant solutions\n\nAcademics and scientists start good and get original. Hackers, from the start, are doing original work; it’s just very bad. They start original, and get good.\nOn Tenure\nDaniel Dennet:\n\n“The juvenile sea squirt wanders through the sea searching for a suitable rock or hunk of coral to cling to and make its home for life. For this task, it has a rudimentary nervous system. When it finds its spot and takes root, it doesn’t need its brain anymore, so it eats it! It’s rather like getting tenure.”\n\nThe ‘rat race’ of theoretical academia\n\nThere is no mechanism today, other than time, donations, and personal social platforms,  for researchers to support other researchers’ work. Every act of support is out of selflessness and there is a lack of incentive for cross collaboration other than having your name on another paper. The reward system in this community is highly dependent on your ability to make your research well known and marketed. Shrey Jain\n\nSource: Just ask for Generalization\nMy guess is that the theoretical research community (e.g. computer science, math) tends to reward narratives that increase intellectual complexity and argue that “we need better algorithms”. People pay lip service to “simple ideas” but few are willing to truly pursue simplicity to its limit and simply scale up existing ideas.\nThe typical computer science research project might last 4 years at the longest because that is the max duration of a PhD and post-doc. An average PhD candidate wants their own unique project; they don’t want to continue someone else’s project. So the academic world is a rapid succession of short-lived projects\nHad a talk with Stephen Fay about this and he mentioned that this isn’t necessarily true for ‘hard’ or ‘physical’ sciences. “In physics there are many huge international collaborations spanning decades (e.g. large radio arrays in poles+south africa+Canada, LIGO + LISA, CERN, James Webb, building quantum computers is going to take large collaborations even if research goes underground)”\nI think any research field with sufficient barrier entry/requirements for access to infrastructure/hardware/physical resources does necessarily require pooling of resources on an institutional and often multi-year/decade long timespan. But now the question is how we can encourage this sort of intergenerational research in theoretical fields?\nTunnel Vision\nSource: Mimetic by Brian Timar, see also mimetic\n“Graduate programs select for intensely competitive individuals with highly specific skills, often with negligible market value outside of universities. A strong desire for publications on esoteric topics is inherited from senior postdocs and professors, making tunnel vision especially acute.”"},"thoughts/access-control":{"title":"Access control","links":["thoughts/consistency","thoughts/CRDT","thoughts/Unix"],"tags":["seed"],"content":"\nAccess control systems guarantee that every action performed adheres to a set of rules, which can be dynamically changed at runtime.\n\nIn traditional systems, this guarantee can be enforced by relying on a central server. But this becomes a lot more difficult for eventually consistent systems (e.g. CRDTs)\nThere is a common perception of ACL systems and capability systems as merely alternative perspectives on Lampson’s access matrix:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAsset 1Asset 2FileDeviceRole 1read, write, execute, ownexecutereadwriteRole 2readread, write, execute, own\n\nAccess Control Lists\nAn Access Control list (sometimes called ACL) is a data structure containing entries that specify an individual user or group’s rights to specific system objects. Generally good when the groups of individuals remains relatively static and objects change a lot.\nIn the Lampson’s access matrix, they are normally seen as the columns.\nCapabilities\nThe capability list (sometimes called C-list) of a user or a process or domain is a list of rights that it has on the various objects. Generally good when groups of individuals changes a lot and objects remain relatively static.\nIn the Lampson’s access matrix, they are normally seen as the rows.\nCapabilities provide much better support for least-privilege operation and for avoiding confused deputy problems\nFrom Capability Myths Demolished:\n\nEquivalence Myth: access control list systems and capability systems are formally equivalent\n\nNo description of any security mechanism is complete without a specification of how access relationships are allowed to evolve over time\nFalse as shown in descriptions above\n\n\nConfinement Myth: capability systems cannot enforce confinement\n\nThe argument assumes that subjects can transmit capabilities anywhere they can transmit data, which is not the case in most capability systems.\nWe can get around this by limiting connections between objects\n\nIn partitioned or type-enforced capability systems such as KeyKOS, W7, EROS, or E, capabilities and data are distinguished by the kernel or runtime\n\n\nNo capability transfer can introduce a new connection between two objects that were not already connected by some path\nSuppose, for example, we decide not to trust Bob. To prevent Alice from delegating to Bob, we simply refrain from giving Alice access to Bob.\n\n\nIrrevocability Myth: capability-based access cannot be revoked\n\nIt is true that capabilities themselves are not literally revocable.\nFurther, we know that the capability alone is sufficient to establish access to the resource.\nHowever, we can create a pair of forwarders, F and R to get around this. Of this pair, we may call F the forwarding facet, and R the revoking facet. Any messages sent to F get forwarded through R to Carol, so Bob may use F as if it were Carol. This works as long as inter-object interactions are mediated by messages, and messages are handled generically, so that a reusable mechanism can forward any message. \nWhen Alice wants to revoke Bob’s access to Carol, she invokes R, telling it to stop forwarding. R then drops its pointer to Carol, and F becomes useless to Bob.\n\n\n\nAmbient Authority:\n\nWe will use the term ambient authority to describe authority that is exercised, but not selected, by its user.\nThe corresponding analogy is to imagine a world with doors but without keys. When a person walks up to a door, the door magically opens if it deems the person worthy.\nFor example, Unix filesystem permissions constitute an ambient authority mechanism, because the caller of a function such as open() does not choose any credentials to present with the request; the request merely succeeds or fails\n\nConfused Deputy Problems:\n\nA deputy is a program that must manage authorities coming from multiple sources.\nA confused deputy is a deputy that has been manipulated into wielding its authority inappropriately.\nA big part of preventing confused deputy problems is by removing ambient authority\n\nIf the authority to write to BILL were not ambient, then the compiler could hold one key to BILL for the purpose of writing billing information, and accept another key from the user for the purpose of writing debugging information. Then, as long as the compiler uses each key for its intended purpose, the confused deputy problem cannot occur.\n\n\n"},"thoughts/accountability":{"title":"Accountability","links":["thoughts/pseudonymity","thoughts/digital-commons","thoughts/virtual-worlds","thoughts/Moderation","thoughts/infrastructure","thoughts/group-limits"],"tags":["sapling"],"content":"How are parties (human and non-human) held accountable for their actions?\nReparations and reconciliation can only occur after both parties have acknowledged it. How does this work if one party ’ghosts’ or otherwise leaves the conversation? This is especially prevalent in online/pseudonymous interactions.\nHow do we keep people accountable in digital spaces and virtual worlds? Is it better to create better tools to help users manage digital boundaries or to shift the responsibility to the platforms? Do we do moderation at the infrastructure level?\nScaling emotional labour\nAlso note that the emotional labour that goes into holding a group accountable goes up in a democratic setting when more people are involved (see also: group limits). Is there a point where it is almost always better to have an authoritarian/impersonal third party? (e.g. the state and the law)"},"thoughts/affordance":{"title":"Affordance","links":["thoughts/Design-Justice"],"tags":["seed"],"content":"An affordance signifies what action is possible. It is the relationship between the properties of an objects and the capabilities of the user that determine how the object could be used. For example, a chair affords (“is for”) support and therefore affords sitting. If an affordance or anti-affordance cannot be perceived, some means of signaling its presence is required.\nAn object’s affordances are never equally perceptible to all, and never equally available to all.\nSee: Design Justice"},"thoughts/agency":{"title":"Agency","links":["thoughts/Tools-for-Conviviality","posts/agentic-computing","thoughts/In-Over-Our-Heads","thoughts/burnout","thoughts/taste","thoughts/credible-exit"],"tags":["sapling","pattern"],"content":"Agency is the ability and freedom for an individual to act in their immediate context or environment.\nSee also: Tools for Conviviality, agentic computing\nSelf-Determination Theory (SDT)\nWhen our social environments, including the places where we receive health care, are more supportive of these psychological needs, the quality of our motivation is more autonomous. Alternatively, when our psychological needs are not well met or even thwarted through our social interactions, the quality of our motivation is more controlled\nSelf-determination theory suggests that all humans have three basic psychological needs, which when met, help us be more intrinsically motivated in our actions (more internal locus of control)\n\nAutonomy: the feeling one has choice and agency in their own lives\nCompetence: experience of mastery and being effective in one’s activity\nRelatedness: need to feel connected and belongingness with others\n\nSee also: In Over Our Heads, burnout, taste\nIn software\n4 Principles, Brooklyn Zelenka at Causal Islands\n\nEmpower users to participate (entry)\nOption to leave (exit)\nControl access to your data (safety)\nProvide capacity to others (serve)\n"},"thoughts/algorithmic-decision-making":{"title":"Algorithmic decision making","links":["thoughts/GDPR","thoughts/consciousness","thoughts/bias","posts/bias-bug","thoughts/Algorithms-of-Oppression"],"tags":["seed"],"content":"GDPR in the EU states that any significant decision making cannot be based solely on automatic information processing (though, what is to say that the human consciousness isn’t just automatic information processing?)\nRelated: bias, bias-bug, Algorithms of Oppression"},"thoughts/antifragility":{"title":"Antifragility","links":["thoughts/fault-tolerance","thoughts/Universal-Scaling-Law"],"tags":["seed"],"content":"\nThe resilient resists shocks and stays the same; the antifragile gets better.\n\nAntifragility is a property of systems in which they increase in capability to thrive as a result of stressors, shocks, volatility, noise, mistakes, faults, attacks, or failures.\nThis is an ideal property to have for fault tolerant systems.\nSee also: Universal Scaling Law"},"thoughts/aperture-problem":{"title":"Aperture Problem","links":["thoughts/computer-vision","thoughts/context"],"tags":["sapling"],"content":"A problem in computer vision that arises when there is limited spatial context.\n\nWithout distinct features to track, the true visual motion is ambiguous\nLocally, one can compute only the component of the visual motion in the direction perpendicular to the contour\n"},"thoughts/art":{"title":"Art","links":["thoughts/intentionality","thoughts/money","thoughts/attention-economy","thoughts/pace-layers","thoughts/attention"],"tags":["sapling"],"content":"\nAn activity in which perfection can be pursued\n\nIn discussion with Wendi Yan\nWhat makes something “art” comes from its intention. How did the artist create the work? What is the personal experience or historical question their art responds to? Or did they seek to evoke certain emotions through their work?\nIn this sense, art is a form of derivative intentionality where the artist imparts it into the work. Is generative art still art? I still think so. You just have a fancier paintbrush.\n\nTrue art doesn’t care to be appreciated, obsessed over with, or owned. It holds a certain self-respect that knows enough of its own value to not plea for attention.\n\nTrue art is like a gift. They should not be explicitly marked with a monetary value. They should not be used as holds of value to be predicted and sold. “If art is important to you, you want to hold onto it, instead of trading it with others.”\nOn Crypto Art\nHas a lot of money in it right now but most (keyword: most) of the ‘value’ derived is from speculation and not necessarily new ways of exploring the medium or good/interesting art.\nYet, this new form of ‘art’ still carries the burden of its past. Crypto art still has not solved the curation problem. There is so much art (just take a look at open sea). How does one ensure their art is discovered, other than pleading to the whims of Twitter and to constantly blast their work in hopes that someone will pay attention?\nArt as entertainment\n\nMost of what the crypto world calls art is entertainment. They snatch your attention. They care more about making a tweet or a headline declaring they SOLD OUT, they set a HISTORICAL RECORD, and they tell you this is the art’s value.\n\nSituated at the top of the pace layers, true art should be much lower.\nOn AI Art\nSource\nHorning observes, “presume that thought is entirely a matter of pattern recognition, and these patterns, already inscribed in the corpus of the internet, can [be] mapped once and for all, with human ‘thinkers’ always already trapped within them. The possibility that thought could consist of pattern breaking is eliminated.”\n“The best art isn’t about pleasing or meeting expectations,” as Dan Cohen has put it in a recent essay about generative AI. “Instead, it often confronts us with nuance, contradictions, and complexity.”\nOn the contrary, Cohen concluded, “The desire of AI tools to meet expectations, to align with genres and familiar usage as their machine-learning array informs pixels and characters, is in tension with the human ability to coax new perspectives and meaning from the unusual, unique lives we each live.”\nIn other words, AI produces what it thinks we want to see\nThis is one way of thinking about what it means for a work of art to have depth. You can press in, and it won’t dissolve under a more attentive gaze. AI art does dissolve under deep attention\nQuestions to ask:\n\nHow will AI-generated images train our vision?\nWhat habit of attention does it encourage?\nWhat modes of engagement do they sustain?\n"},"thoughts/ask-vs-guess-culture":{"title":"Ask vs Guess culture","links":[],"tags":["seed"],"content":"phrasing from Jasmine\nIn a guess culture, there is a high social cost in asking for things. Individuals often guess at the reaction of the other in order to mitigate potential social cost (e.g. will not ask for things if it could be seen as out of place). They tend to ask for things less with the assumption that the other will most likely accept\nOn the other hand, ask culture implies a social cost in asking for things. As a result, individuals will just ask and expect to be rejected more often, generally more forward"},"thoughts/attention-economy":{"title":"Attention Economy","links":["thoughts/types-of-goods","thoughts/attention","thoughts/How-to-do-Nothing","thoughts/digital-mindfulness","thoughts/information-scaling-threshold"],"tags":["sapling"],"content":"\n”… a wealth of information means a dearth of something else - a scarcity of whatever it is that information consumes. What information consumes is rather obvious: it consumes the attention of its recipients. Hence a wealth of information creates a poverty of attention, and a need to allocate that attention efficiently among the overabundance of information sources that might consume it.”\n(Herb Simon (1971))\n\nMichael Goldhaber, who, in a series of essays in the late 1990s, argued that a new “attention economy” was emerging alongside the traditional economy of goods and services. “Ours is not truly an information economy,”\nAs a commodity\nAttention as a commodity, it is increasingly competitive to compete for everyone’s attention. People are not just the products but also the producers of the product (data).\nAttention is the main currency of production — what limits you from doing everything at once. Attention, then, is a common pool resource. It is non-excludable (anyone can bid for their attention) and rivalrous (limited attention).\nMore in Odell’s How to do Nothing, designing for slowness\nInformation Scaling\nSource\nSee: information scaling threshold\nThe wealth of information means a dearth of something else—a scarcity of whatever it is that information consumes. What information consumes is rather obvious: it consumes the attention of its recipients"},"thoughts/attention":{"title":"Attention","links":["thoughts/attention-economy","thoughts/time","thoughts/friendship"],"tags":["seed","pattern"],"content":"\nAttention is the stuff of manifestation. In the same way that looking at something while you’re driving takes you there, paying attention to anything (a field, an attitude, a person), will drive you uncontrollably toward it.\n(Spencer, secret places)\n\n\n“Attention, taken to its highest degree, is the same thing as prayer. It presupposes love and faith. Absolutely unmixed attention is prayer.” – Simone Weil\n\n\nA Thoreau quote: “It’s not what you look at that matters, it’s what you see.”\nWilliam Shaw: “we make things holy by the kind of attention we give them.”\n\n“Give” reminds us of the freedom of our choice to attend, or not; “pay” reminds us of attention’s costliness, and of the value of that to which we attend\nSee also attention economy\nJigs as attention guides\nSource\nA physical jig reduces the physical degrees of freedom a person must contend with. By seeding the environment with attention-getting objects (such as a knife left in a certain spot) or arranging the environment to keep attention away from something (as, for example, when a dieter keeps certain foods out of easy view), a person can informationally jig it to constrain his mental degrees of freedom. The upshot is that to keep action on track, according to some guiding purpose, one has to keep attention properly directed.\nIllich refers to “eutrapelia (or graceful playfulness),” and indeed there is grace in all true play. It is the grace of acting freely — and attending freely, to what delights or moves.\nDeep Attention\nThe bookstore with a single book. This is a tiny bookstore in Tokyo that sells a single book at a time in a small room. I really love the emphasis on getting to know a single book and author intimately — especially in an age of digital consumerism.\nHeraclitus said, “No man steps in the same river twice.” The second time around, both man and river are different than they were before. The paints and books are the same, but we change between reads and brushstrokes. Deep attention allows us to observe these changes not just as a snapshot but through time.\nSee also: friendship\nHolding (and scrolling) attention\n\nhttps://www.nytimes.com/interactive/2022/03/06/books/auden-musee-des-beaux-arts.html\nhttps://www.nytimes.com/interactive/2022/01/16/arts/design/jasper-johns-memory-of-my-feelings.html\n"},"thoughts/authenticator-complexity":{"title":"Authenticator complexity","links":["thoughts/consensus","thoughts/system-model","thoughts/digital-signatures"],"tags":["seed"],"content":"A measure of complexity (lower is better) for distributed consensus mechanisms in partially synchronous system model.\nDefinition: The sum, over all replicas i∈[n], of the number of authenticators received by replica i in the protocol to reach a consensus decision after GST.\nAn authenticator is either a partial signature or a signature.\nFigure 2: Our Network Protocol (from The Saddest Moment)"},"thoughts/authorization":{"title":"Authorization","links":["thoughts/digital-signatures","thoughts/UCAN","thoughts/access-control"],"tags":["seed"],"content":"Authorization is the process of verifying what a user has access to (whereas authentication is the process of verifying who someone is)\nJWT\n\nEach JWT contains encoded JSON objects, including a set of claims. JWTs are signed using a cryptographic algorithm to ensure that the claims cannot be altered after the token is issued.\n\nThree components (looks something like this: xxxxx.yyyyy.zzzzz)\n\nHeader: contains the type of token and signing algorithm\nPayload: contains the claims\nSignature: ensures the token hasn’t been altered\n\nThe party that creates the JWT signs the header and payload with\n\na secret that is known to both the issuer and receiver, or\na private key known only to the sender\n\nWhen the token is used, the receiving party verifies that the header and payload match the signature.\nSee also: UCAN, access control"},"thoughts/automatic-differentiation":{"title":"Automatic Differentiation (AD)","links":[],"tags":["seed","CPSC340"],"content":"\nInput: code computing a function\nOutput: code to compute one or more derivatives of the function.\n\nAD writes functions as a sequence of simple compositions\nf5​(f4​(f3​(f2​(f1​(x)))))\nAnd writes derivatives using the chain rule:\nf′(x)=f5′​(f4​(f3​(f2​(f1​(x)))))∗f4′​(f3​(f2​(f1​(x))))∗f3′​(f2​(f1(x)))∗f2′​(f1​(x))∗f1′​(x)\nWe decompose code using the chain rule to make derivative code. This can lead to a lot of redundant computations. We can use dynamic programming to avoid redundant calculations.\nMulti-variable AD\nWe define a computation graph as a DAG\n\nRoot nodes are the parameters (and inputs).\nBranch nodes are computed values (𝛼 values).\nLeaf node is the function value.\n\nTwo stages (example of a function that takes x and y and calculates a z):\n\nForward AD pass is called forward propagation:\n\nComputes z from x and y and passes it to its outputs\nStoring intermediate calculations ∂x∂z​ and ∂y∂z​\n\n\nBackward AD pass is called backpropagation:\n\nStarts from the end with ∂L/∂L which is just 1\nComputes ∂x∂L​ and ∂y∂L​ from ∂z∂L​\n\nUsing intermediate calculations stored during forward pass\n∂x∂L​=∂z∂L​∂x∂z​\n∂y∂L​=∂z∂L​∂y∂z​\n\n\n\n\n"},"thoughts/automation":{"title":"Automation","links":["posts/agi","thoughts/blockchain","posts/play","thoughts/positive-sum","thoughts/plurality"],"tags":["sapling"],"content":"Source: What Tech Futurists Get Wrong About Human Autonomy by Divya Siddarth and Kelsi Nabben\nAI systems and blockchain are both means to achieve the same “underlying desire: to implicitly or explicitly abstract away human fallibility in service of a fully automated vision of perfection.”\nAI focuses on autonomy of outcome, blockchain focuses on autonomy of process.\nPrevalent in post-work society thinking, especially one of a positive sum sum, post-scarcity world.\n\n“However, the link between individual autonomy and the collective good has always been pernicious — and nowhere more so than in the visions of technology that promise to give us both in unparalleled measure.”\n\nInstead, we need to push not for technological determinism but for technological pluralism."},"thoughts/autopoiesis":{"title":"Autopoiesis","links":["thoughts/emergent-behaviour"],"tags":["seed"],"content":"\nA system capable of producing and maintaining itself by creating its own parts\n\nOften times, emergent behaviour is a form of autopoiesis. This concept mostly refers to biological behaviour, but we see analogs in artificially created things like quines for examples\n"},"thoughts/autoregressive-model":{"title":"Autoregressive Model","links":[],"tags":["seed"],"content":"A statistical model is autoregressive if it predicts future values based on past values. For example, an autoregressive model might seek to predict a stock’s future prices based on its past performance."},"thoughts/awareness":{"title":"Awareness","links":["thoughts/the-Self","thoughts/phenomenology"],"tags":["seed"],"content":"Generally referred to as an awareness of the Self\nAn inverse is absence, particularly visual absence\n\nAbsence by occlusion: parts are hidden\nAbsence by vacancy: characterized by lack of visual information\nPure visual absence: no visual information is provided\n\nOn Having No Head\nD. E. Harding\nOn ego-death\n\nWhat actually happened was something absurdly simple and unspectacular: just for the moment I stopped thinking. Reason and imagination and all mental chatter died down. For once, words really failed me. I forgot my name, my humanness, my thingness, all that could be called me or mine. Past and future dropped away. It was as if I had been born that instant, brand new, mindless, innocent of all memories. There existed only the Now, that present moment and what was clearly given in it. To look was enough.\n\nFirst-person methods are central to the philosophical movement known as phenomenology."},"thoughts/bandwidth":{"title":"Bandwidth","links":["posts/networked-thought","thoughts/hypertext","thoughts/the-garden-and-the-stream","thoughts/qualia"],"tags":["sapling"],"content":"Hypertext\nSource: Hypertext Montage by Gordon Brander\n\nWhen we talk together, we flatten our N-dimensional thoughts into 1D linear narratives, in order to fit them through the 1D bottleneck of words. When we draw or sketch, we flatten our N-dimensional thoughts into 2D images in order to fit them through the 2D bottleneck of sight.\n\nWhat is the shape of networked-thought?\nSee also: hypertext, the garden and the stream\nInterpretation\nDoes moving closer to real-time communication mean less interpretation is required? Less expressive mediums like text mean that a lot of the emotion and meaning is left up to the reader to interpret and guess at. As we move to higher bandwidth mediums (e.g. calls and video), is there less room to interpret?\nWhat does this mean for art which inherently requires interpretation? Will we ever get to a communication medium so direct (e.g. mind-to-mind) that it doesn’t require interpretation? What about qualia and the subjective human experience?"},"thoughts/banking":{"title":"Banking","links":["thoughts/money"],"tags":["seed"],"content":"Source: Banking in the Shadows, Kernel\nAccount or balance sheet-style thinking is a banker’s way of being able to hold complementary opposites and demand in mind.\nSee also: money\nPrinting Money\nIt’s often not correct to say that when the Fed is ‘printing money’ they are straight up just injecting into one side. Most times, it expands both sides of the balance sheet.\nPrices of Money\n\nInterest Rate (future) — price of money now in terms of price of money later\nPart (now) — price of one money in terms of another money (e.g. between states, between private and central bank)\nExchange Rate (foreign) — price of domestic money in terms of foreign/world money\nPrice level (commodities) — price of money in terms of price level of commodities\n"},"thoughts/bias":{"title":"Bias","links":["posts/bias-bug","thoughts/Design-Justice","thoughts/To-Live-in-their-Utopia","thoughts/Social-Bias-in-Information-Retrieval","thoughts/Algorithms-of-Oppression","thoughts/data-distributions","thoughts/context","thoughts/Do-Artifacts-Have-Politics"],"tags":["sapling"],"content":"Bias: a slant or preference\n\n“We use the term bias to refer to computer systems that systematically and unfairly discriminate against certain individuals or groups of individuals in favour of others… A system discriminates unfairly if it denies an opportunity or a good or if it assigns an undesirable outcome to an individual or group of individuals on grounds that are unreasonable or inappropriate” (Friedman and Nissenbaum, 1996)\n\nMore detailed post on Bias in AI. Related readings: Design Justice, To Live in their Utopia, Social Bias in Information Retrieval, Algorithms of Oppression, data distributions\nCaptchas\nHow do you distinguish between human and non-human without discriminating against certain types of people (e.g. ethnicity, cultural background)? How does one prove their humanity without betraying anything else about them?\n\n“What is the universal human quality that can be demonstrated to a machine, but that no machine can mimic? What is it to be human?”\n“You need something that’s easy for an average human, it shouldn’t be bound to a specific subgroup of people, and it should be hard for computers at the same time. That’s very limiting in what you can actually do. And it has to be something that a human can do fast, and isn’t too annoying.”\n\nPossibility of reverse CAPTCHAs where you can only pass if you get it wrong in the ‘right’ way? (e.g. optical illusions)\n3 groups of study\nfrom Design Justice and Friedman\n\nPreexisting Bias: bias that exists in broader society, culture, and/or institutions is reproduced in the computer system, either intentionally or unintentionally, by systems developers. (e.g. notions of quality and authority bias embedded in the web content itself)\nTechnical Bias: some underlying aspect of the technology reproduces bias (e.g. design of crawlers/aggregate/surfacing algorithms for content, ranking features)\nEmergent Bias: may not have been biased given its original context of use or original user base but comes to exhibit bias when the context shifts or when new users arrive (e.g. responses to spam, content moderation, search suggestions)\n\nCathy O’Neil: algorithms are “opinions embedded in code” — artifacts do indeed have politics\nBaeza-Yates\n\nActivity Bias: who contributes to the data? who is seen by these algorithms?\nData Bias: is the underlying data biased/non-representative?\nSampling Bias: what data is used by algorithms?\nAlgorithmic Bias: what gets shown to users?\nInteraction Bias: how do people use the algorithms?\nSelf-selection Bias: who uses these algorithms?\nSecond-order Bias: digital trace data, how do our data-residues\n\nForbidden Rates\nCoined by Tamar Gendler\nWe do not live in perfectly egalitarian societies, and race, gender, class and other identities can significantly affect how our lives work out.\n\nNow suppose you’re at a reception for engineers and their spouses, and you’re introduced to a male–female couple about whom you know next to nothing. Odds are, he’s the engineer. But if you have anti-sexist instincts, you may feel pulled towards keeping an entirely open mind about which of these two strangers is the engineer, rather than allowing your statistical knowledge to incline you towards the man. If you do ‘slip’ into assuming the man to be the engineer, and this turns out to be a mistake, you’re likely to be more embarrassed than you would be had you wrongly assumed the couple to live in the local area, on the grounds that most guests at the reception live locally.\n"},"thoughts/binary-classification":{"title":"Binary Classification","links":["thoughts/SVM"],"tags":["seed","CPSC340"],"content":"\nSet yi​=+1 for one class (“important”)\nSet yi​=−1 for the other class (“not important”)\nTo predict, we look at whether wTxi​ is closer to +1 or -1\n\ny^​i​=sign(wTxi​)\n\n\n\nLeast squares error may overpenalize. Only thing we care about is the sign, not how far away it is from the decision boundary.\nCould we instead minimize number of classification errors? This is called the 0-1 loss function: you either get the classification wrong (1) or right (0).\nL(i,j)={01​i=ji=j​\n\nIllustration above is if yi​=1. Flip for yi​=−1\nUnfortunately, 0-1 Loss is non-convex. We can, once again, use a convex approximation which is called the Hinge loss:\nL(i,j)=max(0,1−yi​wTxi​)\nSee also: SVM\nThis is an upper bound on the 0-1 loss (as illustrated by the picture). For example, if the hinge loss is 18.3, then the number of training errors is at most 18.\nSimilarly, we can use the log-sum-exp trick to get the logistic loss which is convex and differentiable.\nL(i,j)≈log(1+exp(−yi​wTxi​))\nPerceptron\nOnly works for linearly-separable data\n\nSearches for a w such that sign(wTxi​)=yi​,∀i\nIntuition is that you search for the ledge\nStart with w0=0\nClassify each example until we reach a mistake\n\nThen, update w to wt+1=wt+yi​xi​\n\n\nIf a perfect classifier exists, this algorithm finds one in finite number of steps\n"},"thoughts/bitcoin":{"title":"Bitcoin","links":["thoughts/peer-to-peer","thoughts/blockchain","thoughts/Protocol","thoughts/proof-of-work","thoughts/consensus"],"tags":["seed"],"content":"Peer to peer electronic cash built using blockchain. Capital ‘B’ Bitcoin is the network and protocol, lower case ‘b’ bitcoin is the actual currency.\nHere, centralized intermediaries (banks) are replaced by a trustless network of ‘miners’ which use proof of work for consensus.\nMining\nCompeting to solve a cryptographic puzzle to earn rights to add a new block to the blockchain. Reward is new bitcoin.\nHashing then, is the process of guessing a ‘nonce’ (pseudo-random number which is used to initialize communication) that when entered with the previous block information into SHA-256, generates an output deemed satisfactory by the Bitcoin protocol. If the nonce found can be verified by the other miners, then can add the new block to the network and earn bitcoin.\nTo prevent transaction data from being altered, Bitcoin employs an algorithm in which tampering of transaction details will result in large difficulty increases in the puzzle. As a result, it would be extremely difficult to achieve consensus around tampered data."},"thoughts/bitemporal":{"title":"Bitemporal","links":["thoughts/time","thoughts/Merkle-DAG","thoughts/bitemporal","thoughts/incremental-view-maintenance"],"tags":["seed"],"content":"Bitemporality is a technique for modeling time in databases and data structures such that…\n\nThe state at any point in history can be recovered\nAlternative timelines can be forked\n\nIt’s basically time travel! Some Merkle-DAGs, if structured properly, can have this property\nDatalog\nWe can make Datalog fact stores bitemporal if we represent Datalog facts as 4-tuples: (entity, attribute, value, causality)\nTime travel and conflict resolution mean recomputing views from arbitrary points. How do we avoid starting from the beginning? This is where incremental view maintenance comes in"},"thoughts/black-box":{"title":"Black boxes","links":["thoughts/paperclip-optimizer","thoughts/infrastructure","thoughts/trust","thoughts/epistemology","thoughts/transparency"],"tags":["sapling"],"content":"See also: paperclip optimizer\n\nScientific and technical work is made invisible by its own success. When a machine runs efficiently, one need only to focus on its inputs and outputs and not on its internal complexity. Thus, paradoxically, the more science and technology succeed, the more opaque and obscure they become\n\nIf we start to disect the black box and understand that it\n\nis made by people\nsubstitutes their actions\nis a permanent delegate of the work\nshapes human action by prescribing what sorts of people can pass through it\n\nThen this is called “opening the black box” or “infrastructural inversion” for larger scale infrastructures\nJim Johnson: building and rebuilding walls everytime you use it is a waste, thats why we have doors as infrastructure that saves a lot of this repetitive work.\nComputational Reliabilism (CR)\nWho is afraid of black box algorithms? On the epistemological and ethical basis of trust in medical AI by Juan Manuel Durán, Karin Rolanda Jongsma\nOn trust in black box algorithmic decision making systems\nBlack boxes are algorithms that humans cannot survey: they are epistemically opaque systems that no human or group of humans can closely examine in order to determine its inner states. Physicians have a hard time offering accounts of how the algorithm came to its recommendation or diagnosis\nTerms:\n\ntransparency: algorithmic procedures that make the inner workings of a block box algorithm interpretable to humans\n\ntransparency is an epistemic manoeuvre intended to offer reasons to believe that certain algorithmic procedures render a reliable output and that the output of the algorithm is interpretable by humans\n\n\nopacity: inherent impossibility of humans to survey an algorithm both understood as a script as well as a computer process. Burrell proposes 3 types of opacity\n\nIntentional corporate or state secrecy\nTechnical illiteracy\nArising out of the scale of machine learning algorithms\n\n\n\nClaim: transparency will not provide solutions to opacity, and therefore having more transparent algorithms is not a guarantee for better explanations, predictions, and overall justification of our trust in the results of an algorithm\nComputational reliabilism (CR)\n\noffers epistemic justification for the belief that the algorithm is reliable and its results are trustworthy\nmain claim: researchers are justified in believing the results of AI systems because there is a reliable process that yields, most of the time, trustworthy results.\nformal definition “the probability that the next set of results of a reliable (AI system) is trustworthy is greater than the probability that the next set of results is trustworthy given that the first set was produced by an unreliable process by mere luck”\n\nin regular language: given two results are the same, we should consider the one generated by a reliable system to be more trustworthy\n\n\nreliability indicators\n\nverification and validation methods: building and measuring dev confidence in the computer system. Verification is assessment of accuracy with comparison to known solutions, validation is the assessment of accuracy with comparison to experimental data\nrobustness analysis: figure out whether results of a given model are an artefact of the model or related to the core features of the model\na history of (un)successful implementations: scientific and engineering methodologies and practices related to designing, coding, and running algorithms\nexpert knowledge: experts’ judgements, evaluations, and sanctioning\n\n\n\nResponsibility gaps\n\na physician cannot be held responsible for results of algorithms they don’t understand though, we do generally accept ex-post explanations and deem these sufficient of human actors in decision making\nphysicians typically operate other technologies and machinery which they do not fully understand or cannot fully explain the inner working of (e.g. MRI scans)\n\nDebatable; because they are not making decisions, just presenting information. Additionally, these other technologies generally can be understood by an expert. This is not the case for AI systems\n\n\n\nCounterpoints raised:\n\nBlack box algorithms can hide normative assumptions: we often know nothing about the priors of the black box algorithm\nModel and data drift: computationally reliable black box algorithms can be reliable in one setting and time and not everywhere and forever\n"},"thoughts/block-reference-mechanisms":{"title":"Block-reference mechanisms","links":["thoughts/Postel's-Law"],"tags":["seed"],"content":"Source\nPurple\nSource\n\nProduce HTML documents that can be addressed at the paragraph level\n\nIt does this by automatically creating name anchors with static and hierarchical addresses at the beginning of each text node, and by displaying these addresses as links at the end of each text node.\nPurple numbers are stable. That is, they stay the same once generated, even if the text around them changes. This gives you a “hook” to link to that won’t disappear.\nIn HTML, you might implement Purple Numbers through writing IDs into elements. It’s like dog-earing the page of a book to find your way back.\nHowever, this doesn’t work if you want to go anymore granular than a paragraph.\nStandoff markup\nPopularized by Project Xanadu\nThe core metadata is contained in a file called an EDL, or Edit Decision List. An EDL is a list of links, together with a range. The range describes a start position, offset from the beginning of the document, plus a length.\nspan: http://hyperland.com/xuCambDemo/WelcXu-D1y,start=25,length=567\nspan: http://xanadu.com/xanadox/MoeJuste/sources/0-Moe.pscr.txt,start=7995,length=274\nspan: http://hyperland.com/xuCambDemo/WelcXu-D1y,start=592,length=37\n\nText fragment links\nThe basic notion is that you can reference parts of a document by including a snippet of the text you want to reference in the URL. A text fragment link will only break if the specific text fragment being referenced disappears from the document.\n\nSee also: Postel’s Law"},"thoughts/blockchain":{"title":"Blockchain","links":["thoughts/web3-critique","thoughts/bitcoin","thoughts/Solana","thoughts/ethereum","thoughts/State-Machine-Replication-(SMR)","thoughts/Byzantine-Faults","thoughts/longest-chain-consensus","thoughts/system-model","thoughts/decentralization","thoughts/proof-of-work","thoughts/proof-of-stake","thoughts/pace-layers","thoughts/infrastructure","thoughts/truth","thoughts/plurality"],"tags":["sapling"],"content":"See also: web3 critique\nSource: A Primer to Web3\nOn a technical level, blockchain is just a linked list or graph that is replicated. Can be used to build cryptocurrencies like Bitcoin, Solana, and Ethereum.\nA blockchain is a SMR protocol that has a 3-layer architecture.\n\nConsensus core: forms agreement over an immutable sequence of updates to a shared state: byzantine fault-tolerant. The consensus algorithm most commonly used are longest-chain consensus algorithms and solve BFT for a permissionless system model\nState-machine API: Bitcoin’s state-machine and state-updates use a limited scripting language; Ethereum expands the state-machine and state-updates with a Turing complete abstraction (whose resources are bounded using gas).\nApplication Layer: In Bitcoin the application is the shared provenance tracking of digital assets, and in Ethereum, could be anything decentralized.\n\nCharacteristics:\n\nDistributed: data is stored by and updates are broadcasted to everyone\nSmart Contracts: codified agreements. Once the predetermined conditions of the contract are met, the transaction and attached computation are completed and recorded on the blockchain.\nImmutable: A completed transaction can never be changed or hidden. This gives us provenance of assets (you can determine any asset’s entire history as long as all transactions happen on-chain)\nDecentralized: communal consensus rather than one party’s decisions determines access/update to the chain\n\nEach block contains the hash of the previous block header and the Merkle root representing the hash of all the transactions in that block.\nTransactions happen as follows:\n\nA transaction is initiated by a client\nTransactions are checked for validity to see if the state transition is legal (validation)\nMultiple transactions are packaged into a block and sent to members of the chain (block proposal)\nConsensus and approval by rest of network (either proof of work or proof of stake)\nBlock is added to chain and distributed to members\n\nWhy it Matters\nIt feels like the level of change which blockchain impacts is huge but at the same time latent. After going through a bunch of Kernel modules, Austin came upon a name for the concept: “blockchain changes what soil is”\n“The “new world” seems to look very similar to the old. i.e. isn’t a DAO just voting on how to donate money, which we could do today?”\nIn actuality, the layer at which change is happening is much deeper (more infrastructure level than solution/product level)\nData\nSource: Ethereal Dreamers by Kernel\n“Back in early history, the databases were singular, existing in an atomic state with one DB per enterprise. The network existed in some relational sense between enterprises, but because DB’s were so fragile they never spoke directly to the network because then they broke. Even if you did connect DBs somewhat directly, the DB encodes the worldview of the organization and different organizations have different worldviews, so the DBs can’t speak to each other clearly.”\nThere were no large-scale computer-to-computer connections that allowed us to create a shared world view between lots of different organizations.\n\nThis is the promise of blockchains — to create a global ‘distributed database’\n\nThe goal is to build a single, shared story of reality, spread across all the machines simultaneously. And when it changes in one place, it changes everywhere.\nThoughts\nWaiting for the day web3 projects get large enough to amortize and offset the gas fees that consumers pay now (much like centralized orgs nowadays front hosting costs for ‘free’ tier or the average consumer).\nAct II — Smart Contracts\n“First we merged the network and the database in a blockchain. Then, we take computer software and put it into the shared database. That means everyone that is connected has a copy of exactly the same program: same data, same code, same result”\nAct III --- IOT\nThe ‘scaled blockchain’\nIdeally we get to a point where we have a ‘global computing service’ (very Asiimov’s Last Question-esque) through which we can embed IOT devices. This would turn all the sensors and bits of computing power into a global unified knowledge resources that manages the infrastructure of our society\nI’m curious how this relates to truth, specifically how different people have different views on ‘reality’ and ‘truth’. How do we reconcile that at a global scale? What might pluriversal computing look like?"},"thoughts/boundary-object":{"title":"Boundary Objects","links":["thoughts/contact-language","thoughts/notation"],"tags":["seed","pattern"],"content":"See also: contact language\nTrading Zone\nPeter Galisons’ notion of the trading zone: sites where representatives of multiple disciplines come together to work and establish contact languages for purposes of collaboration\nBoundary Object\nSusain Leigh Star and James Griesemer’s boundary objects: objects which both inhabit several intersecting social worlds and satisfy the informational requirements of each (e.g. maps, diagrams, standardized forms, notation, etc.)\nEcotone\nSource: Ecotone, Wikipedia\nAn ecotone is a transition area between two biological communities, where two communities meet and integrate. It may be narrow or wide, and it may be local (the zone between a field and forest) or regional (the transition between forest and grassland ecosystems)"},"thoughts/building-in-public":{"title":"Building in Public","links":["posts/collaborative-thinking","thoughts/effective-altruism"],"tags":["seed"],"content":"Source: You and Your Research by Richard Hamming\n“He who works with the door open gets all kinds of interruptions, but he also occasionally gets clues as to what the world is and what might be important.” — Richard Hamming\nYou may invite distractions but you also get a better sense of the world and the larger picture direction.\nRelated: Collaborative thinking\nAs a form of EA\nWorking and being vocal about your work is a way to amplify your effective impact radius. By doing so, you can change other people’s thinking around giving, money, and charity through discussion"},"thoughts/burnout":{"title":"Burnout","links":["thoughts/self-confidence","thoughts/In-Over-Our-Heads","thoughts/pain"],"tags":["seed"],"content":"Freudenberger (who first coined the term burnout): “[it is] almost always an indication that the person’s goals have been externally imposed. Somehow [they] embarked on his present course because it was expected of [them]… [they were] never the authentic source of [their own] choices and consequently they afford little real satisfaction”\nMaslach describes the “burnout prone individual” as one who mostly yield to the other without adapting to their own capacity: “is often unable to exert control over a situation and will passively yield to its demands rather than actively limiting them to his capacity to give… faced with self-doubt this person tries to establish a sense of self-worth by winning the approval and acceptance of other people”\nFrom In Over Our Heads\nRelated: pain is not the unit of effort"},"thoughts/calculus":{"title":"Calculus","links":[],"tags":["seed"],"content":"Partial Derivatives\nLinear\nFunctions with more than one variable. e.g. f(x) where x∈R3 the following multivariate linear\nf(x1​,x2​,x3​)​=a1​x1​+a2​x2​+a3​x3​+b=i=1∑3​ai​xi​+b=aTx+b​​​\nThe gradient is then the partial derivative with respect to each variable\n∇f(x)=​∂x1​∂f​∂x2​∂f​∂x3​∂f​​​=​a1​a2​a3​​​\nQuadratic\ne.g. f(x) where x∈R2 and A=[2−1​−11​]\nf(x)​=21​xTAx+bTx+c=i=1∑2​j=1∑2​aij​xi​xj​+i=1∑2​bi​xi​+c​​​\nIf A is symmetric, ∇f(x)=Ax+b. In the non-symmetric case, ∇f(x)=21​(A+AT)x+b\nGeneralizations of gradients for d dimensions given (expressed as matrices and vectors):\n\nA=XTX\nb = XTy\nc=21​yTy\n\nSo\n\n∇21​wTAw=Aw (if A is symmetric)\n∇wTb=b\n∇c=0\n"},"thoughts/calendar-as-task-manager":{"title":"Calendar as task manager","links":["thoughts/time"],"tags":["seed"],"content":"Source\nMost task management falls into three categories\n\nTask managers: To organize the things we need to get done\nEmail: To communicate with others\nCalendars: To manage our time\n\nHowever, these are all just ways of expressing ‘events’ in some calendar; emails are just tasks and tasks are just calendar events.\nIn fact, most of today’s email clients are built around the concept of Inbox Zero, which effectively turns your email inbox into a todo list with public write access.\n\nSee also: time"},"thoughts/carbon-credits":{"title":"Carbon Credits","links":[],"tags":["sapling"],"content":"Source: An Even More Inconvenient Truth from ProPublica\nCan we create ‘credits’ that companies can buy to offset their emissions by ‘proving’ that a forest is sequestering a certain amount of carbon?\nIn most of these cases, these carbon offsets don’t actually work. They either\n\ndon’t offset the amount they were supposed to do\nbought gains that were quickly offset\namount of carbon couldn’t be measured accurately\n\nUltimately leading polluters to get off scott-free while the planet still suffers. Is this still ok thought? As long as some trees are saved and foreign aid is going toward protecting forests\nCarbon offsets are just “charismatic carbon” — they offer a feel-good story of environmental and social good without doing much\nOriginally was a compromise between European, American, and developing nations\n\nEuropean leaders wanted to create policies to encourage industry to pollute less\nAmericans wanted flexibility\nDeveloping nations (e.g. Brazil) wanted money to deal with climate change\n\nOn paper, this seemed perfect: if it cost too much to reduce emissions in one area, you could ‘buy’ the equivalent offset. This in theory could incentivize orgs to try and develop more green tech (e.g. solar plant in favour of a coal plant) in anticipation of carbon credit sales\nIn practice doesn’t work super well\n\nClean Development Mechanism: 85% of offsets had a ‘low likelihood’ of creating real impacts\nJoint Implementation: 75% of the credits issued were unlikely to represent real reductions\n\nLeakage: protecting one patch of land could lead to deforestation somewhere else as loggers work to try and meet quotas\nPreserving land and trees is hard in any developing country is hard: the poverty and lack of infrastructure “drives people to violate whatever protections are in place to plant crops or mine for gold or just have enough lumber to build their homes”\n\nWhen a tree is destroyed, all the carbon accumulated over its lifetime is released back into the atmosphere.\n\nBut there’s not a lot of incentive for local residents\n\nRubber sells for about 2 reais/kg (roughly enough for a cup of coffee) whereas a single cow is ~800 reais ($200)\nSustainable logging didn’t pay enough\nGovernment subsidies don’t consider enough things\n\nDigging fish ponds but they don’t produce enough\nRubber-tapping tries but can’t afford fertilizer for the poor Amazon soil\n\n\n\nGerrymandering but for trees? A lot of the offset calculations are based off of regional averages for carbon, projects can then take advantage of ‘unusually high’ concentration areas with certain types of trees to game the system and gain more credits than they should have.\nThis could actually benefit indigenous tribes because of their more conservative approach to logging — in fact, offsets can enable tribes to reduce logging. However, “some tribal members are deeply uncomfortable with the idea of selling offsets to companies like this even if they are legitimate, fearing they’re effectively profiting from pollution.”\nGood Offsets?\nSource: Buying Offsets\n\nTransparency: any individual credits can not be purchased more than once\nPermanence: credits should not be liable to be undone\nAdditionality: by facilitating this activity, more carbon was drawn down than would have been\nMonitoring and Verification: transparent and public processes for ensuring captured carbon stays where it should\n"},"thoughts/cargo-culting":{"title":"Cargo Culting","links":["thoughts/Chesterton's-Fence"],"tags":["seed"],"content":"A group of people that imitate the behaviors, rituals, and symbols associated with technologically advanced societies, particularly those characterized by transportation and material wealth, in the apparent hope of attracting similar benefits\nSee also: Chesterton’s Fence\nOriginally stems from a a Melanesian indigenist millenarian belief system"},"thoughts/cascading-failures":{"title":"Cascading failures","links":["thoughts/Internet"],"tags":["seed"],"content":"Effect can be approximated using a power law distribution\np(s)∼s−α\nwhere α is the avalanche exponent. α tends to hover around 1.5-2.\nSystems that display some sort of cascading failure are generally characterized by 3 key features:\n\nThe system is characterized by some flow over a network, like the flow of electric current in the power grid or the flow of information in communication systems.\nEach component has a local breakdown rule that determines when it contributes to a cascade, either by failing (power grid, earthquakes) or by choosing to pass on a piece of information (Twitter).\nEach system has a mechanism to redistribute the traffic to other nodes upon the failure or the activation of a component.\n\nHalting Cascading Failures\nTwo approaches come to mind\n\nAdding new links to increase connectivity and thus fc​. However, in most real systems the time needed to establish a new link is much larger than the timescale of a cascading failure.\nRemoving redundant links and nodes. The size of a cascade can be reduced if we intentionally remove additional nodes right after the initial failure (i), but before the failure could propagate.\n\nThe mechanism of 2. is similar to the method used by firefighters, who set a controlled fire in the fireline to consume the fuel in the path of a wildfire. In a counterintuitive fashion, controlled damage can be beneficial to a network: the Lazarus Effect\nThe growth rate of a bacteria is determined by its ability to generate biomass, the molecules it needs to build its cell wall, DNA and other cellular components. If some key genes are missing, the bacteria is unable to generate the necessary biomass. Scientists can revive these dead bacteria by removing five additional genes.\nExamples\nExamples from the Network Science Book’s Chapter on Network Robustness:\nBlackouts (Power Grid)\nAfter the failure of a node or a link the electric currents are instantaneously reorganized on the rest of the power grid. For example, on August 10, 1996, a hot day in Oregon, a line carrying 1,300 megawatts sagged close to a tree and snapped. Because electricity cannot be stored, the current it carried was automatically shifted to two lower voltage lines. As these were not designed to carry the excess current, they too failed. Seconds later the excess current lead to the malfunction of thirteen generators, eventually causing a blackout in eleven U.S. states and two Canadian provinces.\nSimilarly, one of the largest blackouts in North America took place on August 14, 2003, just before 4:10 p.m. Its cause was a software bug in the alarm system at a control room of the First Energy Corporation in Ohio. Missing the alarm, the operators were unaware of the need to redistribute the power after an overloaded transmission line hit a tree. Consequently a normally manageable local failure began a cascading failure that shut down more than 508 generating units at 265 power plants, leaving an estimated 10 million people without electricity in Ontario and 45 million in eight U.S. states.\nDenial of Service Attacks (Internet)\nIf a router fails to transmit the packets received by it, the Internet protocols will alert the neighboring routers to avoid the troubled equipment by re-routing the packets using alternative routes. Consequently a failed router increases traffic on other routers, potentially inducing a series of denial of service attacks throughout the Internet.\nFinancial Crises\nCascading failures are common in economic systems. For example, the drop in the house prices in 2008 in the U.S. has spread along the links of the financial network, inducing a cascade of failed banks, companies and even nations. It eventually caused the worst global financial meltdown since the 1930s Great Depression.\nScheduling\nAirline schedules include a buffer period between consecutive flights to accommodate short delays. When a delay exceeds this buffer, subsequent flights that use the same aircraft, crew or gate, are also delayed. The consequences of bad weather or mechanical failures can cascade through airline schedules, delaying multiple flights and stranding thousands of passengers.\nSupply and Food Chains\nThe disappearance of a species can cascade through the food web of an ecosystem, inducing the extinction of numerous species and altering the habitat of others.\nThe shortage of a particular component can cripple supply chains. For example, the 2011 floods in Thailand have resulted in a chronic shortage of car components that disrupted the production chain of more than 1,000 automotive factories worldwide. Therefore the damage was not limited to the flooded factories, but resulted in worldwide insurance claims reaching $20 billion."},"thoughts/catch-22":{"title":"Catch 22","links":[],"tags":["seed"],"content":"A double bind is a psychological predicament in which a person receives from a single source conflicting messages that allow no appropriate response to be made\n\nIf a pilot is deemed insane, they don’t have to fly.\nTo be deemed insane, a pilot must request to be evaluated.\nIf a pilot requests to be evaluated, this demonstrates that he must be sane.\nTherefore, no pilot can ever be deemed insane, and no pilot can get out of flying.\n\nThus, there is no way to win."},"thoughts/causal-decision-theory":{"title":"Causal decision theory","links":["thoughts/Decision-theory","thoughts/causality","thoughts/Decisions-under-ignorance","thoughts/Newcomb's-Problem"],"tags":["seed"],"content":"See also: evidential decision theory\nChoose the act that is most effective in bringing about the best result. Use causal conditional probabilities instead of evidential probabilities to compute expected utility.\nWhen states are causally independent of the actions (e.g., when they are fixed prior to the choice), use Dominance Reasoning (see DUI).\nSubjunctive Conditionals\nX→Y means that if I were to do X, then Y.\n\nP(X→Y) is the causal conditional probability of Y given that I do X\n\nTo calculate, fix the causal history of the world up to the moment you do or don’t do X\nThen determine how your choice of X or not X influences the probability of Y\nNormally, this is equivalent to P(Y∣X) but not true for Newcomb’s Problem\n\n\nP(Y∣X) is the evidential conditional probability of Y given X\n\nCausal Dependence\nThree cases\n\nAct A causes of influences state S\nState S causes or influences act A\nSome common C causes or influences both S and A\n\nNote some properties:\n\nIn all cases, P(S∣A)&gt;P(S∣¬A)\nIn cases 2 and 3 P(A→S)=P(¬A→S)=P(S) in other words, A has no effect on S\n\nGranger Causality\nThe Granger causality test is a statistical hypothesis test for determining whether one time series is useful in forecasting another, first proposed in 1969.\nA time series X is said to Granger-cause Y if it can be shown, usually through a series of t-tests and F-tests on lagged values of X (and with lagged values of Y also included), that those X values provide statistically significant information about future values of Y.\n\nUsing the term “causality” alone is a misnomer, as Granger-causality is better described as “precedence”, or, as Granger himself later claimed in 1977, “temporally related”. Rather than testing whether X causes Y, the Granger causality tests whether X forecasts Y."},"thoughts/causal-tree":{"title":"Causal Tree","links":["thoughts/hypertext","thoughts/CRDT","thoughts/causality","thoughts/Order-theory"],"tags":["seed"],"content":"\nTowards real-time read-write hypertext\n\nTLDR; a simpler and more understandable form of CvRDT that relies on a strong notion of happens-before causal relationships and unique identifiers.\nPaper summary\n\nThe objective is to automate information dissemination the way hyperlink automated associations and search engines automated search.\nSpeculatively, as people become more and more densely connected, they are more and more aware of each other’s details. As a consequence, communications naturally gravitate to compact and speedy update-only forms\nCausal Trees\n\nA tree of atoms. Atoms are triples of (id, causing_id, letter).\n\nAny set of changes can be represented as a set of atom removals and insertions.\nAtom removals are represented by a special “backspace” atom\nid is generated in the form of a owner UUID and a Lamport timestamp\n\n\nAn atom’s id is always greater than its causing_id. This defines a partial order\n\nThus, the tree is a causality tree where each causing atom acts as a parent to its caused atoms\nAtoms are stored in append-only causality feeds. Every feed complies with that order: the causing atom always precedes any of its caused atoms\n\n\nInserts happen directly to the right of its causing_id (or parent)\nA yarn is a full contiguous sequence of operations at a node\n\n\nMerging feeds: sort by id\n\nThis is actually called a weave, which contains every piece of the file to ever exist as well as all the special characters (e.g. backspace)\nNote that backspace only marks an atom as inactive, it is never actually removed\n\nThis “backspace” atom has high priority so always hugs its parent in the resulting weave\n\n\nRecovering the plaintext version of the weave is constructed by removing inactive atoms from the weave.\n\n\n\n\nAnyone writing something based on causal trees only needs to define two functions:\n\nReducers: inserts arbitrary atoms into an ordered log\nMapper: traverses the structured log to arrive at a state\n\n"},"thoughts/causality":{"title":"Causality","links":["thoughts/Order-theory","thoughts/time"],"tags":["seed"],"content":"Causality\nWhen a→b then a might have caused b\nWe say event a happens before event b (a→b) iff:\n\na and b occurred at the same node, and a occurred before b in that node’s local execution order\na is the sending of some message m and b is the receipt of that same message m (assuming sent messages are unique)\nthere exists some c such that a→c and c→b\n\nConcurrency\nWhen a∥b then a cannot have caused b (and vice versa)\nConcurrent does not mean simultaneous, it means two things did not know about each other when they occurred (a is concurrent with b is written as a∥b)\nCausal Order\n≺ is a causal order, it is consistent with causality, a strict total order on events. Usually called the ‘happens-before’ relation\nSee also: Order theory\nTime\nIn Einstein’s theory of general and special relativity\nCausality means that an effect can not occur from a cause that is not in the back (past) light cone of that event. Similarly, a cause can not have an effect outside its front (future) light cone.\n\nThis causes us to be trapped in causal islands. Because data can only travel at the speed of light, we can’t keep up with each other all the time.\nThe normal solution is to work in parts (Spanner does this) where the light cones of the different sites intersect. However, only working in this case called causal subjectiity.\nSee: time"},"thoughts/censorship":{"title":"Censorship","links":["thoughts/infrastructure","thoughts/public-goods","thoughts/freedom"],"tags":["sapling"],"content":"Source: Moderation in Infrastructure\nHow do we draw the line between an end product and infrastructure? How should infrastructure regulate usage on its platform (if at all)? I’ve been thinking about AWS’s decision to remove Parler recently and whether it was warranted for AWS to do so. At what level of infrastructure should something become a ’public good’? As more and more of our digital infrastructure is built out under private companies, does it change how we govern content on top of it?\nThe benefit of retroactive correction in some instances—imagine fixing a typographical error in the proportions of a recipe, or blocking out someone’s phone number shared for the purposes of harassment—should be contextualized against the prospect of systemic, chronic demands for revisions by aggrieved people or companies single-mindedly demanding changes that serve to eat away at the public record.\nIs there a difference between censorship at the broadcast level and receiving level? People should choose what they want to see but have no overarching rules over what is ‘illegal’ content to create/broadcast. States can still mandate for example, required filters for receiving content.\n3 Types of censorship\n\nGovernment monopolization (e.g. former Soviet union owned all the media outlets)\nPrepublication Review (e.g. government prevents information from being revealed, like nuclear weapons program details)\nLicensing and registration (e.g. media with limited bandwidth like radio means a centralized authority divides up the space)\n\nSelf-censorship of press\nPublishers know that if they offend the government, their reporters may not be given access to as much information as reporters for rival publications, putting them at a competitive disadvantage. This knowledge can lead a “free” press to censor itself.\nLiberalism\nJohn Stuart Mill’s argument for free speech and that censorship should never be justified (arguing for liberalism)\n\nAll of us are capable of error. If we prevent someone from voicing their opinion, we may actually be silencing the voice of truth.\nErroneous opinions may contain kernels of truth. We ought to let all opinions be voiced so that all parts of the truth are heard\nThe whole truth left untested is simply a prejudice.\nAn opinion that has been tested in the fire of a free and open discourse is more likely to have a “vital effect on the character and conduct”\n\nWhen should intervention happen? Mill’s Principle of Harm states that the only purpose for which power can be rightfully exercised over any member of a civilized community, against his will, is to prevent harm to others.\nSelf-censorship\nLibrex and the free exchange of ideas on college campuses\nPeople go to colleges not to just read textbooks but to talk to professors who’ve studied it deeply and get their opinions on it.\nBut what happens when those professors feel like their can’t express their opinions? What happens when a student is offended by that opinion and the professor is at risk of losing their job?\n\n“61% of students on Ivy League campuses are afraid to speak their minds because of campus culture”\n\nIt is much more emotional labour to separate the public and private selves; one needs to remember what parts of yourself to portray to what people"},"thoughts/change-of-basis":{"title":"Change of basis","links":["thoughts/Gaussian-RBF","thoughts/latent-factor-model"],"tags":["seed","CPSC340"],"content":"Effectively by constructing new features that take the variable to certain powers. To get a y-intercept (bias), we just raise x to the 0th power to get 1. We can fit polynomials of degree p by raising other powers:\nZ=​11⋮1​x1​x2​⋮xn​​x12​x22​⋮xn2​​……⋱…​x1p​x2p​⋮xnp​​​\nAs the polynomial degree increases, the training error goes down but the approximation error goes up.\nChoosing a basis is hard! We can do something like Gaussian radial basis functions (RBFs) or polynomial basis as these are both universal approximators given enough data.\nKernel Trick\nLet Z be the basis. With multi-dimensional polynomial bases, actually forming Z which is k=O(dp) is intractable.\nRepresent each column of Z as a unique term. For example, with an X of d=2, we can use p=2 to get\n\nWe compute u=(K+λI)−1y\nand for testing:\ny^​​=Z~v=Z~ZT(ZZT+λI)−1y=K~(K+λI)−1y=K~u​minimum of L2-regularized least squares: v=ZT(ZZT+λI)−1yu is a (n,1) of kernel weights we learn​\nThe key idea behind “kernel trick” for certain bases (like polynomials) is that we can efficiently compute K and K~ even though forming Z and Z~ is intractable.\nWe call K=ZZT the Gram Matrix.\nFinally, we call the general degree-p polynomial kernel function Kij​=k(xi​,xj​)=(1+xiT​xj​)p. Computing k is only O(d) time instead of O(dp).\nThus, computing K is O(n2d+n3):\n\nForming K takes O(n2d) time\nInverting K+λI which is a (n,n) takes O(n3)\n\nAll of our distance-based methods have kernel versions\nLearned Basis\nWe can also learn basis from data as well. See latent-factor model"},"thoughts/climate-tech":{"title":"Climate Tech","links":["thoughts/carbon-credits","thoughts/infrastructure","thoughts/degrowth","thoughts/types-of-goods","thoughts/Do-Artifacts-Have-Politics"],"tags":["seed"],"content":"Not just about investing in carbon removal or carbon credits also about investing in the infrastructure to make it possible.\nThe Land Ethic simply enlarges the boundaries of the community to include soils, waters, plants, and animals, or collectively: the land. It changes the role of Homo sapiens from conqueror of the land community to plain member and citizen of it.\nRelated: degrowth\nMeasuring Offsets\n\nYou can not mitigate what you can not measure.\n\nThe Greenhouse Gas (GHG) Protocol consists of “Scopes”, each of which has multiple sub categories. In order to calculate our current carbon footprint, we looked into the emission categories within Scope 1, Scope 2, and Scope 3.\n\nScope 1: Direction Emissions. These are the business activities the company performs that directly create emissions.\nScope 2: Indirect Emissions. These consist of the purchased electricity, steam, heating, and cooling for own use.\nScope 3: Indirect Emissions. Includes goods and services, goods, fuel, transportation distribution, and business travel.\n\nNeutral Infrastructure\nSource: Microsoft Finds Underwater Data Centers Practical and Power, Pollution and the Internet, NYT.\nMight be software infrastructure but it has lots of political implications (e.g. climate impact, territory disputes, etc.). It’s ‘reduced cost’ does not factor in long-term negative externalities like warming the ocean.\n“Many of these solutions are readily available, but in a risk-averse industry, most companies have been reluctant to make wholesale change.” This is largely due to the anonymity involved in the industry where even the federal government was unable to determine how much energy its own data centres consume.\n\n“They don’t get a bonus for saving on the electric bill. They get a bonus for having the data centre available 99.999 percent of the time.”\n\nLow utilization can mean that the energy wasted is as much as 30x the amount of electricity used if it was at 100% utilization.\nCompany Initiatives\nStripe\nhttps://stripe.com/climate\ntldr; we need to develop new carbon removal tech, the ones we have right now are unlikely to scale to well enough to tackle the problem as it stands right now\n“Today, carbon removal solutions face a chicken-and-egg problem. As early technologies, they’re more expensive, so don’t attract a critical mass of customers. But without wider adoption, they can’t scale production to become cheaper.”\n“Early purchasers can help new carbon removal technologies get down the cost curve and up the volume curve. Experience with manufacturing learning and experience curves has shown repeatedly that deployment and scale beget improvement, a phenomenon seen across DNA sequencing, hard drive capacity, and solar panels.”\nShopify\nGeneral ethos: we don’t know what works so let’s invest in a little bit of everything\n\nThere’s no easy fix for climate change, and no solution that will singlehandedly solve the problem. We have to try every possible nature-based and engineered option.\n\nMix of solutions that make a difference now (evergreen tech: 24%) but also emerging tech with big impact down the line (frontier portfolio: 76%) Rationale\nFrontier\n\nDirect Air Capture: directly pulling carbon out of the air and storying it permanently\nProduct: storing carbon into usable products (e.g. concrete)\nOcean: large bodies of water as carbon sinks while also reducing ocean acidification\nBiomass: sequester carbon in organic material, using organic material for energy\nMineralization: turning carbon into a mineral to store\n\nEvergreen (creating a buffer against climate change until we figure out how to unscrew ourselves)\n\nForest: planting, restoring, and protecting forests\nSoil: farming techniques that improve soil health and carbon storage\nRenewable Energy: investing in fossil fuel alternatives\nTransportation: reducing emissions from transportation\n"},"thoughts/clocks":{"title":"Clocks","links":["thoughts/time","thoughts/Unix","thoughts/Network-Time-Protocol","thoughts/Order-theory","thoughts/causality"],"tags":["seed"],"content":"Measuring time in the context of computer systems\nPhysical Time\nTwo types of clock\n\nPhysical clock: number of seconds elapsed\nLogical clock: count events, e.g. messages sent\n\nTime is hard! So many different ways of measuring time\n\nGreenwich Mean Time (GMT): the normal human time format, based on Earth rotation\nInternational Atomic Time (TAI): some multiple of Caesium-133 resonant frequency\nCompromise, UTC is TAI with corrections to account for Earth rotation\nUnix Time: number of seconds since the epoch (Jan 1, 1970) not counting leap seconds\nISO8601: year, month, day, hour, minute, second, and timezone offset relative to UTC\n\nWe periodically adjust our local clocks with a server that has a more accurate time source using Network Time Protocol (NTP) or Precision Time Protocol (PTP)\nLogical Time\nComputers typically have a service like NTP which synchronizes computer clocks with well known time sources on the internet. Because of this, two consecutive readings of the system time on a given server can have time going backwards. As there is no upper bound on clock drift across servers, it is impossible to trust timestamps on two different servers as a way to infer causality!\nWe use logical clocks to work based off of the number of events that have occurred rather than actual time passed.\nLamport Clocks\nProvides a partial order on events\nLogic\n\nOn initialization, set t := 0 for each node\nOn any event on local node, fn tick() -&gt; t += 1\nOn sending message m, fn send(m) -&gt; tick(); actually_send(t, m)\nOn receiving fn receive(t&#039;, m) -&gt; t = max(t, t&#039;) + 1; do_something(m)\n\nProperties\n\nIf a→b then L(a)&lt;L(b)\nHowever, L(a)&lt;L(b) does not imply a→b\nPossible that L(a)=L(b) for a=b\n\nThis means that two identical Lamport timestamps might not correspond to the same unique event. However if we include the node N(e) for the node where event e occurred, then (L(e),N(e)) uniquely identifies event e.\nWe attempt to define a total causal order\n(a≺b)⟺(L(a)&lt;L(b))∨(L(a)=L(b)∧N(a)&lt;N(b))\nHowever even now, given timestamps L(a)&lt;L(b), we can’t tell whether a→b or a∥b\nTo separate causality from concurrent events, we need vector clocks!\nVector Clocks\nProvides a causal order on events\nInstead of having a single counter t for all nodes, we keep a vector timestamp a of an event for each node so we have V(a)=⟨t1​,t2​,…,tn​⟩ where ti​ is the number of events observed by node Ni​\nEach node has a current vector timestamp T, on an event on node Ni​, increment vector element T[i]\nLogic\n\nOn initialization , set t := [0] * n\nOn any event on node Ni​, fn tick() -&gt; t[i] += 1\n\nEach time a process experiences an internal event, it increments its own clock in the vector by one\n\n\nOn sending message m from node Ni​, fn send(m) -&gt; tick(); actually_send(t, m)\nOn receiving fn receive(t&#039;, m) -&gt; t = tick(); zip(t, t&#039;).map(max); do_something(m)\n\nThus, a vector timestamp of an event e actually represents all of its causal dependencies: {e}∪{a∣a→e}\nE.g. ⟨2,2,0⟩ represents first two events from N1​, first two events from N2​, and no events from N3​\nOrdering\n\nT=T′⟺T[i]=T′[i] ∀i∈{1,…,n} (T and T’ are same if each element has the same value)\nT≤T′⟺T[i]≤T′[i] ∀i∈{1,⋯,n} (T happened at the same time or earlier than T’ if each element in T is less than or equal to its value in T’)\nT&lt;T′⟺T≤T′∧T=T′ (T happened earlier than T’ if each element in T is less than its value in T’, at least one element in T differs from T’)\n\nT∥T′⟺T≰T′∧T′≰T (T is incomparable to T’)\n\n\n\nProperties (based on Order theory)\n\nV(a)≤V(b)⟺({a}∪{e∣e→a})⊆({b}∪{e∣e→b})\nV(a)&lt;V(b)⟺(a→b)\nV(a)=V(b)⟺(a=b)\nV(a)∥V(b)⟺a∥b\n\nYou can tell that versions are in conflict when neither vector clock “descends” from the other. In order for vector clock B to be considered a descendant of vector clock A, each marker in vector clock A must have a corresponding marker in B that has a revision number greater than or equal to the marker in vector clock A. Markers not contained in a vector clock can be considered to have revision number zero.\nVector Clock Example\n\nHybrid Logical Clocks\nPhysical and logical clocks both have non-ideal properties.\n\nLogical clocks don’t actually store any sort of date-time when events happen. Clients usually have a notion of time through actual wall time\nBUT wall time isn’t perfect either as clock drift is non-trivial and users can manually turn time backwards on their local machines\n\nNote that this is not a substitute for Vector Clocks as they only provide partial order instead of causal order\nCan we combine them to achieve better properties?\nHybrid Logical Clocks (HLCs) achieve\n\npartial ordering\nconstant space\nbounded different from physical time\n\nWe can store a tuple containing:\n\npt: physical time (wall time)\nl: logical time (holds maximum pt so far)\nc: capturing causality when l is equal\n\nThis tuple can be used directly as a replacement for a physical clock timestamp (and in fact works as a superposition on top of the NTP protocol without any interference)\nPseudocode\n\nInitial state\n\nl := 0\nc := 0\n\n\nSend / local event\n\nl&#039; := l\nl := max(l&#039;, pt) update l to pt if applicable\nif pt is the same (l == l&#039;):\n\nc += 1 increment causality as logical time is the same\n\n\nif pt is updated:\n\nc := 0 reset c\n\n\ntimestamp message with (l, c)\n\n\nReceive of message m\n\nl&#039; := l\nl := max(l&#039;, m.l, pt)\nif all logical clocks are the same l == l&#039; == m.l:\n\nc := max(c, m.c) + 1 set to max causality known\n\n\nif our logic clocks are the same but message logical clock is behind l == l&#039;:\n\nc += 1 (ignore as message clock is behind)\n\n\nif our logic clock was behind the message logical clock and just got updated l == m.l\n\nc := m.c + 1\n\n\notherwise pt was just updated\n\nc := 0 reset c\n\n\ntimestamp message with (l, c)\n\n\n"},"thoughts/clustering":{"title":"Clustering","links":["thoughts/K-means","thoughts/density-based-clustering","thoughts/Random-Forest","thoughts/hierarchical-clustering"],"tags":["seed","CPSC340"],"content":"Set of techniques to find components that belong together.\nNote: Grouping is how the human visual system perceives things and clustering is the actual algorithm itself.\nWe want to assign examples to “groups”\nMethods\n\nK-means (most popular)\ndensity-based clustering\nEnsemble Clustering\n\nLike random forest but for voting for clustering\nThis is problematic because of the label switching problem — we can get clustering with permuted labels on each initialisation\n\nDon’t vote on what specific class each cluster is\nInstead, vote on whether points are in the same cluster (label independent)\nThen, come up with labels after voting\n\n\n\n\nhierarchical clustering\n"},"thoughts/collaborative-software":{"title":"Collaborative software","links":["thoughts/CRDT","thoughts/Operational-Transform","thoughts/git","thoughts/privacy"],"tags":["seed","pattern"],"content":"\nCollaborative computing focuses on group rather than individual problem solving and decision making tasks necessary to accomplish business and scientific objectives. It provides an environment in which people can share information without the constraints of space and time.\n\nEach user device has a local replica of the data and this local replica can be updated anytime (ideally even while offline), and re-sync with others when network is available\nChallenge: how do we reconcile concurrent updates?\nTwo main families of algorithms\n\nConflict-free Replicated Data Types (CRDTs) — persists the causal order of operations\nOperational Transform — persists the final output of operations\n\nI hosted a session about this at Andy Matuschak’s unconference! It turns out a lot of people are thinking about collaborative software. See what we brainstormed: Full FigJam file\nA spectrum of collaboration\nSource\n\nfull async - no collaboration ever - has to be completely disconnected\nasync branching - git flow, working in parallel universes temporarily, then merging back together\nperipheral awareness - working in the same space while working separately (parallel play)\nfully sync - pair programming\n\nWhen we’re collaborating with others, there’s a natural human tendency to desire some privacy while working through something, the freedom to take a piece of the creative work and play out different ideas, move things around, edit and refactor, without fear of judgement or the burden of having to explain or communicate our thinking or concern for overhauling sections where another is actively reading or working.\nHow do we make private ‘forking’ really easy and seamless?\nKnobs over Switches\nSource\nCoCo has been doing a lot of important pioneering work in this direction.\nTypically, in the context of digital creative tools, there are primarily two predominant modes of engagement that are often being developed or thought about.\n\nTools for working on your own on creative projects and then sharing the published works with others.\nTools for working together on the same creative project with others (such as, drawing on the same canvas, making edits to the code in the same project, or writing together in the same document, etc.)\n\nJust like how a knob affords finer control over one’s preferences, similarly, a CoCo space is designed to provide creators with the agency to choose when and in what way they’d like to engage with their peers, based on their own comfort and preferences at any moment.\n\nJust being together: parallel play\n\nJust being together and sharing a context with someone is itself a form of communication. In a CoCo space, creators can work on their projects in a shared digital context, in the presence of others.\nThis is cursor chat and presence\n\n\nFinding inspiration\n\nIt can be challenging for young people to get started on a blank canvas in any open-ended creative environment. In a CoCo space, even though learners get to work on their own projects, they are always surrounded by multiple points of inspirations in the form of the mini live windows showing their peers’ creative explorations in real-time.\n\n\nLive remixing\n\nCoCo spaces also support live remixing of code and other digital assets between peers. Any creator can drag and drop a piece of code from another creator’s project on to their mini window and it will instantly appear in their own project.\nIt is similar in essence to a sight of children sitting around the same table and freely passing and sharing craft materials with one another.\nPart of why I think Midjourney was so successful was this! People could see what other people used as prompts and remixed it in their own work\n\n\nCreative Interaction\n\nThe CoCo Blocks environment introduces a variety of new peer programming blocks — Signals, Waves, and Shared Variables. Creators can use these blocks to make their project trigger something in their peers’ projects in real-time and also get to see the outcome instantly.\n\n\nCollaborative experiences\n\nIn addition to supporting collaborative projects where creators can contribute different parts to a single project, CoCo spaces also afford building new types of collaborative experiences as a group.\nFor instance, creators can collectively imagine and program a new kind of multiplayer pacman game where the pacman passes through each of their projects turn by turn and they can use shared variables to have a common score!\n\n\n\nEras of Computing\nBefore the era of mainframe time sharing, computing relied heavily on batch processing, particularly punch card systems. In the early to mid-20th century, machines like the IBM 650 Magnetic Drum Data Processing Machine were pivotal in handling large volumes of data. Users would prepare decks of punched cards representing their tasks, and these cards would be processed sequentially in batches by the computer. The reliance on batch processing meant that users had to wait for the entire batch of cards to be processed before receiving results which meant that people make queues and schedules around when to run their jobs.\nThe transition to mainframe time sharing marked a departure from this sequential processing model. The IBM System/360, introduced in 1964, played a crucial role in this shift. It allowed multiple users to interact with the computer simultaneously, increasing efficiency compared to the earlier batch processing systems. Now, multiple people could connect their own terminal to the machine while maintaining their own individual workspaces. Because the underlying machine was the same, resources could be shared across sessions relatively easily.\nThe 1980s and 1990s witnessed the advent of personal computing, with machines like the IBM Personal Computer (IBM PC) and the Apple Macintosh. These personal computers became household fixtures, shared among family members. Because of the initial high price of these machines combined with the lack of sophisticated account management, these were mostly communal fixtures where there was only often one ‘family computer’ that was shared.\nAs the price of personal computers dropped and they became smaller, we finally saw a departure from shared home PCs as individuals sought the convenience of truly personal computing.\nNow, we are seeing a return to communal computing as people are realizing the importance of collaboration and shared digital spaces.\nQuestions for communal computing\n\nIdentity: do we know all of the people who are using the device?\nPrivacy: are we exposing (or hiding) the right content for all of the people with access? (see also: Contextual Privacy)\nSecurity: are we allowing all of the people using the device to do or see what they should and are we protecting the content from people that shouldn’t?\nExperience: what is the contextually appropriate display or next action?\nOwnership: who owns all of the data and services attached to the device that multiple people are using?\n\nCRDT Unlocks\nConcurrent Editing plugins\nWe can treat all plugins/external things (e.g. git, filesystem) as actors. CRDTs allow these changes to happen asynchronously\n\nAn actor makes an editing request, which is an insertion of a sequence at a point relative to its snapshot of the buffer\nWhen the result eventually returns, the editor commits that request which might require a coordinate transform based on other edits that have arrived in the meantime\n"},"thoughts/collections":{"title":"Collections","links":["thoughts/library","thoughts/organizing-system"],"tags":["seed"],"content":"\nA collection is a group of resources that have been selected for some purpose.\n\n\nResource-type: a novel, biography, etc. (ordering on the type of artefact)\nResource-kind: coin collection, stamp collection, etc. (ordering on the medium of artefact)\n\nA collection itself can also be a resource. An index is a description resource that contains information about the location and frequencies of terms in a collection to enable more efficient search.\nWhat then is a library? According to Glushko, they are organizing systems which “select, collect, organize, conserve, preserve, and provide access to information on behalf of a community of users.”\nSee also: organizing system"},"thoughts/collective-intelligence":{"title":"Collective Intelligence","links":["thoughts/Transformative-Technology-Trilemma"],"tags":["seed"],"content":"The key to the rise of Homo Sapiens and the anthropocene wasn’t due to the rationality of any individual human, but rather our collective unparalleled ability to think and share knowledge in large groups.\nCIP\nSource\nCollective intelligence capabilities: decision-making technologies, processes, and institutions that expand a group’s capacity to construct and cooperate towards shared goals.\nSee also: Transformative Technology Trilemma\nKnowledge Illusion\nHumans also have this ‘knowledge illusion’ where we think we know a lot, even though individually we know very little.\nIf I were to ask you if you knew how a zipper works, the vast majority of you would exclaim “yes, of course!” Yet, if asked to describe in detail every single step, most would fail to do so. Even with something that seems so basic and intuitive seems to elude an explicit explanation.\nWe treat the knowledge of the human collective as if it were our own, even subconsciously."},"thoughts/colonial-debt":{"title":"Colonial Debt","links":["thoughts/Technosolutionism","thoughts/Data-Capitalism","thoughts/Design-Justice","thoughts/Internet","thoughts/housing"],"tags":["seed"],"content":"Debt that is not just exclusive to British and Spanish colonialism but also encapsulates extractive practices of other forms:\n\nChinese, French, and American imperialism\nneocolonialist internet companies\n\nSee also: Technosolutionism, Data Capitalism\nPaying off colonial debt\n\nImportance of bring in non-WEIRD perspectives especially when trying to solve problems outside of these demographics. How do we empower and enable local people to solve their own local problems?\n\nLocal people know their own problems the best. However, funding complicates things as local people are forced to think and problem solve the way their funders think and problem solve\n\n\nExamples of where ‘good intentions’ were not enough to prevent negative externalities from impacting marginalized communities\n\nDuring middle of Syrian war, have folks from Syria that want to learn about secure communications practices. US people started explaining public-key encryption for a while, then the Syrian people said “oh we just wanted to learn about how to setup a telecommunications network”\nConferences that say they are ‘international’ but make their speakers speak in English\nAmerican women coming to Kenya to explain reproductive health to Kenyan women but providing a very western perspective with resources that the vast majority of the children there did not have the resources to access\nDesign and design thinking is inherently colonial: “you have a problem and we can make the tools to solve it” (see: Design Justice)\nWhen advocating for more Black-led music programs: “oh, we already have a prison program!”\nThe Open Library program at the Internet Archive, run fully with good intentions. This type of work is inherently skewed and biased toward people who have lots of spare time and energy and a result, it is still run predominantly by white dudes who live on the west coast. The Internet Archive is notorious to people who need a white education but does not serve any other populations.\n\nThings that embed implicit Western bias: multi-language calls and time zones\n\n\n\n\nHow does the homogenization of the Internet impact the diversity of cultures and languages of the world?\n\nMoving away from meritocracy\n\nEspecially in technology, candidacy for opportunities heavily skews and favours those with free time, energy, labour\nWe see this happen with HOAs, city town halls, and other ‘democratic’ processes. It turns out these are not really representative but in fact only those with free time attend (see: housing)\n"},"thoughts/colour":{"title":"Colour","links":["thoughts/texture"],"tags":["seed"],"content":"See also: texture\nColour\nTwo lights whose spectral power distributions appear identical to most observers are called metamers\nChromatic Adaptation: colour perception starts to skew if exposed to a certain colour light for an extended period of time\nContrast effects: nearby colours affect what is perceived\nColour Filter Arrays\nOne implementation of photo sensors\n\nMicrolens\n[optional] Colour Filter (what spectral sensitivity functions do we use for each colour? Human colour sensitivity differs between colours as well)\nPhotodiode\nPotential well\n\nTwo design choices to make when designing CFAs:\n\nWhat spectral sensitivity functions (SSFs) do we use for each colour filter?\nHow do we spatially arrange filters to create the best mosaic for CFAs?\n\nRAW Bayer Image gives us direct pixel data\nCFA Demoisaicing\nHow do we produce the full RGB image from mosaiced sensor output?\n\nBilinear Interpolation: average your 4 neighbours\nBicubic Interpolation: needs more neighbours, may overblur\nEdge-aware interpolation\n\nGrassman’s Law\nTL;DR, colour matching is, to an accurate approximation, linear\nFor colour matches\n\nsymmetry (U=V⟺V=U)\ntransitivity (U=V and V=W⟹U=W)\nproportionality (U=V⟺tU=tV)\nadditivity (U=V and W=X, then (U+W)=(V+X))\n\nColour Space\nChoice of primaries is equivalent to choice of colour space. In RGB, we choose monochromatic energies.\n\n\nRGB is additive whereas CMY is subtractive.\n\n\nRGB and CIE are linear.\n\n\nMcAdam ellipses are regions where colour differences are imperceptible to the average human eye.\n\n\nA colour gamut is the range of colours that can be displayed or captured by a particular device or technology.\n\n\nCIE is defined with 3 imaginary lights X, Y, Z\n\nAny wavelength λ can be matched perceptually by positive combinations\nX is approximately R, Y is approximately G, Z is approximately B\nThe XYZ colour space is a device-independent colour space, which means that it is not tied to any particular device or display technology.\n\n\n\nAs the RGB colour cube sits within the CIE colour space, it can only display a subset of perceivable colours\n\nOne way to clamp: construct ray to white point, find closest displayable point within gamut\n\n\n\n(C, M, Y) = (1 - R, 1 - G, 1 - B)\n\n\nPhysiology\nRetina contains an uneven distribution (clustered around fovea) of rods and cones\n\nRods: sense in black and white, mainly for edge detection\nCones: 3 types, mostly for colour sensing\n\nL: most sensitive to red\nM: most sensitive to green\nS: most sensitive to blue\n\n\n\nColour blindness results from missing cone types\n\nDeuteranope (green deficiency)\nProtanope (red deficiency)\nTritanope (blue deficiency)\n"},"thoughts/communities":{"title":"Communities","links":["thoughts/group-limits","thoughts/in-group-bias","thoughts/computability","thoughts/Turing-Test","thoughts/value-setting","thoughts/Dunbar's-Number"],"tags":["sapling"],"content":"Types of Communities\n\nInterest-based → same interests/passions\nLocation-based → same geographical location, place of work, etc.\nVibe-based → group energy\nCircumstance-based → external influence, put into the same group\n\nHow large can communities get before they decay? Relevant: group limits, in-group bias\nTuring Test for communities\nheuristic from Austin Wu\nA very computational view of communities but is it possible to test for value alignment within a community like a Turing Test? If the community feels and behaves like a person, then its values are aligned?\nHow do we quantify this if vibes are unoptimizable?\n90/9/1 Rule\nThe “90–9–1” version of this rule states that for websites where users can both create and edit content, 1% of people create content, 9% edit or modify that content, and 90% view the content without contributing.\nWas called “participation inequality” by researchers at AT&amp;T labs\nThe 80/20 rule known as the Pareto principle states that 20 percent of a group will produce 80 percent of the activity, however the activity is defined.\nSize of the community might matter? a form of Dunbar’s Number e.g.\n\nsmall communities like family group chats: almost everyone creates content\nlarge communities like LinkedIn: most people consume, vocal minority\n\nAlthough size might just be a proxy for sense of belonging in a group. i.e. if you strongly identify with said group (family) you are more likely to participate and contribute. In larger communities then, the approximately normal distribution of people who are engaged in the community may lead to some type of power law"},"thoughts/compiler":{"title":"Compilers","links":["thoughts/program-analysis","thoughts/computer-architecture","thoughts/systems-design","thoughts/disjoint-set","thoughts/Rice's-Theorem"],"tags":["seed"],"content":"A compiler is a transformation between languages, transforming a source language into a target language.\nWe design compilers by starting from a fixed abstraction boundary (the existing target language) and building a new layer of abstraction atop it (the new source language).\nStarting from the target language, we ask a question: what’s wrong with this language?\nRules of thumb:\n\nGenerally: simplifying something for the many users at the expense of the few compiler writers is usually a good trade-off\nWhen we create a new language, we want to ensure we understand the meaning of that grammar separate from how it is compiled.\n\nOptimizations depend on when various programs in a language are equivalent\nWe cannot know whether the compiler is correct if we do not know the meaning of programs before they are compiled\n\n\nWe can define the meaning of a language by writing an interpreter.\n\n\nEliminating undefined behaviour by adding static or dynamic checks in the source language improves the ability of programmers to predict behaviour of all programs in your language. However, it is not always practical to achieve. See: program analysis\n\nRuntimes\nThe run-time system provides all run-time support required by the language but that that is not provided by the underlying machine. Exactly what this run-time support is depends on the language.\nTypically, the language run-time provides memory allocation and deallocation, initialization of the process environment such as the stack, handles returning values to the user, and provides any built-in procedures that all programs in the language can expect to use\nCorrectness\nA compiler is correct if:\n\nthe meaning (as defined by the interpreter) of a program p is the value v1\nwe compile p and execute it as a x64 program and get the value v2\nthe values v1 and v2 are equivalent.\n\nIn general, we have to define equivalence for each pair of source and target languages.\nParameterization\nOur compilers make decisions based on certain parameters (e.g. the return register, the frame base pointer register, etc.)\nParameterizing the language this way lets us avoid committing to particular register choices, making the language inherently more machine and convention agnostic. This is helpful in designing a compiler with multiple machine backends. If our language definitions were sufficiently parameterized, few if any compiler passes would need to differ between target machines.\nAdministrative Languages\nAn intermediate language whose semantics does not differ at all from its parent language, but whose syntax is potentially decorated with additional data that simplifies the next step of the compiler.\nIn a production compiler, we would probably not represent these administrative languages at all, but instead store the contents of the info field “on the side”, as a separate data structure. This would prevent us from deconstructing and reconstructing the syntax tree when modifying or accessing the info field.\nAbstractions\nAbstracting system calls\nNormally, we never program the raw CPU—we program the operating system. The CPU together with the operating system implements a different programming language than the CPU by itself.\nSystem calls are x64 primitives provided by the OS. Once we start using system calls, code becomes OS-specific. One of the first things a compiler writer will do is abstract away from system calls.\nAbstracting memory locations\nFrame Variables\nHuman memory is much less reliable than computer memory, so we should design languages that make the computer remember more and free the human to remember less. This will prevent the human from causing run-time errors when they inevitably make a mistake and overwrite a register that was still in use.\nTo address this, we will introduce abstract locations, of which there are an arbitrary number and that the programmer does not need to know what physical location they end up using.\nThe displacement mode operand is a new operand that can appear in some location positions as the operand of an instruction. This allows accessing memory locations using pointer arithmetic. It is written as QWORD [reg - int32] or QWORD [reg + int32] in x64, where reg is a register holding some memory address and the int32 is an offset number of bytes from that address to access, as a 32-bit integer. Offsets are always multiples of 8.\nThis ensures all memory accesses are machine-word aligned, meaning we leave space for all bytes in the word between each access. Note that the offset is negative; we access the stack backwards, following the x64 “stack grows down” convention (we call this stack discipline).\nAbstract Locations\nWe define an abstract location to be a unique name for some physical location, that is unique for some unit of allocation. Each abstract location must be allocated a physical location somewhere on the machine.\nValue-orientation\nWe want to move towards a value-oriented language, i.e., a language where operations consume and produce values directly, and away from an imperative language that manipulates some underlying machine state. This would free the programmer from keeping the state of the machine in mind at all times.\nControl Flow\nLabels and jumps are a small change to the language syntactically, but have a large effect on the semantics.\nWe can no longer write the interpreter in one simple loop over the instructions. Instead, we need some way to resolve labels. When running the interpreter, we must be able to jump to any expression at any time—a possibility the language now allows. This process of resolving labels is called linking.\nWe use a low-level linking implementation that is similar to the operating system’s linker: we first resolve all labels to their address in memory (in our case, their index in the instruction sequence) and then implement jumps by simply setting a program counter to the instruction’s address.\nTo simplify reasoning about programs with control flow, we can organize code into basic blocks, labeled blocks where control can only enter the beginning of the block and must exit at the end of the block. In particular, we will be able to annotate which registers are undead on entry to and on exit from a block, so our analysis does not have to resolve labels and jumps.\nProcedures/Functions\nWe introduce a common method of reusing code: procedural abstraction.\nThe only question is how to pass arguments. The call instruction needs to know in which locations to store the arguments, and the called procedure needs to know from which locations to read its parameters. The problem is deciding how to ensure the locations end up the same. To solve this, we introduce a calling convention.\nWe could also use the stack to implement the calling convention. This is simpler, as we can keep registers abstract and need to expose memory high in the compiler pipeline anyway, but slower since every procedure call must now access memory.\nInstead, we fix a set of physical locations. Both the caller and the callee agree that those locations are the only thing they need to know about.\nOur calling convention passes the first n arguments as registers, using the set of registers defined in the parameter current-parameter-registers. The default value is &#039;(rdi rsi rdx rcx r8 r9), which is defined by the x64 System V ABI to be where the first 6 arguments of any procedure are stored. For the rest, we use fresh frame variables.\nTo handle procedure calls in effect position, we must use a stack of frames. A frame is a procedure’s set of frame variables needed after a non-tail call. Note that arguments are placed on the callee’s frame instead of the caller’s frame.\n\nWe arrange that all values live after a non-tail call are stored in frame variables\nWe push the callers’s frame onto the stack. This is done by decrementing the frame base pointer past the last frame variable\nThe call happens and returns\nAfter returning from a call, we pop the caller’s frame from the stack by incrementing the frame base pointer back to its original value\n\nThe size of a given non-tail call is the maximum of:\n\nthe number of locations in the call-undead, or\none more than the index of the largest frame location in the call-undead set.\n\nData types\nA static type system may take care to prevent the user from calling an integer as a procedure, for example. Even dynamically typed languages may need to distinguish different kinds of data at run time.\nTo enable the language to distinguish different kinds of data, we can steal a few of our 64 bits to represent a data type tag. This limits the range of machine integers, but allows us to us to distinguish data dynamically, enabling safety and abstractions that must differentiate data dynamically.\nThis approach is called object tagging. Each data type in our language will now be represented by a ptr (pronounced like footer). A ptr is a machine word whose n least-significant bits represent the primary tag, and whose upper (- (* 8 (current-word-size-bytes)) n) bits represent the data (this allows us to represent 2n data types).\nThis lets us implement data-type checking by providing the following primitive operations (which we implement with further, low-level primitives in x64):\n\nTagging, i.e., given some machine integer, tag it to indicate what data type it represents, producing a tagged representation of the underlying data. This tagged representation will happen to correspond to some machine integer, since all sequences of bits do, but maybe not in any meaningful way.\nUntagging, i.e., given some tagged representation, remove the tag returning the underlying data that can be used with primitive x64 instructions.\nTag checking, i.e., given some tagged representation, get the tag or compare the tag to something.\n\nHeap Allocation\nWe need three additional operations\n\nmemory allocation (to implement constructors)\ndynamically computed memory assignment\ndynamically computed memory dereference\n\nTo implement allocation, we need some strategy for managing memory. Our run-time system or compiler needs to know what memory is in use, how to allocate (mark free memory as in use), and how to deallocate memory (return in-use memory to the pool of free memory and ultimately to the system). There are many strategies for this, such as\n\n“let the user deal with it” (managed languages like C)\n“add a process to the run-time system that dynamically manages memory” (garbage collected languages like JavaScript and Go)\n“make the type system so complex that the compiler can statically manage memory” (Rust)\n\nClosures\nEvery instance of lambda compiles to a procedure. The procedure now has three pieces of information:\n\nits arity for dynamic checking;\nthe label to its code, the computation it executes when invoked; and\nits environment, the values of the free variables used in the definition of the procedure\n\nOur procedure data structure is essentially a vector containing a label to the code and the values of each free variable in its environment.\nClosure conversion is not the only way to implement first-class procedures. An alternative that can avoid some of the allocation cost of closures is defunctionalization, but this does not work well with separate compilation.\nOptimizations\nAny compiler optimization should be seen as a transformation between programs in the same language, i.e.,, an intra-language transformation.\nOptimizations should not change the correctness of a solution, only improving its performance characteristics\nRegister Allocation\nWhile memory accesses have improved a lot compared to old computers due to caching, accessing memory are still orders of magnitude slower than accessing a register when our variable is not in the cache (see: systems design). Our compiler will have better performance if we help the machine out by using registers as much as possible\nConceptually, register allocation is a simple idea.\n\nUndeadness (liveness) analysis: figure out which abstract locations might still be needed after each instruction.\n\nWe assume that any variable that gets used, or might get used, might be not dead\nWe consider a variable dead only when we have conclusive proof (e.g. storing a new value in that variable)\nTo calculate this, we loop over the instruction sequence backwards\n\nThis algorithm requires a default undead-out set for the last instruction in the scope of our analysis (for most, we assume this is the empty set though this is not always the case. e.g., functions assume that the return value location is live at the end of the function).\nIn each iteration, we start by assuming the undead-in set is the same as the undead-out set, then update it depending on what happens in the instruction\n\nIf a variable is defined, i.e., its value is overwritten in the instruction, it is definitely dead upon entry to this intruction, so we remove it from the undead-in set.\nIf a variable is referenced in the instruction, it ought to be live and is added to the undead-in set.\n\n\n\n\n\n\nConflict analysis: figure out which abstract locations cannot be assigned to the same physical location because they both contain values that are needed at the same time.\n\nAny variable defined during a non-move instruction is in conflict with every variable (except itself) in the undead-out set associated with the instruction.\nAny variable defined during a move instruction is in conflict with every variable in the undead-out set associated with the instruction, except itself and the variable referenced in the move.\n\n\nRegister allocation: assign each abstract locations to a register that is different from any conflicting abstract locations.\n\nRecursive graph-colouring register allocation. This normally uses a disjoint-set\n\n\nIf the set of abstract locations is empty, return the empty assignment.\nOtherwise, choose a low-degree abstract location from the input set of abstract locations, if one exists. Otherwise, pick an arbitrary abstract location from the set. A low-degree abstract location is one with fewer than k conflicts, for some for pre-defined k. We pick k to be the number of registers in the set of assignable registers.\nRecurse with the chosen abstract location removed from the input set and the conflict graph. The recursive call should return an assignment for all the remaining abstract locations.\nAttempt to select a register for the chosen abstract location. You cannot select registers to which conflicting abstract locations were assigned by the recursive call. This attempt succeeds if a low-degree abstract location was chosen, and might fail otherwise\n\n\nSpilling: if we fail to find a register for an abstract location, put it in a frame variable instead.\n\nIf you succeed in selecting a register, then add the assignment for the chosen abstract location to the result of the recursive call.\nOtherwise, we cannot assign the chosen abstract location to a register. Instead, we spill it, i.e., we assign it a frame variable. We can assign a fresh variable, but we can reduce memory usage by trying to assign a non-conflicting frame variable.\n\n\n\nIn general, we will never do a perfect job, due to Rice’s Theorem.\nWith procedures\nWhen analyzing the program to determine how variables are used, we can either:\n\nIntraprocedural: interpret the program as a tree, analyzing and allocating registers to each block separately and essentially ignoring jumps, or\nInterprocedural: interpret the program as a graph, trying to follow control and data flow to determine the destination of a jump, in order to analyze conflicts and allocate registers across jumps.\n\nLanguage Forms\n\nMonadic form (MF): a syntactic form that allows composing operations that operate on values and have no side-effect (such as changing the value of an abstract location), but requires explicit sequencing any effectful operations\n\nCanonical monadic form (CMF): a syntactic form in which equal programs (for some notion of equality) have the same representation. The form is canonical in the sense that there is one right way to represent every program\nWriting transformations and optimizations over CMF is often easier since we do not have to manually consider two equal programs as they have the same representation.\n\n\nA-normal form (ANF): a syntactic form that restricts all operations to trivial values, and forbids nesting in our value position. It is roughly equivalent to other compiler intermediate forms, such as static-single assignment. All ANF programs are in MF but the inverse does not hold\n\nRacket\nQuasiquoting\n\nThink about it like JavaScript template literals (`some value: ${x}`) or Python f-strings (f&quot;some value: {x}&quot;) but operating on actual source code\nA single quote &#039; indicates the start of a piece of code we can treat as data\nA backtick ` is the same as a single quote except it allows us to template the code using variables\n\nWe can use the unquote operator , to insert the value of the variable\n\nIf we had a (define x 42) and then `(module ,x) becomes &#039;(module 42)\n\n\nWe can use the unquote-splicing operator ,@ to ‘spread’ a list\n\nIf we had a list (define xs (list 1 2 3)) and then  `(module ,@xs) becomes &#039;(module 1 2 3)\n\n\n\n\n"},"thoughts/complexity":{"title":"Complexity","links":["thoughts/Gall's-law"],"tags":["seed"],"content":"Measures of Complexity\n\nVapnik–Chervonenkis dimension: cardinality of the largest set of points that a binary classification algorithm can learn.\nKolmogorov complexity: length of the shortest computer program that produces the object as output. (see also Gall’s Law)\nEssential Complexity: irreducible, non-eliminable part of the system\nAccidental Complexity: everything else to make it work\n\nMaking Simple Software\nSource talk by pvh\n\nWhy is example code normally so simple? Well, it lives in an ideal world. It only cares about the happy path. Defensive code is complex\nVigilance works for small systems… but not large complex ones!\n\nType systems off-load mental state and assumptions about the code and make it explicit\n\n\n\nEssential vs Accidental Complexity\n\nEssential complexity is inherent in the problem domain of the application.\nAccidental complexity arises only because of limitations of our tooling.\n\nUnfortunately, this distinction is also flawed, because boundaries between the essential and the accidental shift as our tooling evolves\nOne of the best tools we have for managing complexity is abstraction. A good abstraction can hide a great deal of implementation detail behind a clean, simple-tounderstand façade"},"thoughts/composable":{"title":"Composable","links":["thoughts/emergent-behaviour"],"tags":["seed","pattern"],"content":"The ability to freely combine smaller programmed artifacts into larger ones, to accomplish larger goals\nThere exist building blocks which span a range of useful combinations. Composability is, in a sense, key to the notion of “programmability” and every programmable system will have some level of composability\nSee also: emergent behaviour\nMediums versus Tools\nLinus Lee on Browsers as Tools of Thought\n\nA tool is something that takes an existing workflow, and makes it more efficient. A nail is an efficient way of holding pieces of wood together; a to-do app is an efficient way of remembering your responsibilities. A medium, on the other hand, gives us new agency or power by which we can do something we couldn’t do before.\n\nThe best mediums are instead collections of generic, multi-purpose components that mesh together well"},"thoughts/computability":{"title":"Computability","links":["thoughts/representation"],"tags":["seed"],"content":"General Computability\nSource text from The Mechanical Mind by Crane\nA computer is a device which processes representations in a systematic way\nAn algorithm is a method for calculating the value of a function\n\n“effective procedures” → procedures which, if applied correctly, are entirely effective in bringing about their results (always work)\nComputable if the algorithm gives the value of a function for any argument\nChurch’s thesis → anything that can be executed by a Turing machine is computable\nConditions to be considered an algorithm\n\ndefinite next step\nfinite number of steps\n\n\n\nTuring Machine\nThe simplest possible device that could perform any computation no matter how complicated\nComponents:\n\nMemory:\n\nA long (infinitely long) tape with squares\nA device that can write/read the symbols on the tape\nDevice can move tap one left or one right\n\n\nInstructions:\n\nPossible operations are dictated by machine’s ‘machine table’\nA set of instructions of the form ‘if the machine is in state X and reading symbol S, then do Y and move tape right/left’\n\n\n\nInstantiating vs Computing function\n\nInstantiating → being an instance of/describable by a function\nComputing → employs representation of input and output\n\nEven if a person could be modeled by a Turing machine, that would not show that thinkers are computers, rather, it would show that a thinker instantiates a function, not that it computes that function.\nThe computational metaphor\nSource: Digital Salon with Stephen Wolfram: Building a New Kind of Science, Palladium Mag\n\nThere is a creaking issue in universities where 70% of incoming students want to study computer science but only 5% of faculty is in computer science. What’s really going on is we want to study computer science as a proxy for the computational paradigm for everything around us\nIs there a computable x for all x?\n\nIs there a computational simulation of a mind is sufficient for the actual presence of a mind?\n\n\n"},"thoughts/computer-architecture":{"title":"Computer Architecture","links":["thoughts/systems-design"],"tags":["seed"],"content":"A lot of content courtesy of cpu.land written by the wonderful Kognise!\nCPUs\nThe central processing unit (CPU) of a computer is in charge of all computation.\nIt works by taking a list of instructions and executing them one-by-one. After executing each instruction, the CPU moves the pointer and repeats.\nAn instruction is made up of a bunch of binary data that looks like this in descending levels of abstraction:\n\nadd ebx, 10 (Assembly)\n0x83 0xC3 0x0A (Machine Code - hex)\n10000011 11000011 00001010 (Machine Code - binary)\n\nGenerally, the first byte (0x83) in the example above, corresponds to the opcode which represents that actual operation and the rest of the bytes are the arguments.\nThe CPU stores an instruction pointer (called the program counter) which points to the location in memory where it’s going to fetch the next instruction. Most instructions, after they are executing, just move the instruction pointer to the very next instruction, but some instructions (like the jmp instruction) can jump to different places depending on certain conditions (e.g. a value in a specific register or whether one value is positive or negative).\nRegisters and Memory\nRegisters are small storage buckets that are extremely fast for the CPU to read and write from. These are often very limited (most CPUs only have a handful) so your computer also has various other levels of storage in the form of memory that get progressively slower but higher capacity the more you move away from the CPU.\nSee also: latency numbers for various levels of memory access.\nRoughly:\n\nRegister read/write: ~0.1ns latency and ~16 bits of capacity\nL1 cache: ~0.5ns latency and ~32KB of capacity\nL2 cache: ~7ns latency and ~4MB of capacity\nMain memory (RAM): ~100ns latency and ~8GB of capacity\n\nIsolating memory\nIt turns out that when the CPU reads from or writes to a memory address, it’s not actually referring to that location in physical memory (RAM). Rather, it’s pointing to a location in virtual memory space.\nThe CPU talks to a chip called a memory management unit (MMU). The MMU works like a translator with a dictionary that translates locations in virtual memory to locations in RAM.\nWhen the computer first boots up, memory accesses go directly to physical RAM. Immediately after startup, the OS creates the translation dictionary (called the page table) and tells the CPU to start using the MMU.\nThese pages are always a fixed size and each processor architecture has a different page size. x86-64, for example, has a default 4 KiB page size.\nThis is how each process can have its own isolated memory space — when the OS switches context from one process to another, an important task is remapping the virtual memory space to a different area in physical memory and allows code and data that refer to memory addresses use the same addresses that get remapped to different physical memory locations.\nPage tables are often hierarchical:\n\nIf an instruction for example asks for an address that the MMU doesn’t know about, it can fail the memory access with a page fault. Then, the page fault interrupt handler can then load the data into memory and let the MMU try again.\nThis is also what the swap or page file is actually doing under the hood: Operating systems can free up physical memory by writing memory pages to disk and then removing them from physical memory but keeping them in virtual memory with the present flag set to 0. If that virtual memory is read, the OS can then restore the memory from disk to RAM and set the present flag back to 1.\nKernel\nThe kernel is the core of the operating system. When you boot up your computer, the instruction pointer starts at a program somewhere. That program is the kernel. The kernel has near-full access to your computer’s memory, peripherals, and other resources, and is in charge of running software installed on your computer (known as userland programs).\nProcessor Mode\nThe mode (sometimes called privilege level or ring) a processor is in controls what it’s allowed to do.\n\nKernel mode: CPU can do anything and access any memory\nUser mode: only subset of instructions are allowed and memory is limited (this is also why you get segmentation faults if you try to read outside of your allowed memory space!)\n\nProcessors start in kernel mode. Before executing a program, the kernel initiates the switch to user mode.\nSyscalls\nPrograms run in user mode because they can’t be trusted with full access to the computer. But programs also need a way to do privileged things like access I/O, allocate memory, etc.\nA system call is a special procedure that lets a program start a transition from user space to kernel space, jumping from the program’s code into OS code.\nInterrupts\nWhen the processor receives an interrupt signal INT, it drops everything it is currently doing and handles the interrupt. Every software interrupt signal is associated with a particular interrupt handler in the interrupt vector table (or IVT).\nSome syscalls take arguments. These arguments are normally passed by placing data in certain registers.\nAfter the kernel finishes handling the interrupt, it uses an instruction IRET to return back to the instruction pointer it was at before it got interrupted.\nMultitasking and Parallelism\nWe can fake parallelism by letting processes take turns on the CPU.\nWe can do this by programming a timer chip to trigger a switch to an OS interupt handler after a certain amount of time passes.\nOS schedulers use timer chips like PITs to trigger hardware interrupts for multitasking:\n\nBefore jumping to program code, the OS sets the timer chip to trigger an interrupt after some period of time (this period of time is called the timeslice)\nThe OS switches to user mode and jumps to the next instruction of the program and the program does its thing\nWhen the timer elapses, it triggers a hardware interrupt to switch to kernel mode and jump back to OS code\nThe OS can now save where the program left off, load a different program, and repeat the process\n\nThis is called preemptive multitasking; the interruption of a process is called preemption.\nThere are a few strategies to determine the timeslice that each process gets:\n\nFixed timeslice round-robin: every process gets the same timeslice (e.g. 10ms)\nDynamic timeslice round-robin: divide a target latency equally among all processes (e.g. with a target latency of 15 ms and 10 processes, each process would get 15/10 or 1.5 ms to run). Note: too small a timeslice can result in performance problems with processes switching too rapidly. Timeslice duration can be given a lower bound (minimum granularity)\nCompletely Fair Scheduler\n\nELFs\nExecutable and linkable format. The structure is roughly as follows:\n\nELF Header: basic information and where’s what (like the PHT and SHT). Also points to where the ‘entrypoint’ is.\nProgram Header Table (PHT): series of entries that describes how and where to load the ELF file’s data into memory. Common header types:\n\nPT_LOAD: load into memory\nPT_NOTE: free form informational text (i.e. copyright notices, version info, etc.)\nPT_DYNAMIC: info about dynamic linking\nPT_INTERP: path to the location of an ‘ELF interpreter’\n\n\nSection Header Table (SHT): optional map of the data to assist in debugging. Common sections are:\n\n.text: machine code\n.data: hard coded data (e.g. global variable, statics, embedded binaries/images/other things)\n.rodata: read-only .data\n\n\nData: all of the actual code and data packed together.\n\nLinking\nProgrammers tend to build their programs on top of libraries of reusable code — for example, libc, which we talked about earlier.\n\nStatic linking: library functions are copied from the developer’s computer into each binary at build time\nDynamic linking: binaries reference the names of library functions which are loaded from the user’s computer at runtime (these are typicaly packaged into files with .so, .dll or .dylib file extensions)\n"},"thoughts/computer-graphics":{"title":"Computer Graphics","links":["thoughts/rendering","thoughts/imaging","thoughts/illumination","thoughts/colour","thoughts/GLSL"],"tags":["seed"],"content":"See also: rendering, imaging, illumination, colour, GLSL\nCoordinate Frames\nLet A be the original basis and B be the new basis\n​xy1​​B​=​ad0​be0​cf1​​​xy1​​A​\nThen:\n\n[ad​] is iA​, how to transform the x coordinate\n\na is how much of iB​ we need to make one iA​\nd is how much of jB​ we need to make one iA​\n\n\n[be​] is jA​, how to transform the y coordinate\n\nb is how much of iB​ we need to make one jA​\ne is how much of jB​ we need to make one jA​\n\n\n[cf​] is OA​, the translation of the entire frame\n\nc is how much of iB​ we need to get from OB​ to OA​\nf is how much of jB​ we need to get from OB​ to OA​\n\n\n\nThe translation from PA​ to PB​ can be represented as PB​=OA​+xA​iA​+yA​jA​\nTransformation Matrices\n\nTranslate(x,y,z)\n\n​x′y′z′1​​=​1​1​1​abc1​​​xyz1​​\n\n\nRotate(z,theta)\n\n​x′y′z′1​​=​cosθsinθ​−sinθcosθ​1​1​​​xyz1​​\n\n\nScale(x,y,z)\n\n​x′y′z′1​​=​a​b​c​1​​​xyz1​​\n\n\n\nTransformations\n\nObject Coordinate System: modeling transformation\nWorld Coordinate System: viewing transformation\nViewing Coordinate System (Camera): projection transformation\nClipping Coordinate System: /h\nNormalized Device Coordinate System (NDCS): viewport transformation\nDevice Coordinate System\n\nIn a scene hierarchy, the Camera Coordinate Frame (FVCS​) is generally the root.\nTransformations in scene graphs are written right to left, starting with source frame and ending with target frame.\nViewing Transformation\n\nDefined using\n\neye point\ntarget point\nup vector\n\n\n\nk=∥Peye​−Pref​∥Peye​−Pref​​\ni=∥Vup​×k∥Vup​×k​\nj​=k×i\nMcam​=​i1​i2​i3​0​j1​j2​j3​0​k1​k2​k3​0​Peye1​Peye2​Peye3​1​​\nMview​=Mcam−1​"},"thoughts/computer-networking":{"title":"Intro to Computer Networking & P2P","links":["thoughts/Internet","thoughts/r-K-Selection-theory","thoughts/Application-Layer","thoughts/HTTP","thoughts/Transport-Layer","thoughts/Network-Layer","thoughts/Physical-Layer","thoughts/peer-to-peer","thoughts/Rhizome-Proposal","thoughts/NAT","thoughts/DHT","thoughts/State-Machine-Replication-(SMR)","thoughts/CRDT","thoughts/Hypercore","thoughts/Yjs"],"tags":["fruit","technical"],"content":"\nThis post is the source material for my workshop on ‘Intro to Computer Networking &amp; P2P’ at Hack the North 2022\n\nA brief history\nA hallmark of the modern day human is the ability to communicate with each other. From spoken word to written language and through to even radio and telephone, we’ve spent a lot of time innovating on new ways to transmit data between people and places. With the introduction of the first computers, it was only a matter of time before we figured out how to make our computers talk too.\n\nEarly computer networks looked pretty similar to landline telephone connections. There is a physical wire directly connecting you from point A to point B for the duration of the connection. This was called circuit switching.\nOf course, during the early days when you could still count the number of computers connected to the Internet on your fingers, this was not problematic. But as the size of the internet grew, telecommunication networks created huge hubs to be able to handle the volume of traffic. The problem was, this hierarchical network — like all hierarchical networks — was vulnerable to targeted attack. Knock out a single node and you cut off lots of users, even whole regions.\nThis was not a comfortable position to be in during the height of the Cold War. There was a demand for a resilient and fault-tolerant network topology that would adapt to changing configurations and load on the fly.\n\nTo be efficient is to be fragile, to be fragile is to go extinct.\n\nNature of course, has its way of selecting for resiliency. In ecology, there is a concept called r-K Selection Theory related how certain species trade off between quantity and quality of their offspring. It notes that there are predominantly two types of species:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr-selectedK-selectedmany offspring, low investmentfew offspring, high investmentthrive in unstable habitatsthrive in stable habitats\nHow could move the internet away from being a K-selected monolith?\nIn 1959 RAND, a Californian think-tank, assigned Paul Baran, a young engineer at that time, to develop a communication system that could survive a nuclear attack. Instead of a static network with hardened switching stations, he proposed a dynamic network with disposable routers that could freely join or leave. Each router would help forward individual little bits of information called packets around the internet. Thus, packet switching was born and with it, the internet became an r-selected network.\n\nZooming out\nLet’s zoom out a bit to see how this fits into the grander vision of the internet.\nThe modern internet, like many things in computer science, has layers of abstraction. It is also a network of networks, composed of many systems connected to each other.\n\nWhen you request a website or make an API call, you go down all these layers and then back up at the other end.\n\nApplication Layer — where applications can access the network services using protocols like HTTP.\nTransport Layer — ensures data arrives in order, recovers lost data by retrying, sends data to the right process on your machine\nNetwork Layer — routes packet through routers to destination machine\nPhysical Layer — connecting individual machines together, transferring the actual bits\n\nWhen we talk about making a request to a server, we normally think about this at the Application Layer. If we make a HTTP request to Twitter to get the latest posts, we think of our request as going directly to Twitters’s servers, not hopping around from router to router, snaking its way through various networks to get to its destination and back.\nOnce we look at the web from the Application Layer, we see something curious. Although the underlying Transport, Network, and Physical layers of our web are decentralized, centralization has re-emerged on the web once again. When you message a friend on Facebook, it doesn’t go directly to them. Instead, it goes to Facebook’s servers and it forwards it to your friends device\n\nThis is the exact centralizing behaviour that we were worried about before.\n\nTo be efficient is to be fragile, to be fragile is to go extinct.\n\nTo decentralize the web is to ensure its resilience and long-term functioning.\nWhen we refer to peer-to-peer today, we don’t mean the underlying networking stack, but rather on the Application level. How can we bypass going through servers to facilitate all of our actions and instead connect directly with our peers?\nI’m not saying that platforms are inherently bad. They can enable efficiency at scale by making the average distance between nodes. Platforms become problematic when there is no meaningful way to easily switch from one platform to another when we are dissatisfied.\nPeer-to-peer applications abate this somewhat. There is no singular failure point; as long as two peers have the application code that connects them together, they don’t need to rely on a centralized provider.\nOf course, peer-to-peer applications are not without their disadvantages either. Not everything should be peer-to-peer, but we should empower developers to at least know about what peer-to-peer is and how to use it in the right contexts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClient-serverPeer-to-peerBetter for persistent applications that need to store data for a long timeBetter for low-latency applications like games and video calling (no need to make an extra trip to the server)Easier to make applications that are mostly about manipulating data and resourcesEasier to make applications that are mostly users interacting with each otherServers waits for requests from clients and responds to them (one-directional)Nodes have persistent connections so can send updates to each other wheneverNeeds hosting services to keep databases or application servers onlineThere are no privileged nodes that need to be up. No hosting costs!\nA lot of my independent research is around how to make it easier to write peer-to-peer software and make it possible to do the things client-server models are good at in these peer-to-peer contexts (e.g. data persistence).\nPeer-to-peer today\nSo, how do we make peer-to-peer connections today? Like talking with a friend, there are a two key criteria that need to be met before you even start to communicate:\n\nYou need to know how to find where they are to initiate a conversation\nYou need a shared language to understand each other\n\nIn the context of peer-to-peer, these are:\n\nWhat is the other user’s IP address?\nWhat application or protocol are we running?\n\nIt turns out, we can reuse much of the internet’s existing infrastructure to answer both of these questions with some caveats.\nToday’s web is peer-to-peer hostile — over 79% of peers on the Internet are not directly connectable. Firewalls are frequently configured to allow only outgoing connections, based on the assumption of the client-server model of communication. If a router uses NAT, it hides the ‘true’ port and IP combination of any machine that is behind it, meaning that connecting to an arbitrary port is often not allowed1.\nTo get around this, we can use a technique called hole-punching. The idea is that to allow packets to come in from a remote endpoint, the computer behind the NAT or firewall needs to send something to the remote endpoint first. By doing so it creates a “hole” in the NAT or firewall through which communications can proceed.\nFor hole-punching to work, peers need some way of looking at themselve s like they would in a mirror to figure out what their external facing IP address is so others can figure out how to reach out.\nOne common method is through the use of signalling servers. These servers just forward information between peers all interested in a given topic (e.g. connecting to a given application) so they can exchange the minimum amount of information to be able to directly connect. This ends up effectively being a ‘registry’ of all people who are interested in connecting with each other.\n\nUnfortunately, this method still relies on a server to handle special responsibilities. One completely decentralized way of doing this is doing something called a distributed hash table — a DHT. Each node holds a small shard of the DHT, so the burden of participation isn’t painful for any one agent. The DHT stores multiple redundant copies of each entry so that the information is available even when the author is offline. Peers can then store info about each other in this DHT to figure out what each other’s IP addresses are.\n\nOk perfect, now we can start sending messages across to our peers. We can send arbitrary data payloads but we can only send it to one person at a time. If we want to talk to a group of people, we need to start conversations with all of them! In this case, the underlying medium is not perfect and we can’t ensure that messages will always arrive in the right order.\nIn the face of this uncertainty, we still need some way to come to some shared understanding of what the ‘state’ of the application is. In literature, this is called the state machine replication problem.\nTypically, this can be done using a consensus mechanism where nodes all vote and agree on what the ‘right’ state of the application is, much like a government election. This is how distributed databases work to come to a consistent state amongst all the replicas.\nMore recently, researchers found another way of doing this is using a specific type of replicated data structure called a commutative replicated data type or CRDT. This uses mathematical properties of operations to guarantee that even if messages are received out of order, they will all eventually converge to the same result. CRDTs are at the base of a lot of popular ‘shared type’ libraries like Hypercore or Yjs, which make it really easy to use JSON-like data-types in your code that automatically receive updates from other peers.\nConclusion\nThis post was a rough mile-high overview of how peer-to-peer differs from client-server models of computer communication. I hope this enables people like you to explore new realms of possibilities of creating lively digital spaces.\nFootnotes\n\n\nFun story: IPv4 only supports 4,294,967,296 total addresses. This is less than the total number of devices that are currently connected to the internet. In fact, we ‘ran out’ of IPv4 addresses in 2011. NAT exists so that instead of every computer getting a public unique address, every home router gets a single public unique address. Computers then only get a private address assigned to them by the router and it translates the address so that to anyone external to the network, it looks like all the traffic is coming from and goes to the router. ↩\n\n\n"},"thoughts/computer-vision":{"title":"Computer Vision","links":["thoughts/computer-graphics","thoughts/imaging","thoughts/colour","thoughts/texture","thoughts/optical-flow","thoughts/object-detection","thoughts/object-classification"],"tags":["seed","technical"],"content":"\nCV, broadly speaking, is a research field aimed to enable computers to process and interpret visual data, as sighted humans can\n\nIt can also be thought of as the inverse of computer graphics.\nTypically, it’s a pipeline from\n\nImage\nSensing Device\nInterpreting Device\nInterpretation\n\nProblems in CV\n\nMeasurement. Algorithms for computing properties of the 3D world from visual data. This is literally impossible to invert the image formation process. The best we can do is guess.\nPerception and interpretation. Algorithms and representations to allow a machine to recognize objects, people, scenes, and activities. We don’t fully understand how human processing mechanisms work yet!\nSearch and organization. Algorithms to mine, search, and interact with visual data. Scale is absolutely enormous.\nVisual imagination. Algorithms for manipulation or creation of image or video content\n\nProblem subtypes\n\nCategorization\nDetection\nSegmentation\nInstance segmentation\nImage captioning\n\nSubnotes:\n\nImages, Cameras, Lenses, Filters, Sampling\nColour and texture\nOptical Flow and Stereo Vision\nrecognition (Template Matching, Keypoint Descriptors)\nObject classification (Model fitting, CNNs, Clustering) \n"},"thoughts/conceptual-model":{"title":"Conceptual models","links":["thoughts/desktop-metaphor","thoughts/mental-model"],"tags":["sapling"],"content":"Conceptual models are explanations (usually highly simplified) of how something works. These can be the desktop metaphor where files, folders, and icons help represent the model of data on the hard drive. We are concerned with mentally held conceptual models — mental models.\nOften times the designer’s conceptual model, the system image (how it manifests in reality), and the user’s conceptual model are not aligned.\nSimilar: mental model"},"thoughts/connectionist-networks":{"title":"Connectionist networks","links":["thoughts/GOFAI","thoughts/representation"],"tags":["seed"],"content":"Architecture\n\nA large number of units which are connected to others (connectionism) and the connections have different strengths (weights)\nUnits are arranged in layers\nComputation happens in parallel\n\nRepresentation\n\nLike in a classical GOFAI system, representations are assigned to connectionist networks by the people who build them\nThere are two types of connectionist representation\n\nlocalist → each unit is assigned a feature as a whole that represents\ndistributed → state of the network as a whole represents something\n\noften claimed to be one of the distinctive features of connectionism\nconnectionist networks is sometimes called parallel distributed processing (PDP)\n\n\n\n\nCan be trained to learn and resembles the structure of the brain much more closely than any classical computer\n"},"thoughts/consciousness":{"title":"Consciousness","links":["thoughts/emptiness","thoughts/Hard-problem-of-consciousness","thoughts/The-Upanisads","thoughts/qualia","thoughts/self-knowledge","thoughts/Primacy-of-Consciousness","thoughts/Nagel's-Bat-Argument","thoughts/phenomenology","thoughts/Neural-Correlates-of-Consciousness-(NCC)","thoughts/Integrated-Information-Theory-of-Consciousness-(IIT)","thoughts/Stream-of-Consciousness","thoughts/information","thoughts/Consciousness-is-not-Information","thoughts/play","thoughts/language","thoughts/praxis","thoughts/potemkin-village","thoughts/telerobotics","thoughts/telepresence"],"tags":["sapling","PHIL451A"],"content":"\nIf x is conscious, then there is an experience of what it is like to be x\n\nWhat does it mean to be conscious? Is consciousness emergent from matter? Or is it something else entirely (the ether or the soul)?\n“Consciousness is always consciousness of something, and when the object is subtracted, nothing remains to be characterized” (cannot be of independent origin, emptiness)\nSee also: Hard problem of consciousness, The Upanisads, qualia, self-knowledge, Primacy of Consciousness, Nagel’s Bat Argument\nTypes of consciousness:\n\nPhenomenal consciousness (felt consciousness) vs Access consciousness\nGross consciousness (obvious waking state) vs Subtle consciousness (sleep/deep sleep). In more Indian philosophy, consciousness is that which is luminous (revealing) and has capacity for cognition.\n\nTheories of consciousness\n\nCognitive Theories: consciousness is based in areas that are devoted to cognitive processing (thinking, reasoning, evaluating, memory, etc.)\n\nGlobal workspace theory: consciousness arises from highly coordinated, widespread activity in the brain\n\nWhen signals are broadcast to the global workspace, we become conscious of the sensation\nWhen they remain localized, they are not perceived consciously\n\n\n\n\nSensory Theories: consciousness is based in areas that are devoted to sensory processing\n\nBlock argues that recurrent processing in sensory areas is the NCC for phenomenally conscious perception\ne.g. Neural Correlates of Consciousness (NCC)\n\n\nFirst-order theories: consciousness is a product of the cognitive processing of sensory information\n\ne.g. Integrated Information Theory of Consciousness (IIT)\n\n\nHigher-order theories: consciousness involves something done to build on that cognitive representation of the sensory experience\n\ne.g. Stream of Consciousness\n\n\n\nMinimal Phenomenal Experience1\nDreamless sleep experience is a candidate for this\nBackground phenomenologies claim these experiences combine\n\nminimal dimensionality (of awareness)\nmaximal prototypicality (overlap with other states/modes of awareness)\n\nIf we find a minimal form of phenomenal experience in all forms of phenomenal content and underlies all conscious experience, then this model would amount to solving the problem of consciousness\nProblems for interpreting reports of pure awareness in meditative experience\n\nEmbodied theory contamination: background theoretical assumptions strongly condition personal beliefs and thus processing and reporting\nConceptual consistency: how does one interpret words like “perceive”, “luminous”, “radiant” in reports of experiences that are supposed to be “contentless”\nPerformative self-contradiction: “I experienced a selfless state” is a self-contradictory\n\nProblems with information theories of consciousness\nFrontiers in Systems Neuroscience\nTwo main camps\n\nRadical behaviourists: all talk of the mind could be translated, without scientific loss, into talk about behaviour\nCognitivists: all talk of the mind (including consciousness) could translated, without scientific loss, into talk about information processing\n\nCognitivists\nTheories that equate consciousness with information or information processing are dualist in nature\nChalmers (in The Conscious Mind: in Search of a Fundamental Theory) defines information in the actual world as having two aspects\n\nPhysical\nPhenomenal\n\nHowever, information is usually defined at odds to this\n\nInformation is often used to refer to non-mental, user-independent, declarative semantic contents, embedded in physical implementations (Floridi, 2005)\n\nImportant distinction: Physical things encode and embed information but themselves are not information\nJames Joyce’s copy of Dante’s Inferno may carry information, but it is not itself information—it’s just an arrangement of paper and ink\nYour brain when he looks at an octopus may encode information, but it is not itself information\n\n\nAn objective (mind-independent) entity… information can be encoded and transmitted, but the information would exist independently of its encoding or transmission\n\nSeriality/stream problem\n\nWhile the brain works as a parallel distributed network with virtually unlimited resources, conscious events appear consecutive and momentary capacity is strongly limited. Multi-tasking is notoriously hard. Theories regarding consciousness as a particular case of brain information processing must suggest a specific mechanism for creating serial processes from a collection of parallel ones\nNot a problem for behaviour-based theories as behaviour is a series of agent-environment interactions. Attention budget, more complex behaviours take more bandwidth and thus require more focus (need to be done in serial). Things can be trained to reduce focus needed (e.g. walking and talking)\n\nProblematic: thermostats (for example) clearly carry information, but are not widely regarded as having any degree of consciousness\nChalmers has two options:\n\nPerhaps only some kinds of “physically realized information spaces” are conscious.\nPerhaps thermostats are conscious (option that Chalmers chose). Suggests that “the level of organization at which consciousness “winks out” might be lower than a thermostat but higher than a rock.”\n\nTononi’s Integrated Information Theory of Consciousness (IIT)\n\nResolution to this problem is choosing that only integrated information is conscious\nIIT does not even answer this question directly, only correlating the two and correlation is most definitely neither definition nor causation: “To recapitulate, the theory claims that consciousness corresponds to the capacity to integrate information”\nSee also: Consciousness is not Information\n\nBehaviourist approaches\nBoris Kotchoubey in Frontiers in Psychology\nHuman consciousness emerges on the interface between three components of animal behaviour:\n\nCommunication\nPlay\nUse of tools\n\nBehaviour, in this context, is a biological adjustment by means of movements and all kinds of movement-related physiological activity\nCommunication and play yields symbolic games and, more importantly, language\nInteraction between symbols and tools results in human praxis\nLife is a continuous battle against the second law of thermodynamics. All organisms’ needs can be subsumed as a need in negentropy\n\nPlay: Play, therefore, introduces something that can be called “second reality” (Vygotsky, 1978). In this reality the life is going on as if it is the “primary reality,” but with the nice difference that whenever I don’t like what happens, I simply stop the process and go out, or start it anew. This makes play suspiciously like consciousness\nTools: A stick is eventually manipulated “just for fun,” and then, suddenly, it turns out to be useful. Thus no animals unable to play can use tools.\nCommunication: behaviour whose main effect is changing the behaviour of another animal\n\n\nConsciousness is then the simulated or potemkin village of reality. In words of Karl Popper, instead of dying as a result of our errors, we can let our hypotheses die on our site (Popper, 1963).\nWe do not consciously think (construct this virtual reality) all the time. Consciousness is the capacity to consciously think (telerobotics vs telepresence)\nFootnotes\n\n\nThis content is sourced from Professor Evan Thompson’s course materials for PHIL451A at UBC. All rights to this content is retained by Evan Thompson. ↩\n\n\n"},"thoughts/consensus":{"title":"Consensus","links":["thoughts/fault-tolerance","thoughts/truth","thoughts/governance","thoughts/communities","thoughts/CRDT","thoughts/State-Machine-Replication-(SMR)","thoughts/Byzantine-Agreement","thoughts/Byzantine-Faults","thoughts/consistency","thoughts/Tendermint","thoughts/Tangaroa","thoughts/HotStuff","thoughts/PBFT","thoughts/longest-chain-consensus","thoughts/bitcoin","thoughts/ethereum","thoughts/safety","thoughts/liveness","thoughts/FLP-Result","thoughts/Sandglass","thoughts/system-model","thoughts/Raft-Consensus-Algorithm","thoughts/authenticator-complexity","thoughts/SBFT","thoughts/TCP","thoughts/33-percent-Impossibility-Result","thoughts/PSL-FLM-Impossibility-Result","thoughts/LR-Permissionless-Result"],"tags":["sapling"],"content":"Consensus in human systems is actually usually pretty easy because of the social layer of society. This fault tolerance against 51% attacks is due to the fact that convincing the community that any engineered ’truth’ is the real on requires subverting every trusted member in the community, most notably media and news sources (also why systems of authoritarian power are so scary).\nA difficult problem for governance within communities\nNote that this is inherently different from collaboration methods like CRDTs. Collaboration involves keeping all edits and merging them. Consensus involves picking one of several proposed values and agreeing on it.\nExample applications include: SMR, Byzantine Agreement\nConsensus and Humming in the IETF\nSource: IETF\nOn rough consensus\n\nHumming as ‘temp checks’ for people to voice disagreement but default assumption is optimistic trust\n“While counting heads might give a good guess as to what the rough consensus will be, doing so can allow important minority views to get lost in the noise. One of the strengths of a consensus model is that minority views are addressed, and using a rough consensus model should not take away from that.”\n“We can’t know who the “members” of any given working group would be at any one time, and we certainly can’t know who all of the “members” of the IETF would be: That’s why we refer to “participants” in the IETF; the IETF doesn’t really have “members”. Indeed, we often recruit additional implementers and other experts into working groups in order to ensure that broader views are brought into the discussion. So, voting is simply not practical.”\n\nAlgorithmic Consensus\nThere are four requirements to such an algorithm:\n\nValidity. The result must be a value that was submitted by at least one of the processes. The consensus algorithm cannot just make up a value.\nUniform agreement. All nodes must select the same value.\nIntegrity. A node can select only a single value. That is, a node cannot announce one outcome and later change its mind.\nTermination. Also known as progress, every node must eventually reach a decision.\n\nThere are two main protocol paradigms for achieving consensus in the presence of Byzantine nodes:\n\nClassic BFT protocols: typically uses two voting rounds to ensure consistency\n\nOne phase to guarantee proposal uniqueness using a quorum certificate of n−f votes\nThe other phase is to convince replicas that the leader is safe to propose new entries\nExamples include: Tendermint, Tangaroa, HotStuff, PBFT\n\n\nLongest-chain consensus\n\nExamples include: most consensus mechanisms for cryptocurrencies like Bitcoin, Ethereum\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassic BFTLongest-chain ConsensusSafety/Liveness tradeoffFavours safety in the face of an attackFavour liveness in the face of an attackFinalityInstant and deterministicProbabilistic (at risk of potentially large chain reorganizations and double-spend attacks)Fork behaviourRare but difficult to recover fromEmbrace forks, uses in-protocol methods for resolving ambiguity as to which fork is correctFLP Result Behavioursacrifice either liveness or consistency in the face of an attack (assuming &lt;33% Byzantine as per FLP Result)Does not apply as longest-chain consensus is non-deterministicPermission modelGenerally permissioned (see Sandglass)Permissionless\nNote that there have been attempts to bridge Classic BFT models with Nakamoto-style consensus ones with hybrid consensus models which use a permissionless chain to determine a participant/proposer rotation in a reconfigurable BFT engine.\nComparisons between different BFT SMR protocols\nAll protocols are of the following:\n\nprotocols for byzantine fault-tolerant SMR\nAll work in the partially synchronous system model and obtain safety (always) and liveness (after GST) in the face of an adversary that controls f replicas out of a total of n=3f+1 replicas (per FLP Result)\nAll these protocols are based on the classic leader-based primary-backup approach where leaders are replaced in a view-change (or election to use Raft terminology) protocol.\n\nBelow is a comparison of a few top protocols and their tradeoffs in authenticator complexity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBest-case Latency (rounds)Normal-case CommunicationView-change CommunicationLeader RotationResponsivenessPBFT2O(n2)O(n3)On suspected faultYesTendermint2O(n) using thresholded signatures, O(n2) otherwiseO(n)Every roundNoSBFT1O(n)O(n2)On suspected faultYesHotStuff3O(n)O(n)Every roundYes\nResponsiveness here refers to the property that a non-faulty leader can drive the protocol to consensus in a time depending on actual message delays and not the theoretical upper bound on message transmission delays. In partially synchronous models, we use optimistic responsiveness, which requires responsiveness only after GST is reached.\nLeader rotation tradeoff:\n\nmaintaining a stable leader means less overhead and better performance due to stability when the leader is honest and trusted\nconstantly rotating the leader provides a stronger fairness guarantee against stable malicious leaders\n\nPipelining\nIn PBFT, SBFT, and HotStuff the leader maintains a window of open slots and is allowed to concurrently work on committing all open slots in his active window. Conceptually, this is like TCP where a sender does not have to wait for the ACK of packet i before sending message i+1. This window can significantly increase throughput by allowing the leader to concurrently coordinate several actions of slot commitments.\nImpossibility Results\nWhen consensus is impossible to achieve:\n\n33 percent Impossibility Result\nPSL-FLM Impossibility Result\nFLP Result\nLR Permissionless Result\n"},"thoughts/consistency":{"title":"Consistency","links":["thoughts/CAP-Theorem","thoughts/quorum","thoughts/message-broadcast","thoughts/linearizability","thoughts/causality","thoughts/system-model"],"tags":["seed"],"content":"Types of convergence (when all replicas eventually agree)\nDefinitions\nACID Consistency\nThe state satisfies application-specific invariants (e.g. every course with students enrolled must have at least one lecturer) at any given point in time\nSee also: CAP Theorem\nReplication Consistency\nMany models to choose from! Most common being read-after-write consistency\nImagine a scenario where\n\nClient writes to servers A and B but request to A fails (so only B has correct state)\nClient reads from servers A and B but request to B fails (so only client gets A’s incorrect state)\n\nClearly, client is getting inconsistent results. We can fix this via a quorum read.\nAtomic Commitment Problem\nBig problem with distributed transactions: atomic commitment problem\n\nEither all nodes must commit or all must abort\nIf any node crashes, all must abort\n\nUsually done through two-phase commit (2PC)\n\nClient begins a transaction with database nodes A and B\nWhen done, the client commits the transaction with the coordinator\nCoordinator tells both A and B to prepare for the commit\nIf both A and B think it is fine for them to commit, then coordinator tells both to commit (A and B cannot go back on their response to prepare, if they said they are prepared for the commit they must commit when the coordinator tells them to)\n\nBut what if the coordinator crashes? The algorithm is blocked until coordinator recovers. We can use a fault-tolerant two-phase commit (uses total order broadcast)\nEventual Consistency\nAlternative to linearizability is eventual consistency.\nIf there are no more updates, eventually all replicas will be in the same state.\nBut how do we know when there are no more updates? This can be an indefinite amount of time. An upgraded version of this is strong eventual consistency which has a few additional rules:\n\nEventual delivery: every update made to one non-faulty replica is eventually processed by every non-faulty replica\nConvergence: any two replicas that have processed the same set of updates are in the same state\n\nProperties\n\nDoes not require waiting for network communication\nCausal broadcast can disseminate updates\nConflicts arising from concurrent updates need to be resolved\n\nSummary\nSummary of minimum system model requirements for various forms of consistency\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblemMust wait for communicationRequires synchronyatomic commitall participating nodespartially synchronousconsensus, total order broadcastquorumpartially synchronouslinearizable get/setquorumasynchronouseventual consistency, causal broadcast, FIFO broadcastlocal replica onlyasynchronous"},"thoughts/constructionist":{"title":"Constructionism","links":["thoughts/Mindstorms","thoughts/language-development","thoughts/Design-Justice"],"tags":["seed","pattern"],"content":"\n“As children construct things in the world, they construct new ideas and theories in their minds, which motivates them to construct new things in the world, and on and on.”\n\nMindstorms\nFrom the book\nPiaget has demonstrated that children learn fundamental mathematical ideas by first building their own, very much different (for example, preconservationist) mathematics. And children learn language by first learning their own (“baby-talk”) dialects. So, when we think of microworlds as incubators for powerful ideas, we are trying to draw upon this effective strategy: We allow learners to learn the “official” physics by allowing them the freedom to invent many that will work in as many invented worlds.\nWhy there is no good answer to “how do I learn CS”: “I see Piaget as the theorist of learning without curriculum and the theorist of the kind of learning that happens without deliberate teaching… But ‘teaching without curriculum’ does not mean spontaneous, free-form classrooms or simply ‘leaving the child alone.’ It means supporting children as they build their own intellectual structures with materials drawn from the surrounding culture”\n\n“They’ll handle the details. Indeed, they insist on it. For a project to feel like your own, you must have sufficient autonomy. You can’t be working to order, or slowed down by bureaucracy.” — Paul Graham\n\nDesign Justice\nFrom the book\nPapert rejected the banking method of education: educator transmits a piece of information to the learner’s brain\nInstead, learning is experiential: it takes place through an active process where the learning develops the ability to modify or transform an object or idea.\nCore principles\n\nPeople do not get ideas, they make them\nPeople construct new knowledge with particular effectiveness when they are engaged in constructing personally meaningful products\n\n“Instead, design justice pedagogies must support students to actively develop their own critical analysis of design, power, and liberation, in ways that connect with their own lived experience”\nHowever, if resource constraints become an excuse to avoid examining the root of the problem area, then designers will almost always end up, at best, providing Band-Aids for deep wounds and, at word, actively serving existing power structures."},"thoughts/contact-language":{"title":"Contact Language","links":["thoughts/boundary-object","thoughts/language","thoughts/CID"],"tags":["seed"],"content":"See also: boundary object\nDigital Contact Languages\nContact Language: broadly speaking, this language served to facilitate collaboration across the groups it interconnected.\nThe rise of the Kindle points out that even the concept of a link—a “uniform resource locator,” or URL—is under great stress. Since Kindle books don’t live on the World Wide Web, there’s no URL pointing to a particular page or passage of them. The same goes for content within any number of mobile apps, leaving people to trade screenshots—or, as The Atlantic’s Kaitlyn Tiffany put it, “the gremlins of the internet”—as a way of conveying content.\nCID is a form of universal contact language"},"thoughts/content-addressed-storage":{"title":"Content addressed storage","links":["thoughts/HTTP","thoughts/CID","thoughts/block-reference-mechanisms","thoughts/Merkle-DAG","thoughts/IPFS","thoughts/hypertext","thoughts/plurality"],"tags":["seed"],"content":"\nIf I identify the book by its content, saying “Check out the book called Why Information Grows by César Hidalgo. The ISBN is 0465048994.”, you will be able to get any copy of the book from any source and know that you’re reading the information I recommended.\nBy contrast, if I used location-addressing to identify the book, I would have to point to a location, saying something like “Go to the news stand at Market &amp; 15th in Philadelphia and ask for the thing 16 inches from the south end of the third shelf on the east wall”\n\nContent-addressed storage or abbreviated CAS, is a way to store information so it can be retrieved based on its content, not its location.\nLocation-addressed: e.g. HTTP, you lookup a content by its location (URI). Whoever controls the location controls the content. This location-addressed approach forces us all to pretend that the data are in only one location (even if multiple people have copies of it!)\nContent-addressed: using the content’s cryptographic hash to identify it. These links are permanent because the cryptographic hash for a piece of content never changes.\nSee also: CID, block-reference mechanisms\nImmutable Objects, Mutable References\nThe Merkle-DAG, immutable content-addressed objects, and mutable pointers to the Merkle-DAG, instantiate a dichotomy present in many successful distributed systems\nIPFS accomplishes this by creating a separate prefix /ipns/&lt;NodeID&gt; ofr mutable paths. One can prove ownership because only the owner of the private key of NodeID can publish to it\nLiterary Machines\nIn Ted Nelson’s 1981 Literary Machines, he imagined a universally addressed public repository system\n\nIn the computer world this will change, especially if — as I foresee — there will be one great repository and everything will be equally accessible. This means that “different” articles and books will more likely be different versions of the same work, and different pathways through it for different readers\n\nThe system promotes the coexistence and resolution of many viewpoints, through the sharing of private documents and comments, and the publication of hypertext complexes whose interrelationships remain orderly.\n\nSee also: plurality"},"thoughts/context":{"title":"Context","links":["thoughts/emptiness","posts/context-collapse","thoughts/interoperability","thoughts/Internet","thoughts/DNS","thoughts/information-scales","thoughts/idea-list"],"tags":["sapling"],"content":"Content only has meaning if it has context. Content thus must be empty of independent origination (it cannot exist on its own, see also: emptiness).\nSee also: context-collapse\nContext in writing\nFootnotes and marginalia are layers of context on top of the original media. These annotations situate the original media in new contexts.\nContext of data\nFor most data in web2, the application is the context. How do we make data as first class citizens?\nDoes interoperable data mean context-free data? I don’t think so. I think interoperable data is data that can be translated across contexts\nContext on the web\nIn relation to the Internet: our browsing is also very static and in a small window. By default, you can only browse on a page-level scale. What about subdomains? Apex domains (see more in DNS)? How do we expand the aperture of the web browser to encompass more information scales? (potential idea)"},"thoughts/convex":{"title":"Convex","links":[],"tags":["seed"],"content":"A set is convex if line between two points in the set stays in the set.\n\nA function f is convex if all the points above f form a convex set. That is,\n\nIf f′′(w)≥0,∀w\nA convex function multiplied by a non-negative constant is convex\nNorms and squared norms are always convex\nThe sum of convex functions is convex\nThe max of convex functions is a convex function\nThe composition of a convex function g and linear function h, g∘h is convex\n"},"thoughts/convolutional-neural-networks":{"title":"Convolutional Neural Networks","links":["thoughts/imaging"],"tags":["seed","CPSC425","CPSC340"],"content":"Rather than picking from fixed convolutions, we learn the elements of the filters. A convolution is a linear filter that measures the effect one signal has on another signal.\nIf x is the (n,n) input signal (image) and w is the (2m+1,2m+1) filter, then the 2D convolution is given by\nz[i1​,i2​]=∑j1​=−mm​∑j2​=−mm​w[j1​,j2​]x[i1​+j1​,i2​+j2​]\nConvolutional Layer\nStandard is DxWxH\nK is the number of filters, F is the spatial extent of filters (kernel size), S is the stride, and P is the padding\n\nWout​=(Winput​−F+2P)/S+1\nHout​=(Hinput​−F+2P)/S+1\nDout​=K\n\nTotal number of learnable parameters: (F×F×Dinput​)×K+K.\nPooling Layer\nMakes representation smaller, more manageable and spatially invariant.\n\nWout​=(Winput​−F)/S+1\nHout​=(Hinput​−F)/S+1\nDout​=Dinput​\n\nTotal number of learnable parameters: 0.\nLayer Summary\n\nConvolutional Layer: applies a set of learnable filters\nPooling Layer: performs spatial downsampling\nFully-connected Layer: same as any regular neural network\n\nA CNN then just learns a hierarchy of filters\nProperties of Convolution\n\nAssociative. G⊗(F⊗I(x,y))=(G⊗F)⊗I(x,y)\nSymmetric. (G⊗F)⊗I(x,y)=(F⊗G)⊗I(x,y)\n\nCorrelation, on the other hand, is generally not associative.\nFor 1D Gaussians, we note Gσ1​​(x)⊗Gσ2​​(x)=Gσ12​+σ22​​​(x). Convolving with Gσ​(x)⊗Gσ​(x)=G2​σ​(x)\nBoundary Effects\n\nIgnore these locations: make the computation undefined for the outsize k rows/columns\nPad with zeroes: return zero whenever of value of I is required at some position outside the image\nAssume periodicity: wrap image around\nReflect border\n\nPillbox\nA 2D pillbox is rotationally invariant but not separable\nf(x,y)=πr21​{10​ifx2+y2≤r2otherwise​\nAn efficient implementation would represent a 2D box filter as the sum of a 2D pillbox and some “extra corner bits”\nGaussian Filters\n\nBox filter doesn’t apply well for lens defocus. A circular pillbox is a much better model for defocus\nGaussian is a good general smoothing model\n\nfor phenomena\nwhenever the CLT applies\n\n\n\nGaussian filters are rotationally invariant.\nWe get Gσ​(x,y)=2πσ21​exp−2σ2x2+y2​ where σ is the standard deviation\nFor a 3x3, we then need to quantize and truncate it, evaluating Gσ​(x,y) wherever in the filter. Increasing σ means more blur. Problem with 3x3 is that it truncates too much of the distribution (does not sum up to one), this can cause unintentional darkening.\nIn general, the Gaussian filter should capture ±3σ for σ=1 which gives us a 7x7 filter.\nEfficiency\nAs both the 2D box filter and 2D Gaussian filter are separable, it can be implemented as two 1D convolutions which convolve each row and then each column separately.\nA 2D filter is separable if it can be expressed as an outer product of two 1D filters\nA seperable 2D Gaussian only does 2m multiplications at each pixel (one for each 1D filter). Considering the image has n×n pixels, then this is a 2m×n2 multiplications. Assuming m≈n, this is O(n3)\nFourier Transform\nThe basic building block of the fourier transform is the periodic function.\nAsin(ωx+ϕ)\nwhere A is the amplitude, ω is the angular frequency and ϕ is the phase. Fourier’s claim was that you could add enough of these to get any periodic signal!\nThe Convolution Theorem\nLet i′(x,y)=f(x,y)⊗i(x,y) be the convolution.\nThen, I′(wx​,wy​)=F(wx​,wy​)I(wx​,wy​) which is just a simple element-wise multiplication after applying a Fourier transform to each.\nAt the expense of two Fourier transforms and one inverse Fourier transform, convolution can be reduced to (complex) multiplication. This speeds up the cost of FFT/IFFT for the image and filter to O(n2logn) and O(m2logm) respectively, dropping the total cost of convolution to O(n2)\nConvolution Sizing\nConvolving two filters of size m×m and n×n results in a filter of size\n(n+2⌊2m​⌋)×(n+2⌊2m​⌋)\nMore broadly for a set of K filters of sizes mk​×mk​ the resulting filter will have size\n(m1​+2∑k=2K​⌊2mk​​⌋)×(m1​+2∑k=2K​⌊2mk​​⌋)"},"thoughts/coordinate-system":{"title":"Coordinate systems","links":["thoughts/computer-graphics"],"tags":["seed"],"content":"Homogenous\nWe add a new coordinate h (sometimes also w). To convert homogenous coordinates (x,y,z,h) back into Cartesian coordinates, we divide through by h: (hx​,hy​,hz​)\nAs groundedness\nIf we interpret h as groundedness, it measures how ‘translateable’ it is:\n\nh=0 for directions/vectors\nh=1 for positions\nh∈{0,1} need to be divided by h to come back into real space\n\nAs depth\nWe can also interpret h as camera depth.\nWhat makes something look 3D?\n\nRelative size (far objects are smaller)\nPerspective (far objects converge on the origin)\nMotion parallax (far objects move slowly)\n\nDividing through h gives us these three effects.\nIn order to make distant objects appear smaller and closer to the center of the screen, we use a coordinate system with (0,0,0) at the center of the screen (this is done in the CCS).\nThis effectively projects onto the ‘near’ plane\n\nBarycentric Coordinates\nMostly used for interpolation. Given values known at the vertices (e.g. depth, colour, texture, surface normals), we wish to linearly interpolate their values for the interior pixels\nBarycentric coordinates can be seen as a weighted combination of vertices.\n\nP=αP1​+βP2​+γP3​\nα+β+γ=1\n0≤α,β,γ≤1\n\nTo interpolate, v=αv1​+βv2​+γv3​\nWe use barycentric coordinates instead of screen space coordinates because the mid-point in screen-space may not be the midpoints along the real line/object.\nCalculating the weighted value α for the vertex P1​:\n\nWrite an explicit line equation for the edge opposite the vertex\nConvert it into an implicit line equation (F2,3​ in this example, line connecting P2​ and P3​)\nEvaluate the implicit line equation at the vertex, let this be k1​\nα(P)=F2,3​(P)/k\n\nSpherical\nθ represents the polar angle (angle from the vertical z axis) and ϕ represents the azimuthal angle (angle from the x axis)\n"},"thoughts/counterculture":{"title":"Counterculture","links":[],"tags":["seed"],"content":"50s and 60s\nPeople in 50s and 60s were protesting because they were worried about being turned into information and data\nTwo distinct movements, both which are anti-bureaucracy, anti-big tech, anti-mass culture)\n\nNew Left: saw political organizing as the site of social change\nNew Communalists: saw internal consciousness and the mind as the site of social change\n\nWhy were the 50’s kids so enamoured with technology?\nWhere is todays counterculture\n\nNot the left, but actually the right\nConcern is not big data and ML but with corporations and states\nPlace of rebellion is not at the data level but at the institution and union level (in big corporations) + legal level (ex: protection of Uber drivers)\nThere are bodies and factories and server farms → lets not forget the basis of our abstractions\n"},"thoughts/cozy-software":{"title":"Personal and cozy software","links":["thoughts/instrumentalism","thoughts/small-technology","thoughts/World-Building","thoughts/Tools-for-Conviviality","thoughts/Design-Justice","thoughts/cozy-software","thoughts/Data-Capitalism","thoughts/A-Pattern-Language"],"tags":["sapling","pattern"],"content":"\ncozy eternal beta test mode\n\nNot everything needs to be built at scale — sometimes it can just be for family and friends :)) Normalize creating playful and non-instrumental technology\nSee: small technology\nAn app can be a home-cooked meal\nSource: Robin Sloan\nClay Shirky: “Situated software, by contrast, doesn’t need to be personalized — it is personal from its inception.”\nI can, at this point, make the things happen on computers that I want to make happen. At the same time, I would not last a day as a professional software engineer. Leave me in charge of a critical database and you will return to a smoldering crater.\nI am the programming equivalent of a home cook.\nSpring ‘83\nSource\nI love the disclaimer at the top: “It’s okay to share this link, but I want to underscore that I am sending it specifically to you with the hope that you will … really think about it! At such a primordial stage, a proposal like this doesn’t need diffuse, drive-by attention. It needs, instead, close consideration and generous imagination.”\n\n“There are no analytics, obviously. Nothing is counted. Boards can’t load tracking pixels, and they can’t ping remote analytics APIs. Sorry, folks … you’ve got to let go of the numbers. In 2022, they are not helpful feedback, but rather a clear warning you are in the wrong place.”\nWhen you operate a server, you get a universe for free (see also: world seed).\n\nFolk Software\nSource\n\nPeople reappropriating existing software to solve their own unique problems\n\nFolksonomies are informal taxonomies developed by users on social sharing platforms. Usually in the form of tags on sites like Flickr, Tumblr, or Instagram.\nFolk creations fill a gap. They solve problems for individuals and small communities in a way that that centralised, top-down, industrial creations never can. They are informal, distributed practices that emerge from real world contexts.\nRelated to Tools for Conviviality\nAgainst universal design\nIn Design Justice\nUniversal Design (UD) as defined by Center for UD at North Carolina State University: The design of products and environment to be usable by all people, to the greatest extent possible, without the need for adaptation or specialized design\nAt least in the digital domain, adaptive design that enables personalization and flexible configuration of shared core objects, tools, platforms, and systems provides a path out of the tension between the diverse needs of individual users and the economic advantages of a large-scale user base.\nUniversalization erases difference and produces self-reinforcing spirals of exclusion, but personalized and culturally adaptive systems too often are deployed in ways that reinforce surveillance capitalism.\nWe should destabilize the underlying assumption that what is best for the majority of users is best for all users.\n\n“People are different sizes; they sit in different ways. And yet there is a tendency in modern times to make all chairs alike” (p.1158, Different Chairs, A Pattern Language)\n\nBicycles for the mind\n\nWe were promised bicycles but instead we got aircraft carriers\nBicycles: personal, light, moddable\nAircraft carriers: industrial, heavy, manufactured\n"},"thoughts/creation-vs-maintenance":{"title":"Creation vs Maintenance","links":["thoughts/innovation","thoughts/maintenance","thoughts/traditional-knowledge"],"tags":["sapling"],"content":"So much of today’s society revolves around creating new things rather than maintaining existing things. Yet, the two go hand in hand - innovation without the ability to scale and maintain is frivolous, and maintenance without innovation is stagnation.\nThere is importance in indigenous knowledge and traditional knowledge, which strides this fine line very well.\nMVP vs Product\nSource: I could do that in a weekend! by Dan Luu\nIt’s not just building out an initial system, but also about how maintainable and scalable the system is for the foreseeable future.\nBusinesses that actually care about turning a profit will spend a lot of time (hence, a lot of engineers) working on optimizing systems, even if an MVP for the system could have been built in a weekend.\nThis reminds me of a common fallacy we see in unreliable systems, where people build the happy path with the idea that the happy path is the “real” work, and that error handling can be tacked on later. For reliable systems, error handling is more work than the happy path. The same thing is true for large services — all of this stuff that people don’t think of as “real” work is more work than the core service\nOn shifting the focus too quickly\nSource: Aeon\n“One important topic of conversation is the danger of moving too triumphantly from innovation to maintenance. There is no point in keeping the practice of hero-worship that merely changes the cast of heroes without confronting some of the deeper problems underlying the innovation obsession.”"},"thoughts/creative-writing":{"title":"Creative writing","links":["thoughts/writing","thoughts/consciousness"],"tags":["seed"],"content":"We have a name for the tendency to go on a reasonable number of random walks: we call it “creativity.”\nCommon misconceptions\n\nCreative writing is about creating ‘literature’ and not genre stories: it is more so a question of quality than content\nWriting is an innate talent: writing is a tool that can be honed and practiced\nNo real world application for creative writing: almost all conversations, media, arguments, involve some form of convincing the other side! what better way to do that than through story telling\nThe riddle of storytelling\n\nImagine two groups of early humans competing for the same resources who lived pretty much the same\nThe first group gossiped and told stories during their leisure time while the second group continued working\nWe know the first group survived because that’s us! So why is story telling so evolutionarily beneficial? It’s a form of simulation (see also: behaviourist approaches to consciousness)\n\n\n\nClassic Story Structures\n\nFreytag’s Pyramid\n\nExposition\nRising Action\nClimax\nFalling Action\nDenouement\n\n\nKurt Vonnegut’s Shape of Stories\nJoseph Campbell’s “Hero’s Journey” (story wheel)\n\nCharacter is in a zone of comfort\nBut they want something\nThey enter an unfamiliar situation\nAdapt to it\nGet what they wanted\nPay a heavy price for it\nThen return to their familiar situation\nHaving changed\n\n\nThree Act Structure\n\nAct I: Get your guy up a tree\nAct II: Throw rocks at him\nAct III: Get him outta the tree\n\n\n\nAspects of Stories\n\nMcGuffin: an object, device, or event that is necessary to the plot and the motivation of the characters, but insignificant, unimportant, or irrelevant in itself.\nThe universal grammar for stories: character + conflict = change\nPlot: the how of the story. Explicit. A series of actions, taken by characters, towards their wants, needs or desires\nTheme: the why of the story, the “so what”. Should be\n\nUniversal\nSpecific\nImplied\n\n\nSummary (general information, tell) vs Scene (specific descriptions, show)\n\nWriting Pitfalls\n\nStructural ambiguity: missing important story beats (e.g. Four undramatic plot structures) — you need character, conflict, and change\n\n\nCliché/Familiar phrasing: don’t do things that are incredibly overdone\nAwkward exposition: having conversations that characters would never actually have for the sake of reader understanding\nAbstractions: don’t invoke abstractions, make it concrete, reify it\n\n\nVagueness (unclear) vs Ambiguity (up to interpretation)\nDeus Ex Machina: god from the machine, heavy handed use of magic or coincidence to solve a conflict\nCharacters: be aware of your defaults\n\nThere not always a person but instead an element of storytelling — a vehicle on which the action of the story plays out\n\n\n\nStorytelling techniques\n\nVoice\n\nHow your characters speak, their voice and diction\nThe language you use as the author\n\n\nIrony\n\nVerbal irony: the device by which we say one thing and mean another\nDramatic irony: the device by which the audience has crucial information that the characters do not\nCosmic irony: our understanding of the human condition, in which efforts are thwarted despite our best intentions\n\n\n\nPoetry\n\nAssonance: repetition of a vowel sound between consonants that may or may not match\nConsonance: repetition of the consonant that concludes a word or syllable\nAlliteration: repetition of an initial consonant sound\nRhyme\n\nTrue rhyme: both the vowel and consonant of the last accented syllable correspond\nInternal rhyme: the end of one line rhymes into the beginning or middle of noather\nOff-rhyme: near-rhyme, slightly discordant or ‘not quire there’ rhymes (four-inch, door hinge)\n\n\nLine: a typographical break representing a slight oral pause or hesitation\n\nIt adds a kind of emphasis both to the last word of the line and the first of the next line\n\n\n"},"thoughts/credible-exit":{"title":"Credible exit","links":["thoughts/game-theory","thoughts/interoperability"],"tags":["seed","pattern"],"content":"One sub-branch of the Exit, Voice, and Loyalty model (based on game theory).\nExit, Voice, and Loyalty\nThere are two agents.\n\nThe Citizen\nThe Government\n\nThe assumption is that there is a change implemented by the Government which negatively harms the Citizen. The Citizen has one of three actions:\n\nExit: Citizen leaves\nVoice: Citizen attempts to change the situation\n\nGovernment can either respond with 1) Respond or 2) Ignore\nIf the Government chooses 2) Ignore, then the Citizen must choose between Exit or Ignore\n\n\nLoyalty: Citizen does nothing, waits passively for conditions to improve\n\nCredible Exit\nA few dimensions for credible exit in software:\n\nI can export my data. But usually, this doesn’t work as data is application specific. No other app knows how to interpret that export unless it is explicitly supported. Export has the downside of being static. If you continue to use the app, your export becomes invalid. This makes export only really useful for hard exit. (see also: interoperability)\nData should be in a useful format: exit happens through a common formats that work in other apps\nYou have all the tools to use the data without the app: in other words, either the data is independent of application logic, or the application logic is transparent (i.e. the app is open source)\n"},"thoughts/crutch-and-shoe-metaphor":{"title":"Crutch and Shoe metaphor","links":[],"tags":["seed","pattern"],"content":"Those with broken legs will use crutches. Shoes however, will enhance performance no matter what."},"thoughts/cryptography":{"title":"Cryptography","links":["thoughts/hash-function","thoughts/content-addressed-storage","thoughts/CID","thoughts/encryption","thoughts/Symmetric-Key-Cryptography","thoughts/RSA","thoughts/Elliptic-curve-Cryptography-(ECC)","thoughts/Asymmetric-Key-Cryptography","thoughts/MAC","thoughts/digital-signatures"],"tags":["seed"],"content":"\nCryptography is the practice and study of techniques for secure communication in the presence of adversarial behaviour.\n\nThe following is a list of primitives one may want to accomplish using cryptography\nData Representation\nStoring plain versions of some data is often risky (e.g. passwords.) Can we create one-way functions that transform potentially large amounts of data or sensitive data into a unique1 value that can be used for comparison or addressing.\nThe main primitive for this are hash functions which can enable content-addressed storage (e.g. CIDs)\nSecure Communication\nEncryption can be used to make sure that only intended recipients can receive the message or data you want.\nMostly accomplished using\n\nSymmetric Key Cryptography (e.g. RSA or ECC)\nAsymmetric Key Cryptography\n\nMessage integrity and Authentication\n\nIntegrity: can the recipient be confident that the message has not been accidentally modified?\nAuthentication: can the recipient be confident that the message originates from the sender?\nNon-repudiation: can the message’s authenticity be unchallengeable? (i.e. if I send a message, I can’t later maintain I did not)\n\nThere are multiple ways of accomplishing this:\n\nHash functions\nMACs\nDigital Signatures\n\nGuarantees\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHashMACDigital SignatureIntegrityYesYesYesAuthenticationNoYesYesNon-repudiationNoNoYesKind of keysNoneSymmetricAsymmetric\nEfficiency\nMACs can be computed three orders of magnitude faster than digital signatures. For example, a 200MHz Pentium Pro takes 43ms to generate a 1024-bit modulus RSA signature of an MD5 digest and 0.6ms to verify the signature, whereas it takes only 10.3μs to compute the MAC of a 64-byte message on the same hardware in our implementation. There are other publickey cryptosystems that generate signatures faster, e.g.,\nelliptic curve public-key cryptosystems, but signature verification is slower.\nFootnotes\n\n\nUp to the limits of probability (e.g. more unlikely than picking the same grain at random as someone else on the beach) ↩\n\n\n"},"thoughts/ctrlv-next":{"title":"Rewriting ctrl-v using Next.js","links":[],"tags":["fruit","technical"],"content":"\nEver since I released ctrl-v, friends from HackClub have kept bugging me about switching my current React code over to use Next.js. I wanted to see what all the fuss was about, so I finally set aside some time this weekend to rewrite the frontend using Next.js and deployed it on Vercel instead of Firebase Hosting. I thought it would take the whole weekend, but I managed to go from zero knowledge about Next.js to a fully finished refactor in just under 6 hours!\nThis will be more of a technical blog post walking through my process in converting ctrl-v from using React Router and Firebase Hosting to Next.js and Firebase.\nSource code: https://github.com/jackyzha0/ctrl-v\nLearning Next.js\nI’m a very hands-on learner. I learn the fastest stumbling my way through a project with a framework I barely know my way around than watching multiple video examples or reading blog posts. Having a tangible project to work with and break as I learned things really helped speed up the learning process for me. That being said, here are a few other resources I found really helpful in my journey!\n\nNext.js in 100 seconds + good beginner tutorial\nTwiddling around with create-next-app\nNext.js official documentation for migrating from React Router\n\nWhy Next.js\nThe main reason I wanted to switch to Next.js is because it makes it dead easy to implement both Static Site Generation (SSG) and Server Side Rendering (SSR) for your React app. For those uninitiated, SSR and SSG are different approaches to rendering content.\nIn Client Side Rendering (CSR), the client is responsible for rendering the content on the page. All they receive is an empty HTML document, and a bunch of JavaScript bundles through which the browser populates the page. The downside of this approach is that no content exists on the page when it is first loaded. As a result, web crawlers won’t be able to find your content (not great for SEO), and you won’t get link previews to your content as the page metadata won’t have loaded. This is the behaviour you get when you use plain old React.\nIn Server Side Rendering (SSR), the server is responsible for rendering the content of the page and sending the fully rendered the content to the user to display. In this approach, all the content is already loaded when the user is receives it, we only need JavaScript to make it interactable (i.e. hydrate it). As a result, this approach is a lot more SEO friendly.\nOn the very other end of the spectrum, Static Site Generation (SSG), means that the server does all of the page rendering at build time rather than on each request. This has all the same benefits as SSR but doesn’t really make a lot of sense for this project as I don’t have a full list of pages I need to render handy.\nA hybrid approach\nHowever, Next.js doesn’t force you to choose between the two. It actually prides itself in just how easy they make it to switch between them. Here’s the approach I decided to take for ctrl-v.\nIn the old frontend, everything was completely client-side rendered. My ‘productionized’ application was quite literally an index.html with a bunch of JS bundles. It sucked as whenever I sent a ctrl-v link to a friend, there wouldn’t be a preview about what the content was and most people were scared to just open a random link like that (rightfully so). There was also a not-very-pleasant ‘loading paste…’ period before any content actually appeared on the screen.\n\nI realized that there were only really two main types of pages\n\nView Paste (/:paste)\nCreate Paste (/)\n\nNothing on the create paste page actually required hitting the backend, it was effectively completely static. I could safely just replace that entire page and have it be statically generated at build time. The view paste page required me to make a call to ctrl-v’s backend API but I still wanted the page to have that content rendered on server so I opted for client side rendering on any view paste pages.\nSteps\nReplacing React Router with next/router\nNext.js uses a file-based routing system rather than routes-as-code approach that React Router takes. I decided to first convert the new paste page as that would be completely SSG. Luckily, I had originally structured my React components into a pages folder so pulling out that component into a page wasn’t terrible.\nI wasn’t super certain of how SSR worked right off the bat so I first played with the /raw/:paste page first before tackling the comparatively more scary /:paste page with password handling and actual error handling.\n// old CSR\nconst Raw = ({hash}) =&gt; {\n   const { err, result } = useFetchPaste(hash)\n   return &lt;RawText&gt;{result?.content || err}&lt;/RawText&gt;\n}\n \nexport default Raw\n \n// new SSR\nexport async function getServerSideProps(ctx) {\n  // ctx.params.hash allows us to access the slug (the :paste part of the url)\n  const data = await resolvePaste(ctx.params.hash)\n  return { props: { ...data } }\n}\n \nconst Raw = ({error, data}) =&gt; {\n  return &lt;div&gt;\n    {/* Only load title/description metadata if no error */}\n    {!error &amp;&amp; &lt;NextHead data={data} /&gt;}\n    &lt;RawText&gt;\n      {/* Just render the content if it exists, otherwise render the errror */}\n      {data?.content || error}\n    &lt;/RawText&gt;\n  &lt;/div&gt;\n}\n \nexport default Raw\nThe biggest part I needed to wrap my head around was the getServerSideProps async function. I didn’t realize that the only job that getServerSideProps actually does, is fetch data and pass that data as props to your actual component. Once that clicked, the rest of the refactoring went relatively smoothly.\nI had to refactor my useFetchPaste hook to just fetch data and then delegated the responsibility of state and password validation shenanigans to whatever component. I ended up changing the name to resolvePaste as it no longer followed the Rules of Hooks.\nIn hindsight, I probably should’ve done this step incrementally using something like Next.js’s rewrite rules to gradually transition ctrl-v from React Router to Next.js. Luckily I didn’t have that many pages to migrate but this is good to know for the future.\nstyled-components and theme provider\nFrom what I’ve seen, Next.js doesn’t play nicely with styled-components out of the box. Throughout the entire previous step, I was looking at painfully broken CSS. Turns out you need to install an additional Babel plugin to add SSR support to styled-components. Then, I added a custom Document to pages/_document.js. This code augments the root &lt;html&gt; tag of our application which will allow us to inline our CSS styles.\n// shamelessly stolen from\n// https://github.com/vercel/next.js/blob/master/examples/with-styled-components/pages/_document.js\nimport Document from &quot;next/document&quot;\nimport { ServerStyleSheet } from &quot;styled-components&quot;\n \nexport default class StyledDocument extends Document {\n  static async getInitialProps(ctx) {\n    const sheet = new ServerStyleSheet()\n    const originalRenderPage = ctx.renderPage\n \n    try {\n      ctx.renderPage = () =&gt;\n        originalRenderPage({\n          enhanceApp: (App) =&gt; (props) =&gt; sheet.collectStyles(&lt;App {...props} /&gt;),\n        })\n \n      const initialProps = await Document.getInitialProps(ctx)\n      return {\n        ...initialProps,\n        styles: (\n          &lt;div&gt;\n            {initialProps.styles}\n            {sheet.getStyleElement()}\n          &lt;/div&gt;\n        ),\n      }\n    } finally {\n      sheet.seal()\n    }\n  }\n}\nAs I was also using theme provider from styled-components, I needed a way to wrap the ThemeProvider component around everything. Luckily a custom App created by adding a pages/_app.js will allow us to do just that. This also allows us to add any ‘global’ structure like padding or margins around the edges.\nimport ThemeProvider from &quot;../theme/ThemeProvider&quot;\nimport GlobalStyle from &quot;../theme/GlobalStyle&quot;\n \nconst Main = styled.div`\n  margin-top: 10vh;\n  padding: 0 20vw 30px 20vw;\n`\n \nconst App = ({ Component, pageProps }) =&gt; (\n  &lt;ThemeProvider&gt;\n    &lt;GlobalStyle /&gt;\n    &lt;Head&gt;\n      &lt;title&gt;ctrl-v | a modern, open-source pastebin&lt;/title&gt;\n    &lt;/Head&gt;\n    &lt;Main id=&quot;appElement&quot;&gt;\n      &lt;Component {...pageProps} /&gt;\n    &lt;/Main&gt;\n  &lt;/ThemeProvider&gt;\n)\n \nexport default App\nState caching for password pastes\nThis part stumped me for a good bit. I needed to find a way to do an initial data fetch, check to see if the paste has a password, prompt the user for a password, then reload the page data. I contemplated using SWR for data fetching but realized in the shower that it wouldn’t really make sense as the initial load is handled in the getServerSideProps function anyways. I realized the best way forward was to fetch the initial state in getServerSideProps then store it as a React State so I can update it when a password is entered and data is re-fetched. This way there are no page reloads to disrupt user experience and keeps code change minimal.\n// simplified version of pages/[hash].js\nexport async function getServerSideProps(ctx) {\n  const data = await resolvePaste(ctx.params.hash)\n  return { props: { ...data } }\n}\n \nconst ViewPaste = ({ data, unauthorized, error }) =&gt; {\n  const router = useRouter()\n  const { hash } = router.query // equivalent to ctx.params.hash\n  const [enteredPass, setEnteredPass] = useState(&quot;&quot;)\n  const [correctPass, setCorrectPass] = useState(!unauthorized)\n  const [clientData, setClientData] = useState(data)\n  const { content, language, expiry, title } = clientData\n \n  const getWithPassword = (password, errorCallback) =&gt; {\n    resolvePaste(hash, password)\n      .then((resp) =&gt; {\n        setCorrectPass(true)\n        setClientData(resp.data)\n      })\n      .catch((e) =&gt; errorCallback(e.response.data))\n  }\n \n  return (\n    &lt;div&gt;\n      {!error &amp;&amp; &lt;NextHead data={data} /&gt;}\n      ...\n      &lt;PasteInfo\n        hash={hash}\n        lang={language}\n        theme={theme}\n        expiry={expiry}\n        err={unauthorized ? &quot;&quot; : error}\n      /&gt;\n    &lt;/div&gt;\n  )\n}\n \nexport default ViewPaste\nDeployment\nGreat! At this point, everything was working great — locally. Every developer knows just how much of a leap it is to go from local machine to hosted and on the cloud. Right?\nMy first thought was to see how difficult it would be to port my existing frontend on Firebase Hosting to run using Next.js instead. Turns out I would’ve needed to enable Firebase Functions and to do that I needed to enable the Blaze plan (pay as you go). Seeing just how much of a hassle the process was made me look for other alternatives.\nVercel really surprised me with how easy it was to get setup. All I had to do was signup for Vercel using my GitHub account and select a repository to import. It took a total of 5 minutes to go from GitHub repo to deployed website which was a first.\nFunnily enough, the hardest part of this entire deployment process was deleting the old frontend off of Firebase. I accidentally deleted the entire project (including my Cloud Run deployed backend) twice and quite nearly misconfigured my domain.\nImpact\nAll in all, trading 6 hours for such a huge quality-of-life improvement on a project that I personally use a lot has been absolutely worth it. To list a few tangible improvements:\n\nSmoother user experience (no more annoying ‘loading paste…’ messages)\nCleaner codebase\nBetter link previews (sending links to friends)\n\nIf you haven’t had the chance to try Next.js out for yourself and you’ve made it this far, you owe it to yourself to at least give it a shot. Thanks to the HackClub community (specifically Rishi, Safin, and Ani) for their constant encouragement and criticism of my ‘outdated’ tech stack.\nSource Code: https://github.com/jackyzha0/ctrl-v\nTry it out for yourself: https://ctrl-v.app/"},"thoughts/dao":{"title":"DAO","links":["thoughts/pseudonymity","thoughts/identity","thoughts/tragedy-of-the-commons","thoughts/value-setting","thoughts/group-limits"],"tags":["seed"],"content":"\nCommunities organized around a shared set of rules enforced on the blockchain through a smart contract.\n\nSource: A beginner’s guide to DAOs by Linda Xie\nDAOs give us structured ways to collect, invest, and build together.\nInherently transparent, every aspect of the org is public. The most infamous DAO, The DAO, was a decentralized VC fund. Eventually raised 150Mbuthad60M hacked, leaving a negative impression/skepticism around DAOs for a while.\nAll members can still be anonymous and built reputations attached to their pseudonym rather than real identity, hopefully helps to create a more even playing field\nPotential Issues\n\nHow does this work for people who want to keep things under NDA/keep IP safe?\nNot everything can be codified, what happens in vaugely defined/legal gray zones?\nOften times no legal protections outside the rules governed by the smart contracts facilitating the DAO\nTragedy of the Commons: how do we ensure all members of the DAO have stake and participate?\nHow/can we enforce who joins a DAO? Would this lead to lower quality discussion and higher noise due to lack of value alignment and group limits?\n"},"thoughts/data-changes-the-application":{"title":"Data changes the application","links":["posts/towards-data-neutrality"],"tags":["sapling","pattern"],"content":"Existing large owners of data won’t innovate because they have monopoly over data, new innovators cant enter because they lack data\nSee: data neutrality\nSocial\nAchieving product-market-fit of social is the intersection of a feature and a graph\n\npeople keep copying each others features\nbut if the underlying graph is different, the intersection might be different too\n"},"thoughts/data-distributions":{"title":"Data Distributions","links":["thoughts/machine-learning","posts/agi","thoughts/To-Live-in-their-Utopia","thoughts/Matthew-Effect","thoughts/information","thoughts/context"],"tags":["sapling","pattern"],"content":"Machine Learning and AI Systems excludes the tail ends of the distributions. Synthetic/generative/federated models suck at these. And, for a lot of industries, the most interesting cases are outliers (esp. in medical AI)\nThis is where minorities live, and the result is that most ML systems end up reproducing existing systems of power (re: To live in their Utopia, Matthew Effect)\nOverfitting\nDoes “not trying to overfit” mean we perform badly on some groups?\n\nIf you have 99% “Group A” in your dataset, model can do well on average by only focusing on Group A\nTreat the other 1% as outliers\nDoing well at test-time might mean ignoring outliers and minorities\n\nContextual Data\nShould data and information be contextualized all the time?\n\nContext is important when dealing with historical data. Knowing why certain decisions were made is extremely important\nWe want data to be anonymized to a certain extent. Exposing patient data, for example, is a huge risk.\n\nHow do we choose what context to include and what not to include?"},"thoughts/data-mining":{"title":"Data mining","links":[],"tags":["seed"],"content":"Data mining is a way to generate new information by combining facts found in multiple transactions, and it can also be a way to predict future events.\nTypical steps of data mining\n\nLearn about the application\nIdentify data mining task\nCollect data\nClean and preprocess the data\nTransform data or select useful subsets\nChoose data mining algorithm\nData mining\nEvaluate visualize and interpret results\nUse results for profit or other goals\n\nIn a table\n\na row is an example or sample\na column is a feature\n\nFeature types:\n\nCategorical\n\nbinary\nnominal: name-like\n\n\nNumerical (counts, ordinal, continuous)\n\nAllows us to interpret examples in points in feature space\n\n\n\nWays to approximate other data with numerical features\n\nText:\n\nBag of words: word counts\n\n\nImages: gray-scale intensity\nGraphs: adjacency matrix\n\nData can not be clean when data is\n\nduplicated\nmissing\nfull of outliers\nnoisy\n\nCoupon collector problem: you generally need to see O(nlogn) samples to see all n possible values which have equal probabilities"},"thoughts/data-structure":{"title":"Data Structures","links":[],"tags":["seed"],"content":"Rope\nThis is essentially an immutable String except many operations that would be O(n) with normal strings are instead O(logn) or O(1)\nRopes will be slower and take more memory than small Strings but they have an asymptotic advantage when working with large documents."},"thoughts/death":{"title":"Death","links":["thoughts/The-Upanisads","thoughts/Epicurueanism","thoughts/consciousness","thoughts/time"],"tags":["seed","PHIL451A"],"content":"\nDeath is thus separation from everything that gives our life form. It is the loss of everything that we hold dear… the finality of it, is harrowing\n\nThe Universal Fear of Death and the Cultural Response\n\n\nConceptions of death in The Upanisads and Epicurueanism\nDeath in contemporary terms1\n\nState of being dead\nThe momentary event of dying\n\nEnding of the dying process (denouement death): separates the dying process from the subsequent disintegration\nPoint when extinction is assured (threshold death): irreversable loss of consciousness/personhood/functioning\n\nMedical Criteria\n\nIrreversible cessation of circulatory-respiratory function (traditionally)\nIrreversible cessation of the functioning of the entire brain, including the brain stem (the modern biomedical criterion)\nIrreversible cessation of the functioning of the cerebral cortex (newer controversial idea that the cortex is the locus of consciousness)\n\n\n\n\nLoss of integrated functioning (integration death)\n\n\nThe process of dissolution (dying)\n\n\nNear Death Experiences (NDEs)1\n\nSubjective experiences occurring when people are physiologically near death or when they believe themselves to be near death\n\nGreyson Scale\n\nCognitive Features\n\nTime distortion\nThought acceleration\nLife review\nRevelation\n\n\nAffective Features\n\nPeace\nJoy\nCosmic unity\nEncounter with light\n\n\nParanormal Features\n\nVivid sensory events\nApparent extrasensory perception\nPrecognitive visions\nOut-of-body experiences\n\n\nTranscendental Features\n\nSense of “otherworldly” environment\nSense of a mystical entity\nSense of deceased/religious spirits\nSense of border/point of no return\n\n\n\n\nProblem with these is that we don’t know exactly when the experiences happen in relation to the time-course of the near death experience\n\nReports only give us the patient’s subjective representation of time, not the objective time when the experience was actually happening\nHave virtually no direct evidence about what states of the brain are associated with NDEs\n\nFootnotes\n\n\nThis content is sourced from Professor Evan Thompson’s course materials for PHIL451A at UBC. All rights to this content is retained by Evan Thompson. ↩ ↩2\n\n\n"},"thoughts/debt":{"title":"Debt","links":["thoughts/money","thoughts/quantization"],"tags":["seed"],"content":"Source: Debt - the first 5000 years, David Graeber on Kernel\nRelated: money\nHuman Exchange\nOn the basis of human exchange: we only have a relationship with the other (as equals) when the exchange is incomplete, i.e. when there is debt.\n\nSomeone will give you something and the expectation is that you will give them something in return, though not something of exactly the same value as that would indicate you no longer wish to relate with them\n\nBy this definition then, a community is one where everybody is a little bit in debt to everyone else which gives us an excuse to continue seeing each other.\nMoney then, appears to be a way to have quantified and transferable promises. (An aside, why do we even feel the need to quantify promises??)\nHowever, the philosophy of debt as something which cannot be forgiven only ever crops up in situations of structural coercion and extreme inequality.\nAgainst the Econ 101 argument\nBarter → money → credit is not only wrong, it’s backwards.\nWe have never found any evidence for an entirely barter-based society. In places without money, there are usually rough credit systems instead: “a cow is roughly like a canoe is roughly like a good necklace”. Of course, people can shirk this but your reputation is then tarnished and that can be social suicide in small communities.\nMarkets and Government\nTraditionally, free markets and governments have been seen as opposing principles.\nHistorically, in fact, markets tend to be created by governments as a side-effect of military operations and to fund armies."},"thoughts/decentralization":{"title":"Decentralization","links":["thoughts/inevitability-of-centralization","thoughts/bitcoin","thoughts/ethereum","thoughts/network-effect","thoughts/positive-sum","thoughts/zero-sum","thoughts/federation","thoughts/fault-tolerance","thoughts/BitTorrent","thoughts/Protocol","thoughts/proof-of-stake","thoughts/game-theory","thoughts/consensus","thoughts/Internet"],"tags":["sapling"],"content":"The value of decentralization is in empowering people to act decisively within their social contexts\n\nSee also: inevitability of centralization\n3 Axes of Decentralizatiaon\n\nArchitectural Decentralization: how many physical computers is a system made of? how many computers can fail before the network fails?\nPolitical Decentralization: how many individuals/organizations ultimately control the computers that the system is made up of?\nLogical Decentralization: are the data structures used to represent the system more monolithic or swarm-like? If you cut the system in half (both providers and users), will both halves continue to fully operate as independent units?\n\nNotes:\n\nArchitectural centralization often leads to political centralization (at least in the physical world, less true for digital spaces), this happened with Bitcoin and Ethereum mining\n\nCentralization\nSource: Why Decentralization Matters by Chris Dixon\nMost things follow a predictable life cycle along the S-shaped adoption curve.\nAt the beginning, will do everything they can to garner usage and appear more valuable as platforms with multi-sided positive network effects. However, when they move up the S-curve, their power grows. Eventually, the relationships turn from positive sum to zero sum. Thus, to continue growing, they must extract from users (e.g. selling user data, taxing profits, etc.)\n\nCentralized systems often start out fully baked, but only get better at the rate at which employees at the sponsoring company improve them. Decentralized systems start out half-baked but, under the right conditions, grow exponentially as they attract new contributors.\n\nTypes of Centralization\nFrom IETF Draft\n\nProprietary Centralization: creation of a protocol/application with a fixed role for a specific party (e.g. making a protocol for streaming on Zoom). Generally undesirable as it most often reflect commercial goals (strong desire to capture financial benefits by “locking in” users to a proprietary service)\nBeneficial Centralization: need for a single, globally coordinated “source of truth” (e.g. DNS). Need for coordination in establishing p2p connections (endpoint mutual discovery typically requires a third party)\nInherited Centralization: depending on a centralized “lower-layer” protocol. Having only a single implementation of a protocol is also an inherited centralization risk because applications that use it are vulnerable to the control it has over their operation (can still happen with open source! maintaining forks for example is costly)\nPlatform Centralization: platform for centralization — while the protocol itself is not centralized, it facilitates the creation of centralized services and applications (can help to mitigate this through federation)\n\nStandards efforts should focus on providing concrete utility to the majority of their users as published, rather than being a “framework” where interoperability is not immediately available.\nWhy Decentralize?\n\nFault tolerance, less likely to fail accidentally because they rely on many separate redundant components\nAttack resistance, no central point to attack\nCollusion resistance\n\nDecentralization as Activism\nSource: Resistant protocols: How decentralization evolves by John Backus\n\nDecentralization doesn’t work in a vacuum, mainstream decentralized systems require a degree of activism to keep the system working\n\n”BitTorrent seems to represent the minimum viable decentralization required to stay alive as defined by the law at the time”\nDecentralization is a tactic for diffusing risk for many and lowering the risk for the activists that operate the most sensitive parts of the system. Over applying decentralization isn’t a strategy unless your goal is obscurity\nCounterpoints\nFaul Tolerance\nCommon mode failure, all pieces can fail for the same reason. (e.g. all nodes in a blockchain run the same software but that software has a bug)\nTo counteract this,\n\nhave multiple competing implementations\nknowledge of technical considerations behind underlying protocol must be public and democratized\ncore stakeholders should be from multiple companies/orgs or just multiple volunteers\nuse proof of stake to move away from hardware centralization risk\n\nAttack Resistance\nFrom a purely mathematical and game theory perspective, decentralization may not even matter. In a finality reversion (e.g. 51% attack), a huge loss of say $50M is still $50M regardless of whether you have validators in 1 org or 10.\nHowever, after considering coercion, decentralization becomes much more important. It’s much harder to threaten 100 people than 1.\nCollusion Resistance\n\nCollusion is “coordination that we don’t like”\n\nConsensus model relies on uncoordinated choice model, the assumption that the game consists of many small actors that make decisions independently\n90% of the entire Bitcoin network’s mining power can show up at the same conference (as 7 men). Yet, some coordination is good (e.g. strong community spirit and banding together to implement patches and fix bugs)\nWays to counteract:\n\nFind a happy medium that allows enough coordination for a protocol to move forward but not enough to enable attacks\nTry to make a distinction between beneficial coordination and collusion and make the former easier and the latter harder\n\nRelated: decentralization on the Internet"},"thoughts/decentralized-marketplace":{"title":"Decentralized marketplace","links":["thoughts/decentralization","thoughts/funding","thoughts/web3","thoughts/state-channels","thoughts/ethereum"],"tags":["seed"],"content":"Related: decentralization, funding, web3\nNanopayments\n“When you wake up in the morning and flick on a light switch, do you pause to think about how many tiny fractions of a penny that electricity costs? Or do you just flick on the light so you don’t bump your head?\nAnd if you could pay for other kinds of services the same way you pay for electricity — a tiny flow of resources that could be turned on or off at any moment — what possibilities would that open up?”\nCan be done through 2 main ways:\n\nState channels\nAmortized cost through probabilistic payments (i.e. 1% chance to win 100insteadof1 payment). Over time, the value transmitted on-chain will in expectation match the value represented in the probabilistic nanopayments.\n\nOrchid\nWebsite\nTLDR; decentralized VPN with nanopayment channels based on xDAI L2 chain\nProviders on Orchid run the Orchid server which accepts connection requests and provides VPN service in exchange for immediate payment via nanopayments. Orchid providers stake OXT tokens in an Ethereum smart contract (the directory) to advertise their services to clients. Orchid clients then select providers randomly, weighted by proportional stake, so that the probability of picking a particular provider is equal to their fraction of the total stake.\nThey also offer prepaid access credits: A frictionless payment system\nOrchid’s Prepaid Access Credits provide users the option to pay in fiat for VPN credits denominated in the xDAI stablecoin through a simple in-app purchase on mobile devices. The credits are only spendable with Orchid’s preferred providers for VPN service.\nGolem\nWebsite\nBased on Polygon L2 using a native ERC-20 GLM\nTLDR: Golem democratizes society’s access to computing power by creating a decentralized platform where anyone can build a variety of applications, request computational resources and/or offer their idle systems in exchange for cryptocurrency tokens\n\nFrom large universities to scientists or artists, anyone can leverage the world’s unused computing power to conduct data intensive research and complex computations through the Golem Network\n\nThe basic premise of the Golem Network is as follows:\n\nproviders make some resources available to potential requestors for a price,\nrequestors rent those resources and pay the providers in exchange.\n\nThe costs of an activity are based on a pre-agreed set of coefficients that specify the price the requestor is required to pay for the time the activity is running, the processor time used and for starting any activity in the first place.\nPayments and transactions happen through Ethereum.\nRegistry\nUses an application registry so anyone can publish their own applications. Goals:\n\nGive developers a way to publish their integrations and reach out to users in a decentralized manner\nGive requestors a place to look for specific tools fitting their needs\n\n3 categories of users:\n\nAuthors: publish applications\nValidators: review and certify applications as safe by adding them to a whitelist\nProviders: can choose whom to trust by selecting validators’ whitelist\n"},"thoughts/decision-tree":{"title":"Decision Tree","links":[],"tags":["seed","CPSC340"],"content":"A simple program consisting of if-else decisions (decision stumps) based on the features.\n\nWe can create a bunch of decision stumps and define a “score” for each possible rule.\n\nAn intuitive way of thinking about this is as classification accuracy: “if we use this rule, how many examples do we label correctly?”\nWe can create a tree using greedy recursive splitting: using a sequence of stumps to fit a tree\n\n\nHowever, accuracy isn’t perfect. Sometimes splitting doesn’t immediately improve accuracy. We can get around this by generally selecting feature test that maximizes information gain\n\nEntropy of set S of data samples is defined as H(s)=−∑c∈C​p(c)log(p(c)), where C is the set of classes represented in S and p(c) is the empirical distribution of class c in S.\nGenerally, select feature test that maximizes information gain: I=H(S)−∑i∈children​∣S∣∣Si∣​H(Si)\n\n\n\nInput: Vector y of length n with numbers {1,2,..k}\ncounts = zeros(k)\nfor i in 1:n\n  counts[y[i]] += 1\nentropy = 0\nfor c in 1:k\n  prob = counts[c] / n\n  entropy -= prob * log(prob)\nreturn entropy\nTradeoffs\nAdvantages\n\nEasy to implement\nInterpretable\nLearning is fast, prediction is very fast\nCan elegantly handle a small number of missing values during training\nDisadvantages\nHad to find optimal set of rules\nGreedy splitting often not accurate, can require very deep trees\nCan only express ‘and’ relations, not OR\n\nPseudocode for choosing decision stumps\n// time complexity: O(ndk), O(nd) if k=1 (binary classifier)\ndecision_stump(feature matrix X, and label vector Y):\n  compute error: number of times y_i does not equal most common value for feature j\n  for each threshold t:\n\t  set y_yes to most common label of objects i satisfying rule (x_ij &gt; t)\n\t  set y_no to most common label of objects i satisfying rule (x_ij &lt;= t)\n\t  set y_pred[i] to be our preditions for each object i based on the rule\n\t\t  y_pred[i] = y_yes if satisfied, y_no otherwise\n\t  compute error e which is the number of objects where y_pred_i != y_i\n\t  store the rule (j, t, y_yes, y_no) if it has the lowest error so far\n  return the best decision stump based on score\n"},"thoughts/declarative-programming":{"title":"Declarative programming","links":["thoughts/Datalog","thoughts/react"],"tags":["seed"],"content":"\nTo declare what is, not instruct what to do\n\nSee: Datalog, react"},"thoughts/degrowth":{"title":"Degrowth","links":["thoughts/funding","thoughts/climate-tech"],"tags":["sapling"],"content":"Source: People are realizing that degrowth is bad by Noah Smith\nIf we interpret “degrowth” as the decision to fix global GDP at its current level, this completely screws over an the existing 40% of the global population that earns less than $2.50 a day while the already rich continue to enjoy their cushy life styles.\nIf we wanted to avoid keeping a good chunk of the globe in poverty, we then need to define a new income distribution where everyone who is above the mean is driven down to the mean and everybody above can comfortably grow to the mean. Yet in this case, convincing the vast majority of people living in developed countries to freeze world income at about $17,000/year would be near impossible — pretty much political suicide for any platform.\nIn fact, this would also just not work economically because of how interconnected the global economy is now.\n“When COVID hit, poor countries were devastated not just by the virus but by the aftershocks of virus-induced slowdowns in consumption in rich countries.”\nEK on degrowth:\n\nI think that if the political demand of the [degrowth] movement becomes you don’t get to eat beef, you will set climate politics back so far, so fast, it would be disastrous. Same thing with S.U.V.s. I don’t like S.U.V.s. I don’t drive one. But if you are telling people in rich countries that the climate movement is for them not having the cars they want to have, you are just going to lose. You are going to lose fast…This is where the politics of [degrowth] for me fall apart…\n\nThe ideal would not be to reduce economic growth, but to reduce consumption as a whole in order to increase investment and funding for new green technology.\n\nInstead of “consume less”, their message should be “consume less today, so you can consume more tomorrow”\n"},"thoughts/deletion":{"title":"Deletion","links":["thoughts/Byzantine-Faults","thoughts/right-to-be-forgotten"],"tags":["seed"],"content":"What does deletion mean when copying bytes is cheap and can happen without consent anywhere along the line. A conjecture is that deletion can only exist in non-byzantine environments. The more observers there are the harder it is to convince everyone to delete/forget it.\nDoes deletion ever actually occur? How does this reconcile with theories like conservation of information? (erase vs whiteout)\nSee also: right to be forgotten\nTurns of Phrase\nDamnatio memoriae is a modern Latin phrase meaning “condemnation of memory”, indicating that a person is to be excluded from official accounts\nDeletion trilemma\nAny implementation of deletion in a p2p context cannot have all of:\n\nByzantine fault-tolerant\nActual deletion\nFast/efficient\n"},"thoughts/democracy":{"title":"Democracy","links":["thoughts/epistemology","thoughts/attention","thoughts/plurality"],"tags":["seed"],"content":"For representational democracy to work, we need to trust people to make informed decisions at the polls, so that we can hold politicians accountable to the real interests of their constituents\nSalience principles for democracy\nBy Susanna Siegel\n\n“Salience principle of importance”: idea that the press should make clearly available info that the public needs to know for democracy to work\n\nBy default, political consciousness stays in the background of consciousness (Lippmann revers to these people as ‘deaf spectators’). Politics can become salient through elections, contact with the government (e.g. jury duty, drivers license, or a ‘brush’ with the law)\nJohn Rawls: “In a well-governed state only a small fraction of persons may devote much of their time to politics”\nAs such, Lipmann suggests we need to bridge by means of mass communication. Journalists then, are the epistemic bridges between government and the public. The Importance principle is that we should “make salient information that is important for the public to know about.” Define newsworthiness as whatever is actually important for the public to know about\nTo make something salient is to put it forward as both demanding attention and deserving it. In can be distinct from actual uptake (as the receiving end can experience it as demanding of attention but not classify it as deserving it)\nHowever, this just demonstrates the problem of democratic attention. The library is full of good books, and the sad fact is that none of us will ever read them all.\nThis arises from a combination of three points\n\nProfessional journalism is governed by an importance principle of salience\nIf journalism fulfilled the importance principle, some roles would depend on readers taking in information made salient by journalism\nFor much important information, many readers have no interest in it, feel no prior motivation to learn it, and face substantial obstacles to paying attention to information even when it is widely available, and even when it would yield knowledge that would be useful to have\n\nThe problematic upshot is that democracy imposes an attentional demand that can’t easily be met\nRepresentative democracy relies on a population with stable, well-formed opinions about public policy who are disposed to select representatives ready to respect their preferences. However, important news is often not sensational.\nSocial media platforms on the other hand highlight content that captures user attention (maximizes occurrent engagement). Markers of virality are not co-extensive with information that is important for the public to know about\nGood reporting then follows a public-as-protagonist principle\n\n“recommends framing and selecting information to invite readers to view themselves and one another as potential political participants”\nStories should make explicit when it can the ways in which the reading public has a stake in how the story unfolds and how the reader can affect its outcome\n\nNot every reader will be a potential protagonist in every story. However, every regular reader is likely to encounter a story in which they feel addressed as a potential protagonist eventually\n\n\nUnderlying belief that there are many publics (see: plurality)\n"},"thoughts/density-based-clustering":{"title":"Density-based clustering","links":[],"tags":["seed","CPSC340"],"content":"\nClusters are defined by “dense” regions.\nExamples in non-dense regions don’t get clustered\nClusters can be non-convex\n\nIt is non-parametric (there is no fixed number of clusters k)\nDBSCAN\nMain idea: merge all neighbouring core points to form clusters\nDefines\n\nEpsilon (ϵ): distance we use to decide if another point is a “neighbour”\nMinNeighbours: number of neighbours needed to say a region is “dense”\n\nIf you have at least minNeighbours “neighbours”, you are called a “core” point\n\n\n\nPseudocode;\n\nFor each example xi​ :\n\nIf xi​ is already assigned to a cluster, do nothing.\nTest whether xi​ is a ‘core’ point (≥ minNeighbours examples within ϵ).\n\nIf xi​ is not core point, do nothing (this could be an outlier).\nIf xi​ is a core point, make a new cluster and call the “expand cluster” function.\n\nAssign to this cluster all xj​ within distance ϵ of core point xi​ to this cluster.\nFor each new “core” point found, call “expand cluster” (recursively).\n\n\n\n\n\n\n\nIssues\n\nAmbiguity of “non-core” boundary points\nSensitive to the choice of ϵ and minNeighbours\n"},"thoughts/design-goals":{"title":"Design Goals","links":["thoughts/context","thoughts/affordance"],"tags":["seed"],"content":"Goals in the context of HCI are things that can be defined in a given context, then evaluated.\nHuman task involves the steps to do something, whereas human needs are general wants of the user, what they want to get accomplished. Usability goals are primarily objective, user experience goals are more subjective.\nUsability Goals\nUsability refers to ensuring that interactive products are easy to learn, effective to use, and enjoyable from the user’s perspective. Following 6 goals:\n\nEffective to use (effectiveness). Is the product capable of allowing people to learn, carry out their work efficiently, access the information that they need, or buy the goods that they want?\nEfficient to use (efficiency). Once users have learned how to use a product to carry out their tasks, can they sustain a high level of productivity?\nSafe to use (safety). What is the range of errors that are possible using the product, and what measures are there to permit users to recover easily from them?\nHaving good utility (utility). Does the product provide an appropriate set of functions that will enable users to carry out all of their tasks in the way they want to do them?\nEasy to learn (learnability). Is it possible for the user to work out how to use the product by exploring the interface and trying certain actions? How hard will it be to learn the whole set of functions in this way? Should have good transfer effects (where knowledge acquired earlier improves one’s ability to learn/perform in another context)\nEasy to remember how to use (memorability). What types of interface support have been provided to help users remember how to carry out tasks, especially for products and operations they use infrequently?\n\nExamples of commonly used usability criteria\n\ntime to complete a task (efficiency)\ntime to learn a task (learnability)\nnumber of errors made when carrying out a task over time (memorability)\n\nUser Experience Goals\nThese are concerned with how users experience an interactive product from their perspective, rather than assessing how useful or productive a system is from its own perspective\nDesirable aspects of UX\n\nSatisfying\nEnjoyable\nHelpful\nFun\nProvocative\n\nUndesirable aspects of UX\n\nMaking one feel stupid or guilty\nCutesy\nGimmicky\nFrustrating\n\nThese are combined in a user’s multi-faceted experience of a product\nDesign Principles\n\nGeneralizable abstractions intended to orient designers toward thinking about different aspects of their designs\n\nConcerned with what users should see and do when carrying out their tasks using an interactive product: the dos and don’ts of interaction design.\n\nHigh visibility: ensure functions are easy to find and intuitive.\nFeedback: time sensitive information about what actions has been done and what has been accomplished.\nConstraints: ways of restricting the user interaction to those which are valid in any given moment.\nConsistency: an interface which follows rules, such as using the same operation to select all objects.\naffordance: attributes should obviously signal what they can use it for.\n\nThe problem when applying these to real world is that trade-offs can arise between principles (e.g. increased constraint might mean less visibility)."},"thoughts/design-requirements":{"title":"Double Diamond Model of Design","links":[],"tags":["seed"],"content":"Two main ‘diamonds’ of diverge-converge, one to find the right problem and the other to find the right solution (fulfilling human needs).\nDiscover, define, develop, deliver\n\nThe 4 basic activities for interaction design are as follows:\n\nDiscovering requirements (discover/define phase). Deciding what to design is key, and exploring the problem space is one way to decide. The goal is understanding the target users and the support an interactive product could provide.\nDesigning alternatives (develop phase). This can be 2 parts, conceptual design (producing the conceptual model of the product) or concrete design (detail of the product like colours, sounds, images, etc.)\nPrototyping alternative designs to be communicated and assessed (develop phase). The process of designing behaviour of interactive products as well as look and feel. Doesn’t necessarily need working software!\nEvaluating the product and the UX it offers (develop phase). Determine the usability and acceptability of the product or design measured in terms of a variety of usability and user-experience criteria.\n\nA few good questions to ask to better understand the problem:\n\nwhat are the human needs?\nwho are the stakeholders?\nwhat are the central tasks?\nwhat might we want to learn? (evaluation goals)\nwho should our participants be?\n\nRequirements\nRequirements are stable descriptions of users’ aspirations, goals, constraints, expectations etc that form a sound basis from which to start design activities (not rigid, might shift over long periods of time). They are also statements about an intended product that specifies what it is expected to do or how it will perform.\nDiscovering and communicating reqs is important bc defining what needs to be built supports technical developers and allows users to contribute more effectively\nSteps\n\nidentify human needs\n\n\nwhich the proposed interactive system will support; task, goals, conditions; current problems and strengths\n\n\nidentify all users and other stakeholders\n\n\nwho do or perform the activity: groups, capabilities, motives, needs\n\n\nset levels of support (metrics)\n\n\nfunctionalities the system will provide; environmental constraints and user/stakeholder characteristics\nmetrics: how we know if we have succeeded → can be quantitative or qualitative\nfit criterion is something that can be used to assess when the solution meets the requirement\n\nKinds\n\nfunctional requirements: describe what the product will do\nnonfunctional requirements: describe the characteristics (sometimes called constraints) of the product\n\ndata requirements: capture type, volatility, size/amount, persistence, accuracy, and value of required data\n\neg. app for buying/selling stocks has to have up-to-date and accurate data which is likely to change many times a day\n\n\nenvironmental requirements: context of use; circumstances in which the interactive product will operate. made up of:\n\nphysical environment: lighting, noise, movement, dust expected in operational environment. Will users need to wear protective clothing, which may affect the choice of interface type?\nsocial environment: will data need to be shared? does sharing have to be synchronous or async?\norganizational environment: how good is user support likely to be, how easily is it obtained, how efficient or stable is the communication infrastructure, etc?\ntechnical environment: what tech will the product run on or need to be compatible with, and what technological limitations might be relevant?\n\n\nuser characteristics: capture key attributes of user group. collection of characteristics for a typical user is a user profile\n\neg. abilities and skills, educational background, preference, personal circumstances, disabilities, novice/expert/casual or frequent user etc\n\n\nusability goals and user experience goals\n\nusability engineering: approach in which specific measures for usability goals are agreed upon early in the dev process and used to track progress\n\n\n\n\n"},"thoughts/desire-paths":{"title":"Desire path","links":[],"tags":["seed","pattern"],"content":"\nThe path usually represents the shortest or most easily navigated route between an origin and destination. The width and severity of erosion are often indicators of the traffic level that a path receives. Desire paths emerge as shortcuts where constructed paths take a circuitous route, have gaps, or are non-existent.\n\nSource: Fools and their time metaphors by Aaron Z. Lewis\nDesire paths or free-will ways: “paths and tracks made over time by the wishes and feet of walkers, especially those paths that run contrary to design or planning.” In other words, an unexpected behaviour that emerges as the result of poor design."},"thoughts/desktop-metaphor":{"title":"Desktop Metaphor","links":["thoughts/interaction-design","thoughts/workflows"],"tags":["sapling"],"content":"The representation of familiar desktop items in a user interface to help make the capabilities of a computer clear\nBeyond the Desktop Metaphor\nAn interface that intentionally exploits the vulnerability of human beings for financial or otherwise selfish gain is inhumane and also despicable.\nHow do we move beyond sliding and tapping and explore other ways of interaction design? Reimagining applications and instead using modules and workflows\nBrowser Metaphors\nLinus Lee on Materials\nWhat is the right software metaphor for “a point in my browsing history”?\n\nSomething discrete like files? Or something continuous like a video recording, from which you can “clip out” a section?\nDo we want hierarchy, so we can organize sessions into sub-sessions? Is browsing history a flat sequence of events, or sections with sub-parts?\nWhat should each “session” remember about its contents? Just the URL? Maybe occasional screenshots? Scroll history?\nDo browsing sessions have weight? A session with two windows and 30 tabs each certainly feels heavier than one where I just Googled a question and found an answer in the first tab I opened. How does this manifest in the metaphor?\nSome browsing sessions are definitely more salient and important to remember than others. How should we express this property?\n\nA lot of this feels familiar to Atlas Recall"},"thoughts/digital-commons":{"title":"Digital Commons","links":["thoughts/urban-planning","thoughts/friction","thoughts/software-principles","thoughts/pace-layers","thoughts/play","thoughts/exploit-explore","thoughts/social-graphs","thoughts/games","thoughts/From-Counterculture-to-Cyberculture","thoughts/ephemereal-content","thoughts/virtual-worlds","thoughts/Internet","thoughts/Overlay-Network"],"tags":["sapling"],"content":"How might we analogize urban planning to social media and digital spaces?\nOnline Parks\nEli Pariser in Wired\n\nWhen technologists refer to platforms like Facebook and Twitter as “walled gardens”—environments where the corporate owner has total control—they’re literally referring to those same private pleasure gardens that Whitman was reacting to. And while Facebook and Twitter may be open to all, as in those gardens, their owners determine the rules.\n\n“Public spaces are so generative precisely because we run into people we’d normally avoid, encounter events we’d never expect, and have to negotiate with other groups that have their own needs.”\nFriction is essential to public space. Rapid growth can quickly overwhelm and destroy it—as anyone who has lived in a gentrifying neighborhood knows.\nThere is also a nuanced labor of governance and maintenance—finding the balance between welcoming everyone and providing safety and comfort for everyone—is critical to the health of online communities\nInternet Studio Gardens\nBy Jon Borichevskiy\nPrinciples for Internet Spaces\n\nPlaces where they might drift over to peers in adjacent spaces for the chance – but not the obligation – to respond or otherwise reflect upon, completing the loop at the speed of a lazy river instead of a light circuit (see: friction, pace layers)\nThere might even be different seasons of play: periods of divergent planting and nurturing followed by collective harvesting and pruning (see: exploit explore)\nJust permeable enough to be discovered by those curious enough to add their own drawings and words. Details just hidden enough to be carefully unearthed by the intentional visitor\n\nCreating Digital Spaces\nCan we create digital common spaces like parks and things without everyone online needing to be exceedingly intentioned?\nCommons should be safe, low pressure contexts for random interaction. They are public spaces where a lot of people coincidentally share the same space for a short period in time. It has the same energy as commuting — a familiar yet ever-changing context.\nThe park and the trees may stay the same most times you visit yet the people on the benches and walking on the paved paths are always different. One can sit and observe all the people moving by, wondering what their life is like..\n\n“What would it look like to do that with your favourite internet neighbourhoods?”\n\nThis is a small reflection on potential new avenues to explore for digital spaces. Not to say that any existing ones are bad but I am interested to see what new direction we can take to explore how we use technology to further human connection.\nSerendipity in Public Spaces\nSerendipity is bumping into new people you otherwise wouldn’t have talked to or sought out. It’s the casual bus or subway chatter, the queue neighbour, or stranger reading on a park bench. It’s the opposite of intentionality.\nAre there ways to be less intentional with digital interactions?\nIf you want to meet with someone you need to schedule it or visit a link, etc. There is no ‘random’ interaction. Even algorithmic experiences are like being carried away by the TikTok or Facebook algorithm rather than something out of the blue. Though these experiences may seem random at times, they are explicitly curated with an end goal in mind.\nAre there any spaces that don’t have ulterior motives and are just places of gathering? The only example that comes to mind where ‘coincidental’ interaction happen is within currated interest groups like online network forums around games or technologies or early communities lead by superconnectors like the Whole Earth ‘Lectronic Link (WELL). How can we recreate these public ‘watering holes’ for people to gather around?\nPermanence in Private Spaces\nPermanence means that the environment reflects that people have been there. A shared garden to tend to, a bookshelf to work through, a guestbook of all the people who have dropped in and out.\nSo many of our mediums have now tended towards real time in an effort to replicate the fleeting nature of face-to-face conversation; vanishing messages, video chats, and audio rooms. Yet, rarely do any of these platforms leave any indication whether a conversation ever happened between two people.\nIs there any way we can create shared artifacts that are permanent and can be grown over time? A digital indication that a space is lived in and occupied?\nAs of now, most platforms keep a primative chat log or history but thats it. What if there was a way to create digital gardens to foster and maintain existing relationships? A commonspace you could both take care of, share, and contribute to. Completely private common spaces often allow users to put whatever and allow people can construct their own digital nooks and cozy spaces.\nMaybe this involves having a shared calendar, todo list, books. Or even just a space to co-live and co-exist in virtual worlds. Most online RPG games (think Animal Crossing, Minecraft, Stardew valley) give the option for users to have a shared space to exist and build together (and where both people don’t necessarily both need to be present for the space to function). Why doesn’t this exist outside of the gaming sphere?\nCan we create permanence of artifacts without sacrificing ephemerality in medium?\nTools for Digital Spaces\nGiven that there are so many different types of digital spaces, I wanted to explore how different tools are supporting and facilitating different sorts of digital human interaction. Can we use urban planning to help us plan digital spaces?\nThe hope is to be able to move away from the ‘feed’-based model of browsing the social internet and to create safe spaces to interact at different scales.\n\nThe “feed”–an archaic form of content consumption that is effectively just a direct visual manifestation of the data structure that powers it – is a medium that is effectively designed to be consumed alone. —Humphrey Obuobi\n\nTown Square\nMany-to-many relationships like clubs, families, larger interest groups.\nQuestions\n\nHow do we ensure that people feel connected in large groups and find\nwhat they are looking for?\nShould communities be gated or public? Does this matter?\nHow do we moderate content while ensuring individuals feel safe?\n\nTools\n\nForums → Gathering based on interest\nGame lobbies and public squares → A temporary bringing-together of otherwise completely unrelated individuals\nMeeting rooms and Gather.town → Intentional spaces to meet and mingle with coworkers and friends\n\nParasocial Relationships\nOne-to-many ‘broadcast’ relationships.\nQuestions\n\nHow do we ensure these relationships are healthy for all parties involved?\nShould content be moderated in this relationships? Is this the responsibility of the individual, the platform, or the viewer?\n\nTools\n\nTwitch, Celebrities, and Internet Figures → Gathering around a single person because of personality/content\nNewsletters and personal sites → Public places of exploration and self-identity, self-owned corners of the internet\n\nPrivate Channels\nSpaces for one-to-one interaction.\nQuestions\n\nAlmost all of these mediums are on a sliding scale of how ’real-time’ the medium is, yet none of them are obsolete. Why is that?\nWho has power in the channels? What tools are there for safety?\n\nTools\n\nLetters and Mail → Completely asyncronous text based communication\nMessages → Instantaneous text or audio based communication with a history log. No immediate urgency to reply\nCalling → Audio/video based live communication\n\nField Guide to Digital and/as Public Space\nSource\nPublic spaces are virtual and augmented realities. They are filled with marks of inhabitation and habit, desire and unfinished stories. The challenge in digital public spaces is that such traces are not always easy to leave or feel. When, for instance, you visit a website, rarely does anything about your visit change the site. Your presence doesn’t leave a mark, the way walking on grass does. A website registers an impression of your visit, but there’s nothing inherently virtual about that. How do we make our desires felt in digital spaces?\nThe publicness that is created in online spaces is still a relation between actual offline bodies. What’s actually happening in an encounter in a virtual world? Two offline bodies are using flickers of light travelling across wires of sand and metal to make each other’s bodies feel and perceive things. One thing that digital networks have done to public spaces is to intensify the capacity of distant relations to affect local spaces.\n\nThere is always only IRL - transformed, layered, and intensified by digital techniques\n\n(All digital publics as overlay networks onto IRL networks?)\nIn architecture, adaptive reuse is about repurposing old sites to new functions. Factories get converted to lofts, industrial districts to cultural zones, shipping containers to ghost kitchens, or the underneath of expressways to public spaces. What does adaptive reuse look like for digital spaces? How can we make the aesthetic of repurposed but worn spaces appealing for the digital? What is the brownstone of software?"},"thoughts/digital-mindfulness":{"title":"Digital Mindfulness","links":["thoughts/attention-economy","thoughts/friction","thoughts/ephemereal-content","thoughts/A-Tale-for-the-Time-Being","thoughts/human-computer-interaction","thoughts/infrastructure"],"tags":["sapling"],"content":"On methods to reclaim ourselves from the attention economy. How do we enable friction to give people the space to think and reflect?\nTime\nSource: The present time by Kernel\n\nInstead of having sequential conferences on bulletin boards like The Well where you would take hours to craft a response to something, we ended up with a digital space where we were constantly being interrupted […] We end up in this state of constant emergency interruption which I don’t think is healthy neurologically or culturally.\n\nThis is an opportunity for us to reclaim our attention, to properly build out asynchronous experiences so that people can work in their own time rather than the rushed, ephemereal content we’re so used to now.\n“I don’t want a job - I want my basic needs fulfilled and a degree of comfort and security which will enable me to make a meaningful contribution. Jobs are an artifact of the industrial age and a certain way of existing in time.” Jobs are just the societally accepted norm for fulfilling basic needs. (Bullshit Jobs)\nDesigning for slowness\n\n“what if you had to commute to websites”\nhttps://twitter.com/spencerc99/status/1401320086183366657\n\nExploring the Reflective Potentialities of Personal Data with Different Temporal Modalities: A Field Study of Olo Radio\nRather than organizing music through name or playlist (inherent or user groupings), Olo Radio chooses to organize information through data residue (unintentional data generated through regular usage). Allows the user to browse music by time of day listened to, how long ago you listened to it, or when in the year you listened to it.\nIncentivizes revisiting of the past rather than the constant discovery of the new. (w the exception of once-a-year recaps like Spotify Wrapped)\n\nHow do we introduce temporality into objects to appreciate their inner processes?\n\nWater fountain display of number of water bottles saved in lifetime\nTransit time between websites? (bring back friction)\n“Natural decay” of physical objects — how do we transfer this into the digital realm?\nDispo as a way to fake ‘scarcity’ in digital photos\n\n\n\nSlowness as a form of reflection. Key features of the Olo Radio:\n\ntakes time to understand\nmanifests change through time (big A Tale for the Time Being energy)\nleverages different forms of time to prompt reflection by manifesting their presence in everyday life\n\nThis feels so magical to me; being able to create technology that evokes and recalls, creating spaces for reflection and a chance to reincorporate rather than gogogogopleasefocusonme all the time.\n\n“There are also growing calls in the HCI community to develop alternative approaches to designing interactions with personal data that better support experiences of contemplation, interpretation, and slowness”\n\n\nTemporal seeking of memory and the element of surprise/anticipation\n\n“I took to blocking off time to use it. It was less of a casual thing for me. …I’d start with Life mode. I’d pick a position, turn [the volume] up, lie on the couch, and close my eyes. …I’d get a vague idea of where I was [in time]. A pattern would eventually emerge. [Songs would] sometimes spark a flash of a specific memory, like a date I was on years ago or visions of my parents’ home. …I’d start to get these tense feelings about what the next song would be. Where would it bring me back to? …When I was in the zone, time flew by. I could listen for an hour without really noticing.”\n“It was playing on Day, just in the background. …This Daft Punk song came on. That caught my ear. I sensed it could’ve been on a playlist I’d listen to before my [ski] competitions when I was younger. …I switched to Life and the slider shot back to the start [of my Last.FM]. This was years ago, so it would’ve been around the right time. …I anticipated Year would put [the slider] somewhere in the winter months. And, it did! I put it back to Life. …The next one in the queue was a Chemical Brothers song. And that’s when I knew I was in my old playlist.”\nIncreasing focus on active and intentional listening (I’m going to block off time to enjoy this) rather than just passive listening (put it on in the background while I work)\n\n\nBlackspots emerging in one’s digital history when devices and services breakdown\n\nThe result of infrastructural inversion\nWhat happens when data is lost in a ‘complete’ record? How does that affect the experience? The recall?\n\n\n"},"thoughts/digital-permanence":{"title":"Digital Permanence","links":["thoughts/Internet","thoughts/deletion","thoughts/right-to-be-forgotten","thoughts/ephemereal-content","thoughts/peer-to-peer","thoughts/blockchain","GDPR","thoughts/GDPR"],"tags":["sapling"],"content":"Once you share something, you can’t unshare it, the internet feels intractable.\nWhat do we lose when we lose deletion?\nRelated: right to be forgotten, ephemereal content\nThe Internet is a collective hallucination\nSource: The Internet Is Rotting in The Atlantic\n“Of course, there’s a keenly related problem of permanency for much of what’s online. People communicate in ways that feel ephemeral and let their guard down commensurately, only to find that a Facebook comment can stick around forever. The upshot is the worst of both worlds: Some information sticks around when it shouldn’t, while other information vanishes when it should remain.”\nSocial media\nSource: Lifting the mask by Edward Snowden\n“One history of the Internet — and I’d argue a rather significant one — is the history of the individual’s disempowerment, as governments and businesses both sought to monitor and profit from what had fundamentally been a user-to-user or peer-to-peer relationship. The result was the centralization and consolidation of the Internet — the true y2k tragedy. This tragedy unfolded in stages, a gradual infringement of rights: users had to first be made transparent to their internet service providers, and then they were made transparent to the internet services they used, and finally they were made transparent to one another. The intimate linking of users’ online personas with their offline legal identity was an iniquitous squandering of liberty and technology that has resulted in today’s atmosphere of accountability for the citizen and impunity for the state. Gone were the days of self-reinvention, imagination, and flexibility, and a new era emerged — a new eternal era — where our pasts were held against us. Forever.”\n“Everything we do now lasts forever… The Internet’s synonymizing of digital presence and physical existence ensures fidelity to memory, identitarian consistency, and ideological conformity. Be honest: if one of your opinions provokes the hordes on social media, you’re less likely to ditch your account and start a new one than you are to apologize and grovel, or dig in and harden yourself ideologically. Neither of those “solutions” is one that fosters change, or intellectual and emotional growth”\n“The forced identicality of online and offline lives, and the permanency of the Internet’s record, augur against forgiveness, and advise against all mercy. Technological omniscence, and the ease of accessibility, promulgate a climate of censorship that in the so-called free world instantiates as self-censorship: people are afraid to speak and so they speak the party’s words… or people are afraid to speak and so they speak no words at all…”\n“Even the most ardent practitioners of cancel culture — which I’ve always read as an imperative: Cancel culture! — must admit that cancellation is a form of surveillance borne of the same technological capacities used to oppress the vulnerable by patriachal, racist, and downright unkind governments the world over. The intents and outcomes might be different — cancelled people are not sent to camps — but the modus is the same: a constant monitoring, and a rush to judgment.”\nBlockchain\nThe sheer immutability of blockchain data—you can’t delete a block without redoing the chain, something semi-impossible in practice—puts it in obvious violation of GDPR.\nSo long as the wallet is unassociated with real personal data, either on chain or off, it shouldn’t be subject to the many data restrictions of GDPR by a strict reading of it. But as soon as a company like Coinbase does a KYC to verify your identity, this data gets linked at GDPR violation happens once again."},"thoughts/digital-signatures":{"title":"Digital signatures","links":["thoughts/security","thoughts/hash-function","thoughts/MAC"],"tags":["seed"],"content":"\nSignatures are cryptographic functions that attest to the origin of a particular message.\n\nIt is infeasible for Alice to generate a signed message that appears to have been generated by Bob.\n\nAggregating signatures: have multiple signatures signed by various people and then you can aggregate it into a single signature, which makes it more efficient in terms of size\nThresholding signatures: multiple people split a key into multiple parts, and you require some fixed number of people to agree to sign a message to be able to actually sign it with the full key\n\nIn a (k,n)-threshold signature scheme, there is a single public key held by all replicas, and each of the n replicas holds a distinct private key.\nJaclyn implemented Proactive Refresh for BLS Threshold Signatures during TreeHacks which was super cool. “It’s a way to renew signature shares every 30 seconds. Think of it as Google Authenticator for threshold signatures.”\n\n\n\nSignatures Schemes\nRequire 3 algorithms\n\nKey generation algorithm: seed -&gt; public_key, private_key\nSigning algorithm: msg, private_key -&gt; msg, signature\nVerification algorithm: msg, signature, public_key -&gt; boolean\n\nSigned Blobs\nFrom Farcaster Docs\nBlobs are cryptographically signed so that it cannot be tampered with\nThe structure that holds this data is called a Signed Blob, and it contains three properties:\n\nbody - the JSON object that the user wants to store\nmerkleRoot - the hashed body (should be renamed to hash)\nsignature - the signed hash\n\nSigning\n\nConstruct the JSON object with the properties in the exact order as specified.\nConvert the object to a string to make it hashable.\nHash the string using keccak256 and store this value as the merkleRoot\nSign the merkleRoot with the user’s Ethereum wallet, creating a recoverable ECDSA signature and store this in the signature property.\n\nVerifying\n\nConvert the body to a string to make it hashable.\nHash the string using keccak256 and check that it matches the merkle root\nPerform an ecRecover on the signature with the merkle root to retrieve the address.\nCheck that the recovered address matches the expected address.\n\nSigned Message Digests\n\nSignature of long messages is computationally expensive\nWe can compute a fixed-length “fingerprint”\n\nApply hash function H to message m, giving a fixed size message digest, H(m)\n\n\nSigned message digest\n\nBob sends message m and signed digest KB−​(H(m))\nAlice receives m and computes Hnew​(m)\nAlice receives signed digest KB−​(H(m)) and computes KB+​(KB−​(H(m)))\nIf KB+​(KB−​(H(m)))=Hnew​(m), the message is considered signed (and untampered)\n\n\nAlternatively, MACs\n"},"thoughts/digital-urban-design":{"title":"Digital urban design","links":["thoughts/urban-planning","thoughts/Dark-Forest-Theory-of-the-Internet","thoughts/cozy-software"],"tags":["seed"],"content":"We spend so much time thinking about how to design cities we want to live in. Why don’t we spend the same level of care thinking about how to design software we’d like to live in too?\nA provocation: what does it feel like to move into software?\nWhat if it was as easy to customize your software as it is to move furniture around your website. We can customize our homes so easily because you don’t need full sets of furniture — you can have an IKEA desk and Noguchi lamp. But with software, there are “ecosystems” that don’t interoperate.\nSee also: urban planning, cozy web, cozy software"},"thoughts/disjoint-set":{"title":"Disjoint-set","links":[],"tags":["seed"],"content":"A data structure that stores a collection of disjoint (non-overlapping) sets.\nIt provides operations for:\n\nadding new sets,\nmerging sets (replacing them by their union), and\nfinding a representative member of a set.\n\nImportantly, Kruskal’s algorithm uses the disjoint-set data structure to efficiently determine whether two vertices are part of the same tree."},"thoughts/disrupting-routine":{"title":"Disrupting routine","links":[],"tags":["seed"],"content":"From The Morning Paper\nThe ‘reward equation’ models how ants communicate using pheromones, and our own brains keep track of rewards using dopamine. The reward equation includes a decay or ‘forgetting’ parameter, so what happens if you disrupt established solutions for long enough that their hold is broken?\nIf you disrupt all of the pheromone trails around their nest, is that they converge on a new solution in the environment, but it won’t necessarily look the same as the one they had before the disruption"},"thoughts/distributed-systems":{"title":"Distributed Systems","links":["thoughts/fault-tolerance","thoughts/RPC","thoughts/system-model","thoughts/clocks","thoughts/causality","thoughts/message-broadcast","thoughts/replication","thoughts/quorum","thoughts/consensus","thoughts/consistency"],"tags":["seed"],"content":"A distributed system can be defined as multiple computers (nodes) communicating via a network trying to achieve some task together.\nMartin Kleppmann’s Course\nNotes from Martin Kleppmann’s Distributed Systems Course. He has a set of course notes on his teaching site as well.\n\nHow do we share data amongst different concurrent entities?\n\n\nRecommended Reading\n\n“Distributed Systems” by van Steen &amp; Tanenbaum: Implementation detail heavy, more practical\n“Introduction to Reliable and Secure Distributed Programs” (2nd ed) by Cachin, Guerraoui &amp; Rodrigues: Theory heavy\n“Designing Data-Intensive Applications” by Kleppmann: More oriented toward distributed databases\n“Operating Systems: Concurrent and Distributed Software Design” by Addison-Wesley: links to Operating Systems\n\n\n\nWhy distributed?\n\nThings are inherently distributed: sending a message from your phone to your friend’s phone\nReliability: even if one node fails, the system as a whole keeps functioning\nPerformance: get data from a nearby node rather than one centralized server halfway around the world\nSolve bigger problems: some amounts of data can’t fit on just one machine\n\nWhy not distributed?\n\nCommunication may fail (and we might not even know it has failed)\nProcesses may crash (and we might not know)\nAll of this can happen nondeterministically\nThus we need to think about fault tolerance\n\nNotes\n\nRPCs\nFault Tolerance\n\nSee Two Generals Problem and Byzantine Generals Problem\n\n\nSystem models\nPhysical and Logical Time\nMessage ordering and Causality\nMessage broadcast\nReplication\nQuorum\nConsensus\nConsistency\n"},"thoughts/distributed-web":{"title":"Distributed Web","links":["thoughts/Rhizome-Proposal","thoughts/distributed-systems","thoughts/IPFS","thoughts/Hypercore","thoughts/Syndication"],"tags":["seed"],"content":"New web technologies and communities seeking to reduce or eliminate central points of control on the web.\nThe promise of the Distributed Web (DWeb) is that it gives everyone the ability to control their digital networks and platforms. In distributed networks, the underlying code, data, and network infrastructure are managed by many.\nSee also: Rhizome Proposal, distributed systems\nDistributed Press\nSource\nThe Distributed Web enables us to share content that resist centralized forms of censorship\nDistributed Press uses IPFS and Hypercore as our initial content sharing protocols. In addition to the familiar https, you can view our published content using the IPFS and Hypercore schemes on compatible browsers.\nSee also: syndication"},"thoughts/docker":{"title":"Docker Explained","links":[],"tags":["fruit","technical"],"content":"Content from my own ‘Docker Explained’ repo. Check out the repository for example code!\nIntroduction\nWhat is Docker?\nDocker is a tool that makes it really easy to package applications into self-sustaining ‘containers’.\nWhat are containers?\nContainers, as their name suggests, contain things. In the case of Docker, these contain all the parts the application needs to run, everything from libraries and dependencies to the actual source code.\nWhy containers?\nContainerization means that everything to do with your application stays inside the container. You shouldn’t need to worry about how stuff on your machine (e.g. which version of Python you have) affects how your program runs. As a side benefit, this means that Docker containers are dependency-free. Never worry about “oh, it works on my machine” ever again! After a Docker image is created, all of its contents are frozen so it should work exactly the same on your computer as it does for someone else (assuming you both have Docker).\nWhy Docker?\nDocker makes it super easy to work with these containers and, by proxy, you can get all the cool benefits of containers easily too! It also allows you to programmatically define a container through code, meaning you can collaborate and work on Docker containers just as you would with a regular piece of code through version control like git.\nInstalling Docker\nMore detailed instructions can be found here.\nParts of Docker\nDocker Containers\nI think the intro covered this pretty well so I’ll repeat it again here.\n\nContainers, as their name suggests, contain things. In the case of Docker, these contain all the parts the application needs to run, everything from libraries and dependencies to the actual source code.\n\nThis means that ‘containerized’ applications don’t need to rely on a system to have certain dependencies (e.g. Node.js) installed on the user’s system to run because the container will have it packaged.\nYou can think of Docker containers like a fully self-contained and running version of your application.\nDocker Image\nYou can think of Docker images like a sort of ‘template’ that describes to Docker how to create a container from scratch. You can build these images by providing instructions on how to build them in the form of layers.\nLayers\nDocker images, like ogres (or cakes if you’re a boring person), have many layers. The base layer often provides some basic functionality like providing git, bash or apt — otherwise your container has nothing to run off of! We can then add our own layers on top of that base layer, like installing dependencies, copying files into the image, and defining the command to run when the container starts up. These instructions are programmatically defined through a Dockerfile.\n\nOne of the coolest parts of Docker is that these layers get cached between builds if nothing has changed. That means that if you rebuild an image and only changed the last layer, it’ll only need to rebuild the last layer rather than rebuilding the whole image, making for some really fast iteration times.\n\nDockerfile\nThe Dockerfile are the actual specific instructions for how to create the actual image or ‘template’. The Dockerfile starts off by defining a ‘base-layer’, which serve as the basis for your actual image. Some common base layers are ubuntu (which contains a minimal install of the actual Ubuntu operating system) and python (which contains everything needed to run a basic Python app).\nI won’t dive into too much details about each command you can use as these will be described more in-depth within the examples. You can find detailed documentation on the commands you can use in a Dockerfile here: https://docs.docker.com/engine/reference/builder/\nDocker CLI\nGreat, so now I know a little bit about how Docker actually works, how do I get started? First, let’s make sure our Docker install works correctly. You can do this by opening your favourite terminal and entering docker run hello-world. You should get something that looks like:\n$ docker run hello-world\n \nHello from Docker.\nThis message shows that your installation appears to be working correctly.\n...\nNow, let’s go over a few basic CLI commands that you’ll probably be using as you work with Docker.\ndocker build\n\nHow do I turn a Dockerfile into an actual image?\n\nTo built an image, you can do docker build . -t &lt;name-of-image&gt; which tells Docker to look in the current directory for a file called Dockerfile and to follow the instructions inside to build an image. After doing so, tag the image so we can easily find it later. Docker image tags let you version your images as well. Say you wanted to build a v1 for your image, you would do docker build . -t &lt;name-of-image&gt;:v1. If you have a different name for your Dockerfile, you can also refer to it using the -f flag like so: docker build . -t &lt;name-of-image&gt; -f &lt;name-of-dockerfile&gt;\nMore info can be found here: https://docs.docker.com/engine/reference/commandline/build/\ndocker image ls\n\nHow do I get a list of all the images I’ve built?\n\nThe command will give you an output that looks something like the following.\nREPOSITORY                 TAG                 IMAGE ID            CREATED             SIZE\nimage1                     0.1.1               9eb95c7f06b0        2 days ago          343MB\nimage2                     &lt;none&gt;              c99ac06cf60a        2 days ago           23MB\n...\nMore info can be found here: https://docs.docker.com/engine/reference/commandline/image_ls/\ndocker run\n\nHow do I create a container from an image?\n\nNow that you’ve built an image, you can just run it by doing docker run &lt;name-of-container&gt;:&lt;version&gt;. More often than not, you can just use the latest version of the image, docker run &lt;name-of-container&gt;:latest. However, certain applications (like servers) need to listen on specific ports. By default, Docker doesn’t allow containers to use ports on your local machine, but you can allow this by specifying ports using the -p flag, docker run -p &lt;host-port&gt;:&lt;container-port&gt; &lt;name-of-container&gt;. If your container listens on port 3000, but you want it to appear as port 5000 on your local machine, it would look like docker run -p 5000:3000 &lt;name-of-container&gt;\nIf you want to run your container in the background in a detached manner, you can just add the -d flag.\nMore info can be found here: https://docs.docker.com/engine/reference/commandline/run/\ndocker ps\n\nHow do I figure out what containers are currently running?\n\nYou can get a list of currently running containers by doing docker ps, which will give you each container running along with details about its Container ID, what image it was created from, when it was created, as well as which ports are open.\ndocker exec\n\nHow do I run a command inside a container?\n\nYou can use the command docker exec -it &lt;container name&gt; /bin/bash to get a bash shell in the container, allowing you to run commands from within the container as if it was a full-fledged machine. If you know specifically what command you want to execute, you can use docker exec -it &lt;container name&gt; &lt;command&gt; to execute whatever command you specify in the container.\nDocker Examples\n\nBasic Python app with dependencies\nNode.js and Express app\nMulti-stage Go app\n\nFurther reading\nThese topics will not be talked about within this repository, but I’ve added a few resources I’ve found helpful in my understanding of each of them.\nDocker Compose\nDocker Compose is a tool that lets you start multiple Docker containers together and configure how they interact.\n\nhttps://docs.docker.com/compose/\nhttps://github.com/docker/compose\n\nKubernetes and Microservices\nmIcRoSerViCeS you may hear them say. What’s all the hype about? Basically, its the single responsibility principle but applied to services. This means that each responsibility should, ideally, be split out into its own service and be completely responsible for that one thing. This lends itself really easily to Docker and containers. Kubernetes is a system that makes it really easy to deploy, scale, and manage a bunch of containers, making it near ideal in creating a microservices architecture using Docker.\n\nhttps://microservices.io/\nhttps://medium.com/hashmapinc/the-what-why-and-how-of-a-microservices-architecture-4179579423a9\nhttps://kubernetes.io/docs/concepts/overview/what-is-kubernetes/\nhttps://kubernetes.io/docs/tutorials/kubernetes-basics/\nhttps://opensource.com/article/17/11/getting-started-kubernetes\n\nDeploying to the Cloud\nNow that you got some cool new containers? How do I run them in the Cloud like all the other cool kids? Thankfully, Docker makes this super easy too.\nGoogle Cloud Run (GCR)\n\nhttps://cloud.google.com/run\n\nGCR completely manages scaling and deploying your containers for you so you don’t need to worry about server management (yay serverless)! Simply upload your images to Google Container Registry, and create a new Cloud Run deployment.\n\nhttps://cloud.google.com/run/docs/quickstarts/prebuilt-deploy\nhttps://cloud.google.com/run/docs/quickstarts/build-and-deploy\n\nGoogle Kubernetes Engine (GKE)\n\nhttps://cloud.google.com/kubernetes-engine\n\nWant to run your own Kubernetes cluster and have a lot of money to burn? GKE may be right for you! Other than the price, GKE is super user friendly and makes it really easy to manage and visualize your deployments.\nAmazon Fargate\n\nhttps://aws.amazon.com/fargate/\n\nThink GCR but Bezos.\n\nhttps://medium.com/@ariklevliber/aws-fargate-from-start-to-finish-for-a-nodejs-app-9a0e5fbf6361\n"},"thoughts/double-consciousness":{"title":"Double-consciousness","links":["thoughts/hermeneutical-injustice","thoughts/epistemic-injustice"],"tags":["seed","PHIL240A"],"content":"Term from DuBois, 1989\nRelated: hermeneutical injustice, epistemic injustice\nIn the context of marginalized knowers needing to model how their actions are perceived by dominantly situated knowers.\nTom Robinson exhibits this in To Kill a Mockingbird, when he recognizes his ‘mistake’ in saying he felt sorry for Ewell.\n\nHe knows what actually took place because he has the epistemic resources (for example, a non-subordinating notion of pity) to do so. At the same time, he also knows what is happening in the courtroom, for he has the epistemic resources to know something else: how his words and actions will be perceived by those without the epistemic resources required to know him and his world of experience.\n"},"thoughts/downward-causation":{"title":"Downward Causation","links":["thoughts/causality","thoughts/telerobotics"],"tags":["seed"],"content":"In hierarchical systems, downward causation implies that higher, more abstract, levels of a system can influence lower level parts of the system.\nOne example of this is telerobotics, where we have ‘higher-level’ mental thoughts that coincide with lower level atomic and physical actions (i.e. muscles moving to get us to the fridge)"},"thoughts/doxastic-partiality":{"title":"Doxastic partiality","links":["thoughts/friendship","thoughts/epistemic-authority","thoughts/trust","thoughts/bias","thoughts/causality"],"tags":["seed"],"content":"From Believing the best: on doxastic partiality in friendship by Lindsay Crawford\nArgues that there is no conflict between friendship and epistimic norms as being a good friend constitutively involves forming attitudes about one’s friends that are appropriately responsive to the features that one’s friends have that appear to warrant those attitudes\nNote: property x being constitutive of p is not a normative reason for p to have x\nIt is not true that friendship can normatively require doxastic (logic/belief) partiality but not for the reasons given against partiality by “evidentialists”\nTypes of deliberation\n\nPractical deliberation:\nTheoretical deliberation:\n\n\nPartiality: You are more likely to do certain things for someone that you wouldn’t do for others.\nDoxastic partiality: You are more likely to believe in it\n\nWe can be partial to your friends but they can be wrong given state-given reasons. We can be partial to our friends but not because it is normative to be always partial to our friends\n\nPartialist: those who argue that friendship can normatively require doxastic partiality\n\nSomeone who failed to be doxastically partial to you would fail to be a good friend to you\nWe ought (normative statement) to do the various things that partly constitutive of what it is to be a good friend to someone (e.g. be doxastically partial)\n\nThis statement stems from the idea that good friendships are invaluable and indispensible component of living a good life. In turn, it makes sense that we ought to cultivate and sustain good friendships\n\n\n\n\nEvidentialists: people who claim that what we ought to believe only subject to epistemic reasons, which are directly, transparently available as evidence during deliberation\n\nThere are no reasons to be doxastically partial, because, more generally, there are no non-evidential reasons for belief\nTransparency: the question of whether to believe p gives way to the factual question of whether p holds true because the answer to the latter question determines the answer to the former\n\nFirst gloss: we inevitably first consult the evidence in deliberation\nSecond gloss: normative point that evidence is the only thing relative in deliberation\nHowever, it is true that p not being true does not stop one from believing that p\n\nBeing a good friend, in my view, need not require being right about what sorts of qualities genuinely deserve one’s love or admiration.\n\n\n\n\n\n\n\nOn deliberation: deliberation concludes in an attitude of some kind, not the action itself\n\nPractical deliberation about whether to do ϕ concludes in an intention to ϕ\n\nHowever, practical deliberation can be about theoretical deliberation\n\n\nTheoretical deliberation about whether to believe that p\n\nState-given vs appropriately responsive attitudes\n\nA reason for an attitude is state-given when its status as a reason is grounded in some relation it bears to a property of having that attitude in one’s circumstances. e.g. an evil demon will torture you unless you believe that 2 + 2 = 5 is a state given reason to believe that 2 + 2 = 5.\n\nThus, the approach to thinking that esteeming a good friend because you expect them to esteem you in return is a state-give reason for esteeming them (e.g. makes esteeming the friend practically advantageous)\n\n\nAs opposed to appropriately responsive attitudes. These reasons for attitudes are appropriately responsive when they are responsive to what one takes to be “object-given reasons” — that is, it is grounded in some relation that bears to a property of the object of the attitude\n\nThe belief that a friend is kind purely because your friend’s kindness is a property of your friend (that is, they actually have the feature)\n\n\n\nPartiality and prejudice in trusting\nBy Katherine Hawley\n\nIs it reasonable to trust your friends?\n\nCommon definition of trust:\n\ninvolves relying on them to do it\nan extra factor which distinguishes genuine trust from the attitude of mere reliance we take to inanimate objects\n\nInterestingly, there is a gap between relying on someone to do something and believing that they will do it\n\nTrust is a choice — so “trusting someone to do something need not involve belief that she is trustworthy, nor belief that she will do what she is trusted to do, nor even belief that it is likely she will do it”\nYou can trust someone to do something without relying on them to do it\n\nI can trust my friends to keep my secrets by not believing that they won’t keep my secrets\n\n\nYou can rely on them without trusting them - I can rely on my neighbour to tidy the garden we share but I don’t trust her to do so (nor distrust) — I would feel disappointed but not resentful if she didn’t do the job\nTrust doesn’t require active knowledge — checking whether x will p is unnecessary when I already know whether x will p\n\nStroud (2006) and Kelly (2004) argue that we should have partiality towards friends not only in actions but beliefs as well, though this isn’t always the right thing to do. As Stroud says, ‘friendship requires epistemic irrationality’\n\nConfirmation bias: tendency to notice evidence which confirms our existing beliefs (that our friends are indeed good people)\nMay resist bad news that undermines a shared history of friendship (sunk cost fallacy)\n\n“should we quietly drop the friend, provoke a confrontation, or accept a continuing friendship contaminated with doubts?”\n\n\n\nThe considerations are all in some sense selfish—they play on our wish to be right, to have been right, to be a good judge of character, and to avoid difficult situations.\nExploring potential conflicts between different types of trust\n\nEpistemic Trust: trust in someone as a speaker or source of knowledge\nPractical Trust: trust in someone as an actor\n\nIn fact, there is often a two-way causal interaction between friendship and trustworthiness. Roughly, people are more likely to behave in a trust-worthy manner towards their friends, and we are more likely to form friendships with people we consider trustworthy. Clearly, there are exceptions\n\nFriend might let you down instead of disappointing someone else as your friend hopes you will understand and forgive them\nMight find it more tempting to lie about some matters because they are concerned about maintaining a good image of themselves in your mind\nBut if she takes these liberties too often, you will feel you have been taken for granted, and come to resent your friend. Friendship requires mutual respect and openness as well as forgiveness.\n\nStroud’s 4 demands of friendship\n\nSerious scrutiny: scrutinize negative claims about our friends\nDifferent conclusions: draw different conclusions and make difference inferences than they otherwise would about non-friends given the same information\nInterpretive Charity: interpret evidence against friends more charitably than with non-friends\nReason: treat the fact someone is a friend as a reason when we believe about them\n"},"thoughts/easy-is-not-always-better":{"title":"Easy is not always better","links":["thoughts/workflows"],"tags":["seed","pattern"],"content":"Are ‘easy-to-use’ products always better?\nInteresting read about the ‘seductive, destructive appeal of ease of use’: the belief that “ease of use” is somehow conflated with better products. I always hear that complex tools and apps being not implemented because user research proved that it was ‘too difficult to use’ but is this costing us in the long run?\nOne great example the article talks about is the tricycle/bicycle analogy. “It is clear that for an unskilled user, the tricycle is much easier to use. But, as we know, the payoff from investing in learning to ride on two wheels is enormous.”\nRelated: building tools around workflows"},"thoughts/economics":{"title":"Economics","links":["thoughts/tribe-flourishing","thoughts/Utilitarianism","thoughts/selfish","thoughts/positive-sum"],"tags":["seed"],"content":"\nOur goal as economic designers is to craft the systems that drive our selected set of values.\n\nSource: Prosocial economics for game design by Daniel Cook\nEconomics is predominantly concerned with the central Economic Problem, namely\n\nLimited resources: There are limited resources in the world;\nUnlimited needs: Greedy humans have essentially unlimited needs for those resources.\n\nHowever, assumptions in the early economic models of human behaviour are problematic:\n\nHomo economicus: The most common behavioral model assumes that humans are atomic individuals who operate rationally and selfishly.  We know now that humans have limited attention, are contextually altruistic, are highly tribalistic, and exhibit a wide range of irrational cognitive biases.\nIndividuals are the best judge of their needs: Our brains are not conscious of the base psychological processes driving some of our most pressing needs and are thus unable to value relationships rationally.\nWeak social modeling: Most economics models ignore basic human behavior such as friendship networks, affiliation networks, limits on cognitive resources (ex: attention) or altruism.\n\nEconomics embraces reductive utilitarianism and posits you can put a price on anything. We tend to be intrinsically motivated to connect to others and invest long term in a relationship where no extrinsic value is ever publicly admitted. When a relationship transitions into being an extrinsically rewarded transactional relationship, the relationship often suffers a catastrophic loss of value.\nSelfishness\nSource: Does Studying Economics Breed Greed? by Adam Grant\nThose that practice economics — and to a degree modern American capitalism — are heavily invested in an implicit system of self-centered moral values.\n“The repetitive doctrine that humans are best modeled as selfish rational optimizers creates a set of selfish social norms that practitioners consciously or subconsciously follow. The act of studying economics makes you a morally worse human-being (by most definitions of morality.)”\nHow do we move beyond the study of scarcity and towards a study of abundance and positive sum goods?"},"thoughts/effective-altruism":{"title":"Effective Altruism","links":["thoughts/Utilitarianism","thoughts/The-ones-who-walk-away-from-Omelas","thoughts/quantization","thoughts/Mutual-Aid","thoughts/probability","thoughts/traditional-knowledge"],"tags":["sapling"],"content":"The suffering of a few is okay as long as the greater good benefits? A utilitarian way of doing good.\nRelated: The ones who walk away from Omelas, quantization\nWhat is EA?\nEarning to give. Work can have huge impacts on the type of change we can make in the world. We spend ~80k hours, it makes sense to spend atleast 1% of it thinking about what to dedicate the other 99% to.\nHow do we time donations to maximize impact? Turns out there’s no ‘goldilocks zone’ in the continuum.\n\nWhen you’re young: justify putting it off by saying “oh I’ll donate when I have more money”\nWhen you’re old: justify not doing it by saying “oh that’s too much of my money I’m giving away”\n\nShallow Puddle Analogy\nStory from source: The Drowning Child and the Expanding Circle by Peter Singer\nOne morning, you notice a child has fallen in and appears to be drowning. To wade in and pull the child out would be easy, but it will mean that you get your clothes wet and muddy, and by the time you go home and change you will have missed your first class.\nDo you have any obligation to rescue the child? Unanimously, most people say they do. Does it matter if other people walk by the pond and do nothing? Most people still say no. Does it matter if the child were far away, in another country perhaps? Most people still say no.\n“We are all in that situation of the person passing the shallow pond. We can all save the lives of people, both children and adults, who would otherwise die, and we can do so at a very small cost to us: the cost of a new CD, a shirt, or a night out at a restaurant or concert, can mean the difference between life and death to more than one person somewhere in the world.”\nIneffective Altruism\nHal in Reboot\nIneffective altruism eschews metrics, because “What does doing good look like?” should be a continuously-posed question rather than an optimization problem.\n\nStrengthening community is also important for our shared future, even if it isn’t measurable.\n\nWhat is the best we can do as a collective and community rather than at the individual level?\nI’m curious if there has been attempts to quantize efforts of mutual aid and solidarity? If so how what does that computation come out to? Thinking about it, don’t most quantizations have huge error bars? How do EA folks choose between prioritizing something that is almost certainly good vs something that could have a very small change of huge upside? Seems to me they are basing it off of expected value of the distribution which… isn’t reasonable in most cases I feel (especially in cases when the long tail for these distributions can explode as t→∞) (see: interpreting small probabilities)\nPotentials include capping tail end of distribution?\n\nSeventh-generation decision making, for example, is an indigenous principle that is enshrined in the Constitution of the Iroquois Nation. It mandates Iroquois leaders to consider the effects of their actions over seven generations, encompassing hundreds of years. Seven generations is a long time, but it is also a finite amount of time. (see: traditional knowledge)\n\nKarl Popper had a really good way of putting this, which now sides him very solidly on the side which would choose to save the child in The Ones Who Walk Away From Omelas\n\nSimilarly we must not argue that the misery of one generation may be considered as a mere means to the end of securing the lasting happiness of some later generation or generations; and this argument is improved neither by a high degree of promised happiness nor by a large number of generations profiting by it. All generations are transient. All have an equal right to be considered, but our immediate duties are undoubtedly to the present generation and to the next.\n"},"thoughts/embedded-AI":{"title":"Embedded AI","links":["posts/agi","thoughts/Heidegger","thoughts/frame-problem","thoughts/taste","thoughts/Chinese-room-argument","thoughts/representation","thoughts/GOFAI"],"tags":["seed"],"content":"More in post on AI systems\nDreyfus\nDreyfus believed that, for any AI system to achieve any sort of general intelligence, it must also exhibit Dasein (being in the world). Thus, “a successful Heideggerian AI would need a perfect model of the human body – and by implication, that Dasein must be expressed as a human being, organically as well as existentially”.\nDreyfus’s critique of the frame approach involves look at descriptions of typical situations like going to a birthday party. This quickly grows out of hand once again as any AI program using frames to organize millions of meaningless facts so as to retrieve the relevant frames is going to be caught in a cycle of finding frames for recognizing relevant frames for recognizing relevant facts, forever in a vicious regress\nApproaches to embedded AI\nRodney Brook’s behaviourist approach\nIt turns out to be very difficult to reproduce in an internal representation for a computer the necessary richness of environment that would give rise to interesting behaviour by a highly adaptive robot\nThis is generally avoided by human beings because their model of the world is the world itself\nSolution is to build a mobile robot that uses the world itself as its own representation (referring to its sensors rather than to an internal world model)\nHowever, the problem is that a robot operates in a fixed world, responding only to a small set of possibly relevant features that their receptors can pick up\nPhil Agre’s pragmatist model\nUse of deictic representations: instead of representing a particular object in the world, we represent a role that an object might play in a certain time-extended pattern of interaction between an agent and its environment\ne.g. when a virtual ice cube defined by its functions is close to the virtual player, a rule dictates a response (for example, kick it)\nProblems: no learning takes place. As such, finesses rather than solves the frame problem\nMerleau-Ponty’s discriminatory model\nAs an agent learns, skills are not stored as internal representations. Rather experiences are presented to the learned as more finely discriminated situations\ne.g. as you learn to cook, experiences are presented that are more finely discriminated like having a better cooked egg vs a poorly done one. If the situation does not clearly solicit a single response or if the response does not produce a satisfactory result, the learner is led to further refine the discrimination\nSee also: taste\nWalter Freeman’s neurodynamic model\nBasic Cartesian model\n\nthe brain receives input from the universe by way of its sense organs\nout of this stimulus information, the brain abstracts features, which it uses to construct a representation of the world\n\nTreat the computer/brain as a passive receiver of bits of meaningless data, which then have significance added to them. The big problem is how the brain binds the relevant features together (see: Chinese room argument)\nFreeman solves this using the concept of energy states. States tends toward minimum “energy” which are called attractors.\nBrain states that tend towards a particular attractor are called that attractor’s “basin of attraction”. When learning, the brain forms a new basin of attraction for each new significant class of inputs and thus the significance of past experience is preserved in an attractor landscape.\nEach new attractor does not represent a thing, rather, the brain’s current state is the result of the sum of the animal’s past experience with the thing.\nThe constantly updated landscape of attractors is correlated with the agent’s experience of the changing significance of things in the world.\nThus, there are no fixed representations, when an animal learns to respond to a new odor, there is a shift in all other patterns (even those not directly involved with the learning)\nThis is a notably different approach from GOFAI where each item is positioned by a discrete address or branch of search tree"},"thoughts/emergent-behaviour":{"title":"Emergent Behaviour","links":["thoughts/communities","thoughts/consciousness","thoughts/Reductionism","thoughts/Gall's-law","thoughts/LLMs","thoughts/Panpsychism","thoughts/autopoiesis","thoughts/emergent-behaviour","thoughts/computer-graphics"],"tags":["sapling"],"content":"How complex behaviour can arise out of seemingly simple rules? Is there anything special that causes emergent behaviour?\nWe see examples like\n\nAnt simulations\nMold simulations\nCommunity dynamics\n\nInteresting to think about in context of single agents in multi-agent systems. How does consciousness arise? Is it just because of the rules itself (a reductionist approach) or is there something larger at play?\nGall’s law: simple alphabets produce behaviors. Even simple rules like Conway’s game of life can be Turing Complete!\nEmergentism\n\nOnce a certain level of complexity is reached, there is a kind of qualitative leap where completely new sorts of physical laws can “emerge”—ones that are premised on, but cannot be reduced to, what came before\n\n\nIn this way, the laws of chemistry can be said to be emergent from physics: the laws of chemistry presuppose the laws of physics, but can’t simply be reduced to them.\nIn the same way, the laws of biology emerge from chemistry: one obviously needs to understand the chemical components of a fish to understand how it swims, but chemical components will never provide a full explanation.\nIn the same way, the human mind can be said to be emergent from the cells that make it up.\n\nLLMs seem to exhibit this sort of behaviour.\nCombination Problem\nHow do microlevel experiences combine to form macrolevel ones?\n“Take a sentence of a dozen words, and take twelve men and tell to each one word. Then stand the men in a row or jam them in a bunch, and let each think of his word as intently as he will; nowhere will there be a consciousness of the whole sentence”\nPanpsychist response\nMental properties belong only to genuine individuals, not to mere aggregates\nThen, what is the boundary of the genuine individual? This is a problem of embodiment. To be a genuine individual is not simply to be a particular (as opposed to a conglomerate or universal) but also a system that has some kind of bounded organizational unity through autopoiesis — this is a body\nTheir own emergence problem: how do genuine individuals emerge from particular that aren’t individuals/subjects of experience? It doesn’t solve the problem of accounting for the place of consciousness in nature, it just relocates it\nCellular Automata\nSource\nMesmerizing video by Sebastian Lague on how complex (and beautiful) behaviour can arise out of simple rules. I’ve been wanting to get into experimenting with shaders and graphics stuff and this might just be the slight nudge I needed."},"thoughts/emptiness":{"title":"Emptiness","links":["thoughts/interdependence","thoughts/ontology"],"tags":["seed","PHIL240A","PHIL451A"],"content":"\nEmptiness is not nothingness. Emptiness is form.\n\nEmptiness is like the mathematical concept of 0.\n\nIt seems to have no value, yet is it the foundation of mathematics.\nWithout 0, you cannot have 1, 2, 3, so forth.\n\nEmptiness thus does not mean nothingness (0 is not null), emptiness is the base case through which existence is meaningful.\n\nIn order for a glass to even be empty, it should first and foremost be there.\n\nEmptiness means empty of independent existence — all things are interdependent. Thus, form is emptiness but emptiness is form (existence is to depend on others). This is sometimes referred to as dependent origination.\nIndra’s Net\nSource: Indra’s net on Wikipedia\nImagine a net stretching out infinitely in all directions with a monadic-like jewel at each node that reflects every one, including itself. These objects are nodes in a net of ontological interdependence, one in all and all in one.\n"},"thoughts/encryption":{"title":"Encryption","links":["thoughts/Asymmetric-Key-Cryptography","thoughts/Symmetric-Key-Cryptography","thoughts/RSA","thoughts/Elliptic-curve-Cryptography-(ECC)"],"tags":["seed"],"content":"\nA process of converting the original representation of the information (plaintext) into an alternative form (ciphertext). Ideally, only authorized parties can decipher a ciphertext back to plaintext and access the original information.\n\nAn encryption algorithm comprises\n\na method for encrypting data\na method for decrypting data\na secret key used in the decryption/encryption method\n\nThe two types of encryption are\n\nAsymmetric Key Cryptography (sometimes called public-key cryptography)\nSymmetric Key Cryptography\n\nTrapdoor: a mathematical function that is easy to go one way but hard to go the other way (an effectively one-way function)\n\nCommon functions include RSA (prime factorization) and ECC\nRSA for example, is a trapdoor because multiplying primes is easy but factoring the result back into its component primes is hard.\nThe bigger the spread between the difficulty of going one direction in a Trapdoor Function and going the other, the more secure a cryptographic system based on it will be\n\nLanguage\n\nA: Alice\nB: Bob\nKA​: Alice’s encryption key\nKB​: Bob’s decryption key\nm: plaintext message\nKA​(m): ciphertext, encrypted with key KA​\nKB​(KA​(m))=m\n\nTypes of attacks\n\nCiphertext-only attack: knowns KA​(m) but not m\nKnown-plaintext attack: for some m knows KA​(m)\nChosen-plaintext attack: knows KA​ but not KB​\n"},"thoughts/ephemereal-content":{"title":"Ephemereal and Real-time Content","links":["thoughts/neutrality","thoughts/digital-permanence","thoughts/library","thoughts/Internet","thoughts/friction","thoughts/Mangrove-Theory-of-the-Internet","thoughts/Dark-Forest-Theory-of-the-Internet"],"tags":["sapling"],"content":"One of the things that keeps face-to-face friendships strong is the nature of shared experience: you laugh together; you dance together; you gape at the hot-dog eaters on Coney Island together.\n\nThe internet is often hailed for bringing people closer together. The apparent collapse of the physical space between users is achieved by slashing down the time between the moment in which a message is sent and received, until it’s close to real time. For millions of years, the only real-time communication we’ve had as a species involved physical presence. Thus, real-time digital communication makes us feel physically close (from The Neutrality Pyramid)\n\nOpposite of digital permanence\nRaider’s of the Lost Web\nSource: RAIDERS OF THE LOST WEB on the Atlantic\n“In Supreme Court opinions, every word matters … When they’re changing the wording of opinions, they’re basically rewriting the law.”\n“It’s gone gone. A piece of paper can burn and you can still kind of get something from it. With a hard drive or a URL, when it’s gone, there is just zero recourse.”\nThe promise of the web is that Alexandria’s library might be resurrected for the modern world. But today’s great library is being destroyed even as it is being built. Until you lose something big on the Internet, something truly valuable, this paradox can be difficult to understand.\n“Ephemerality is built into the very architecture of the web, which was intended to be a messaging system, not a library.” (maybe we need more friction?)\nModern content is sometimes actually assembled on the fly through the likes of Ruby, Django, Next.js, etc. Not actual ‘flat’/self-enclosed pages. The data may exist but not the in the format it was originally delivered in\nRelated: Mangrove Theory of the Internet, Dark Forest Theory of the Internet\nEphemeral vs Accretive Content\nSource\nSee also: digital permanence\n\nEphemeral tools are about the conversation. Slack is one. Zoom is another. Here, memory resides in the users\nAccretive tools build over time. Developer tools tend to work like this: platform-as-code. Wikis are the main one: Notion is accretive. Memory resides in the tool\n"},"thoughts/epistemic-authority":{"title":"Epistemic Authority","links":["thoughts/trust","thoughts/epistemology","thoughts/Nagel's-Bat-Argument","thoughts/philosophy-of-mind","thoughts/the-Self","thoughts/testimony","posts/collaborative-thinking","thoughts/positive-sum","thoughts/zero-sum","thoughts/attention","thoughts/democracy","thoughts/consensus","thoughts/knowledge-distillation"],"tags":["seed","PHIL240A"],"content":"On when to trust epistemic claims\nTrusting the word of others is necessary to expand knowledge beyond perception, e.g. we will never know what it is like to be another person (see Nagel’s Bat Argument)\nEpistemic Authority with Anand Vaidya\nYouTube talk\nHow do we distinguish between appeals to authority that are rationally/ethically problematic and ones that are morally and rationally wholesome and skillful?\nQuestion about demarcation of appeals to authority between good and bad turns into a much larger question that goes into the philosophy of mind and philosophy of neuroscience, the nature of the self, how that is created, and how that thing epistemically gains and accesses information in the first place through retention\nTwo concepts and distinctions\n\nEpistemic Culture: what do members of the culture take to be valid sources of knowledge\nEpistemic Self: an individual epistemic agent within a culture might have their own idiosyncratic belief about how the source work, how they’re weighted, etc.\n\nKnowledge Sources:\n\nTestimony\nPerception\nInference\n\nShared Epistemic Culture\nThrowing something from one epistemic culture that’s disjoint from another epistemic culture cannot lead to reconciliation without a shared epistemic culture (see also: collaborative thinking).\nDogmatism is when one cannot understand differences in epistemic cultures\nCritical Thinking\n\nOverriding defeater: given evidence presented, this piece of evidence is a countering piece of evidence that overrides the argument for what they’re saying\nUndermining/undercutting defeater: some kind of source is fundamentally wrong and nothing comes out of it\n\nIs it worth to assess overriding testimonial information from authorities? Is critical thinking good?\nMichael Huemer: you shouldn’t! Critically thinking is epistemically irresponsible. Anand’s counter-argument: you always need to critically evaluate at least the following\n\nwho are the authorities?\nare they currently properly performing their authority?\nwhat does their authority amount to, within their expertise?\n\nIn being an expert and giving expert testimony means that we want the expertise that the person has to be spoken for and performed — we want an expert performance. This can be suspect when the information upon which the expert is drawing is ambiguous. Skepticism can also start to creep in because of the bureaucratic complexity through which the message is being delivered\nArgumentation\nTwo views on argumentation\n\nEpistemic: trading of assertions has a way of leading to a better epistemic position either for both parties or for the conjunction of the two parties (positive sum)\nNon-epistemic: trying to pull one side to another (zero sum)\n\nNot only an issue of trust but also psychological exhaustion. One of the requirements of any epistemic enterprise is the use of attention. Information that puts its receivers into an almost constant state of some kind of cognitive dissonance negative impacts their ability to attend and assess the information. See: salience principles of democracy\nOne of the main components of collective action is collective belief. We want everyone to do f but that requires everyone to believe that doing f is right\nEvaluating authority\nAchieving consensus is difficult, especially among a wide net of epistemic agents. In fact, without formal processes like voting or delegation, just peer-based discussion leads to a factorial explosion in time-taken to reach consensus.\nWhat about a “best bet view”: if the weighted average of authorities who meet these tests say something, we should trust that it’s true (this depends on epistemic culture and epistemic self)\nThus in most cases, rough consensus or trusting delegates (e.g. authorities like the CDC) to not be biased and accurately evaluate other authorities (e.g. scientists and pharmaceutical companies responsible for inventing/deploying the vaccine) is usually suitable when balancing tradeoffs between speed (for example when timeliness during a pandemic is important) and correctness (making sure vaccines are safe to the general public).\nIncreased diversity (of critical thinking skills) leads to a decrease in the probability of the whole group being accurate in evaluating whether or not someone is an authority. You can fool some people some of the time but you can’t fool all the people all of the time\nListen to voices not being heard and figuring out how to repackage what they’re saying in a way that is significant — it is a unique skill to be trained as a cross-cultural philosopher. These philosophers can then teach experts to communicate better. Perhaps this is the role of knowledge distillers like teachers and those who specialize specifically in pedagogy?\nSee also: testimony\nExperts: Which Ones Should You Trust?\nby Alvin I. Goldman (2001)\nCentral Question: the Novice/2-Expert Problem\n\nAn expert\n\nKnows a lot about the domain (first order material)\nKnows about the literature concerning the domain (second order material)\nAble to draw on this knowledge to produce answers about the domain\n\n\nTwo experts disagree, as a novice, which expert do you trust as more credible? 5 kinds of evidence\n\nArguments presented by the contending to support their own views and critique their rival’s views\nAgreement from additional putative experts on one side or other of the subject in question\n\nCounterpoint: Copernican heliocentrism, most ‘experts’ believed that the Earth was the middle of the solar system, not the sun\nGoldman steel-mans his argument through Bayesian analysis, experts should think for themselves and should be ‘conditionally independent’ — they will not invariably parrot the claims of others when those claims are wrong\nWhat about discoverability of experts? If a tree falls but nobody hears it, did it still happen?\n\n\nAppraisals by “meta-experts” of the experts’ expertise\nEvidence of the expert’s interests and biases vis-a-vis the question at issue\nEvidence of the expert’s past track-records\n\n\n\nSee also: testimony"},"thoughts/epistemic-injustice":{"title":"Epistemic injustice","links":["thoughts/power","thoughts/testimony","thoughts/hermeneutical-injustice","thoughts/trust"],"tags":["seed","PHIL240A"],"content":"from Epistemic Injustice by Miranda Fricker\nRelated to social power\nTypes of epistemic injustice:\n\nTestimonial Injustice: giving a deflated level of credibility to a speaker’s word for no other reason other than identity prejudice in a way that harms the speaker\nHermeneutical Injustice: the injustice of having some significant area of one’s social experience obscured from collective understanding owing to hermeneutical marginalization\n\nLearning new epistemic resources is difficult for multiple reasons\n\nLack of trust between groups\nDominantly situated knower being unwilling to confront destabilizing truths\nReluctance or inability of dominant group to recognize the value of the epistemic resources they’re being taught\n"},"thoughts/epistemology":{"title":"Epistemology","links":["thoughts/truth","thoughts/epistemic-injustice","thoughts/hermeneutical-injustice","thoughts/indeterminant","thoughts/testimony","thoughts/qualia","thoughts/self-knowledge","thoughts/the-Self","thoughts/Descartes'-Meditations","thoughts/Knowledge-Argument","thoughts/Root-Verses-on-the-Middle-Way-(MMK)","thoughts/Nyāya"],"tags":["seed","PHIL240A"],"content":"As it relates to knowledge, its validity, and its acquisition.\nValidity\n\ntruth\nepistemic injustice\nhermeneutical injustice\nindeterminancy\ntestimony\n\nOf the self\n\nqualia\nself-knowledge\nthe self\n\nDoubt\n\nDescartes’ Meditations\nKnowledge Argument\n\nSchools of Thought\n\nRoot Verses on the Middle Way (MMK)\nNyāya\n"},"thoughts/ethereum":{"title":"Ethereum","links":["thoughts/blockchain","thoughts/proof-of-work","thoughts/proof-of-stake"],"tags":["seed"],"content":"Blockchain with a built-in Turing-complete programming language to allow individuals to write smart contracts and decentralized applications (dApps) which dictate their own rules around ownership, transaction formats, and state transition functions.\nEther (ETH) is the actual currency. All transactions (sending ETH, using a dApp, executing a smart contract) cost a gas fee to disincentivize bad actors from spamming.\nUsed to use proof of work but switched to proof of stake.\nUnderstanding\n\nHas a built-in Turing-complete scripting language built on top of the Ethereum VM that perform transactions and send them to the blockchain\n\nVM details\n\nStack: up to 1024 32-byte fields\nMemory: infinite in size but more size means more gas\nStorage: permanent for contracts (verifiable using a Merkle trie)\nEnvironment variables: VM can access block number, time, mining difficulty, previous block hash etc.\nLogs: append-only storage in a specific block (not state)\nSub-calling: VM can call other contracts\n\n\nTo prevent halting problem from taking up all computational resources, we implement a “gas fee” to charge every computational step\nThe more bytes the data field in each transaction is, the more expensive it becomes\n\n\nState is the database (key value mapping addresses to account objects)\nContracts are programs that run on top of the database\n\nThis feels like time-share all the way back in the day but decentralized and now on the global scale. Crazy\ndApps\nCombine smart contracts (the backend) with a frontend user interface. Typically,\n\nopen-source\npublic data + records\nuses a cryptographic token to keep the network secure\n"},"thoughts/ethics":{"title":"Ethics","links":["thoughts/morality","thoughts/Collingridge-dilemma","thoughts/religious-authority","thoughts/Kant","thoughts/Utilitarianism","thoughts/Social-Contract-Theory","thoughts/virtue-ethics"],"tags":["seed","CPSC430"],"content":"Ethics is the rational systematic analysis of morality.\nEthics is focused on the voluntary, moral choices. Ethics is not concerned about involuntary choices or choices outside the moral realm.\nWorkable ethical theory: produces explanations that aim to be persuasive to a skeptical, yet open-minded audience about what is “right” or “wrong”\nEspecially important for technologists to grapple with as common wisdom may not exist for novel situations brought about by new technologies (see: Collingridge Dillemma)\n4 Unworkable Ethical Theories\n\nSubjective Relativism: “who are you to criticise my values”\n\nThere’s no universal standard of right or wrong, each individual must decide for themselves\nEverything is equally valid, not that “I can see how people can arrive at different conclusions”\n\n\nCultural Relativism: “who are you to criticise my culture’s values”\n\nMoral actions are based on a culture’s actual moral guidelines\n\nTraditions develop because they meet a need, but once a tradition has been established, people behave in a certain way because it’s what they’re supposed to do, not because they understand the rationality deeply embedded within the tradition.\n\n\nIt is presumptuous to judge another culture’s values\n\n\nDivine Command Theory: “stealing is wrong because the Bible says so”\n\nWe should use holy books (or any sort of appeal to authority) as guides for moral decision making\nOnly effective to those who already believe in the same beliefs as us\nSee also appeals to religious authority\n\n\nEthical Egoism\n\nPeople’s self-interest is their only ethical obligation\nWhat’s good for the market (modelled by selfish agents) is good for society\nHowever, does not respect the ethical point of view\n\nThe ethical point of view is the understanding that other people and their core values are also worthy of respect\n\n\n\n\n\nWorkable Theories\n\nKantianism\nUtilitarianism\nSocial Contract Theory\nVirtue Ethics\n\n"},"thoughts/evaporative-cooling":{"title":"Evaporative Cooling Effect","links":[],"tags":["seed","pattern"],"content":"Sources: The Evaporative Cooling Effect in BumblebeeLabs and Evaporative Cooling of Group Beliefs in LessWrong\n\n“The people who are the most eager to talk are the ones who the least number of people are interested in hearing.”\n\nEvaporative Cooling is the slow decay of a community into mediocrity as its most ‘high energy’ or high value individuals exit.\nIt is a way to think about communities in terms of the average quality of its members.\nWhen the most high value members realize there isn’t much left for them to gain, they leave (Groucho Marx rule: you stop attending any event which would have you as participant).\nThis drop the average quality of the community down and the next level notices and then also finds it underwhelming. This continues until you get to a point where the people in the community are so unaware they no longer notice/care.\n“If anyone can join your community, then the people most likely to join are those who are below the average quality of your community because they have the most to gain.”\nSocial Gating\nMechanisms which allow participants to self-select out of the group (vs traditional direct exclusion).\ne.g.\n\nnicheness/technicality (high prerequisite knowledge)\nsocial stigma\nhigh cost (time and money)\ndifficult entry (hazing)\n\nGeeks, MOPs, and sociopaths in subculture evolution\nFrom the article of the same name\n\nCreators: riff off of each other, produce examples and variants, and share them for mutual enjoyment, generating positive energy\nFanatics: don’t create, but they contribute energy (time, money, adulation, organization, analysis) to support the creators.\nCreators and fanatics are both geeks. They totally love the New Thing, they’re fascinated with all its esoteric ins and outs, and they spend all available time either doing it or talking about it.\nMops: fans, but not rabid fans like the fanatics. They show up to have a good time, and contribute as little as they reasonably can in exchange.\n"},"thoughts/explainability":{"title":"Explainability","links":["thoughts/accountability","thoughts/representation","thoughts/semantics","thoughts/neural-networks"],"tags":["sapling"],"content":"European Union adopted new data-protection rules in 2016 that include a legal right to an explanation of decisions made by algorithms.\nAs AI systems become more influential in their power and incorporated into more and more important decision making, explainability is extremely important for the sake of algorithmic accountability.\nFor now though, the current advances in deep learning mean that most representations of the neural network state have a distributed representation of content, meaning that there is no ‘single-neuron’ for certain decisions as semantic symbols do not arise here.\nSemantic meaning only arises in neural networks because we inject them or through inductive proding (e.g. LIME for explainability)"},"thoughts/exploit-explore":{"title":"Exploit Explore Problem","links":["thoughts/machine-learning","thoughts/gradient-descent"],"tags":["sapling"],"content":"Machine Learning\nSource: Intro to Reinforcement Learning: The Explore-Exploit Dilemma\nExploit-explore tradeoff in machine learning is a hill climbing/optimization problem (see: gradient descent).\nAlways jump to next step up is analogous to greedy search.\nPendulum of life\nSource: The Joy of Wasting Time by Samson Zhang\n“The ideal is to find an equilibrium point between external pulls and internal pushes, between exploitation (of your current opportunities via external pulls) and exploration (of your actual passions via internal pushes)… the process of finding equilibrium often takes the form of a damped oscillation over time”\nHowever in real life, what this hill climbing problem landscape may look like will erode and grow and change with time.\n\nRegret Minimization Framework\nSource: Bezos on the Regret Minimization Framework\nProject yourself to age 80 and look back on your life. How do I minimize the regrets that I have? Will you regret abandoning this idea?\n\nWhen you minimize future regret, you sleep well knowing you’re maximizing fulfilment.\n\nIs a life well-lived one that is fully maximized? What if I just want to live for vibes (which are inherently unoptimizable)?\nMulti-armed Bandit\nThe multi-armed bandit problem models an agent that simultaneously attempts to acquire new knowledge (called “exploration”) and optimize their decisions based on existing knowledge (called “exploitation”). The agent attempts to balance these competing tasks in order to maximize their total value over the period of time considered.\nThe name comes from imagining a gambler at a row of slot machines (sometimes known as “one-armed bandits”), who has to decide which machines to play, how many times to play each machine and in which order to play them, and whether to continue with the current machine or try a different machine."},"thoughts/exploratory-data-analysis":{"title":"Exploratory data analysis (EDA)","links":["thoughts/latent-factor-model"],"tags":["seed","CPSC340"],"content":"How do we “look” at features and high-dimensional examples?\n\nSummary statistics\n\nCategorical Features\n\nFrequencies\nMode\nQuantiles\n\n\nNumerical Features\n\nLocation\n\nMean\nMedian\nQuantiles\n\n\nSpread\n\nRange\nVariance\nInterquartile ranges\n\n\n\n\nEntropy: measured “randomness” of a set of variables where entropy is −Σc=1k​pc​logpc​ and pc​ is the proportion of times you have value c, range from [0,logk]\n\nLow entropy means it is very predictable whereas high entropy means it is very unpredictable (roughly, spread)\nNormal distribution has the highest entropy\n\n\nNot always representative! Don’t mistake the map for the territory\n\n\nDistance or similarities\n\nHamming distance: number of times elements aren’t equal\nEuclidian distance: how far apart are the vectors (square root of sum of squares)\nCorrelation\nJaccard coefficient: set distance, intersection over union\nEdit distance: for strings, how many characters do I need to change to go from one to the other\nDistance in latent space\n\n\nVisualizations\n\nBasic line plots\nMatrix plot: visualize two features as an image\nCorrelation plot\n\nCan add colour to show a third feature (usually categorical)\n\n\nScatterplot\n\n\n"},"thoughts/fair-betting-quotient":{"title":"Fair Betting Quotient (FBQ)","links":["thoughts/Decision-theory","thoughts/Dutch-Book"],"tags":["seed","PHIL321A"],"content":"Your fair betting quotient (FBQ) for A is p if you think that the following is a fair bet (you are willing to take either side):\n\nIf A is true, you win (1−p)S\nIf A is not true, you lose pS\n\nBetting table is different from decision tables. Columns are different bets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nABet for ABet against AT(1−p)S−(1−p)SF−pSpS\nSuppose a set of credences (or FBQ’s) is incoherent. We can always construct a Dutch Book. Have the agent bet for propositions with credences (or FBQs) that are too high, and against propositions with credences (or FBQs) that are too low."},"thoughts/fairness":{"title":"Fairness","links":["thoughts/To-Live-in-their-Utopia","thoughts/Algorithms-of-Oppression"],"tags":["seed"],"content":"On Algorithmic decision making\nSource: Can you make AI fairer than a judge? Play our courtroom algorithm game in MIT Technology Review\nNo matter how much data we collect, two people who look the same to the algorithm can always end up making different choices.\nTwo definitions of fairness:\n\nkeep the error rates comparable between groups, and\ntreat people with the same risk scores in the same way.\n\nBoth of these definitions are totally defensible! But satisfying both at the same time is impossible.\nRelevant reads on algorithms and algorithmic decision making: To Live in their Utopia, Algorithms of Oppression"},"thoughts/fault-tolerance":{"title":"Fault Tolerance","links":["thoughts/distributed-systems","thoughts/game-theory","thoughts/Zooko's-Triangle","thoughts/Sybil-Attack","thoughts/cascading-failures","thoughts/Byzantine-Faults","thoughts/Network-Theory"],"tags":["sapling"],"content":"How do we defend against attacks in distributed systems with no central authority? We want the system as a whole to continue working, even when some parts are faulty\n\nFailure: system as a whole isn’t working\nFault: some part of the system isn’t working\n\nProbability of all n replicas being faulty: pn\nProbability of 1 or more replicas being faulty: 1−(1−p)n\n\n\n\nRelated: game theory, Zooko’s Triangle, Sybil attack, cascading failures, Byzantine Faults\nTwo Generals Problem\nThis thought experiment meant to illustrate the pitfalls and design challenges of attempting to coordinate an action by communicating over an unreliable link. In the experiment, two generals are only able to communicate with one another by sending a messenger through enemy territory. The experiment asks how they might reach an agreement on the time to launch an attack, while knowing that any messenger they send could be captured. It is required that the two generals have their armies attack the city simultaneously to succeed, lest the lone attacker army die trying.\nBecause acknowledgement of message receipt can be lost as easily as the original message, a potentially infinite series of messages is required to come to consensus.\nThis problem is unsolvable.\nByzantine Generals Problem\nThis situation can be expressed abstractly in terms of a group of generals of the Byzantine army camped with their troops around an enemy city. Communicating only by messenger, the generals must agree upon a common battle plan. However, one or more of them may be traitors who will try to confuse the others. The problem is to find an algorithm to ensure that the loyal generals will reach agreement.\nIt is shown that, using only oral messages, this problem is solvable if and only if more than two-thirds of the generals are loyal; so a single traitor can confound two loyal generals. With unforgeable written messages, the problem is solvable for any number of generals and possible traitors.\nSee: Byzantine Faults\nDesigning Robust Networks\nSee also: Network theory, cascading failures\nDesigning networks that are simultaneously robust to attacks and random failures appears to be a conflicting desire\n\nThe hub-and-spoke network is robust to random failures, as only the failure of its central node can break the network into isolated components, but a single targeted attack can fragment the network.\nA completely random network lacks hubs, the impact of an attack is similar to the impact of random node removal — both are equally bad and can easily fragment a network.\n\nTo maximize robustness, we want to maximize the ‘breakdown’ or critical threshold: fctot​=fcrand​+fctarg​\nThis is maximized by having a bimodal degree distribution where an r fraction of nodes have degree kmax​ and the remaining 1−r fraction have degree kmin​\nRobustness vs Resilience vs Redundancy\n\nRobustness: able to maintain basic functions in the presence of internal and external errors (static).\nResilience: able to adapt to internal and external errors by changing its mode of operation to maintain its ability to function (dynamic).\nRedundancy: presence of parallel components and functions that can replace missing components or functions.\n"},"thoughts/feature-selection":{"title":"Feature Selection","links":["thoughts/fundamental-tradeoff","thoughts/linear-regression"],"tags":["seed","CPSC340"],"content":"Better features usually help more than a better model. Good features would ideally:\n\nAllow learning with few examples, hard to overfit with many examples\nCapture most important aspects of problem\nReflects invariances (generalize to new scenarios)\n\nFind the features (columns) of X that are important for predicting y\n\nWhat are the relevant factors?\nWhich basis functions should I use among these choices?\nWhat types of new data should I collect?\nHow can I speed up computation?\n\nThis can help us to remove features. Feature complexity is also correlated with the fundamental tradeoff. Increased complexity leads to increased overfitting risk. Models (like linear regression) can overfit with large d so reducing d to only useful factors may improve results.\nGenerally, there are no right answers but there are wrong answers.\nAssociation\nFor each feature j, compute correlation between feature values xj​ and y.\nUsually gives unsatisfactory results as it ignores variable interactions (e.g. if tacos make you sick, and you often eat tacos on Tuesdays, it will say “Tuesday” is relevant.)\nRegression Weight\nFit linear regression weights w based on all features.\nTake all features j where weight ∣wj​∣ is greater than a threshold\nHas major problems with collinearity\ny^​i​=w1​taco+w2​tuesday=0taco+(w1​+w2​)tuesday\nSearch and Score\n\nDefine score function f(S) that measures quality of a set of features S\nSearch for the variables S with the best score\n\nWe create the set of features S by creating every possible combination of 2d features.\nHowever, as we have a large number of sets of variables we are prone to optimization bias\nO(2d) runtime\nForward Selection\n\nStart with an empty set of features S=[]\nFor each possible feature j, compute scores of features in S combined with feature j\nFind the j that has the best score when added to S\nCheck if {S∪j} improves on the best score found so far\nAdd j to S and go back to step 2\n\nWe can stop when no j improves the score\n\n\n\nO(d2) runtime\nNumber of Features Penalties\nWe can again use complexity penalties and penalize the number of features used. This can also be called the L0​-norm which is the number of non-zero values.\n∥w∥0​=size(S)\nText Features\n\nBag of Words: represents sentences/documents by word counts:\nBigram: an ordered set of two words\nTrigram: an ordered set of three words\n\nGlobal vs Local\nGlobal vs. local features allow for “personalized” predictions.\nWe add a feature for each ‘person’ in the system.\n"},"thoughts/federation":{"title":"Federation","links":["thoughts/decentralization","thoughts/interoperability"],"tags":["seed"],"content":"\nIn a federated network, users are still interacting with a server, but anyone can run a server that interoperates with others in the network, giving users more providers to choose from\n\nDesigning in such a way that new instances of any centralized function are relatively easy to create and can maintain interoperability and connectivity with other instances."},"thoughts/feedback-loops":{"title":"Feedback Loops","links":["thoughts/exploit-explore","thoughts/interaction-design"],"tags":["seed"],"content":"Tight feedback Loops\nSource: Beware of tight feedback loops by Brian Lui\n“Focusing on tight feedback also leads to getting stuck in local maxima. The short turnaround time leads to the pursuit of incremental improvements. Accurate feedback reduces error, but this means reaching and staying at the local maximum. The more accurate and more rapid the loop, the more quickly you’ll arrive at the top of the hill – and the less chance you have of leaving it to climb the mountains of mastery.”\n“This is because feedback loops which are too short for the overall system makes people focus on inappropriate intermediate goals.”\nRelated: exploit explore as a metaphor for optimizing life.\nInteraction Design\nSee: interaction design\n\nGulf of execution: the difference between the intentions and allowable actions\nGulf of evaluation: the difference between the actual system state and user’s understanding\n"},"thoughts/fiction":{"title":"Fiction","links":["thoughts/language","thoughts/skyhooks","thoughts/Where-is-My-Flying-Car","thoughts/truth","thoughts/fiction","thoughts/social-contracts"],"tags":["sapling","pattern"],"content":"\nThe future is manifested\n\nShared fiction as a statement about the future instead of a statement of today. They’re both fictions in that they can’t be (dis)proved and thus can have the same functions in a group of people (bring together, motivate to action, share values/language, etc.)\nBuild as if your creations had skyhooks you could hang them off of.\n\nTo see things as they really are, you must imagine them for what they might be. — Ruha Benjamin\n\nIn part why I really like reading things like The Book of Predictions where people from the 1970s just make predictions about the far future and see how correct they were. A lot of completely wrong predictions but some are eerily accurate (e.g. distributed and accessible compute). Now, you look at todays predictions of the future and it looks the same as the dreams we had 50 years ago about flying cars and space-faring societies.\nWhere are our futures with no famine and disease? With a clean and healthy planet? Of universal free access to information? Where did the ambition go?\nJ. Storrs Hall in Where is My Flying Car:\n\nWe need hopers and dreamers; we need visionaries who can see a better future worth striving for. We need great, important things to do with the staggeringly huge capabilities that lie within our grasp. Science fiction must get back down into the gutter and start looking back up at the stars.\n\nSocial Theory as Science\nSource: Possibilities — Social Theory As Science and Utopia by David Graeber\n“Where, in earlier generations, science fiction projections seemed to regularly become reality a generation later, now they remain trapped on the screen—even if the screen images look increasingly realistic.” (321)\nThe Big Here and Long Now\n\n“Humans are capable of a unique trick: creating realities by first imagining them, by experiencing them in their minds. When Martin Luther King said “I have a dream”, he was inviting others to dream it with him. Once a dream becomes shared in that way, current reality gets measured against it and then modified towards it.”\n\nThe act of imagining something makes it real. Art as processes or the seeds for processes - things that exist and change in time, things that are never finished.\nFiction as shared visions\nVisions matter. Visions give people a direction and inspire people to act, and a group of inspired people is the most powerful force in the world. If you’re a young person setting off to realize a vision, or an old person setting off to fund one, I really want it to be something worthwhile.\nProtocol Fiction\n\nBut we don’t need just design fictions. We need business model fictions, engineering feasibility study fictions, interop protocol specification fictions, investment return fictions.\n\nSpeculative Infrastructure\nShared truth\nCreating shared truth through fiction by dreaming the world we want to see come to fruition. I really like this, helps to explain why we need a collective vision. Reminds me of a big reason contests like this exist.\nIn essence, these are just social contracts (which are just shared truths that we all believe in)\nLegal fiction is one that has huge influence today (enforced by the threat of violence which relies on the asymmetric power of a nation state). But it feels like we can create other fictions that we as a society can believe in"},"thoughts/file-system":{"title":"File system","links":["thoughts/IP-Address","thoughts/CRDT"],"tags":["seed"],"content":"Overlay File System\nSource\nSometimes referred to as union-filesystems. You can think about it like different ‘layers’ of file systems, much like those laminated overhead projector sheets. If there are duplicated files, the file from a higher layer will take precedence.\nVirtual Distributed File System\nFrom Alluxio’s Technical Paper by Haoyuan Li\n\nAlluxio is an append-only file system, similar to HDFS, that supports standard file operations, such as create, open, read, write, close, and delete\n\nThe storage layer of the ecosystem grew from the Apache Hadoop Distributed File System (HDFS) to a variety of choices, such as file systems, object stores, blob stores, key-value systems, and NoSQL databases to realize different tradeoffs in cost, speed and semantics\nOne way of solving the n-to-n problem is to use a VDFS as opposed to exposing APIs. Reduces a lot of overhead in\n\nWork: needing to solve similar problems around storage\nData storage: ETL (extract, transform, load) pipelines\n\nFor the benefits and values the VDFS provides, we can make the analogy to IP. The IP layer is the narrow waist that enables the higher layer to innovate without worrying about the lower IP layer, and vice-versa. In the meantime, the virtual file system is an abstraction layer on top of a concrete file system implementation, and it allows applications to be able to access different types of concrete file systems in a uniform way.\nPerformance\nWhile caching can dramatically improve read performance, unfortunately, it does not help much with write performance. This is because highly parallel systems need to provide fault-tolerance, and the way they achieve it is by replicating the data written across nodes.\nInteresting to note: in big data processing, the same operation is repeatedly applied on massive data. Therefore, replicating programs is much less expensive than replicating data in many cases\nCRDT (ElmerFS)\nSource\nThe design of ElmerFS leverages the properties of CRDTs) to ensure that concurrent operations on different replicas always converge to a correct state while preserving the semantics of a traditional POSIX file system\nChallenges\n\nUnique identifiers: Any operation that creates inodes needs to generate 2 a unique identifier. Without coordination among replicas, generated ids might conflict.\nNamed links: Operations that create or move objects (files or directories) may result in conflicts in which concurrent operations on different replicas create objects with the same name in the same directory.\nCycles: Concurrent move operations without coordination may violate the file system invariant. For example, merging an operation that moves a directory A into a directory B with a concurrent operation that moves B into A can result in a cycle.\nDivergent renames: The rename operation is semantically a move operation, it move a link from one folder to another. When two concurrent renames move the same link to two different places, if both rename are ultimately accepted, a additional link of the inode will be created.\nPermissions changes: Updating permission from a replica may take some time to be enforced in other replicas. Merging an operation that removes a Bob’s permission to write to file with a concurrent operation in which Bob writes to that file will result in a different outcome depending on the order in which operations are applied.\n\nThe four main entities are inode objects, symbolic links, blocks and directories. We represent a file as a collection of fixed-size blocks. Each block is represented using a LWWR.\n\nNote: Further work needs to be done for allowing file content to diverge without loss of data or to use a CRDT that would be appropriate for a given file format. This means this is not ideal for collaborative text editing for example\n\nIt is based on the FUSE protocol, a user-space protocol used to implement file systems. The interface layer receives a FUSE request, calls the corresponding operation in the translation layer, and creates the appropriate response.\nThe translation layer is responsible for translating FUSE requests to CRDT operations."},"thoughts/forgetting":{"title":"Forgetting","links":["thoughts/right-to-be-forgotten","thoughts/lost-knowledge","thoughts/tools-for-thought"],"tags":["seed"],"content":"Forgetting things makes the things you do actually know/remember important.\nSee also: right to be forgotten, lost knowledge\nWhat if books and essays needed their words to be marked up/scribbled on/highlighted regularly or they would fade away? (demo by Winnie Lim)\nWriting\nI think a lot about how a lot of people who use Notion/Obsidian/tool for thought try to write everything down but maybe forgetting is what allows us to remember what is actually salient\nWe don’t write things down to remember them. We write them down to forget.\nWe did the most important work when we wrote the ideas down. “I’m not writing it down to remember it later,” declares every Fields Notes notebook, “I’m writing it down to remember it now.” The action of writing is what counts, what imprints important ideas in our brain. The note itself is a permission slip to let things go.\nSee also: tools for thought"},"thoughts/formality-considered-harmful":{"title":"Formality considered harmful","links":["thoughts/cozy-software","thoughts/interaction-design","thoughts/information-retrieval","thoughts/Seeing-like-a-State","thoughts/terminology","thoughts/Gall's-law","thoughts/tools-for-thought","thoughts/Projects"],"tags":["seed","pattern"],"content":"See also: cozy software\nPaper by Frank M. Shipman III and Catherine C. Marshall with the same name. On experiences, emerging themes, and directions on the use of formal representations in interactive designs\n\nThe cause of a number of unexpected difficulties in human-computer interaction lies in users’ unwillingness or inability to make structure, content, or procedures explicit\n\nWhen people use computer systems, their interaction is usually mediated by abstract representations that describe and constrain some aspect of their work or its content. Computer systems use these abstract representations to support their users’ activities in a variety of ways, like for information retrieval. These abstractions are frequently referred to as formalisms.\nAt the formal end of the spectrum, knowledge-based systems require people to encode materials in a representation that can be fully interpreted by a computer program.\nA la Seeing like a State: excessive formalization may cause people to lose information that falls outside the prescribed structure, and in general require people to make knowledge explicit that may be difficult or undesirable to articulate\nExamples\n\nExperiences training information analysts to use NoteCards revealed that they had difficulties\n\nChunking information into cards (“How big is an idea? Can I put more than one paragraph on a card?”)\nNaming cards (“What do I call this? Do I have to name this card before I can get it off the screen?”)\n\nOnce again, naming things is one of the most difficult parts of anything\n\n\nFiling cards (“Where do I put this?”)\n\n\nPeople tend to cluster things spatially than via links\n\nInstead of building large interconnected networks of nodes (like the designers expected), users created linkless spaces of nodes arranged in regular graphical patterns that indicated relationships among nodes spatially and visually\n\n\nThere are many cognitive costs associated with adding formalized information to a computer system\n\nForemost, users must learn a system’s formal language\nEven if they know a system’s formal language, users face a mismatch between their understanding of the information and the system’s formal representation (good teachers help to bridge this)\n\n\nTacit knowledge poses a particularly challenging problem for adding formal structure and content to any system since, by its very nature, people do not explicitly acknowledge tacit knowledge\n\nWhen a person is asked to breath normally, their normal breathing will be interrupted\n\n\nStructure changes with time\n\nSince a user’s understanding of any non-trivial task, such as performing an analysis or completing a design, evolves as they attempt to complete the task, users resist making such commitments\nSame with terminological anchoring\nThere is a perception that information formalized incorrectly or inconsistently will be more difficult to use or simply be of less use than information not formalized.\n\nHence why we leave many tabs open or things on our desk/desktop\n\n\n\n\n\nFixes\n\nKeep things simple and out of the way for the user (Gall’s law).\nIncremental formalization\n\nIn the Hyper-Object Substrate (HOS) (Shipman, McCall, 1994) we have investigated the potential to support users by suggesting possible formalizations based on recognized patterns in textual information. In HOS, suggestions for new attributes or relations in the knowledge base were presented to the user for acceptance, modification, rejection, or just to be ignored\nThese are effective as long as they don’t overwhelm a user with too many requests to acknowledge inferred structure\n\n\n\nTools for thought\nI think this applies for thinking and note-taking too. My thesis is that having a scratch space be incredibly low friction to access (on par with or lower than Apple Notes) is incredibly important to cultivating good knowledge/deadline management. See: TabSpace"},"thoughts/frame-problem":{"title":"Frame Problem","links":["thoughts/context","thoughts/semantics"],"tags":["sapling"],"content":"Intelligence is (at least partly) a matter of using well what you know. An intelligent being learns from experience, and then uses what is has learned to guide expectations in the future. How does one select the appropriate frame or context for any given situation?\nThere are an infinite number of contextual frames for any given situation. How do we encode this in discrete systems like in AI?\nInstallation problem\nProblem of installing in one way or another all the information needed by an agent to plan in a changing world.\nCan be broken down into\n\nsemantic problem: what information do we need to install?\nsyntactic problem: what system, format, structure, or mechanism do we use to install it\n\nHowever, one cannot realistically create a Spinozistic solution (a small set of axioms and definitions from which we can deduce the rest of our knowledge on demand)\nWe run into the problem of induction: give that I believe all of this (have all this evidence) what ought I to believe as well (about the future, or about unexamined parts of the world)? Clearly this will not work (see: Black swan theory)\nWe need a system that genuinely ignores most of what it knows and operate with a well-chosen portion of its knowledge at any moment\nThis is the qualification problem: how do we design a system that reliably ignores what it ought to ignore under a wide variety of different circumstances in a complex action environment?"},"thoughts/freedom":{"title":"Freedom","links":["thoughts/agency","thoughts/web3"],"tags":["seed"],"content":"See also: agency\nWeb3\nweb3 allows us to codify and define these boundaries for ourselves. By itself, it is not a technology that makes us ‘more free,’ but by explicitly stating the boundaries we create, we can create the freedom.\n“This is a critical point: the products we create should not aim to make people ‘more free’ - that way lies false marketing campaigns and disappointment. Our products should be conscious of, and communicate clearly, how they constrain the people who use them.”\nFree Speech\nIn the US, protection is not given to “libel, reckless or calculated lies, slander, misrepresentation, perjury, false advertising, obscenity and profanity, solicitation of crime,\nAnd personal abuse or ‘fighting’ words,” because these actions do not serve the ends of the First Amendment"},"thoughts/friction":{"title":"Friction","links":["thoughts/Internet","thoughts/writing","thoughts/digital-mindfulness"],"tags":["seed"],"content":"Bring back Friction on the Internet\nSource: The case for slowing everything down a bit by Ezra Klein\nThe reduction of friction on the web is a business strategy. “Less friction means more time spent, more ads seen, more sales made. Tech companies lose customers during login screens and security verification, and as a result of slow load times. The country’s top computer science talent is paid billions of dollars to further reduce the milliseconds of delay separating our desires and their fulfillment.”\n“The philosophy of the Internet has assumed that friction is always part of the problem,” writes Kosslyn. But look around. The problem now isn’t too much friction; it’s too little. “It’s time,” he says, “to bring friction back.”\nFriction creates space in the system where judgment can intercede, where second thoughts can be had, where decisions can be made.\nWriting, by contrast, is full of friction. It’s hard and slow, and the words on the page fall short of the music and clarity I imagined they’d have. But it is, in the end, rewarding. It’s where I have at least a chance to create something worth creating. The work is worth it.\nRelated: digital mindfulness"},"thoughts/friendship":{"title":"Friendship","links":["thoughts/tribe-flourishing","thoughts/Dark-Forest-Theory-of-the-Internet","thoughts/Knowledge-Argument","thoughts/self-effacing-ends"],"tags":["sapling"],"content":"“The ultimate touchstone of friendship is not improvement, neither of the other nor of the self. The ultimate touchstone is witness, the privilege of having been seen by someone, and the equal privilege of being granted the sight of the essence of another, to have walked with them, and to have believed in them, and sometimes, just to have accompanied them, for however brief a span, on a journey impossible to accomplish alone” — David Whyte\nFriendship as witnesses to eudaimonic well-being.\n\nthe type of happiness or contentment that is achieved through self-actualization and having meaningful purpose in one’s life\n\nadrienne maree brown described relationships like a spiderweb—diaphanous yet strong, thick yet porous. “A web allows things to fall through, like a sieve,” she said. “Some things are not meant to be caught.”\nRelated: Tribe flourishing\nOveroptimization\nSource\n\nIt doesn’t feel right to pressure people to be “interesting” in your sense of the word. Everyone is interesting. Every person in the world has a literal lifetime of experiences that have shaped who they are. They have internal thought processes and distinct worldviews that you won’t find within anyone else. It’s a matter of giving your interactions enough time and care to discover these.\n\nInstead:\n\nOne thing I keep coming back to is open-mindedness. I’m drawn to people who don’t take themselves too seriously. People who move through life with a certain nonchalance that makes them eager for the unconventional and unusual. These people can laugh about anything and like to have fun.\n …\n These friends expand our boxes of possible experience. Spontaneity and unconstrained eagerness feel good because they are exercises in doing without thinking too much. Having the thought I want to do this, and then immediately being able to do the thing with people you care about silences the voice of “reason” in our heads.\n\nUnknown unknowns\nFriends shouldn’t tell you exactly how to live your life but rather to help you reflect back parts of your self that you may not be aware of.\nFriends &gt; Communities\nSource: friends &gt; communities, Sari Azout\n“This is about building the picks and shovels for intimate, intentionally small groups of friends and Internet friends to build things together, live together, and create wealth together. Unlike the Discord communities you’re part of, the small groups I’m thinking of have to stay small to survive — they’re small by design. Can you really be yourself in group chats with 50+ people?”\nSee: tribe flourishing, cozy web\nAcceptance Prophecy\nSource\n\n“If people expect acceptance, they will behave warmly, which in turn will lead other people to accept them; if they expect rejection, they will behave coldly, which will lead to less acceptance.”\n\nAssume that people like you. Tempted to ask a gym friend if they want to become a happy-hour friend? Assume they do. Want to reconnect with a friend you’ve fallen out of touch with? Assume they’re in. When we make this assumption, initiative feels less scary. We’re more likely to take some leaps of faith—and eventually navigate the friendship-making process, and life, with more peace, pleasure, and security.\nCommitment\nCommit to some great loves; falling in love with something and building a structure of behaviour for when love falters. Who would they care about if nobody knew?\n“For one human being to love another: that is perhaps the most difficult of all our tasks… the work for which all other work is but preparation,” Rilke wrote to the young poet seeking his advice a century ago.\nEpistemic Norms, The False Belief Requirement, and Love\nBy J. Spencer Atkins\nArgues that the demands of romantic love requires that we sometimes become bad epistemic agents\n\nCompanionship is pleasurable and consequently valuable because it affords the opportunity to feel “seen” by another. We can only, according to Braden, view ourselves conceptually—we know things about ourselves—but we need others to view ourselves perceptually, “as concrete objects ‘out there.’” Other consciousnesses function like a mirror. Being seen in this way is recognition of personhood. The feeling of being seen is psychological visibility. Romantic love affords a “uniquely powerful” experience of visibility because lovers share a fascination with one another unlike any other relationship.\n\nDoxastic voluntarism: we can make ourselves belief a proposition\n\nFalse Belief Requirement\n\nArgues that romantic love sometimes poses the demand to believe falsely\nKnowledge as justified true belief or JTB (see Knowledge Argument): S believes that p, S is justified in believing that p, and p is true\nBelief then, is not knowledge as one can hold false beliefs\nW.K. Clifford: “It is wrong always, everywhere, and for anyone to believe anything upon insufficient evidence”\nWhen love poses the false belief argument, love will be opposed to epistemic norms\nDelaney argues that lovers desire to be loved for the right reasons. That is, person A wants a romantic partner B to love him for properties that A takes central to their self-conception\n\n\nDoxastic Wronging (the demand not to wrong one another)\n\nTrue beliefs can wrong\n\nExample, base rates: “For instance, suppose that the swanky DC night club, the Cosmos Club, has nearly all black employees and nearly all white club members. A person looking for an employee, where employees and club members both wear tuxedoes, would be epistemically rational to believe that some particular black person is an employee, given the base rate at the Cosmos Club”\n\n\nThree conditions of doxastic wronging\n\nDirected Condition: belief wrongs a particular person, not just wrong in general\nBelief Itself Condition: holding the belief is what wrongs (not how the belief was formed or the actions that follow from the belief)\nContent Condition: content of the belief wrongs\n\n\nLovers stand in a privileged position — they know things not usually shared with other people. This position makes them especially vulnerable to doxastic wronging\n\n\n\nOn failures\nSource\nIf I had to distill the problems in failed relationships down to one idea, it would be our colossal failure to make the invisible visible, our failure to invest time and effort into developing awareness of what we otherwise might not notice in the busyness of daily life.\nThird Things\nLittle Prince author Antoine de Saint-Exupéry’s beautiful insistence that “love does not consist of gazing at each other, but in looking outward together in the same direction”\nThis concept of a ‘third thing’ comes from a poem written by Donald Hall after the passing of his wife Jane Kenyon who as also a poet: “We did not spend our days gazing into each other’s eyes. We did that gazing when we made love or when one of us was in trouble, but most of the time our gazes met and entwined as they looked at a third thing. Third things are essential to marriages, objects or practices or habits or arts or institutions or games or human beings that provide a site of joint rapture or contentment. Each member of a couple is separate; the two come together in double attention.”\nJohn mentions how he thinks that third things are essential not only to marriages but also to lots of other relationships too and I agree. Third things are points of “joint rapture”\nIt doesn’t so much matter what the thing is, only that it is.\nhttps://youtu.be/X86QGoGkf74?t=43\nAgainst ‘assessing’ love\nNussbaum on how over-reliance on intellect for clarity about love produces instead a kind of myopia:\n\nIntellect’s account of psychology lacks all sense of proportion and depth and importance… [Such a] cost-benefit analysis of the heart — the only comparative assessment of which intellect, by itself, is capable — is bound, Proust suggests, to miss differences of depth. Not only to miss them, but to impede their recognition. Cost-benefit analysis is a way of comforting oneself, of putting oneself in control by pretending that all losses can be made up by sufficient quantities of something else. This stratagem opposes the recognition of love — and, indeed, love itself.\n\nSee also:\nself-effacing-ends\nSocial Energy\nSource\n\nWe are constantly interpreting ourselves for others.\nThe more foreign the person, the more interpretation they require, and the more narrow the affordances we can take with them.\nI think this also can explain how larger groups often are more draining. We have to do a lot more cognitive work to hold several models of the world in our head. Large groups of foreign people tend to be the most exhausting.\nI like to think about energy expended interpreting other people as “shapes I have to hold my being in”. The further the shape from my normal patterns of movement, the more energy I have to expend\n"},"thoughts/functional-programming":{"title":"Functional Programming","links":["thoughts/Category-Theory"],"tags":["sapling","technical"],"content":"Terminology\n\nCategory Theory\nCategory Theory to Haskell\n\nObjects are types\nMorphisms are functions\nThings that take a type and return another type are type constructors\nThings that take a function and return another function are higher-order functions\nTypeclasses capture the fact that things are often defined for a ‘set’ of objects at once\n\nFunctor\n\nA ‘container’ of some sort, along with the ability to apply a function uniformly to every element in it\n\nEssentially a transformation between categories. Given categories C and D and a functor F:C→D\n\nF maps any object A∈C to F(A)∈D (the type constructor)\nF maps morphisms f:A→B∈C to F(f):F(A)→F(B)∈D (fmap). Importantly, this means that all functors must be generic over at least one parameter (e.g. Maybe and not Integer)\n\napplying fmap is sometimes called ‘lifting’ as it lifts a function from the normal context into the ‘f’ world\n\n\n\nclass Functor f where\n\t-- fmap maps morphisms\n\tfmap :: (a -&gt; b) -&gt; f a -&gt; f b\n \n\t-- applies a &#039;constant&#039; function to replace the values in a container\n\t(&lt;$) :: a -&gt; f b -&gt; f a\n\t-- default implementation\n\t(&lt;$) = fmap . const\nfmap takes a function which maps a value from a to b and applies it to a Functor f. Think of f as the container, (a -&gt; b) as the function that operates on the ‘inner’ values.\nMonad\nMonads are functors from a category A to that same category. A container for values that can be mapped over.\nThink of it like a context-specific environment. You need a function to transform things outside of it to things in it. You also need a function to manipulate stuff inside of that environment.\nA monad is a functor M:C→C along with two morphisms ∀X∈C\n\nunitX​:X→M(X) (return)\njoinX​:M(M(X))→M(X) (can be recovered from bind)\n\nclass Monad m where\n  -- join operation (optional, only one of bind or join need to be defined)\n  join :: m (m a) -&gt; m a\n \n  -- bind operation\n  -- takes an f :: (a -&gt; m b) and applies it to\n  -- the inner value a of m\n  (&gt;&gt;=)  :: m a -&gt; (a -&gt; m b) -&gt; m b\n \n  -- replaces m a with m b\n  (&gt;&gt;)   :: m a -&gt;  m b       -&gt; m b\n \n  -- constructs the simplest monad m using a\n  return ::   a               -&gt; m a\nMonad laws\nreturn a &gt;&gt;= k                  =  k a\nm        &gt;&gt;= return             =  m\nm        &gt;&gt;= (\\x -&gt; k x &gt;&gt;= h)  =  (m &gt;&gt;= k) &gt;&gt;= h\nLeft and Right Associativity\nAssociativity of an operator determines how operators are grouped in the absence of parentheses.\nFor the following examples, we consider a fictional operator ~\n\nAssociative: operations can be grouped arbitrarily (e.g. addition, order doesn’t matter)\nLeft-associative: operations are grouped left to right\n\na ~ b ~ c is interpreted as (a ~ b) ~ c\nExamples include\n\nFunction application operator\n\n\n\n\nRight-associative: operations are grouped right to left\n\na ~ b ~ c is interpreted as a ~ (b ~ c)\nExamples include\n\nVariable assignment (=)\nExponentiation (^)\n\n\n\n\nNon-associative: operations cannot be chained\n\nParser Combinators\n\nParser combinators are a technique for implementing parsers by defining them in terms of other parsers\n\nNotes on Chumsky\nWhere a and b are both parsers.\nParser Methods\n\njust(a) accepts a single string a\na.or(b) parse a, if a fails, try parsing b\na.choice(b,c,d...) try parsing b, c, d, return first one that succeeds\na.or_not() optionally parse a\na.ignore_then(b) ignore pattern a then parse b\na.then_ignore(b) parse a then ignore b\na.then(b) parse both a and b and return a tuple of (a,b)\na.padded() ignore whitespace around a\na.repeated().at_least(n) parse a at least n times\na.filter(fn) only accept a if fn(a) evaluates to true\n\nResult Methods\n\na.collect() turn results of a into an iterator\na.map(b) map results of a into type b\na.chain(b) concatenate results of parsers a and b into collection\na.copy(b) duplicate parser definition\na.flatten() flatten nested collection\na.to(b) marks result of a as type b\na.labelled(b) label result of a with b\na.end() indicate end of parser\n\nHaskell Syntax Quirks\n\n$ :: (a -&gt; b) -&gt; a -&gt; b is function application (adds implicit parentheses and makes it right associative instead of left associative)\n\nNormally, sort &quot;abc&quot; ++ &quot;def&quot; would be interpreted as (sort &quot;abc&quot;) ++ &quot;def&quot;\nIf we use the $ operator, we can do sort $ &quot;abc&quot; ++ &quot;def&quot; which is interpreted as sort (&quot;abc&quot; ++ &quot;def&quot;) as intended.\n\n\n. is function composition. Read the dot as the little dot in f∘g\n&lt;&gt; is a synonym for mappend :: Monoid m =&gt; m -&gt; m -&gt; m or the monoidal append\n&lt;$&gt; is a synonym for fmap :: (a -&gt; b) -&gt; f a -&gt; f b\n\nIntuitively like applying a function to a container\n\n\n&lt;*&gt; is like &lt;$&gt; but for wrapped functions (&lt;*&gt;) :: Applicative f =&gt; f (a -&gt; b) -&gt; f a -&gt; f b\n\nIntuitively like applying a function in a container to another container\n\n\nRemember that (&lt;$) and ($&gt;) point towards the value that will be kept\nvoid :: Functor f =&gt; f a -&gt; f () is implemented as void x = () &lt;$ x. Read as: whatever you give me, I will return the unit value\n"},"thoughts/fundamental-tradeoff":{"title":"Fundamental Tradeoff","links":[],"tags":["seed","CPSC340"],"content":"The fundamental tradeoff has two parts:\n\nHow small you can make the training error\nHow well training error approximates the test error.\n\nEtest​​=Eapprox​+Etrain​=(Etest​−Etrain​)+Etrain​​\nGenerally,\n\nTraining error goes down when the model gets more complicated, however\nApproximation error goes up when the model gets more complicated\n"},"thoughts/funding":{"title":"Funding","links":["thoughts/incentives","thoughts/web3"],"tags":["sapling"],"content":"\nRaising funding is just purchasing time to be together\n\nBounty Model\nPaying people set amounts for set tasks.\nBounty model is tough because\n\nImmediate shared context is required and onboarding new members to do novel work is hard (upfront costs are large)\nKeeping people engaged after the tasks are difficult (no long term sense of investment)\n\nHow can we keep individuals engaged with a project on longer term timescales?\n\nEncouraging individuals to create projects with the technology/ideas\nThrough hiring (contractually bound commitment)\n\nGrants\nFunding individuals/projects/organizations without the expectation of stake. Can be one-time or recurring.\nWhat are the incentives for people to provide grants then? Within web3, a lot of the reason is because of the obsession with profit. Donating to OSS is thereby a way to improve the long term return on their profit/investment. Is this still possible to incentivize grants when the technology/idea itself is not inherently of value (i.e. maybe only has derivative value)?"},"thoughts/game-design":{"title":"Game Design","links":["thoughts/games","thoughts/teaching","thoughts/play","thoughts/constructionist","thoughts/Mindstorms","thoughts/economics","thoughts/game-theory","thoughts/longevity","thoughts/friendship","thoughts/group-limits","thoughts/black-box","thoughts/feedback-loops","thoughts/incentives","thoughts/trust","thoughts/Goodhart's-Law","thoughts/positive-sum","thoughts/virtual-worlds"],"tags":["sapling","pattern"],"content":"Related: games\nGame Design and Teaching\nSource: Game designers vs. education researchers on unguided instruction by Andy Matuschak\n\nA mistake we made early on was looking primarily at metrics like retention and monetization. These simply don’t tell us much about what motivates players to play. If I were to build the game again, I would have implemented the togetherness metric for the very first private alpha.\n\nA lot of pedagogy covers the same questions as game design (especially tutorials): how much explicit guidance should a student/player get in an activity?\nThe ”vow of silence”: to preserve the joy of discovery, games should carefully structure their activities so that players will learn what they need through play. An approach very similar to the constructionist ones found in Mindstorms\nWe can teach stuff using mechanics and level design instead of words.\nThe goal is to give users/players the freedom to manage their own position relative to their own intellectual “sweet spot”\n\nEconomics and Game Theory\nSource: Prosocial economics for game design by Daniel Cook\n\nMultiplayer games can help build a player’s social support network. What would game design look like if our goals included reducing loneliness, decreasing toxicity and boosting a player’s positive connections with others?\n\nLoneliness\nThe loneliness epidemic is a real thing. It has been medically associated with mortality, depression, and more. In aggregate, chronic loneliness is estimated to shorten lifespan by an average of 15 years.\nAdvancing age also makes us more likely to be lonely. Especially as we work on longevity research, this is a pressing issue to try to solve.\nThe best games are designed to be played with friends.\nToxicity\n\nAt the root of much toxicity is the misdirection of our human need to belong\n\nWhen humans feel like they lack membership in healthy, eudaimonic organizations, they experience stress and seek to rapidly remedy the situation (e.g. lashing out at others in hopes that putting others down helps them rise in status).\nIn toxic systems every new user is potentially rewarded if they adopt toxic behaviours.\nProsocial Game Design\nThree main pitfalls\n\nPsychology: there are a lot of requirements to build friendships, like right sized groups of people, correct density, and engagement in mutually dependent reciprocal activities.\nLogistics: rigid human limits on how many relationships they can maintain and how long it takes them to form new ones (see: group limits)\nEconomics: games are built on an economic foundation of resources (creation, transformation, trade, and consumption). However, it is hard to get economic incentives to align with those of social behaviour. “In particular, many of the key elements required by the psychological and logistical aspects of friendship formation are systematically undervalued within common economic practices.”\n\nElements of the internal economy\n\nTokens: base units of quantity (can represent attention, time, or value). They act as goods, products, or currencies\nSources: operations that produce new tokens\nPools: stores of tokens\nSinks: operations that take tokens out of circulation\nTransforms: transform one type of token into another\n\nThen agents who perform operations:\n\nPlayers: human agents who trigger various transforms, sources, and sinks\nBlack Box: computer agents or systems which also trigger transforms, sources, and sinks. Often, players attempt to understand the triggers of this black box.\n\nFrom these, we can model various phenomena like feedback loops and ownership\nWe can then use these to influence incentive structures to balance games (e.g. improve a drop rate of a weapon to better increase the rate at which a key boss is defeated)\nExamples of internal economic systems:\n\nLeveling Systems: XP tokens\nItems: enables players to perform various transforms on the pool (e.g. a weapon is a token which depletes enemy health tokens to generate XP tokens)\nChat: budget of attentional and time resources\n\nProsocial Values\n\nFriendship: The formation and maintenance of healthy, meaningful friendship networks between players.\nThriving individuals: Individuals feel competence, volition, and relatedness, both for themselves and for their friends.\nAltruism: The promotion of activities that involve intrinsically motivated altruism and cooperation.\nPositive group norms: The spread and enforcement of shared altruistic social norms within and across groups.\nShared goals: The definition and adoption of shared group goals. Players work towards those goals via mutual interdependence, and achieve feelings of purpose and meaningfulness.\n\nAnti-values\n\nIndividual Toxicity: poorly socialized individuals resort to antisocial behaviours in an attempt to put themselves above the group.\nGroup Toxicity: intergroup friction results in unhealthy interactions\nLoneliness: sparse relationship networks, we feel this when our social support network fails. Loneliness tells us that our current social situation is untenable long term and we should seek out connection with others.\n\nMeasuring Trust\nTrust is an internal factor that cannot be measured directly so instead we need to rely on proxies.\n\nPairwise bonds: There are two different (asymmetric) bonds for any given pair of player; the bond player A to player B and the bond from player B to player A.\nActive time spent together: One of the easiest symmetric bond proxies is simply tracking if players are in the same area together.\nSuccess together in high trust situations\nTime spent talking positively to each other\n\nHowever, this ‘trust value’ should not be public for the sake of avoiding converting it into a metric to optimize for: Goodhart’s Law. Doing so may transform relationships into ones which are transactional in nature with clear extrinsic motivators in the form of your willingness to make that number go up or down. (This is also one of the reasons why ‘likes’ in social media end up being a source of toxicity and in general a very poor practice.)\nOne also needs to keep in mind that trust differs across social contexts\nFor new players, one of the scariest things is requirement to engage in specialized, high coordination group performances. Building towards high trust using a ladder:\n\nPositive Sum Resources\nAn economy of zero sum resources is a world of scarcity. The challenge economics attempts to solve is how we might split up these limited resources in an efficient fashion.\nIn virtual worlds, we can make almost any resource positive sum. When a monster drops loot for one player, it can also drop loot for any other player that did damage.\nHowever, truly abundant worlds are not super fun to play in (constraints are what make games interesting). How can we design for infinite source and imbalanced economies?\n\nPer players caps: cap number of harvestable positive sum resources per player\nPer group caps: cap total number of harvestable items per group of players\nTransaction costs: can prevent global pooling with large transaction taxes. This encourages local resource use\nAppropriate sinks: find ways to drain resources out of the world\n"},"thoughts/game-theory":{"title":"Game Theory","links":["thoughts/zero-sum","thoughts/positive-sum","thoughts/sequential-games","thoughts/Decisions-under-ignorance","thoughts/Decisions-under-risk","thoughts/Decision-theory","thoughts/Nash-equilibrium","thoughts/trust","thoughts/Evolutionary-game-theory","thoughts/catch-22","thoughts/game-design","thoughts/evaporative-cooling"],"tags":["sapling","PHIL321A"],"content":"Modelling decisions where the outcome partially depends upon choices made by other rational agents. Assumes:\n\nIndividual Rationality: each player maximizes their own utility and knows the full game tree/table\nCommon Knowledge of Rationality: each player knows that all other players are rational\n\nTwo ways to look at game theory\n\nNormative tool: how to make rational choices\nDescriptive/explanatory tool: help explain social behaviour\n\nSome social practices don’t appear rational in normative game theory but can be explained using descriptive game theory\nSome distinctions:\n\nZero-sum vs non-zero sum games (e.g. positive sum)\n\nIn a zero sum game, the payoffs always sum to 0\n\n\nSimultaneous vs sequential games\n\nIn simultaneous games, all players choose independently at the same time without knowing what other players will do\n\n\nPerfect information vs imperfect information games\nSymmetric vs non-symmetric games\n\nIn symmetric games, all actors have the same set of actions to choose from\n\n\nTwo-person vs n-person games\nCooperative vs non-cooperative games\n\nIn cooperative games, players get to choose the outcome together (i.e. form binding agreements in the form of ‘if I do A, then you must do B’)\n\n\nNon-iterated vs iterated games\n\nA non-iterated game is only played once. Iterated games can be played several times.\nA “supergame” is a specified # of iterations of a game. Strategies:\n\nC: Always cooperate\nD: Always defect\nTT: tit-for-tat\n\nCooperate on round 1 and then take same action as opponent in previous round\n\n\n\n\nUpshot is that the case for cooperation (e.g. TT or C) is strong when the number of games is not known in advance\n\nWe can always construct a strategy where D or some version of TT dominates when we know how many games there will be\n\n\n\n\nFinite vs infinite games\n\nNot to be confused with the James Carse definition of infinite games\n\n\n\nRisk, Ignorance, and Uncertainty\n\nDecisions under ignorance (DUI): the agent is ignorant of all probabilities\nDecisions under risk (DUR): the probability of each outcome is known\n\nWe can maximize expected value to figure out what decision to make\nGenerally need to know utility and risk values\n\n\nDecisions under uncertainty (DUU): includes risk, ignorance, and intermediate cases\n\nGame Tables\nUsed to represent simultaneous games\nSimilar to decision tables in decision theory but the column is the action of the other agent, and cells are the outcomes for each agent represented as a tuple of numbers.\ne.g. Stag Hunt where A and B are hunters and the numbers represent amount of food acquired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nABA25,250,5B5,05,5\n(In zero sum games, it is sufficient to only represent the utility of each of the Rows)\n\nThe solution is a set of “profile” of choices that are rational for each agent\nEach cell is a profile which leads to an outcome for both players\n\nSolutions to games\nDominance and admissibility\nS1 dominates S2 iff:\n\nS1 is at least as good as S2 regardless of what other players do.\nS1 is superior to S2 in at least one case.\n\nIf there is a dominant strategy, it will be part of the solution profile. If there is no dominant strategy, we eliminate all dominated strategies as inadmissible.\nEquilibrium\nThis rule subsumes the rules for Dominance and Admissibility. Any solution using Dominance or Admissibility is also a Nash equilibrium.\nMinimax condition: For two-person zero-sum games in particular, a pair of (pure) strategies is an equilibrium if and only if its payoff is the minimum on its row and the maximum on its column\nThere may not always be an equilibrium with pure strategies. However,\nMixed Strategies\nIn a two-person zero-sum game with mixed strategies, there is always an equilibirum. Moreover all equilibria have the same (expected) value\nIf a player has options R1​,…,Rn​, then a mixed strategy selecting Ri​ with probability pi​ written as [p1​R1​,…,pn​Rn​] where the probabilities add up to 1.\n[pA,(1−p)B] where 0≤p≤1. This means: choose A with probability p and B with probability 1−p.\nGiven a table in Standard Form:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nC1C2R1abR2cd\nSteps:\n\nWe calculate the EU for the actor represented by Row, EU(Row) = p(qa+(1−q)b)+(1−p)(qc+(1−q)d)\nWe find a p such that EU(Row) only depends on p (by making the coefficient for q zero). To do this normally:\n\np=(d−c)/(a−b−c+d)\nq=(d−b)/(a−b−c+d)\nIf a−b−c+d=0 then there are pure-strategy equilibria so use those\n\n\nWe do the same but with q\nNow set EU(Row) to 0. We essentially want to make EU(Row) constant so there is no incentive to switch\n\nTrust and Game Theory\nSource: The Evolution of Trust by Nicky Case\nFor trust to evolve:\n\nRepeat interactions must happen. Trust keeps a relationship going, but you need the knowledge of possible future interactions before trust can evolve\nPossible win-win situations. This must be a positive sum game where both players can be better off\nLow miscommunication. High-tolerance players are possible but if the level of miscommunication is too high (low signal to noise ratio), trust breaks down. Even in low miscommunication situations, it pays to be more forgiving\n\nIn the short run, the rules of the game define what the players do. In the long run, players define the rules. Incentive for us to build the environment and rules we want to play in.\nUnfortunately, modern day social media means we have less ‘close-friends’ and thus repeat interactions than ever. With algorithmic news feeds, miscommunication breeds and win-win situations become scarce.\nSee also: Evolutionary game theory\nApplied Game Theory\nGame theory feels hard to apply to the real world due to properties like the Collingridge Dilemma\nGames, however, are a great test bed for a lot of game theory given how information rich they are. See game design\nBartle Taxonomy of Player Types\nSee also Geeks, Mops, and sociopaths\nA classification of types of actors in video games based on character theory\n\n\nAchievers: prefer to optimize metrics or other concrete measurements of ‘succeeding’ in a game\n\nSingle-player appeal: Every game that can be “beaten” are appealing to Achievers.\nMulti-player appeal: Look to socializers for praise, opportunities to show off their skill and hold elite status to others\n\n\nExplorers: tendency to dig around, explore, and immerse themselves in the lore and game world\n\nSingle-player appeal: flock to games which reward close attention\nMulti-player appeal: tire quickly of MMORPGs when they finish the content\n\n\nSocializers: play the game for the social aspect. The game is the tool they use to meet others in-game or outside of it\n\nSingle-player appeal: rich interactions with NPCs and/or strong online community (e.g. forums and streams)\nMulti-player appeal: main purpose for the socializer, enjoys mechanics for socializing like guilds, chats, etc.\n\n\nKillers: thrive on competition from others and feeling superior\n\nSingle-player appeal: want to achieve top ranks or beat other speedrunners\nMulti-player appeal: friendly competitive spirit or ‘thrill of the hunt’\n\n\n"},"thoughts/game-writing":{"title":"Game Writing","links":["thoughts/games","thoughts/creative-writing","thoughts/game-design","thoughts/Games-Agency-as-Art","thoughts/pace-layers"],"tags":["seed"],"content":"See also: games, creative writing, game design\nLoops and Episodes\n\nGames are interesting to write for because of the concept of agential distance (see: Games Agency as Art)\n\nThe agency of the player shapes their own experience of the game\n\n\nTension\n\nWhen the playing strives to fulfil and objective or goal and the game pushes back\nExternal to the narrative\n\n\nConflict\n\nWhen a character strives to fulfil an objective or goal and an antagonist pushes back\nInternal to the narrative\n\n\nGameplay loops (called ludic loops by Nguyen)\n\nA cycle in which the player uses the core mechanics to resolve tension\nThese may occur at different pace layers and serve to anchor the player in the game\n\nThe narrative should support the gameplay loops to be effective\n\n\n\n\nEpisodes\n\nCan be split into more manageable pieces\nCan be released episodically (e.g. DELTARUNE)\nEstablishes patterns (and allows you to break these patterns for exaggerated effect)\n\n\nExample story\n\nProtagonist attends a very elite preparatory school\nAll his friends are incredibly smart\nHe’s the highest scoring kid in the class but… the secret is he has been stealing the answer keys from the teachers lounge since he’s started school there. As such, he hasn’t actually learned anything in school, just how to steal really well\nAll previous episodes have been solo: player versus environment\n\nSecurity cameras, guards, alarms, etc.\n\n\nEpisode: this time when stealing the answer key, he sees his best friend also trying to steal the answers. This leads to a reckoning about whether all his friends are actually smart or actually faking it like him\n\nThis adds a new element: other NPC characters who are trying to reach the answer key before you do\nAdds time pressure and extra dynamism\n\n\nLarger commentary is about whether these ‘preparatory’ institutions really prepare students for the real world or just to get really good at these ‘vanity’ metrics like GPA that schools always portray\n\n\n"},"thoughts/games":{"title":"Games","links":["thoughts/game-theory","thoughts/game-design","thoughts/The-Grasshopper,-Games,-Life-and-Utopia","thoughts/Moving-Castles","thoughts/Gall's-law","thoughts/complexity","thoughts/peer-to-peer","thoughts/interaction-design","thoughts/Games-Agency-as-Art","thoughts/game-writing"],"tags":["sapling"],"content":"Related: game theory, game design, The Grasshopper, Games, Life and Utopia, Moving Castles\nGames are interesting when the underlying rules are simple (see: Gall’s Law, low Kolmogorov Complexity) but exhibit complex behaviour.\nThe more I think about it the more I think I eventually want to build games. All of my p2p infrastructure work now is to build up the tools possible to explore that space of potential games. Games as explorations in interaction design.\nSee: Games Agency as Art, game writing"},"thoughts/garbage-in-garbage-out":{"title":"Garbage in, garbage out","links":[],"tags":["seed"],"content":"There’s a timeless saying “garbage in, garbage out” in the field of Computer Science which essentially states that bad data or bad input will produce an output that’s of equal quality. This holds true for almost all the tech we use today, from trading algorithms to search results. If what we put into the system is inherently unclear or flawed, then the output will also give back something that’s ‘wrong’ or doesn’t align with our objectives."},"thoughts/gate-keeping":{"title":"Gate Keeping","links":["thoughts/information","thoughts/Internet","thoughts/censorship","thoughts/bias","thoughts/cozy-software"],"tags":["seed"],"content":"Mass media communications theory\n\nOrigins in social psychology\nGatekeeper makes decisions about what should be passed on and to whom\nA form of social control\n\nDigital gate keeping\n\nPlatforms gate keep by managing information flows in many different ways\nNon-human agents participate through algorithmic filtering (e.g. upvoting content)\nSee centralization of Internet through search\n\nDifferences from censorship\n\nCensorship is all about blocking\nGate keeping is more about access and quality control\n\nPersonalization as a form of gatekeeping\n\nInformation flow and filtering is based on individual and/or group profiles\nBozdag (2013): personalization leads to more bias rather than less\nCass Sunstein: “information cocoons”\nServes a sort of invisible auto propaganda, indoctrinating us with our own ideas, amplifying our desire for things that are familiar\n\nPotential dangers of personal software"},"thoughts/generational-learning":{"title":"Generational Learning","links":["thoughts/Theory-of-Niche-Construction","thoughts/Extended-Mind-Hypothesis","thoughts/notation"],"tags":["sapling"],"content":"Related: Theory of Niche Construction and Extended Mind Hypothesis\nIntergenerational social learning\nIn intergenerational transmission of ecological and technical expertise, parental acts bias the environment explored by trial and error learning.\nParenting means that trials are guided — social/observational learning. Advantaged through:\n\naid of tools that initially chosen by others\naccess to raw materials in various stages of preparation\n\nNatural bargain\n\nSkilled practitioners ease their own burdens by having apprentices do low to medium skilled work.\nApprentices do grunt work from the perspective of the skilled, but for the beginner, it builds basic skills.\n\nOver both evolutionary and developmental time frames, inner mechanisms have coevolved with and adapted to this rich environment of intergenerational learning. Language and arithmetical notation enhance our capacity to think"},"thoughts/generative-models":{"title":"Generative Models","links":["thoughts/noise"],"tags":["seed","CPSC340"],"content":"Given data, we want to make more data that look like it\nLast 10 years have seen a variety of new deep generative models:\n\nVariational autoencoders (VAEs)\nGenerative adversarial networks (GANs)\nNormalizing flows\nDiffusion models\n\nTake training images, and add noise to them in a sequence of steps.\nUntil the image basically looks like random noise.\nTrain neural network to reverse those steps.\nGenerate a new image by starting from random noise and applying the network\n\n\nText-guided Diffusion\n\nA Diffusion Model starts from randomly sampled Gaussian noise so there is no way to guide this process to generate specific images. We can augment this process with textual embeddings\nGenerate the image and text encoding of each of the image-caption pairs\nTrains to maximize the cosine similarity between image-caption pairs\nAfter this step is finished, the model is frozen\n\n\n\n"},"thoughts/git":{"title":"git","links":["thoughts/content-addressed-storage","thoughts/Merkle-DAG","thoughts/bitemporal"],"tags":["seed","technical"],"content":"git internals. Heavily inspired by The Git Parable, a talk by Tom Preston-Werner\nGit Internals\nSnapshots\nWhat if you could take snapshots of your codebase at any time and resurrect that code on demand?\nThe simplest version of this is something you may have done with Photoshop files.\n\nYou start your work in a directory project\nAs you make changes, you want to make a snapshot so you make a copy of your entire working folder and rename it project-version-1\nAfter the next chunk of work, you make another copy and rename it project-version-2\nIn each folder, you include a message text file that describes your changes\nTo go back to a previous version, you just delete your current working version of rename your snapshot project\n\nBranches\nWell now we run into a new problem. What if we roll back to a previous version of the code and then make some more changes?\nThat is, we create a new snapshot that is not a direct descendent of the preceding snapshot.\nOur previous snapshot system only worked for a linear system of changes. How might we handle having multiple points where there are active changes being made?\nBy looking at your code history as a tree, solving the problem of ancestry becomes trivial. All you need to do is include the name of the parent snapshot in the message file you write for each snapshot.\nThis is also why there is a lot of arboreal terminology in git: branches, trunk, etc.\nNow… we run into another problem. How should we name our snapshots now that we can’t use a linear system of numbers?\nWell, we can actually just name each branch and then list branch-name: snapshot-name pairs that represent the tips of branches. Let’s store this in a file called branches:\nmain: project\ntmp-fix: project-version-2-fix\n\nTo switch to a named branch you need only look up the snapshot for the corresponding name from this file.\nTo ensure this file is always up to date, creating an additional snapshot on a branch means we should update the entry in branches that corresponds to our current branch to the latest snapshot.\nBranches as Pointers\nAfter using branches for a while you notice that they can serve two purposes. First, they can act as movable pointers to snapshots so that you can keep track of the branch tips. Second, they can be pointed at a single snapshot and never move.\nMixing both of these uses into a single file feels messy. Both types are pointers to snapshots, but one moves and one doesn’t.\nWe can create another file called tags to contain static pointers to snapshots.\nCollaboration\nNow, imagine you are working with a friend on this project. You give them a copy of all of your snapshots, branches, and tags. Your friend happens to go offline for a while and when you meet up again, you both realize you’ve been using the same naming system for your snaphsots! Now, you both have snapshots called project-version-23 and project-version-24 that have different contents. Also, we have no idea who authored which snapshot!\nThere are two things we can do to solve this:\n\nSnapshot messages will henceforth contain author name and email.\nSnapshots will no longer be named with simple numbers. Instead, you’ll use the contents of the message file to produce a hash. This hash will be guaranteed to be unique to the snapshot since no two messages will ever have the same date, message, parent, and author. Let’s use the SHA-1 hash algorithm\n\nNice! Now, we can merge our snapshots (and thus our working trees) without conflicts. Because of how we hash our snapshots to get their names, we know that any two snapshots with the same name actually have the same content too.\nMerges\nThis is what we normally call a merge commit!\nHowever, while constructing the snapshot message for the merge, you realize that this snapshot is special. Instead of just a single parent, this merge snapshot has two parents.\nEliminating Duplication\nWe can use content addressed storage and Merkle-DAGs!\n\nCreate a directory named objects\nGo to the most deeply nested directory in the snapshot\nCreate a temp working file\nFor each file in the directory\n\nCalculate the hash of the contents\nAdd an entry to the temp working file: blob {hash} {filename}\nCopy the file into the objects directory and rename it to the hash\n\n\nAfterwards, find hash of the temp working file and place it in the objects directory (also using the hash as the name). This represents the folder we just traversed\nMove up on directory and repeat starting from step 3\n\nWhen we come across the folder we just traversed, we add the following entry to the temp working file: tree {hash} {dir name}\n\n\nOnce this has been accomplished for every directory and file in the snapshot, you have a single root directory object file and its corresponding SHA1. We record this in the commit message\n\nThus, we avoid storing duplicate files!\nCompression\nText can be very efficiently compressed using something like the LZW or DEFLATE compression algorithms. If you compress every blob before computing its SHA1 and saving it to disk you can reduce the total storage size of the project history significantly.\nHandy Commands\nThink about git like a file time machine — it allows you to traverse and manage an entire multiverse of files (see: bitemporal)\n\nUnstaged files: anything you’ve done to your current branch of the world that hasn’t been staged or committed\nStaged files: things that you’ve marked as things you want to commit to a snapshot\nA commit: a snapshot in time\nA branch: a specific branch of the multiverse\nA repository: the entire multiverse\n\nHere are some verbs and commands you’ll find yourself using a lot locally!\n\nVerbs\n\ngit add {PATH}: add things to the staging area. Matches whatever directory/file you pass in as the path. For example, using git add . adds everything in the current directory. git add tests/math adds everything in the tests/math folder. git add &#039;*.js&#039; adds all JavaScript files (note the quotes here!)\ngit commit -m {MESSAGE}: save a snapshot of everything in the staging area with a given message\ngit commit -am {MESSAGE}: amend the previous commit with a new message (if you had a typo, for example)\ngit show {REFERENCE}: show all changes in a given commit\ngit reset {REFERENCE}: set our current HEAD to the specified commit\n\nNormally, working directly is not affected but staging is updated to match commit\n--soft keeps the staging area\n--hard updates both the staging and working directory to match the commit (this is a hard reset)\nUse as a local reset button\n\n\ngit status: Show how many commits ahead/behind we are, as well as staged, unstaged, and untracked files\ngit reset {PATH}: unstage specific files/folders\ngit checkout {BRANCH OR REFERENCE}: switch branches to the given branch or reference\ngit reflog: show a list of commits that moved the tips of branches\n\nThis is useful for recovering deleted branches and hard resets!\n\n\ngit stash: stash away everything in the staging area\n\ngit stash pop: take out the changes from the stash and restore it back into the staging area\nGood for moving changes across branches (oops, I made my changes on main instead of my feature branch!)\n\n\n\n\nNouns (can be used anywhere where git expects a reference)\n\nHEAD: current tip of the branch\nHEAD~2: go 2 commits back from HEAD\n25c3be7b5: go to the commit with hash 25c3be7b5\nHEAD@{2}: go back to where HEAD was 2 moves ago\nmain@{one.week.ago}: go back to where main branch was a week ago\n\n\n\nFor collaboration and online repositories:\n\ngit push {REMOTE} {BRANCH}: push your local commits on BRANCH to a remote repository called REMOTE\n\nTo add a new remote: git remote add {REMOTE} {URL}\n\n\ngit pull {REMOTE} {BRANCH}: pull remote commits from BRANCH onto your current local branch\ngit merge {BRANCH}: merge BRANCH into our current branch\ngit log --oneline --decorate --color --graph: pretty print history\n"},"thoughts/gossip":{"title":"Gossip","links":["thoughts/message-broadcast","thoughts/transitive-closure","thoughts/Network-Theory"],"tags":["seed"],"content":"\nUse random selection of nodes to pass on information to ensure it reaches all the nodes in the cluster without flooding the network\n\nSee also: message broadcast\nWhen we receive a message from a client, we want all the other servers to know about that message eventually.\nThe naive solution is to just send the message to all your peers. However, this doesn’t scale super well as the number of nodes n goes up.\nCheckout Hashicorp’s convergence simulator\nTopology\nNormally, each node tracks its topology (or local neighbourhood). This is either calculated dynamically or given on initialization.\nThen, we broadcast to only our peers (as defined by the topology). Then, through transitive closures, the message will make its way to every node if all nodes form a connected subgraph\nTwo approaches\n\nSync: immediately send updates to peers\n\nMay clog network if updates are frequent\n\n\nAsync: every now and then send updates to peers (I have these values, what values do you have?)\n\nMay take a long time for messages to traverse the network if the path between nodes is long and latency is a non-negligible factor\nCan be done using a Bloom filter\n\n\n\nStochastic\nSource\nGossip Dissemination is based on the mathematical models from epidemiology (see: Network Theory)\nEach node selects a random node to pass the information it has. This is done at a regular interval, say every 1 second. Each time, a random node is selected to pass on the information.\nOptimizing state transfer\nWe tag each value with a version (sequence number).\n\nWhen transferring state\n\nIt diffs the values in the message and the values we have locally\nIf entries are in both, we keep the one with the higher version (more recent)\nWhatever entries are in the message that we don’t have, we add\nWhatever entries that we have that weren’t in the message, we return as a response\n\n\nCassandra uses a three-way handshake\nCockroachDB maintains state for each connected node. For each connection, it maintains the last version sent to that node, and the version received from that node. This is so that it can send ‘state since the last sent version’ and ask for ‘state from the last received version’\nCan also use Bloom filters\nGossip messages can also act as heartbeats to detect whether something is down\n"},"thoughts/governance":{"title":"Governance","links":["thoughts/Moderation","thoughts/digital-commons","thoughts/writing","thoughts/games","thoughts/move-fast-and-break-things","thoughts/Plato's-Ship-of-State","thoughts/Goodhart's-Law","thoughts/Gall's-law","thoughts/Internet"],"tags":["sapling"],"content":"Related: Moderation\nCommunity Governance\nHomeostasis of communities\n\nImportant to have a healthy mix of internal and external influence\n\nHow do the people within the community conform to the community? Or to external labels of people in the community\n\n\nSecluded vs public: digital commons\n\nPreserving the sanctity of natural interactions\n\nNot monetizing human interaction\nScreen for value alignment\n\nInterviews (two-way vibe check)\nPublishing list of values\nResonance with a large body of writing\n\n\n\nThe Art of Online Governance\nSource: The Yak Online Governance Primer\n\nSee also: Bartle Taxonomy of Player Types\nFour Online Governance Regimes:\n\nHobbesian: Governance ideas responding to wild defaults and low alignment that attempt to foster progress despite conflict and chaos\n\nGenerally ok for early years of a subculture or a social network but fails when it scales or becomes popular (e.g. external influence is too strong)\n\n\nGaia: Governance ideas responding to wild defaults and high alignment that attempt to foster progress using the patterns and harmonies of nature\n\nas a warning, see The Tyranny of Structurelessness by Jo Freeman\n\n\nMuddler: Governance ideas responding to structured management with low alignment; commonly known as “herding cats,” attempts to foster progress by structuring local activities\nCitadel: Governance ideas that assume a consciously architected and structured context, usually with top-down alignment forces\n\nTraps\n\nTechno Utopia trap\n\nan attempt ‘to build it better’ from tabula rasa (very ‘move fast and break things’)\n“While this blend of tabula rasa thinking and romantic cherrypicking of reference points can occasionally lead to refreshing new insights and much-needed shedding of historical baggage, it can also lead to naive idealism and utopian, wishful thinking, and governance attempts that fail through inevitable disillusionment.”\n\n\nGrand Old Institution trap\n\nopposite of the Techno Utopia trap\nthose involved with long-standing research/scholarship in governance and management (e.g. from academia) often approach the question as though the context of new digital tools, information ubiquity, and unusual organizing aims changes almost nothing\n\n\n\nTuring-Complete Governance\nSource: Turing-Complete Governance by Saffron Huang\n\n“Auctioning off or loaning away voting power to those uninvested in quality governance, or want to steer governance towards selfish ends misaligned with the community, is generally a terrible idea.” (see: Plato’s Ship of State)\nSkeptical about normalizing airdrops as reward for early investors\n\n“Imagine if we had airdrops for more decisions relevant to the off-chain world, like participating in local council”\nWouldn’t this just lead to optimizing for airdrops? This happened with ENS (Goodhart’s Law)\n\n\n“Turing-Complete governance shouldn’t be advocated as a way to make the most complex algorithm possible. The point is that we can have any inputs we want, and any computational transformation of those inputs to produce some output. Vitalik has pointed out that simple social systems with more predictable outcomes and understandable processes for tweaking the outcome are more desirable” (see: Gall’s law)\n“Imagine if every application on the Internet by default had a zero-downtime, publicly exposed API”\n"},"thoughts/gradient-descent":{"title":"Gradient descent","links":["thoughts/convex"],"tags":["seed","CPSC340"],"content":"When we minimize or maximize a function, we call it optimization.\nGradient descent is essentially an iterative optimization algorithm that takes a guess and refines it using the gradient to make a better guess.\nIf the objective function is a convex function, then it will converge to a global optimum.\nGradient descent finds critical point of differentiable function. Which can be faster than normal equations for large ‘d’ values. It takes O(nd) per iteration so O(tnd) for t iterations.\nFormally,\n\nWe start with an initial guess: w0\nWe repeatedly refine the guess: wt+1=wt−αt∇f(wt)\n\nα here is the learning rate.\nWe move in the negative gradient direction as given some parameters w the direction of largest decrease is −∇f(w)\n\n\nWe stop when ∥∇f(wt)∥≤ϵ\n\nStochastic Gradient Descent (SGD)\nHowever, the runtime of each iteration of regular gradient descent is proportional to n. This is problematic when we have large training sets!\nInstead of computing the gradient over all training examples, we do it for some random training example i. Intuition is that on average, the algorithm will head in the right direction\nWe can use it when minimizing averages (so all regression losses except brittle regression)\nWhen we get close enough to a local minima w∗, we enter a region of confusion where some ∇fi​(w) point towards w∗ and others don’t. This confusion region is captured by the variance of the gradients\n\nIf the variance is 0, every step goes in the right direction (outside region of confusion)\nIf the variance is small, most steps go in the right direction (just inside region of confusion)\nIf the variance is large, many steps point in the wrong direction (middle of region of confusion)\n\nBasically, for a fixed stepsize, SGD makes progress until the variance is too large.\n\nDecreasing Step Size\nIf we decrease the step size as we keep training, we can still converge to a stationary point as long as:\n∑t=1∞​αt∑t=1∞​(αt)2​=0\nA common option is to use αt=O(t​1​)\nMinibatches\nWe can train on a ‘mini-batch’ Bt of examples. Radius of region of confusion is inversely proportional to Bt\nEarly Stopping\nNormally, we stop GD when gradient is close to zero. However, we never know this when doing SGD (as we cannot see the full gradient). We just stop early if the validation set error is not improving (this also reduces overfitting)"},"thoughts/group-limits":{"title":"Group Limits","links":["thoughts/evaporative-cooling","thoughts/Dunbar's-Number","posts/context-collapse","thoughts/niche-at-scale","posts/images/framing/context-collapse.png","thoughts/digital-commons","thoughts/tribe-flourishing","thoughts/Vanilla-Ice-Cream-effect","thoughts/research-institutions","thoughts/communities","thoughts/independent-research","thoughts/Unrepeatable-Miracle-of-Silicon-Valley","thoughts/friendship","thoughts/Moderation","thoughts/incentives"],"tags":["sapling"],"content":"Related to the evaporative cooling effect. Great example of a group limit is Dunbar’s Number\nThe way to ‘scale intimacy,’ if possible, is to scale horizontally rather than vertically. Create many groups of smaller interest groups (warrens) rather than large groups (plazas). This mitigates negative impacts of context collapse\nSide: warrens as a way to be niche at scale\nSocial Capacity\nIs there an inverse relationship between number of social connections and their depth? If you multiply them, do you get a person’s social capacity?\n\n“The amount of social capital you have is pretty fixed,” Dunbar said. “It involves time investment. If you garner connections with more people, you end up distributing your fixed amount of social capital more thinly so the average capital per person is lower.”\n\nBecause of the increased number of ‘social connections’ people have today, is context collapse inevitable?\n“Yet, when researchers tried to determine whether virtual networks increase our strong ties as well as our weak ones (the ones that Hansen had focussed on), they found that, for now, the essential Dunbar’s Number, a hundred and fifty, has remained constant.” Is the Dunbar number a metric for social capacity?\nDoes social capacity differ across different types of social interactions? e.g. One-to-one vs One-to-many vs Many-to-many (relevant: digital commons)\n“Modern Loneliness” by Lauv covers a lot of similar topics\nRelated: tribe flourishing, Dunbar’s Number\nResearch\nI feel there are dangers of large groups, especially within research. Larger groups lead to more bureaucracy and being ok with the average — an effect of too many cooks in the kitchen. This leads to ideas that are at the middle of the distribution (Vanilla Ice Cream effect) and tends to discourage more radical, out-of-distribution thinking and research.\nRelated: a new DARPA, community perception, independent research\nScalability\nWhy do we feel the need to find the billion-person version of communities? Is this just the Silicon Valley question of scale?\nIs human intimacy scalable? I think not.\nWhat if the events were universally accessible but instead of serving everyone, serve those who opt-in to participate more? Cultivation &gt; moderation\nIf we remove misaligned incentives and less people come, then tbh that’s better, that eliminates the people you didn’t want in the first place"},"thoughts/hackathons":{"title":"Hackathons","links":["posts/hackathons"],"tags":["sapling"],"content":"As Steven Levy defines it, ‘hacks’ were projects undertaken by these hackers not to fulfill any sort of end goal other than to take pleasure from working on it. A hackathon is an event where a bunch of people gather to hack on things together for a short duration (usually a weekend).\nMore thoughts on why hackathons have gone downhill and what’s next.\nDiscussion\n\n“I want hackathons to embody a starting point not a set timeframe”\n\nHow do we push for hackathons to be the ‘start’ of the journey rather than saying its the entirety of a short, not super comprehensive event?\nIn other words, how can we make hackathons events for people to dip toes into ideas and concepts without having them need to be ‘judged’ for polish or ability to fully solve a problem?\nQuestions to think about:\n\nhow do we continue projects beyond their initial timeframe\nshould hackathons even focus on maintainable projects when they require so much context?\nwhat does “hacking racism” mean? what about social good hackathons in general? how realistic is it so try and solve some sort of large societal issue in the span of a few days?\n\nHackathons as co-optation ritual\nSource: Hackathons as Co-optation Ritual: Socializing Workers and Institutionalizing Innovation in the “New” Economy\n[Hackathons] reshape unpaid and precarious work. Writing code and building apps for free becomes an extraordinary opportunity, a ritual of ecstatic labour, and a collective imaginary for fictional expectations of innovation that benefits all.\nMany prototypes that are developed during hackathons, even winning projects, are not really usable.\n[Hackathons] translate the values of longstanding hacker subculture into new work norms… using rituals of play and pleasure to co-opt a wide range of talent into the service of corporations and the state without offering participants full-time jobs. As one participant says:\n\n“[Hackathons] are also perfect for my creative spur. I work as a corporate consultant and sometimes miss the research thing, the building of things. Hackathons allow me to do this.”\n\nHackathons are a multi-site mechanism for both “manufacturing” innovation and “manufacturing consent”\nThe hackathon acts a a multimodal platform for building social capital as well as facilitating and institutionalizing innovation. Yet the hackathon’s corporate sponsors are front and center in control of the event.\nSponsors fuel the romance of digital innovation by appealing to hackers’ apsiration to be multi-dimensional agents of chance. “Doers, makers &amp; disruptors,” one announcement goes. Sponsors don’t really think hackathons are a good recruitment tool. Performance on a hackathon team doesn’t give an adequate indication of ability to work on a “real” team. Hackathons are more important for companies because they need to maintain their ‘cool’ profile."},"thoughts/hash-function":{"title":"Hash function","links":[],"tags":["seed"],"content":"\nA hash function is any function that can be used to map data of arbitrary size to fixed-size values.\n\nProperties\n\nOrder should matter, should be very unlikely for two messages two have a hash collision\nExamples of good hash functions\n\nMD5: compute a 128-bit message digest in a 4-step process\nSHA-1: US NIST standard, 160-bit digest\nSHA-256 and SHA-512 are more secure\n\n\n\nHomomorphic Hashes\nbromberg_sl2 is hash function that provides a monoid homomorphism\nThis means there is a cheap operation * such that given strings s1 and s2, H(s1 ++ s2) = H(s1) * H(s2)"},"thoughts/hedonic-treadmill":{"title":"Hedonic Treadmill","links":[],"tags":["sapling"],"content":"What does it mean to ‘improve’ if all we do is take the new baseline as just that — a baseline? Everything relative to that baseline would be relatively the same.\n\n“Improvement in my life — should I not desire it or should I not be in need of improvement? I really want to improve. But it’s precisely because I yearn for it that I’m afraid of remedies that are worse than the disease.” (Van Gogh, 1879)\n\nTable Selection\nFrom Table Selection by Nat Eliason\nIn poker, there’s an important concept called “table selection.” Your success is not just determined by how good you are but also by the table you choose to play at.\nYou can’t win as much money if you sit at a table with small blinds. You’ll get cleaned out if you sit at a table with players much better than you. To have the best result, you need to find a table where the stakes are high enough to be worth playing and where you have a chance of winning against the other players.\nThe “keeping up with the Joneses” effect is a symptom of table selection. If you compare yourself to your neighbors or people who are “near” you in whatever you index your life on, you will find countless ways to make yourself feel insignificant or behind."},"thoughts/hermeneutical-injustice":{"title":"Hermeneutical injustice","links":["thoughts/epistemic-injustice","thoughts/language","thoughts/identity","thoughts/ontology","thoughts/epistemology","thoughts/interdependence","thoughts/terminology","thoughts/Design-Justice","thoughts/double-consciousness"],"tags":["sapling","PHIL240A"],"content":"\nIt is the injustice of having some significant area of one’s social experience obscured from collective understanding owing to a structural identity prejudice in the collective hermeneutical resource\n\nHermeneutics is the theory and methodology of interpretation. Hermeneutical injustice is a subcategory of epistemic injustice wherein one has no labels/common terminology to describe or explain experiences to others. Historically has been applied in the context of exclusion of marginalized groups from activities which shape the language we use.\nIdentity affects experience, and experience makes a difference in our judgment.\nIt is always a form of powerlessness, whether structural or one-off. If one can simply opt-out, then it should not be considered hermeneutical injustice.\n\n“the dominated live in a world structured by others for their purposes” (emphasis added)\n\nThis quotes from Nancy Hartsock has at least 3 different readings of the meaning of the word structured\n\nMaterially: social institutions and practices favour the powerful\nOntologically: the powerful constitute the social world\nEpistemologically: the powerful have an unfair advantage in structuring collective social understandings\n\nWillful Hermeneutical Ignorance\nIn Relational Knowing and Epistemic Injustice: Toward a Theory of Willful Hermeneutical Ignorance by GAILE POHLHAUS, JR.\nPositing that the sociality of the knower is epistemically significant\n\nSituatedness: the knower’s social position draw their attention to particular aspects of the world.\n\nNote, not as simple as the claim that different experiences lead to different knowledge. Not as strong as the claim that social position leads to automatic knowledge\nSituations resulting from one’s social positioning create common challenges that constitute part of the knower’s lived experience and contribute to the context from which they approach the world\n\n\nInterdependence: epistemic resources are by nature collective\n\nLynn Nelson: “there are no ‘immediate’ experiences” Instead, within any given situation, our experience “is shaped and made possible by communal ways of organizing things, and systems of connected theories, methodologies, and practices”\nRelated: language and terminology. Wittgenstein: “a language that in principle could be understood by only one person would not be a language at all”\n\n\n\nIt is important to note that being marginally situated leads not to “different” knowledge, but, as Harding has argued, to more objective knowledge (Harding 1991, 138-163)\nThe dominantly situated knower cannot step outside of her situatedness in order to experience the world as others do; however, she can learn to use epistemic resources developed from the experiences of marginalized knowers (same reason why disability simulation doesn’t work)\nWillful hermeneutical ignorance: dominantly situated knower’s continued engagement in the world while refusing to learn to use epistemic resources developed from marginalized situatedness\nThe marginalized knower possesses a sort of double-consciousness."},"thoughts/hierarchical-clustering":{"title":"Hierarchical Clustering","links":["thoughts/K-means"],"tags":["seed","CPSC340"],"content":"Hierarchical clustering produces a tree of clusterings\n\nEach node in the tree splits the data into 2 or more clusters.\nMuch more information than using a fixed clustering.\nOften have individual data points as leaves.\n\nOften applied in phylogenetics\nAgglomerative Clustering (Bottom up)\nRequires a “distance” measure between two clusters.\nCluster distance measures\n\n\nDistance between closest members of C1​ and C2​. Also called single-link clustering: mind(a,b),a∈C1​,b∈C2​\n\n\nDistance between farthest members of C1​ and C2​. Also called complete-link clustering: maxd(a,b),a∈C1​,b∈C2​\n\n\nAverage distance between members of C1​ and C2​. Also called group average clustering: ∣C1​∣∣C2​∣1​∑a∈C1​​∑b∈C2​​d(a,b)\n\n\nStarts with each point in its own cluster.\n\n\nEach step merges the two “closest” clusters.\n\n\nStop with one big cluster that has all points.\n\n\nNaive implementation cost is O(n3d)\nDivisive Clustering (Top-down)\nStart with all examples in one cluster, then start dividing. (e.g., run K-means on a cluster, then run again on resulting clusters)\nBiclustering\nCluster the training examples and features. Helps to figure out the ‘why’ on why things are clustered together\n\nRun clustering method on X\nRun clustering method on XT\n\nA dendrogram describes the hierarchy of clusters generated by the clustering methods."},"thoughts/hootsuite":{"title":"Interning at Hootsuite in Highschool","links":[],"tags":["fruit","personal"],"content":"(with insights from Calvin and Hobbes)\nSince I’ve started at Hootsuite, I have\n\nadded and/or changed 6223 lines of code in 16 different repos\nclosed 24 tickets\nmade 4 new Slack emotes\nlistened to almost 10,000 minutes of lofi\nhad 1 amazing summer at Hootsuite\n\nIncluding the amount of money I’ve spent on lunch this past summer, everything has been so much more than I expected it to be. This short but sweet experience is something that I definitely will remember for a very long time.\nOctober 18th, 2015\nOver these short 8 weeks, I’ve learned a lot. Everything at Hootsuite has served to be a once-in-a-lifetime learning opportunity, from learning to build complex services in languages that I’ve never even heard to doing technical demos in front of the entire Product Development team. Despite being a ‘software development’ position, I found myself learning to be a better technical communicator, to have better personal confidence, and so much more.\nYet among everything, two big lessons stood out to me:\nThe balance of life and work\n\nPeople often say how elusive good work-life balance is, even to the point of saying that it doesn’t exist. Yet, at my 2 months here at Hootsuite, I can positively say that it is alive and well. We’ve hiked almost 44km across 4 different trails in BC, attended music festivals, and even found niche groups for those interested in bouldering. At the office, there is no shortage of Slack memes and free birthday cake either. On a more serious note, the managers and leads here care deeply about their team members and go out of their way to ensure not only their well-being but also their development as a person.\nMount Seymour Trail Hike with the team!\nThis was a big change from high school where a day at school left me completely drained and just ready to crash. Everything was so focused on grinding for results and doing work that it was hard to make time for myself. At Hootsuite, the community made it feel easy and natural to find the balance that worked for me.\nI was still working 8 hours a day, but somehow I felt energized and ready for more each day. It motivated me to make time to take on a freelance web development job just for practice (and to pay rent — living in Vancouver is rough) and even to make a simple project for a friend’s birthday!\nI know that this has probably been repeated countless times already, but I cannot stress how important it is to make sure you’re taking enough time off to relax.\nRejection\n\nDon’t reject yourself before other people get the chance to\n\nDecember 27th, 1985\nBecomes sometimes, you just gotta go for it. You probably shouldn’t be going around and ruining nice coffee tables, but it works as a metaphor I guess? The point is: take necessary risks. Don’t say “nah, that’s not possible” before someone else tells you it is. At the start of summer, if someone told me I was going to be doing a demo on a proposal for a massive re-architecture of one of Hootsuite’s core services, I would have told you that you were bonkers. Yet here I am, doing just that:\n\nIf your brain is anything like mine, you often get these nagging self-doubts about whether you’re capable of doing something. The first time I was assigned to help define Service Level Objectives (SLOs) for some of Hootsuite’s core services, I was shocked.\n\nThey’re letting a high schooler do this? How am I even close to being qualified?\n\nNot wanting to miss the opportunity, I took it. But as I began work, it became clear that I was the biggest critic in the room. They wouldn’t have asked me to do it if they didn’t trust me to do it and to do it well. Moving forward, I tried to trust myself more and to put myself out there, eventually taking on projects that I never thought would’ve been possible:\n\nDesigning prototype service meshes\nAdding endpoints and writing scripts to cleanup user data\nAnd other crazy things that I probably can’t publicly disclose\n\nIf an opportunity presents itself, jump on it! You don’t know where it might take you.\nIt’s really hard to put in words how much I’ve learned from the Owls here at Hootsuite. To the Owls: hopefully I’ve added something of value to all of you too.\n\nThanks to my manager and team lead Imtiaz and Shaun for being absolutely amazing in supporting my whole journey from start to finish.\nThanks to our team — team Golden Hammer — for helping me realize that yes, work can still be done while having fun. (:harold:)\nThanks to the co-ops (most notably Kevin, Andy, and Albert) for being so welcoming and having such a wholesome community.\nThanks to the other high school interns (Chloe, Kai, and Scarlet) for making sure that I wasn’t the only high school student that was being made fun of at board game night.\n\nAnd finally, thanks to Hootsuite for showing this little owl the world!\n\nHootsuite has changed my attitude on how I see the world and it’s beautifully captured in one quote from Waterson —\n\n“It’s a magical world, Hobbes, ol’ buddy … let’s go exploring!”\n\n"},"thoughts/housing":{"title":"Housing","links":["thoughts/democracy"],"tags":["seed"],"content":"Housing Crisis\nWhy Housing Is So Expensive — Particularly in Blue States\n\nThe five states in the U.S. with the highest rates of homelessness are New York, Hawaii, California, Oregon and Washington. Some of the bluest states in the country, not one red state on that list.\nHousing is fundamental. When you fail to provide it, that failure reverberates throughout society, it lays waste to all your other carefully laid policy plans and ideals\n\nThat means a state like California — that prides itself on all the green energy infrastructure it’s building — is pricing people who would want to live in that infrastructure into states where they use more fossil fuels\n\n\n\nTwo housing affordability problems\n\nSupply hasn’t kept up with the demand: high-cost metros, places with great job markets haven’t been building enough housing to accommodate population growth and job growth for something like the last 30 years.\n\nDemocratic places have very strong demand for housing so a lot of high-income people who bid up the housing prices\nDemocratic administrations have imposed a lot of rules on construction processes (sometimes for progressive reasons like protecting the environment or giving voice to community)\n\nBut in some senses, it’s now been — the impulse to give communities control has been weaponized by wealthy white communities, which then used this to say you can’t build apartments and low-income housing in our wealthy neighborhoods\n\n\n\n\nHousing cost is too high relative to low incomes: the poorest households everywhere in the country spend more than half of their income on housing costs, and that leaves them too little money left over to pay for things like food and transportation and health care.\n\nIf you spend more than 30 percent, HUD says that you are cost-burdened. And if you spend more than 50 percent, they say you are severely cost-burdened\n\n\n\nHousing Vouchers\n\nHousing vouchers are actually one of the most effective anti-poverty programs we have.\nThey can rent an apartment on the private market. They spend 30 percent of their income, whatever dollar value that is, and the federal government picks up the tab for the rest of this.\nCritique: if you just give everybody a voucher, or a check, or a subsidy, all that’s going to happen is that landlords or other kinds of housing suppliers are going to pocket that\n\nResponse: works well for housing-abundant but highly priced areas. For supply-constrained places, we need to also build more homes.\n\n\n\nDemocratic Process\n\nSmall-d democratic processes where people get to engage in their local government and make their voice heard, are not actually that democratic.\nIt’s not representative. And we know this, in part, from the work of political scientists who have looked at the characteristics of people who show up to a neighborhood meeting.\nActually seems to disallow experimentation\n\nIn theory: democracy allows compared to other systems is a lot of different kinds of systems to flourish and we can see what works best.\nBut what is really striking reading your work and looking at housing is actually how little experimentation is possible. And this estimate you make, really, really is wild. That it is illegal to build anything except single-family detached houses on roughly 75 percent of land in most cities today.\n\n\nIf most of the houses that already exist in your city are currently illegal under zoning, it raises questions about what the zoning is trying to do.\n\nCambridge, Massachusetts is a perfect example. The vast majority of parcels in Cambridge, Massachusetts have what are called nonconforming uses. So the structure there is in violation of current zoning laws. Either it’s too tall or too close to the street or a structure that’s illegal.\n\n\nWe’ve made communes illegal in a lot of places\n\nSo the boarding house that had one communal kitchen, and meals got cooked, and everybody had, essentially, a bedroom, but you all ate your meals together or ate out at a restaurant all the time, that was very typical. And certainly earliest cities — workers moved from farms to cities, and they all just rented a room in a boarding house and that was pretty much the option. It was much, much cheaper.\nBut what we’ve essentially done is say middle class preferences for having nuclear families and having your own kitchen and bath, that’s the only housing structure that’s allowed.\n\n\nWhat about the resurgence of co-living arrangements?\n\nAs investment\n\nWe’ve really pushed housing as the engine of middle class wealth. We have really pushed people to stock a ton of their money and wealth and long-term financial security or intergenerational financial security in homes.\nAnd so that also creates a politics where people are very, very nervous about anything that might negatively affect their home values\n\nEnvironment\n\nWe think a lot about “Are there going to be any negative consequences to anybody from building here” — without thinking about the flip side, “if we don’t build here, what are the negative consequences?&quot;&quot;\n"},"thoughts/human-centered-design":{"title":"Human Centered Design (HCD)","links":["thoughts/design-requirements","thoughts/affordance","thoughts/desire-paths"],"tags":["seed"],"content":"What drives design?\n\ntechnology-centered design: building what we are able to build but risks leaving out challenges of real people\ndesigner-centered design: progress made by designer’s intuition, imagining user\nuser-centered design: incorporating users heavily into the desing process (e.g. double-diamond design and iterative design model)\n\nPutting human needs, capabilities, and ways of behaving first. This requires good understanding of human psychology, technology, and good communication (both person-to-person and person-to-machine).\n\nHCD principle is to avoid specifying the problem as long as possible but instead to iterate upon repeated approximations.\n\nImportance of user involvement\nInvolving users helps with expectation management\n\ncan see capabilities from an early stage\nunderstand better how it will affect their jobs/lives and why the features are designed that way\n\nDiscoverability\nWhen we interact with a product, we need to figure out how to work it. This means discovering what it does, how it works, and what operations are possible.\nThis is composed up of 5 fundamental psychological concepts which are the principles of interaction:\n\nAffordance: signifies what action is possible. It is the relationship between the properties of an objects and the capabilities of the user that determine how the object could be used. For example, a chair affords (“is for”) support and therefore affords sitting. If an affordance or anti-affordance cannot be perceived, some means of signaling its presence is required.\nSignifiers: signifies where actions should occur. A mark, sound, or perceivable indicator that communicates appropriate behaviour to a person. Can be a desire paths, a push/pull label, etc. However, we shuld be aware that different cultures associate different meanings with different signifiers.\nConstraints: limits to the possible interactions with an object. For example, the different sized holes in a scissor suggests different numbers of fingers may fit in each hole and not any more.\nMappings: relationship between the elements of two sets of things. For example, a mapping of light switches to light bulbs or the steering wheel to the direction of the wheels. Groupings and proximity are important principles from Gestalt psychology that can be used to map controls to function: related controls should be grouped together.\nFeedback: communicating the results of an action or some way of letting you know that the system is working on your request. Has to be a balance, little and poor feedback and too much feedback can also be unhelpful and annoying to users. Feedback is essential, but not when it gets in the way of other things, including a calm and relaxing environment.\n"},"thoughts/human-computer-interaction":{"title":"Human Computer Interaction (HCI)","links":["thoughts/interaction-design","thoughts/The-Psychopathology-of-Everyday-Things","thoughts/human-centered-design","thoughts/design-requirements","thoughts/interaction-failure","thoughts/design-goals","thoughts/user-involvement","thoughts/interviews-and-data-recording","thoughts/task-centered-design","thoughts/mental-model","thoughts/prototyping","thoughts/page-layout"],"tags":["sapling"],"content":"\nBetter HCI is better empathy\n\nWhat even is HCI?\n\nThe human-machine joint performance of tasks\nHuman-computer or human-human communication mediated by computers\n\nMany a clever invention has been termed ‘before its time’ because the inventor did not see how to build a transition from what was known and in use to what was new.\n3 main goals of HCI\n\nDesigning\nImplementing\nEvaluating\n\nAs designers, we are defining systems, and implementing the structures that create culture. We have the responsibility to think about the world as a complicated, ethically fraught place. We are designing interactions\nAlways ask: who are my users, and what are their needs?\nConcepts\n\nThe Psychopathology of Everyday Things\nHuman Centered Design (HCD)\nDesign Requirements\nInteraction Failures\nDesign Goals\nUser Involvement\nInterviews and data recording\nTask-centered Design\nMental Models\nPrototyping\nPage Layout\n"},"thoughts/hyper-parameter-optimization":{"title":"Hyper-parameter Optimization","links":[],"tags":["seed","CPSC340"],"content":"How do we efficiently find the “best” hyper-parameters?\nMore complicated models have even more hyper-parameters. This makes searching all values expensive (increases over-fitting risk)\nSimplest approaches:\n\nExhaustive search: try all combinations among a fixed set of σ and λ values.\nRandom search: try random values\nStochastic local search: Generic global optimization methods (simulated annealing, genetic algorithms, and so on)\nCoordinate search: Optimize one hyper-parameter at a time, keeping the others fixed. Repeatedly go through the hyper-parameters\n"},"thoughts/hypertext":{"title":"Hypertext","links":["thoughts/the-garden-and-the-stream"],"tags":["seed"],"content":"Coined by Ted Nelson in 1965 in “Literary Machines”\nHe envisioned a world where computers would enable people to write and publish in a new, nonlinear format, which he called hypertext.\nHypertext was “nonsequential” text, in which a reader was not constrained to read in any particular order, but could follow links and delve into the original document from a short quotation.\nSee also: the garden and the stream\nXanadu\nHe also described a project called Xanadu, in which all the world’s information could be published in hypertext. He had the dream of a utopian society in which all information could be shared among people who communicated as equals.\nHe struggled for years to find funding for his project, but success eluded him."},"thoughts/iconic-space":{"title":"Iconic Space","links":[],"tags":["seed"],"content":"Writer George Saunders coined the term “iconic space” to refer to a space one uniquely occupies because of their set of skills; to do work only they can do.\n\n“They arrive already wonderful. What we try to do over the next three years is help them achieve what I call their “iconic space” — the place from which they will write the stories only they could write, using what makes them uniquely themselves…At this level, good writing is assumed; the goal is to help them acquire the technical means to become defiantly and joyfully themselves.”\n"},"thoughts/idea-list":{"title":"Idea List","links":["thoughts/CodeMirror","thoughts/git","thoughts/WebAssembly","thoughts/CID","thoughts/Sloppy-Hashing-DHT","thoughts/IPFS","thoughts/cryptography","thoughts/tools-for-thought","thoughts/A-Pattern-Language","thoughts/latent-factor-model","thoughts/attention-economy","thoughts/Jestermaxxing","thoughts/Byzantine-Faults","thoughts/independent-research","thoughts/taste","thoughts/search","thoughts/Network-Theory"],"tags":["evergreen"],"content":"Technical\n\nblock based text editor\n\nallow you to rip paragraph apart and rearrange sentences spatially\n\nparagraph should be able to have hierarchy like bullet points\nthe ‘bullet points’ should be purely visual (easy toggle to turn it into paragraph/prose mode)\n\n\ncolumn based drafts throughout history (save as draft button but also rewindable history)\n\n\nfuzzy rip grep\nhoogle for rdf shapes\noptimizing network topology for crdts\ncodemirror but CRDT + TreeSitter\n\nsmart git merges\nhttps://www.wilfred.me.uk/blog/2022/09/06/difftastic-the-fantastic-diff/\n\n\nmarkup any site with a webcrawler + yjs + tldraw\n\nmaybe use webrecorder\nmarkupthis.site is not taken!!\nbacked by content-addressed cdn store?\n\n\ndaily link share\n\ninventory packing simulator but its with tabs… what would affect tab size? length of article == perceived weight in bag?\nwhat if you could bring your little backpack of tabs with you to a square or market and trade your tabs with other people\nlist building primitive\n\n\nWASM-based val.town with CIDs distributed using a Sloppy Hashing DHT\n\nIPFS as a versioned package manager for all software\ngood example: https://github.com/dylibso/wasmstore\n\n\nrotmg but actually good lol\n\ndodge mechanic\nno instakill, should be iframes and way less health (similar to EtG)\nkeep soulbound mechanic\nno screen rotation\nmulti-floor dungeons, take more inspo from roguelites\ncrowdsources levels + bossfights - community rating system like how Geometry Dash does it\nshould have one free key a day\nactual good graveyard mechanics to look at past runs/chars\n\n\nlatency based quorum sensing, similar to how bacteria release a particular molecule and behave differently if sensors of the molecule are particularly active\n\nsee also: Sloppy Hashing DHT\n\n\nwebgpt but its for tools for thought\n\n“A reader-generated essay is what you get when you can go into someone else’s knowledge graph and make a linear journey through the network, while GPT-5 generates a just-in-time essay that is human-readable.”\nturning a graph traversal into a beautiful essay\n\n\nwhat does entropy + erosion of data look like?\n\nis they ways to make cryptography that are valid in time windows?\n\n\nbig touchscreen desk\n\nwhat if i just got a huge old flatscreen tv\nmounted 4 pressure sensors on each corner and then got a big sheet of thin glass\n4 points is enough to pinpoint single-point touch accurately for dragging\nwhat if you could use it like a scratch space? like an always available figjam/muse board that also supports linking and trails a la memex\n\na vision of communal computing perhaps\na shared screen which anyone can ‘connect’ to as an external display\nanyone can drag windows/files to and from it\n\n\n\n\nwave function collapse for poetry\n\nbringing shape-shifting text to its most literal form\nhttps://oskarstalberg.com/Townscaper/ for words\nbase representation is word vectors\n\nyou can jiggle word vectors around\napply transformations to vectors (e.g. the past tense vector)\nsome sort of 1d marching cubes which modifies a vector depending on context?\n\npotentially transformer related\n\n\n\n\nimage ←&gt; text interop using CLIP/unCLIP/DALL-E?\n\n\nprocedural city + building generation in minecraft + rtx using a codified version of A Pattern Language\n\nhttps://en.wikipedia.org/wiki/Constraint_satisfaction_problem\n\n\nprocedural visualization of digital garden as an actual garden to help you tend to it better\nbetter search\n\nsearching through vectors\n\n(no clue if this would work) integral images but applied to vector similarity search in text documents\nviola-jones for text embeddings\nhttps://github.com/facebookresearch/faiss\n\nhttps://arxiv.org/pdf/1702.08734.pdf\nhttps://crates.io/crates/kd-tree\nhttps://dl.acm.org/doi/10.1145/3154273.3154307\n\n\n\n\nhow do we encode sentences/paragraphs/documents as vectors?\n\nhttps://beta.openai.com/docs/guides/embeddings/text-search-using-embeddings\nhttps://github.com/ryankiros/skip-thoughts\nhttps://github.com/pytorch/fairseq/tree/main/examples/data2vec\n\n\nwasm-based in-browser\n\nopen question: how do we get the query → vector??\ncompute vectors for documents ahead of time and compile it into a single static binary file\nfrontend will just load this\nin the browser, use webgl for fast dot-products\n\nwrite a fragment shader to compute similarity between search vector and indexed vectors (1×n image where n is number of documents and output is value in [0,1] for similarity)\n\n\n\n\n\n\ndata provenance\n\nhttps://www.cs.cmu.edu/~NatProg/whyline.html\n\n\nLayoutLM + screenshots → auto-categorization of knowledge\n\nhttps://screenotate.com/ but with atlas recall\nmaybe turn this into an app which auto-extracts semantic info from screenshots/images on webpages and does something w it idk\nhow do we prevent https://twitter.com/rsnous/status/1130910375795277824\n\n\nMarginalia for the web? browser as a graph database, chronological browsing?\n\nwhat if you could 3 finger swipe up on a browser to see what pages this page is connected to in a graph\nand you could write on the margins of pages and share those with friends\n\na little annotated web\n\n\n\n\nConversational GPS: why do we even look at a screen when we can just ask for directions as if it was a normal person lol\ngoogle photos + olo radio (see attention economy)\n\n\nDreamCoder boolsat\n\nCreating a LISP-like higher-order language to exploit reusable sub-proofs in specific domains (e.g. graph colouring)\nkinda iffy on the sat problem solver using dreamcoder, not a lot of exploitable structure in the proofs (otherwise we’d have a more reliable human method)\nBackground:\n\nhttps://en.wikipedia.org/wiki/Boolean_satisfiability_problem\nhttps://searchworks.stanford.edu/view/13250178\n\n\n\n\nNeuroSAT Paper:\n\nhttps://arxiv.org/pdf/1802.03685.pdf\n\n\nDreamCoder Paper:\n\nhttps://arxiv.org/pdf/2006.08381.pdf\nhttps://www.youtube.com/watch?v=qtu0aSTDE2I\n\n\n\n\n\n\ntabfs but for emails\n\nhttps://bazil.org/fuse/\n\n\nhttps://blog.gopheracademy.com/advent-2014/fuse-zipfs/\nlisten on any email server\n\n\ndeep foveal VR rendering\n\nWriting\n\nShort stories/speculative fiction\n\nCID as the library of babel\nInterplanetary communication / state machine\nPacket switched electricity\nsaving sun for later (make something we take for granted extremely scarce)\n\n\nEssays\n\nAnalog software: software by analogy and by atomic building blocks that interface with each other\n\nWe should be able to directly manipulate them, like files, rather than only indirectly work with them, like layer activations in a neural network.\nSoftware representations for similar ideas should be obviously similar in some way – they should click together, or look similar, or feel similar to the touch.\nIdeas should remember where they came from – what blog I copied it from, which author I quoted it from, and so on.\n\n\nEssay on epistemic play + Jestermaxxing + mill’s take on why censorship is unethical\nlimits to BFT\n\nsome malicious activity is indistinguishable from legitimate activity (e.g. deleting a document)\n\nsemantic byzantine fault tolerance vs protocol byzantine fault tolerance\n\n\nMaking distributed systems reliable is inherently impossible; we cling to Byzantine fault tolerance like Charlton Heston clings to his guns, hoping that a series of complex software protocols will somehow protect us from the oncoming storm of furious apes who have somehow learned how to wear pants and maliciously tamper with our network packets. (The Saddest Moment by James Mickens)\n“I have never had a real-life experience that resembled a Byzantine fault tolerant protocol.”\n\n\ntwo axes of collaborative vs local-first software\n\nultimately about state reconciliation\none is temporally (offline-first) the other is spatially (collaboration across multiple computers)\ntwo axes, not mutually exclusive\nmaybe worth plotting where everything sits lol\ndata is either\n\ncollaborative: co-editing\npersistent: saved somewhere (as opposed to ephemeral)\nversioned: can time travel\n\n\n\n\nindependent research is applied taste\n\nAesthetics as a heuristic for non obvious optimality\n\n\nGood search (aggregators) turns random networks into scale-free networks (see: network theory)\nThe sequel to SQL (Against table databases)\n\nData interoperability is really hard with SQL\n\nI think interoperability in the context of the web means being able to transparently understand and share data, agnostic of platform.\nClosed platforms disallow this as they curate the information they present to end-users/devs through the frontend but close off access to the actual data itself.\nThis forces one ‘correct’ way of looking at data (which is often not the case!)\n\n\nWhat about evolution of applications?\nTaking evolution to its natural extension, a different application all together?\nHow do we create sources of truth that are legible outside of the application, possibly in ways that the application developer never anticipated?\nWell, we’ve done this already once! The applications on our computers ‘share’ data between them is through the file system\n\nAn application then is a specific view on types of data rather than a standalone thing. Data can be dragged through different applications. Each data is annotated with a type.\n\n\nHow might we do this for more general data?\nRecent years has seen a revival of academic and research interest in the tuple store which will allow us to pull out structure when we need it. It embraces a pluriversal view of data — a world where multiple alternatives co-exist\n\nWe can think of a triple store as a distributed and fragmented SQL database, where instead of tables with rows and value, we have entities with attributes and values. Any application can declare new attributes or alias an attribute to a more common one. The most important part is that applications that share attributes can automatically interoperate their data by using the same attributes\nThis type of ‘decentralized’ database means there is no canonical schema. You can’t mistake the map for the territory because everyone has their own map and can’t force others to view the ’truth’ of the world through your map\nSee more: https://jzhao.xyz/thoughts/Rhizome-Research-Log#october-19th\n(Inkandswitch explored this with Cambria)\n\n\nOne common critique of this is that we lose efficiency and that by knowing the structure of the data ahead of time, we can better optimize queries, etc.\nUsing tuples doesn’t prevent us from creating indices and using incremental view maintenance! A lot of really good recent research on differential data flow\nImportantly, this gives us moldable data which enables people to have the raw material to sculpt and play with\n\n(AtProto making the data and schema public means that we’ve seen an explosion of clients and projects that use BlueSky data)\n\n\n\n\n\n\n"},"thoughts/idea-machine":{"title":"Idea machines","links":["thoughts/funding","thoughts/research-institutions","thoughts/autopoiesis"],"tags":["seed"],"content":"Post by Nadia Asparouhova\n\nIdea Machine: a network of operators, thinkers, and funders, centered around an ideology, that’s designed to turn ideas into outcomes.\n\nRelated: funding, research institutions\nAn Idea Machine is a self-sustaining organism that contains all the parts needed to turn ideas into outcomes:\n\nIt starts with a distinct ideology, which becomes a memetic engine that drives the formation of a community\nThe community’s members start generating ideas amongst themselves\nEventually, they form an agenda, which articulates how the ideology will be brought into the world. (Communities need agendas to become idea machines; otherwise, they’re just a group of likeminded people, without a directed purpose.)\n\nDoes agenda come first or ideas? I would think agenda first for more autocratic orgs and ideas for more autopoietic orgs\n\n\nThe agenda is capitalized by one or several major funders, whose presence ensures that the community’s ideas can move from theory to practice – both in terms of financing, as well as lending operational skills to the effort. (Without funding, an idea machine is just that: an inert system that needs fuel to turn the crank and get it moving.)\n\n\nIdea machines are different from movements, which are focused on achieving a specific outcome and are therefore self-limiting (if they succeed, the movement winds down). On the other end of specificity, idea machines are less broad than paradigm shifts, which are widespread, headless, decentralized shifts in cultural norms and attitudes due to changes in systemic conditions.\nDefinitions\n\nIdeology: a mimetic engine that drives the formation of a community\nAgenda: articulates how the ideology will be brought into the world\nFunders: presence ensures the community’s ideas can move from theory to practice\nScene builder: sustain the community, develop the agenda, attract new members\nOperators: driver operating initiatives which lead to real outcomes\nSupport organizations: infrastructural organizations whose purpose is to strengthen the values and best practices of the idea machine\n"},"thoughts/idealism":{"title":"Idealism","links":["thoughts/self-knowledge","thoughts/ontology","thoughts/the-Self"],"tags":["seed","PHIL451A","PHIL240A"],"content":"Everything is in the mind. The intrinsic nature of ultimate reality is the mind.\nPratyabhijna Self-awareness\nOn Self-knowledge\nNondual Śaiva philosophical tradition concerned with articulating and defending the idea that everything is Śiva, as well as the practices that lead to this realization\nA form of Cosmopsychism which is in turn a form of ontological idealism\n\nUltimate Reality (Śiva)\n\nConsciousness as the infinite potential for any and all manifest worlds\nŚiva is like a sculptor, a playwright, or a yogin because he spontaneously creates worlds from within himself\n\n\nConventional Reality\n\nEmbodied worlds created through exclusion\nManifests because the ultimate desires to experience itself in diverse forms\nWhen we imagine, for example, an elephant, our consciousness presents this object (‘this’) as distinct from our conscious selves (‘I’). Despite this distinction, we know ‘this’ is only imagined and arises from the self.\nConsciousness, when this happens, does not become the elephant, nor does it cease to exist as the ‘I’ when taking on the elephant’s form, it simplify manifests itself as something distinct from the ‘this’.\nThis plasticity is what the Śaivas call “freedom”\n\n\nInternality is the reflective awareness ‘I’, externality is the reflective awareness ‘this’\n\nFor objects perceived there are two kinds of externality\n\nObject of cognition through the external sense\nObject of cognition through the internal sense\n\n\n\n\nNo ontologically independent external objects\n\nThe internal self must have the notion or essence of object to manifest it as external (?)\nCannot infer the existence of something that has never been perceived\n\n\n\nHas many holes in its axioms but again presented here just to learn:\n\nIntrinsic Existence: consciousness must exist from its own intrinsic perspective, independent of external observers\nComposition: consciousness is structured and composed of multiple phenomenological distinctions (elementary or higher order)\nInformation: consciousness is specific. Each experience is particular and uniquely distinct\nIntegration: consciousness is unified. Each experience is irreducible and cannot be subdivided into non-interdependent disjoint subsets of phenomenal distinctions\nExclusion: consciousness is definite — it either has or does not have certain phenomenal distinctions\n\nArgument\n\nWe observe causally specific differentiation in our everyday world\nSomething that is causally specific must be the effect of a specific real cause\nEach real cause produces only the effects that are in accord with its nature\nSuch causes must be either internal or external to consciousness\nThese causes cannot be external to consciousness because, per Vasubandhu and Dharmakīrti, external objects are irrelevant and logically incoherent\nThese causes therefore must be internal to consciousness\nIt is not the nature of something undifferentiated to produce different effects\nConsciousness cannot be totally undifferentiated and produce different effects\nConsciousness must be inherently differentiated if it is to account for the differentiated awarenesses observed in the conventional world\nSince there is no other viable candidate for the cause of this differentiation, the nature of reality is ultimate consciousness that inherently contains the capacity for the expression of all differentiated awarenesses\n\n‘Ill-taught’ people satisfy themselves with conventional reality call it ‘inert’, failing to see it is a subset of Śiva. “Only consciousness can present itself as precisely what it’s not while still remaining itself”"},"thoughts/idempotence":{"title":"Idempotence","links":["thoughts/clocks"],"tags":["seed"],"content":"\nHow do we avoid cases where losing an ACK could lead to users doing an action multiple times (e.g. pressing the like button)? Idempotence!\n\nf is idempotent if f(x)=f(f(x))\nFor example\n\nNot idempotent: f(likeCount)=likeCount+1\nIdempotent: f(likeSet)=likeSet∪{userID}\n\nIdempotent requests can be retried without deduplication.\nHowever, this isn’t perfect when there are other actors/actions that intermix. For example, f(f(x))=f(x) but f(g(f(x)))=g(x) (e.g. liking, unliking, then liking is not the same as unliking!)\nTo somewhat fix this, use tombstones and record logical timestamp for when events happen\n\nThen, we can reconcile replicas by propagating the record with the latest timestamp and discard the records with earlier timestamps\nThen, to fix concurrent writes by different clients\n\nLast writer wins (LWW): resolve conflicts using a logical clock that gives total ordering (e.g. Lamport clock)\nMulti-value register (give all options to let user/algorithm above resolve it): use a Vector clock, v2​ replaces v1​ if t2​&gt;t1​; preserve both {v1​,v2​} if t1​∥t2​\n\n\n"},"thoughts/identity":{"title":"Identity","links":["thoughts/self-knowledge","thoughts/the-Self","posts/digital-identity","posts/context-collapse"],"tags":["seed"],"content":"See also: self-knowledge, the self\nDigital Legibility\nExtensions on relational identity\nSee also: digital identity\nIdentity constantly shifts in real life depending on who you are around. We try to approximate this using multiple accounts online to try to prevent context collapse\nBut identity shouldn’t feel like different accounts you hop between different digital platforms. Identity should be the principles of your experience, it should be applicable across contexts\nDigital identity should enable novel interactions, not try to simplify existing ones. Identity systems rely on the concentration of power in the hands of operators who cannot possibly be fully aligned (financially, socially, or morally) with all of their users—for example, private social media companies with diverse global audiences must often make decisions about what constitutes an act of unjustified censorship versus an act in the interest of public safety, though they are often ill-equipped to do so"},"thoughts/idolization":{"title":"Idolization","links":["posts/a-failure-resume"],"tags":["seed"],"content":"Source: On Idolization by Matthew Wang\n“I struggled with this significantly in high school. I had the pleasure of working with some absolutely wonderful people. We debated together and against each other; ran conferences and events; did community service. I’d constantly rave about how insane, out of the world, and unbelievable they were. It was constant and nagging. It seeped into my personality.”\n“You implicitly invalidate any struggles they have, and eschew their private life. Your praise becomes so diluted that it’s meaningless, just another reflex or polite gesture.”\n\nRecognizing that excessive praise (even if genuine) can be toxic positivity. It sets unrealistic boundaries for how people should act all the time and makes it hard for people to be truly vulnerable\nHow can we approach showing appreciation without being over-the-top about it? We can start by acknowledging and normalizing failure: failure resumes and creating safe spaces for vulnerable conversation as well as genuine celebration\n"},"thoughts/illumination":{"title":"Illumination","links":["thoughts/computer-graphics","thoughts/GLSL"],"tags":["seed"],"content":"Lighting Models\nHow do we simulate how light moves in a computer graphics scene?\nThis is difficult as at any point in 3D space, light of all wavelengths may be going in all directions. We can vastly simplify this by assuming there are ‘only’ three wavelengths, red green and blue. This is a reasonable assumption as these map cleanly to the three types of cone-cells in the human eye\n\nLocal illumination (naive simulation, assumes single bounce)\n\nThis is ignorant of shadows (except simple self-shadowing)\n\n\nGlobal illumination (physical based, computes light transport based off of reflectance, luminance, roughness, etc.)\n\nBidirectional Reflectance Distribution Function (BRDF): Measure how much light is reflected in every outgoing direction for each possible incoming direction of light.\n\n\n\nPhong Model\nambient termIa​ka​​​+L∑​​diffuse termIL​kd​(N⋅L)​​+specular termIL​ks​(R⋅V)n​​​\n\n\nAmbient term provides default minimal illumination\n\n\nDiffuse term provides basic lighting depending on the angle of the surface w.r.t. the incoming light direction\n\nIntuition is that the brightness depends on the angle of the surface with respect to the incoming light direction. Sharper angles mean that the surface receives less light.\nNormally also clamp and normalize N⋅L to between 0.0 and 1.0\n\n\n\nSpecular (phong) term\n\nMany surfaces are not perfect mirrors, but still reflect much light in the general direction of the reflected ray\n\n\n\nN is the surface normal\n\n\nL is the light direction\n\n\nR is the reflected ray (normally R=−L+2(N⋅L)N, in GLSL: R = reflect(-L, N))\n\n\nV is the viewing direction\n\n\nn is the shininess constant (sometimes also α)\n\n\nI is a property of the scene\n\nIa​ is the ambient light colour\nIL​ is the intensity of the diffuse component of the light source L\n\n\n\nk is a property of the surface\n\nka​ is the ambient colour of the surface\nkd​ is the diffuse reflection constant\nks​ is the specular reflection constant\n\n\n\nShadow Mapping\n\nRender scene from the light source and store the distances in a z-buffer (normally called dshadowmap​)\nRender scene from the camera view. If the depth computed at the fragment is greater than the depth in the shadowmap, it is in shadow\n\nHowever, this model only produces hard shadows\nGouraud Shading\nGouraud shading, named after Henri Gouraud, is an interpolation method used in computer graphics to produce continuous shading of surfaces represented by polygon meshes.\nLighting computations based on a reflection model, e.g. the Phong reflection model, are then performed to produce colour intensities at the vertices. For each screen pixel that is covered by the polygonal mesh, colour intensities can then be interpolated from the colour values calculated at the vertices.\nHowever, highly localized lighting effects (such as specular highlights, e.g. the glint of reflected light on the surface of an apple) will not be rendered correctly\nCaustics\nCaustics are patterns of light that form when a light source is reflected or refracted by a curved surface.\n\nSunlight reflecting off a swimming pool: When sunlight enters a swimming pool, it is refracted by the water and then reflected off the bottom of the pool. The caustic appears as a series of bright, rippling lines on the surface of the water.\nLight passing through a wine glass: When light passes through a wine glass, it is refracted by the curved surface of the glass. The caustic appears as a bright, elongated spot of light on the surface of the table or other surface where the glass is placed.\n\nAmbient Occlusion\nAmbient occlusion refers to the soft shadows that form in areas where objects are close together, or where there are tight crevices or other areas where light has trouble reaching.\nSpecifically, ambient occlusion is calculated by casting rays from each point on an object’s surface in multiple directions and then determining how many of those rays intersect with other objects in the scene. The more rays that intersect with other objects, the darker the ambient occlusion effect will be in that area.\nOne common approach to ambient occlusion is to use a technique called screen-space ambient occlusion (SSAO), which calculates ambient occlusion by examining the depth and color values of pixels on the screen.\nLight Transport Notation\n\nL: a light source\nE: the eye\nS: a specular reflection\nD: a diffuse reflection\n\nLight takes a path from L, the light, to E, the eye."},"thoughts/imaging":{"title":"Imaging","links":["thoughts/noise"],"tags":["seed"],"content":"Image Formation, Cameras, and Lenses\nImage formation depends on\n\nLighting conditions\nScene geometry\nSurface properties\nCamera optics\nSensor properties\n\nImage Processing Pipeline\nThe sequence of image processing operations applied by the camera’s image signal processor (ISP) to convert a RAW image into a regular JPG/PNG.\n\nLens\nCFA\nAnalog Front-end → RAW image (mosaiced, linear, 12-bit)\nWhite balance\nCFA demoisaicing\nDenoising\nColour transforms\nTone reproduction\nCompression → final RGB image (non-linear, 8-bit)\n\nExamples\nReflection\nSurface reflection depends on viewing (θv​,ϕv​) and illumination (θi​,ϕi​) direction along with the Bidirectional Reflection Distribution Function: BRDF(θv​,ϕv​,θi​,ϕi​)=πρd​​\nAll angles are spherical coordinates w.r.t. the normal line of the surface.\nA Lambertian (matte) surface is one which appears the same brightness from all directions. A mirror (specular) surface is one where all incident light is reflected in one direction (θv​,ϕv​)=(θr​,ϕr​)\nCameras\nAll scene points contribute to all sensor pixels\nAs a result, the image is really blurry.\nPinhole camera\nThe image here is flipped, but no longer blurry. Roughly, each scene point contributes to one sensor. Pinhole camera means you need to get the right size of pinhole. If the pinhole is too big, then many directions are averaged, blurring the image. If the pinhole is too small, then diffraction becomes a factor, also blurring the image.\nA few perspective ‘tricks’ arise out of the pinhole\n\nSize is inversely proportional to distance\nParallel lines meet at a point (vanishing point on the horizon)\n\nSide note, pinhole cameras are really slow because only a small amount of light actually makes it through the pinhole. As a result, we have lenses\nLenses\nThe role of a lens is to capture more light while preserving, as much as possible, the abstraction of an ideal pinhole camera, the thin lens equation\nz′1​−z1​=f1​\nwhere f is the focal point, z′ is the distance to the image plane, and z is the distance to the object.\nFocal length can be thought of as the distance behind the lens where incoming rays parallel to the optical axis converge to a single point.\nDifferent effects can be explained using physics phenomena. Vignettes are simply light that reaches one lens but not the other in a compound lens. Chromatic aberration happens because the index of refraction depends on wavelength of the light so not all colours can be in equal focus.\nSimilarities with the human eye\n\npupil is analogous to the pinhole/aperture\nretina is analogous to the film/digital sensor\n\nWeak Perspective\nOnly accurate when object is small/distant. Useful for recognition\nP=​xyz​​projects to a 2D image point P′=[x′y′​]where m=z0​f′​,x′=mx,y′=my\nOrthographic Projection\nP=​xyz​​projects to a 2D image point P′=[x′y′​]where x′=x,y′=y\nPerspective Projection\nP=​xyz​​projects to a 2D image point P′=[x′y′​]where m=z0​f′​,x′=f′zx​,y′=f′zy​\nImage as functions\nGrayscale images\n2D function I(x,y) where the domain is (x,y)∈([1,width],[1,height]) and the range is I(x,y)∈[0,255]∈Z\nPoint Processing\nApply a single mathematical operation to each individual pixel\nLinear Filters\nLet I(x,y) be an n×n digital image. Let F(x,y) be the m×m filter or kernel. For convenience, assume m is odd and m&lt;n. Let k=⌊2m​⌋, we call k the half-width.\nFor a correlation, we then compute the new image I′(x,y) as follows:\nI′(x,y)=∑j=−kk​∑i=−kk​F(i,j)I(x+i,y+j)\nFor a convolution, we then compute the new image I′(x,y) as follows:\nI′(x,y)=∑j=−kk​∑i=−kk​F(i,j)I(x−i,y−j)\nA convolution is just the correlation with the filter rotated 180 degrees. We denote a convolution with the ⊗ symbol.\nIn general,\n\nCorrelation: measures similarity between two signals. In our case, this would mean similarity between a filter and an image patch it is being applied to\nConvolution: measures the effect one signal has on another signal\n\nEach pixel in the output image is a linear combination of the central pixel and its neighbouring pixels in the original image. This results in m2×n2 computations. When m≈n, then this is a O(n4)\nLow pass filters filter out high frequences, high pass filters filter out low frequencies.\nProperties of Linear Filters\n\nSuperposition: distributive law applies to convolution. Let F1​ and F2​ be digital filters. Then (F1​+F2​)⊗I(x,y)=F1​⊗I(x,y)+F2​⊗I(x,y)\nScaling. Let F be a digital filter and let k be a scalar. (kF)⊗I(x,y)=F⊗(kI(x,y))=k(F⊗I(x,y))\nShift invariance: output is local (doesn’t depend on absolute position in image)\n\nCharacterization Theorem: Any operation is linear if it satisfies both superposition and scaling. Any linear, shift-invariant operation can be expressed as a convolution.\nNon-linear filters\n\nMedian Filter (take the median value of the pixels under the filter), effective at reducing certain kinds of noise, such as impulse noise (‘salt and pepper’ noise or ‘shot’ noise)\nBilateral Filter (edge-preserving filter). Effectively smooths out the image but keeps the sharp edges, good for denoising. Weights of neighbour at a spacial offset (x,y) from the center pixel I(X,Y) given by a product exp−2σd2​x2+y2​exp−2σr2​(I(X+x,Y+y)−I(X,Y))2​. We call the first half of the product the domain kernel (which is essentially a Gaussian) and the second half the range kernel (which depends on location in the image).\nReLU. x for all x&gt;0, 0 otherwise.\n\nSampling\nImages are a discrete (read: sampled) representation of a continuous world.\nAn image suggests a 2D surface which can be grayscale or colour. We note that in the continuous case that\ni(x,y) is a real-valued function of real spatial variables, x and y. It is bounded above and below, meaning 0≤i(x,y)≤M where M is the maximum brightness. It is bounded in extent, meaning that x and y do not span the entirety of the reals.\nImages can also be considered a function of time. Then, we write i(x,y,t) where t is the temporal variable. We can also explicitly state the dependence of brightness on wavelength explicit, i(x,y,t,λ) where λ is a spectral variable.\nWe denote the discrete image with a capital I as I(x,y). So when we go from continuous to discrete, how do we sample?\n\nPoint sampling is useful for theoretical development\nArea-based sampling occurs in practice\n\nWe also quantize the brightness into a finite number of equivalence classes. These values are called gray-levels\nI(x,y)⟹⌊Mi(x,y)​(2n−1)+0.5⌋\nTypically, n=8 giving us 0≤n≤256\nIs it possible to recover i(x,y) from I(x,y)? In the case when the continuous is equal to the discrete, this is possible (e.g. a completely flat image). However, if there is a discontinuouty that doesn’t fall at a precise integer, we cannot recover the original continuous image.\nA bandlimit is the maximum spatial frequency of an image. The audio equivalent of this is audio frequency, the upper limit of human hearing is about 20kHz which is the human hearing bandlimit.\nAliasing is the idea that we don’t have have enough samples to properly reconstruct the original signal so we construct a lower frequency (fidelity) version.\nWe can reduce aliasing artifacts by doing\n\nOversampling: sample more than you think you need and average\nSmoothing before sampling: reduce image frequency\n\nThis creates funky patterns on discrete images called moire patterns. This happens in film too (temporal aliasing), this is why wheels sometimes look like they go backwards.\nThe fundamental result (Sampling Theorem): For bandlimited signals, if you sample regularly at or above twice the maximum frequency (called the Nyquist Rate), then you can reconstruct the original signal exactly.\n\nOversampling: nothing bad happens! Just wasted bits.\nUndersampling: things are missing and there are artifacts (things that shouldn’t be there)\n\nShrinking Images\nWe can’t shrink an image simply by taking every second pixel\nArtifacts will appear:\n\nSmall phenomena can look bigger (moire patterns in checkerboards and striped shirts)\nFast phenomena can look slower (wagon wheels rolling the wrong way)\n\nWe can sub-sample by using Gaussian pre-filtering (Gaussian blur first then throw away every other column/row). Practically, for ever image reduction of a half, smooth by σ=1"},"thoughts/in-group-bias":{"title":"In-group bias","links":[],"tags":["seed"],"content":"Your Brain on Groups\nEzra Klein in Why We’re Polarized\n\nThe most important principle of the subjective social order we construct for ourselves is the classification of groups as ‘we’ and ‘they’. Once someone has become a ‘they’ we are used to dismissing them, competing against them, discriminating against them even if there is no reason for it in terms of our own interests (Tajfel)\n\nHistorically, people have nurtured their prejudices as they believed they reflected reality — we disliked those we disliked because we had reason to dislike them\nTajfel’s Experiement\n\n64 boys between the age of 14 and 15 all from the same school knowing one another already\nSorted into two groups arbitrarily based off some previous shared experience (in this case, ‘performance’ on a dot estimation test)\nTajfel expected no intergroup behaviour as he wanted a baseline yet, even this test showed group identity taking hold and mutating into bias\nIn fact, far from behaviour showing a pure desire to maximize their group’s gains, they often gave their group less to increase the difference between them and the out-group\n\nHuman beings evolved to exist in groups. To be part of a group and to see that group thrive meant survival. To be exiled from a group, or to see your group crushed by its enemies, could mean death. Is it really so strange that we evolved to feel the life-and-death stakes of group belonging and status?\nPolitics is a team sport\n\n“the behaviour of partisans resembles that of sports team members acting to preserve the status of their teams rather than thoughtful citizens participating in the political process for the broader good”\nWhen people spend endless hours volunteering, planting yard signs, writing checks, what is likely foremost in their minds is that they are furious with the opposing party and want intensely to avoid losing to it — not a specific issue agenda\nIn Open versus Closed\n\nThe least-engaged voters tend to look at politics through the lens of material self-interest “what will this policy do for me?”\nThe most-engaged look at politics through the lens of identity “what does support for this policy position say about me?”\n\n\nOur political identities are not our only identities: polarization is about which identities get activated\n\nThe problem with increasing polarization is that a single vote can now “indicate a person’s partisan perference as well as his or her religion, race, ethnicity, gender, neighbourhood, and favourite grocery story.” This is no longer a single social identity, partisanship is now a mega-identity\n\n\nIyengar: “Political identity is fair game for hatred. Racial identity is not. Gender identity is not.”\nBipartisan cooperation is often necessary for governance but irrational for the minority party to offer\n"},"thoughts/incentives":{"title":"Incentives","links":["thoughts/money","thoughts/tragedy-of-the-commons","thoughts/public-goods","thoughts/maintenance","thoughts/play","thoughts/tribe-flourishing","thoughts/organizing-system","thoughts/social-contracts"],"tags":["sapling","pattern"],"content":"How do we motivate people? Is money effective as an incentive?\nHow do we prevent the tragedy of the commons, help fund public goods, and incentivize maintenance?\nIn all honesty, I think that the only way to incentivize things is to pull at what intrinsically motivates them and to make what they do feel like play. Create spaces of local abundance for people to feel comfortable trying new things and enable tribe flourishing\nLibertarian Paternalism\nThe goal of encouraging the design of organizing systems and policies that maintain or increase freedom of choice but which at the same time influence people to make choices that they would judge as good ones\nSee also: social contracts"},"thoughts/incremental-view-maintenance":{"title":"Incremental view maintenance","links":["thoughts/Datalog"],"tags":["seed"],"content":"Materialized views speed up query evaluation by storing the results of specified queries. One problem of materialized view is its maintenance. Materialized views have to be brought up to date when the underling base relations are updated.\nSee also: Datalog\nDRed\nDelete/Rederive\n\nCompute difference in facts between two points\nDelete all derivations that rely on deleted facts\nRederive all facts with alternative derivations\n\nNoria\nSource\nA materialized view is a cache.\nBy reasoning about every cache as a partially materialized view (assume it can be explicitly defined), it means if we can cache partially materialized views consistently, we solve the cache invalidation problem.\nThe Noria paper from OSDI’18 introduced a new concept called partially-stateful data-flow to solve the consistency problem for partially materialized views.\nNoria will internally compile the materialized view into a DAG of partially-stateful and stateless operators.\nIf the underlying data store supports ordering primitives such as Hybrid Logical Clock, we can probably apply the techniques described in the OSDI’20 FlightTracker paper to provide Causal Consistency in Noria – a big improvement over Eventual Consistency."},"thoughts/independent-research":{"title":"Independent Research","links":["posts/the-fools-who-dream","thoughts/self-confidence","thoughts/building-in-public","thoughts/exploit-explore","thoughts/attention"],"tags":["sapling"],"content":"See also: the-fools-who-dream\n\nIn independent research, one often pendulums between two brains that drive your day-to-day\n\nBrain 1: I want to make change in the world, I want to ship and build\n\nThis is the beginners mind, it helps you escape the conceptual walls of today’s dogma.\nKuhn pointed out, the biggest ideas transcend the existing paradigm and create their own lane.\n\n\nBrain 2: I want to understand why this works the way it does\n\nIt is almost always Brain 2 thinking that leads to incredibly high payoffs in clarity and increased conviction.\n\n\n\n\nDon’t have conviction that you are right because that will lead to disappointment. Have conviction that you will learn regardless. It is sufficient to do things to learn and to understand (even if just about yourself)\n\nThis comes with a certain level of self-confidence and/or foolishness\n“Who am I to even try to solve this problem, when so many people smarter than me have tried and failed in the past?”\nThe people who succeeded had a healthy smidge of arrogance: a belief, perhaps irrational, that while they were not guaranteed to succeed, they had a fair shot.\nIf they also had the thought, would they have even tried their hand at the problem?\n\n\nOften times, it is one core principle that if followed to its natural conclusion/end will result in a fundamental perspective shift (e.g. quantum mechanics).\n\nWhat is that core principle that sits at the heart of everything you find interesting? The connection between the dots is only evident in hindsight so don’t spend too long thinking about it. But just follow your gut, it right more often than not.\n\n\nOn feedback and doing research in public. People often ask me if building in public is actually worth doing.\n\nI’ve found a nice middle ground where I’m public enough about my work where anyone interested can find it without too much trouble but not public or frequent enough that my only reward/validation cycle is likes on Twitter\nStill, being somewhat public about your work is good on two dimensions\n\nBreaking things into legible pieces is important. If not for other people, for yourself to have small wins and to crystallize thought into something that is mostly self-contained\nYou will expand the realm of what is possible for others. People will look at you and be like “wow, I didn’t even know that was even an option”; that being whatever endeavour you are pursuing, whether that be independent research, film-making, or anything outside of the normal 9-5\n\n\nKasra puts this balance this very well in his piece on thinking but I think it applies to research more broadly as well: “When you have an idea, before you dump it onto the world, there is value in cultivating it, massaging it, rewriting it again and again … Do this in silence, and then bring it to others and get their thoughts.”\n\n\nCommunities and feeling lonely\n\nI often say that independent research doesn’t mean you have to do it alone. I think many people wait around to kind of bump into the right community people that are the perfect mix of genuine, kind, and curious.\nI’ve found the best way to do that is to not only active seek these people out, but to create watering holes where these people gather\nVonnegut similarly thought that community building was incredibly important: “What should young people do with their lives today? Many things, obviously. But the most daring thing is to create stable communities in which the terrible disease of loneliness can be cured.”\n\n\nDon’t get trapped in the mindset of having every little thing you do fit perfectly in your grand master plan. An effective hill-climber is one that sometimes goes in the wrong-direction! (see also: exploit explore)\nWhat solitude gives you is an opportunity to study what personal curiosity feels like in its undiluted form, free from the interference of other considerations (Good Ideas, Henrik Karlsson)\n\nGood ideas — actually, no, great ideas are fragile. Great ideas are easy to kill. An idea in its larval stage — all the best ideas when I first heard them sound bad.\n[Grothendieck] could interface with the mathematical community with integrity because he had a deep familiarity with his inner space. If he had not known the shape of his interests and aims, he would have been more vulnerable to the standards and norms of the community—at least he seems to think so.\nPascal, self-teaching mathematics because his father did not approve, rederived several Euclidean proofs. There is also a lot of confusion and pursuit of dead ends. Newton looking for numerical patterns in the Bible, for instance. This might look wasteful if you think what they are doing is research. But it is not if you realize that they are building up their ability to perceive the evolution of their own thought, their capacity for attention.\n\n\n"},"thoughts/indeterminant":{"title":"Indeterminacy","links":["thoughts/Nyāya","thoughts/philosophical-realism","thoughts/representation","thoughts/Twin-Earth-Argument","thoughts/ontology"],"tags":["seed","PHIL240A"],"content":"\nAnirvacanīyatva or indeterminancy\n\nVācaspati Miśra’s Bhāmatī, a commentary on Śamkara’s Brahmasū-trabhāsya.\nIn this chapter/section, objects of erroneous cognition are used to show how the world as it is experienced can neither be denied an independent status nor established as having one. The key idea is the ‘indeterminate’ state as to their ontological status.\nVācaspati does not doubt the correctness of the Nyāya argument for philosophical realism but more so its completeness. I have cognition of a white horse, not of ‘white’ and of ‘horse’. Vācaspati argues that the sum is greater than the whole — it is not sufficient that a description of its constituent elements (‘this’ and ‘silver’) entirely constitute the unitary object of cognition (‘this is silver’).\nCognition-Object relationship\nThe cognition-object relationship here refers to one about representational content.\nRepresentation, in this context, means a cognitive state of being ‘of’ or ‘about’ the object. Important to note that, in this translation, representation is neutral to theories of perception\nExample case where a cognition (thought) is a thought of a specific object\n\nObject gives content (visayatā) to a cognition in the sense it is the subject of the thought and therefore individuates/gives identity to/specifies that cognition\nThe cognition is then a representation (visayitā) of the object\n\nThe form usually is ‘this thing is a shell’ which is in the form of Fα where\n\nα is the object (‘this thing’, dharmin, qualificand, thing being distinguished)\nF is what it is cognised as (the concept, content of thought, dharma, the distinguisher)\nAkin to the Twin Earth Argument, “this” can refer to different things\n\nLike variable naming in programming languages where unification would not unify two ‘this’ from different scopes\n\n\n\nOntologies are theories of objects — a conception of what the world is made up of. Ontologies need to be interpreted\nErroneous Cognition\n\nA cognition is valid if it discriminates between that object and all others.\nDiscrimination is correct identification.\nErroneous cognition occurs when the subject takes it to be true when it is not.\nSublation is the incidence of a later cognition that results in the realization of an earlier cognition being erroneous.\n\nEpistemic indistinguishability: there is no perceived difference between contents of distinct cognitions (i.e. confusing ‘this is shell’ with ‘this is silver’)\nLet us take the simple perceptual demonstrative judgement of the form ‘this is a piece of silver’. This can clearly be a wrong judgement as the relevant object could be a piece of shell.\nWhat does an erroneous cognition say about its object (if anything at all)?\nWell, this states that object and concept must both exist. Even in mistaking the shell of the silver, both shell (instantiated as physical demonstrated object) and silver (instantiated as primary object of cognition) exist. TLDR; erroneous cognition required determinate objects in the determinate world.\nThe state of taking ‘this’ to be silver is not explained simply by explaining that ‘this’ and ‘silver’ occur in it. Take for example, the non-erroneous cognition ‘this looks like silver’ which contains both constituent elements but is epistemically different from the earlier erroneous claim of ‘this is silver’.\nSo how does an erroneous cognition Gα arise?\n\nSuppose the subject knows when the distinguisher G is valid\nG is not necessarily true given the conditions at hand may not be as remembered compared to the conditions under which G-identification was last true (temporally different).\nIt is true that Gβ where β is a piece of silver at some point in the past\nIt is true that α exists\nIt is false that Gα\nIt is false that α is perceived as β\nThe erroneous cognition arises when the subject believes 5 and 6 to be true instead.\n\nThe question then becomes, what is the object α that is being individuated here?\nFirst, let F and G be incompatible (read: mutually exclusive) qualifiers. That is, if Fα then ¬Gα and vice versa.\nAdvaitin makes two contradictory conclusions here\n\nUtilizes P1: asserting that an object exists requires there to be a cognition with that object as its content through pramānas. Concludes that Gα cannot exist.\nUtilizes P2: what doesn’t exist must not be the content of any cognition as it is not experienced as all. Concludes that Gα exists.\n\nBoth P1 and P2 rely on relevant (and fairly uncontroversial principles) to come to contradictory results. This argument shows that there is no way of assigning status to Gα and thus indeterminant."},"thoughts/inevitability-of-centralization":{"title":"Inevitability of Recentralization","links":["thoughts/Network-Theory","thoughts/Internet","thoughts/internet-computing","thoughts/Application-Layer","posts/towards-data-neutrality","thoughts/attention-economy","thoughts/progress","thoughts/network-effect","thoughts/cascading-failures"],"tags":["sapling","pattern"],"content":"Notes on Gordon Brander’s Decentralization enables permissionless innovation, Centralization is Inevitable and Redecentralization\nTo be efficient is to be fragile, and to be fragile, over the long run, is to go extinct.\nNetwork Models\nSee also: network theory\nIn 1959 RAND, a Californian think-tank, assigned Paul Baran, a young engineer at that time, to develop a communication system that can survive a Soviet nuclear attack.\nHe found that:\n\nThe original centralized communication system is obviously vulnerable as the destruction of the hub would destroy communication between the end stations.\nThe decentralized model, a “hierarchical structure of a set of stars connected in the form of a larger star”, is also too centralized to be viable under attack\nThe distributed model, where kmax​≈kmin​, each node has roughly the same number of connections to each other and there are no hubs in the network\n\n\nBaran, 1964 “On Distributed Communications (Memorandum RM-3420-PR)“\nBaran’s ideas were ignored by the military. The internet thus relied on distributed protocols that allowed each node to decide where to link, creating preferential attachment. This gave rise to a scale-free model of the Internet rather than the original mesh-like topology that Baran imagined.\nWhile not the ideal distributed model Baran initially envisioned, the decentralized model that was actually implemented had several features that made it adaptable to failure. Routers could freely join or leave, something that wasn’t possible in the circuit switching model of the internet.\nMessages would be broken up into little packets, and each packet would be independently routed to its destination, finding its own best path. Packets would be passed across the network like hot potatoes until they found their destination. This was the advent of packet switching.\nNow, it didn’t matter if a router was destroyed because the network could dynamically route around the damage and new routers could be dropped in.\nInevitable Recentralization\nAlthough the underlying protocol layers of our web are decentralized, we see that centralization has re-emerged at the Application Layer: one layer up.\nBut this centralization brings the same vulnerabilities that the centralized communication model brings that Baran designed to communication system to avoid. If AWS goes down, it takes much of the internet with it.\nToday, we see recentralization around\n\nTrust: security, and thus identity and payments, are all bound to a given domain\nData: we have many ‘hubs’ for proprietary data like Google Drive, Facebook, etc. (see: Towards Data Neutrality)\nInfrastructure: the actual server farms and cables the internet runs on (failures in companies like CloudFlare and AWS can lead to global disruptions)\nAttention: too much information and not enough attention to give to things. This fundamental scarcity implies search, discovery, ads, spam, and a bunch of other thorny things. (see: attention economy)\n\nBrander proposes a new rule of thumb:\n\nIf you decentralize, the system will recentralize, but one layer up. Something new will be enabled by decentralization. That sounds like evolution through layering, like upward-spiraling complexity. That sounds like progress to me.\n\nWe see this with hubs on the internet forming, creating scale-free networks. We see this pattern emerge over and over again: a solid sign that there are evolutionary attractors pulling the system in that direction. We know this is true due to the presence of network effects and preferential attachment (e.g. rich-get-richer, economies of scale, running servers is hard, etc.)\nThe presence of hubs in scale-free mean that communication can happen efficiently, averaging loglogN hops as opposed to logN hops for random networks (see the note on average hop distance in various networks). Fitter nodes attract more connections just by virtue of staying alive or being better. This is the Fitness Model of scale-free networks.\nBut the problem with scale-free networks is that they are vulnerable to attack. Though they are resilient to random failure of the nodes (generally need to remove ≈N−1 nodes to bring down the network), the removal of a small fraction of hubs is sufficient to break a scale-free network into tiny clusters.\nKnocking out even a few hubs quickly breaks down the network. Y-axis is the ratio P∞​(0)P∞​(f)​ provides the relative size of the largest connected subgraph\nWhen a random shock inevitable causes the collapse of a few hubs, this creates a cascading failure which causes a system-wide collapse. We return to an unstructured, random network.\nThe key insight is that this process has a direction, from decentralized to centralized, and collapsing back again."},"thoughts/information-behaviour":{"title":"Information Behaviour","links":["thoughts/search","thoughts/information-retrieval","thoughts/information"],"tags":["seed"],"content":"Distinction between information behaviour and information practice\n\nBehaviour: how we search (e.g. type in a short query, click top result, etc.)\nPractice: search as conditioned by social and technical norms and practices\n\nA combination of information retrieval, information encountering, information avoidance, etc. Methods of interacting with information\nA model is a simplified, tentative representation of a phenomenon of interest, often depicted as a graphic. A model describes important factors (variables) and proposes their relationships (associations).\nInformation Seeking Behaviour\nDervin’s Sense-making Model\n\nKnowledge ‘is the sense made at a particular point in time-space by someone’\nInformation seeking is conceptualized as a gap-bridging process in which the individual makes moves, influenced by information, in time and space to reach a desired outcome or goal to make sense of a ever-gappy ‘reality’\nIn this model, an information object is not seen simply as an entity that meets or is relevant to an information need. Rather, information embedded in an information object must be conceptualized by the individual in a particular situation in order to influence actions.\n11 Deadliest Sins of Knowledge Management\nFahey and Prusak (1998)\n\nNot developing a working definition of knowledge.\nEmphasizing knowledge stock to the detriment of knowledge flow.\nViewing knowledge as existing predominantly outside the heads of individuals.\nNot understanding that a fundamental intermediate purpose of managing knowledge is to create shared context.\nPaying little heed to the role and importance of tacit knowledge.\nDisentangling knowledge from its uses.\nDownplaying thinking and reasoning.\nFocussing on the past and the present and not the future.\nFailing to recognize the importance of experimentation.\nSubstituting technical contact for human interface.\nSeeking to develop direct measures of knowledge.\n"},"thoughts/information-foraging":{"title":"Information Foraging","links":["thoughts/rationality"],"tags":["seed"],"content":"Developed at XEROX PARC in the late 1990s and was inspired by biomimicry (specifically, animal behaviour theories)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnimal ForagingInformation ForagingGoalFoodInformationPatchSite containing one or more potential sources for foodA source of information (e.g. website)ForageSearch for foodSearch for informationScentHow likely a patch will provide foodHow promising a source of information appear to userDietTotality of food types that can satisfy hunger for an animalTotality of information sources that a user may consider useful\nRate of gain = Information value / Cost associated with obtaining that information (both actual time/effort and opportunity cost)\nBounded rationality is at play here\nObviously we don’t have perfect estimation for 1) how much information a patch contains and 2) how much time it will take to extract that information. This is where info scent comes in.\nThings that contribute to scent:\n\nPerceived credibility\nInformation density\nLength\n\nEnrichment\nTechniques, tools, and interactions, that maximize the utility of the information foraging. This can happen either between patches or within patches.\nBut, a good user experience involves web pages that are designed so that the user can get the maximum relevant information in the minimum amount of time.\nBehaviour enrichments\n\navoiding context switching (page parking: opening multiple pages in rapid-fire succession to save them for later)\navoid reading the entire page but still getting the majority of information (F-pattern scanning)\ntypically ignoring banners and the right rail\n\nInteraction enrichments\n\ncritically thinking about specific keywords for query\nuse of within-page search (ctrl-f) to quickly locate content\n"},"thoughts/information-professions":{"title":"The Information Professions: Knowledge, memory, heritage","links":["thoughts/information-scaling-threshold","thoughts/language","thoughts/generational-learning","thoughts/writing","thoughts/desire-paths","thoughts/Extended-Mind-Hypothesis"],"tags":["seed"],"content":"Source: The information professions: knowledge, memory, heritage by Marcia Bates\nThe traditional view of disciplines is that they lie on a spectrum where the study of arts and humanities (disciplines of the cultural record) lie on one end and mathematics and physical sciences on the other (the sciences of information). I’m not sure I fully agree with this, I think the pursuit of knowledge and truth in physical sciences ties very well with the pursuit of documentation within the cultural record and humanities. It would be more of a circle than a spectrum.\nBates proposes meta-disciplines which study the entire spectrum and how they fit together.\nExamples of these include information disciplines, communication/journalism, and education.\nThe fundamental engine of development of this field of information professions is need (see: information scaling threshold).\n\nHuman beings want to retain informational resources, and, after a very short time, these resources collect at such a rate that some principles of selection, organization, etc., need to be brought to bear, in order for the resources to continue to be available for effective use.\n\n3 information flow lineages through the history of life on Earth\n\nGenetic, information about the history of life on Earth is literally encoded in your DNA\nNeural-cultural, information is passed down between generations through language, storytelling, and sharing (see: intergenerational learning)\nExosomatic, information is embedded into the world around us through writing, digital systems, desire paths, etc. (see: Extended Mind Hypothesis)\nResidue, trace/abandoned information degrading back into nature\n\n\nThe storage and management of exosomatic information was one of the major contributors to the exponential growth of human knowledge and power over nature.\n"},"thoughts/information-retrieval":{"title":"Information Retrieval","links":["thoughts/information-foraging","thoughts/search","thoughts/human-computer-interaction"],"tags":["seed"],"content":"Information foraging as a metaphor for exploring information retrieval and seeking. Heavily tied to search systems.\nA ‘closed stack’ library facilitates information transfer from collection to information seeker through some intermediary who controls access to said objects. ‘Open stack’ libraries often allow direct access to information objects (although sometimes with an intermediary’s help). Both may utilize some ordering/index.\nThe goal of information seekers is to find information that are useful in answering the original query or research questions\nSystem Relevance is the measure or degree to which a document ‘was about’ a topic. This can be used to measure performance of an information retrieval system by seeing the extent to which the system was able to retrieve all the relevant documents in the collection to a query (recall: ratio of relevant documents retrieved to total relevant documents), and only the relevant documents (precision: ratio of relevant documents retrieved to all documents).\nHuman relevance is the degree to which the document is useful to the search query of the user. Composed of 3 separate facets\n\nSituational Relevance: useful, efficient, applicability to the task and situation\nCognitive Relevance: suited to the knowledge level of the searcher; contains novel content of interest\nAffective Relevance: satisfies user goals and motivations, trustworthy, algns with beliefs and is emotionally engaging, aesthetics\n\nTypes of interactions then are\n\nBetween information seeker and intermediary\nBetween the intermediary and the collection\nBetween the information seeker and the collection\n\nInformation retrieval has a whole slew of related terminology surrounding it:\n\nIndexing (characterizing documents)\nRetrieval (characterizing information seeker’s information problem as a query put to the collection)\nInformation retrival system (combination of method to index, organization of the collection, and method of querying and searching the collection)\n\nCranfield/SMART Paradigm\nA ‘test collection’ is constructed, consisting of a collection of documents, a set of descriptions of information ‘needs’, and an exhaustive evaluation of the relevance of each document to each information need description.\nWhy it may be faulty:\n\nrelevance assessments are collected once for each information need for each document alone, without reference to any other document (most queries involve looking at multiple sources, relevance can depend on documents they have seen before)\nevaluation is made of the performance of just the single query that represents each information need (queries are often iterative, you originally may not know what you don’t know or what you are looking for)\nmodel of the user on which the measures are based limits their appropriateness for other possible user goals\n\nThe dominance of the SMART (very technical and computational approach to information retrieval) means the research in the field of information retrieval was split into two disciplines: information retrieval (mainly CS/computational, study of formal models of information retrieval) and information seeking/use (mainly library/information science, study of people’s information-seeking behaviours and uses of and interactions with information systems). As a result, ‘interactiveness’ was more of an afterthought and relegated to the peripheral.\nEarly information retrieval systems\nMostly ‘batch’, non-interactive ‘closed stack’ systems.\nWalker (1971)\n\nA key difference in interactive terminal search is that data is brought to the searcher rather than the searcher going to the data\n\nCreating an online system as an access point for databases. However, these information retrieval systems required substantial knowledge not only of the characteristics of the database and its indexing systems but also of the details of the query language, boolean algebra for effective queries, and effective techniques for modifying queries given search results. This resulted in a new class of librarians and information scientists — search intermediaries — well versed in this new form of information seeking.\n5 main filters to forming an effective query\n\nDetermining the subject of the information query\nDetermining the objective/motivation of information-seeking behaviour\nDetermining the personal characteristics of the inquirer\nEstablishing the relationship between the query description and the information system\nDetermining anticipated/acceptable answers\n\nThe problem: most people don’t actually know the thing they are searching for.\nSavage-Knepshield and Belkin (1999)\nBennet was looking at some very important HCI considerations here, esp. w.r.t. the differences between human searcher and its automated counterpart. Asks some design questions to determine\n\ncharacteristics of the searchers served by the facility\nthe conceptual framework presented to the searcher\nthe role of feedback to the searcher during searches\noperational characteristics of the facility: the command language, display formats and response time\nthe constraints of the terminal and techniques to ameliorate them\nthe effects of the bibliographic database on the user interface for searchers\nhow to introduce the search facility to the user\nthe role of evaluation and feedback in the redesign cycle\n"},"thoughts/information-scales":{"title":"Information Scales","links":["thoughts/context"],"tags":["sapling","pattern"],"content":"Creating a browsable store of knowledge that provides something useful at all distance scales (e.g. viewing the entire library, just a subcategory, a single file, etc.)\nFor example, dictionaries cut indentations into the pages to help you find the neighborhood of your entry then let you flip along glancing at guide words to finish your search.\nCan we create skimmable views on data? Perhaps a spatially-anchored overview of the different ‘branches’ of knowledge\nEngelbart Zoom\nEngelbart on HyperScope\nHyperscope is a browsing tool that enables most of the viewing and navigating features called for in Doug Engelbart’s open hyperdocument system framework (OHS) to support dynamic knowledge repositories\n\nSemantic Zoom\nSource\nOur field of view (or context) on all of that data is defined by the interfaces we use. Rigid, pre-packaged interfaces don’t let us move our field of view around, should we wish to get a different vantage point on some data we care about."},"thoughts/information-scaling-threshold":{"title":"Information Scaling Threshold","links":["thoughts/attention-economy","thoughts/Dunbar's-Number","thoughts/Dark-Forest-Theory-of-the-Internet"],"tags":["seed"],"content":"\nThroughout the Holocene, societies developed additional layers of administration and more information-rich instruments for managing and recording transactions and events as they grew in population and territory\n(Shin, et al. 2020. Scale and information-processing thresholds in Holocene social evolution)\n\nWhen a society hits this information threshold, it stops functioning until it can invent new ways to sense make with the new abundance of information. If they don’t pull that transition off, they die.\nThe attention economy is a symptom human society hitting this information scaling threshold. Philosopher Paul Tillich posits that when social sensemaking fails to keep up with reality, we experience it as a kind of mass neurosis. Everybody has a crisis of meaning at the same time. Life stops making sense.\nWe’ve tried to cope through Dunbar-scale spaces and the cozy web\nMore in Gordon Brander’s Thinking Together\nResearcher Simon DeDeo considers it a phase transition in human culture, dividing history into three eras:\n\nThe premodern/archaic era, when most information was generated by non-human phenomena like seasons, weather, drought, flood, hail, lightning. “The gods”.\nThe modern/postmodern era, when most information was broadcast by a small number of information “sellers”, and consumed by a large number of information “buyers”.\nThe user-generated content era, where most information is produced/consumed by users, in a tight feedback loop between attention allocation and content production/consumption.\n\nAs We May Think\nThe 1945 essay by Vannevar Bush\n\n“The difficulty seems to be, not so much that we publish unduly in view of the extent and variety of present day interests, but rather that publication has been extended far beyond our present ability to make real use of the record. The summation of human experience is being expanded at a prodigious rate, and the means we use for threading through the consequent maze to the momentarily important item is the same as was used in the days of square-rigged ships.”\n\nHe stresses, as many of us believe today, that mechanization — or, algorithms in the contemporary equivalent — will never be a proper substitute for human judgment and creative thought in the filtration process:\n\nMuch needs to occur, however, between the collection of data and observations, the extraction of parallel material from the existing record, and the final insertion of new material into the general body of the common record. For mature thought there is no mechanical substitute. But creative thought and essentially repetitive thought are very different things. For the latter there are, and may be, powerful mechanical aids.\n"},"thoughts/information-system":{"title":"Information Systems","links":["thoughts/Internet","thoughts/design-goals","thoughts/library"],"tags":["sapling"],"content":"Source: Foundations of Information by Amy J. Ko\nInformation Systems are processes that organize people, technology, and data to allow people to create, store, manipulate, distribute, and access information.\nWhy build the ARPANET (precursor to the Internet)?\n\nExplore the area of computer networking research (how to share computing resources)\nSharing resources for research communites (e.g. datasets)\nScaling past the speed of the phoneline (2.4kb/s)\nUtility for command and control of various military sites (military-industrial complex)\n\nQualities of information systems\n\nAccuracy: how accurate and truthful is data in the system?\nReliability: how many dependencies does it need? How often do those fail?\nLearnability: how many skills do you need to acquire to use it?\n\nSee also: design goals, library\n\nChoosing the best system for a particular task then isn’t just about choosing the latest technology, but carefully understanding the task at hand, and what types of systems might best support\n"},"thoughts/information":{"title":"Information","links":["thoughts/information-system","thoughts/information-professions","posts/collaborative-thinking","thoughts/context","thoughts/representation","thoughts/collections","thoughts/visualization"],"tags":["seed"],"content":"\nInformation is a thing that conveys meaning about stuff, transmitted from one place to another.\n\nSource: What is Library &amp; Information Science? by Stacks &amp; Facts on YouTube\nWe store, access, and organize information through information systems.\nCharacteristics\n\nTruthfulness: information is correct\nAboutness: tells us about something else\nThingness: information is a thing, although perhaps intangible\n\nThe study of information in and of itself is a research area too, through and the Information Professions\nHierarchy\nIn informatics, there exists the concept of an Information Hierarchy, Knowledge Pyramid, or DIKW Hierarchy.\nData is transformed into Information, which is transformed into Knowledge, which is then transformed into Wisdom.\n\nData is not god-like; it is not absolute truth. It is empathetic. It is imperfect. It is human.\n\n\nData: raw or elementary observations about properties of objects, events, and their environment\nInformation: data after aggregation, processing, analyzing, formatting, and organization to add meaning and context\n\nInformation is political\nSource: Foundations of Information by Amy J. Ko\nAs information becomes more abundant and saturated in our world, has information been any more of less important/necessary? We may have access to more of it, but it’s not obvious that this is better—just different.\n\nWhile technology has changed the volume, pace, and sources of information in our daily lives, it has not changed its essential power to shape our knowledge and our behavior in the world.\n\nInformation fuels discovery. It enables us to build off of each others work, and accomplish more than what a single person could.\nInformation organizes us. The models of thinking and computation (e.g. numbering system) affect how we think about the world writ large and forms the basis for the computational systems which we interact with daily.\nInformation regulates us. The law, in its codified and written forms, encodes great injustices and inequities through dictating what rights are and who gets them.\nInformation identifies us. Each of us leave informational foot prints as we live our lives through documents, interactions, and conversations.\nContrary to Marshall McLuhan’s famous phrase, “the medium is the message”, it seems as if the vast majority of the power from information is derived from its meaning and context rather than its medium or method of transmission.\n\nOf what value is its microprocessor, its memory, and its applications if there is no content to consume and no people to connect with?\n\nAaron Swartz in the “Guerrilla Open Access Manifesto”\n\nInformation is power. But like all power, there are those who want to keep it for themselves. The world’s entire scientific and cultural heritage, published over centuries in books and journals, is increasingly being digitized and locked up by a handful of private corporations… . Providing scientific articles to those at elite universities in the First World, but not to children in the Global South? It’s outrageous and unacceptable… . We can fight back. Those with access to these resources—students, librarians, scientists—you have been given a privilege. You get to feed at this banquet of knowledge while the rest of the world is locked out. But you need not—indeed, morally, you cannot—keep this privilege for yourselves. You have a duty to share it with the world.\n\nInformation Representation\nHow do we represent documents and information in our databases and collections?\nSee also: visualization\nIndexing\nIndex features\n\nindex term (textual description)\nnumber of words (numerical, length description)\netc\n\nWhen indexing, we want\n\nfeatures that make a document easy to find given some similarity measure between query and document (if a term is too broad, it would apply to too many documents)\nhigh discriminatory power so different documents are sufficiently distinct\n\nSpecificity: degree to which a term is broad or narrow\nAutomatic Indexing\nAutomatic indexing uses a set of algorithms to convert a document into a set of index terms.\n\nTokenization (how do we turn text into tokens?). However, we need to be wary of edge cases like hyphenation, punctutation, abbreviations, numbers, etc.\nStopword removal (removing words that occur very frequently but don’t contribute very much to overall meaning of sentence). However, we need to be wary of creating new phrases that never existed and removal of entire phrases (e.g. ‘to be or not to be’)\nConflation and stemming (recall enhancing technique that takes different forms of words and replaces them by a single form). Example stemmers include Porter and Krovetz stemmers (difference is that Krovetz is more conservative, only stems to words in a dictionary). However, this can reduce precision when stemming results in incorrect words.\nTerm weights (which terms are the most important? One common approach is to use term frequency between documents — sometimes called Document Frequency or DF. The larger the DF, the less useful it is to discriminate between two documents. Measure of importance then is the inverse document frequency. The most common measure is term frequency multiplied by inverse document frequency TF×IDF)\n\nCatalog\nCatalogs allow for retrieval by simple matching whereby the user has an index term in mind and can then find all items that are indexed by that term.\nBy its nature, the catalog is divided, and the subject catalog is on its own. The classification scheme typically mirrors the scheme used to organize the physical items in the library."},"thoughts/infrastructure":{"title":"Infrastructure","links":["thoughts/Myth-of-the-Infrastructure-Phase","thoughts/incentives","thoughts/emergent-behaviour","thoughts/Protocol","thoughts/social-contracts","thoughts/creation-vs-maintenance","thoughts/fiction","thoughts/epistemology","thoughts/public-goods"],"tags":["sapling"],"content":"Infrastructure should be\n\nEmbedded: Infrastructure is “sunk” into, inside of, other structures, social arrangements and technologies\nTransparent: Infrastructure is transparent to use, in the sense that it does not have to be reinvented each time or assembled for each task, but invisibly supports those tasks.\nReach/scope: This may be either spatial or temporal — infrastructure has reach beyond a single event or one-site practice. Occurs across multiple places.\nLearned: Strangers and outsiders encounter infrastructure as a target object to be learned about. New participants acquire a naturalized familiarity with its objects as they become members.\nLinks with conventions of practices: Infrastructure both shapes and is shaped by the conventions of a community of practice (e.g. the QWERTY keyboard).\n\nSource: Steps Toward an Ecology of Infrastructure: Design and Access for Large Information Spaces by Susan Leigh Star and Karen Ruhleder\nPlatforms\n“A platform is when the economic value of everybody that uses it exceeds the value of the company that creates it. then it’s a platform”\nWhat is infrastructure even?\nWhich came first, the infrastructure or the applications that depend on it?\nSee also: Myth of the Infrastructure Phase\n“infrastructure as potential energy held in suspension” (Boyer)\nDo we need to define infrastructure?\n\nThus, infrastructure design requires a more subtle approach: creating the right incentives, environments, and dependencies to encourage well-being while preserving user autonomy.\n\nInfrastructure can be emergent:\n\n“I have admiringly called this the “Procrastination Principle,” wherein an elegant network design would not be unduly complicated by attempts to solve every possible problem that one could imagine materializing in the future. We see the principle at work in Wikipedia, where the initial pitch for it would seem preposterous: ‘We can generate a consummately thorough and mostly reliable encyclopedia by allowing anyone in the world to create a new page and anyone else in the world to drop by and revise it.‘”\n\nHard and soft infrastructure\nHard infrastructure refers to hard rules and goods (e.g. parks, laws, highways, etc.)\nSoft infrastructure is upheld as a social protocol, an institution whose maintenance relies on dedication and value alignment. More closely related to social contracts\nPost-destruction\nTo oblivion and beyond: Imagining infrastructure after collapse\nAn alternate take on creation vs maintenance\nInfrastructures do not function forever. Like everything else, they lose in the universe’s constant battle against entropy.\nAs Boyer (2016) argues, infrastructures possess a temporal persistence that “points deathward.” Oil refineries along the Texas Gulf Coast, for example, take part in a system of resource extraction that hastens climate change; at the same time, climate change threatens the continued functioning of these refineries, and oil companies have requested federal funds to protect their facilities from the destructive environmental phenomena that they have a hand in creating (Associated Press, 2018).\n\nCruel optimism: the experience of placing hope in an object that perpetually prevents the realization of that hope\nAngry optimism: “does not attempt to escape or control the dangers of the present or to return to the comforts of the past but instead looks forward to the possibilities of a time beyond these”\n\nNot just maintenance, but rebuilding. In the context of infrastructure and collapse, cruel optimism is the belief that rebuilding is a way to heal. “Breakdown might instead represent an opportunity to create futures that do not resemble the past.”\nLifetime of an infrastructure doesn’t just cover its functional lifespan. Many components to this:\n\nService life (assuming ongoing maintenance, duration of functionality)\n\nRisk analysis and modelling is one way of calculating thius\n\n\nMaterial life (how long will it take for the material of the infrastructure itself to degrade and erode)\n\nIn fact, for the vast majority of infrastructures, the material life of an infrastructure will far outlast its service life (e.g. concrete aqueducts of the Romans almost 2000 years ago).\nRepair, they argue, is just an excuse for returning things to what they used to be rather than see it as a chance for change. Berlant’s worry is that the repair of infrastructure merely reinstates a comfortable yet crisis producing past\nA new form of utopianism in science fiction: “if present conditions lead inexorably to collapse, how can that collapse be used as a resource from which to build more equitable ways of life”\nCommons\nThe commons: Infrastructures for troubling times\n\nThe common usually refers to an orientation toward life and value unbound by concepts and divisions of property, and points to the world both as a finite resource that is running out and an inexhaustible fund of human consciousness or creativity\n\nSensus communis: “‘common sense’ is merely the bourgeois order of truth standing in for the universal, what Stoler calls ‘‘a folk epistemology.’’”\nModeration\nSource\nHow do we draw the line between an end product and infrastructure? How should infrastructure regulate usage on its platform (if at all)? Been thinking about AWS’s decision to remove Parler recently and whether it was warranted for AWS to do so. At what level of infrastructure should something become a ’public good’? As more and more of our digital infrastructure is built out under private companies, does it change how we govern content on top of it?"},"thoughts/innovation":{"title":"Innovation","links":["thoughts/progress","thoughts/Lindy-effect","thoughts/maintenance","thoughts/innovation"],"tags":["sapling"],"content":"Innovation: the word to hide the lack of substance. A term gains popularity because it resonates with the zeitgeist, reaches buzzword status, then suffers from overexposure and cooptation.\nIn formal economic terms, ‘innovation’ involves the diffusion of new things and practices. The term is completely agnostic about whether these things and practices are good. How much of this is tied back to the definition of progress?\n\nProved to be useful: innovations\nProved to be useful over 40 years: technology (lindy innovation)\n\nInnovation as a form of maintenance?\nIs there even a distinction of maintenance and innovation? Is maintenance just micro-innovation?\nMaintenance is fixing parts of an existing system whereas innovation is wholistic and changing the whole system"},"thoughts/instrumentalism":{"title":"Instrumentalism","links":["thoughts/move-fast-and-break-things","thoughts/telic-action","thoughts/quantization","thoughts/recommendation-system","thoughts/Goodhart's-Law","thoughts/play","thoughts/attention"],"tags":["sapling"],"content":"Source: Value Beyond Instrumentalization by Jasmine Wang\nReminiscent of the move fast and break things Silicon Valley ethic.\n“This tendency to instrumentalize, or to treat something as a means or resource for achieving some end goal, shows up in the personal lives of many technologists. Many types of “fun” are made telic).”\n\n“All that was good becomes data. All that was beautiful is now efficient.” (Jacques Ellul, in The Technological Society)\n\nAn inherent quantization and optimization of everything. Prevalent in recommendation systems where poor proxies are often the target of optimization that ignores the humans under them (e.g. engagement metrics rather than satisfaction and wellbeing). Goodhart’s Law\n“Creation of spaces of local abundance (as opposed to, say, a space like YCombinator, where the space is meant to help you reach a specific external outcome under time-bounded pressure) where technologists can play with ideas and think freely, similar to parks and urban forests in a bustling city”\n\nDecide carefully what to pay attention to out of an infinity of possible ends to apply your time and resources, and what worlds you wish to bring about. This is a call to build a beautiful and deeply good future.\n"},"thoughts/intellectual-property":{"title":"Intellectual Property","links":["thoughts/rights"],"tags":["seed","CPSC430"],"content":"According to the World Intellectual Property Organization, intellectual property “refers to creations of the mind: inventions; literary and artistic works; and symbols, names and images used in commerce”\nPerhaps can also be called intellectually monopoly\nProperty Rights\nSee also: rights\nProposed by John Locke\n\npeople have a right to property in their own person.\npeople have a right to their own labour\npeople have a right to those things that they have removed from Nature through their own labor\n\nno person claims more property than he or she can use\nwhen people remove something from the common state in order to make it their own property, there is still plenty left over for others to claim through their labor\n\n\n\nProtections\n\nTrade Secret: intellectual property that provide a company with a competitive advantage\n\nThey do not expire\nNot suitable for IP that needs to be viewed (e.g. a movie)\nCan be circumvented using reverse engineering\n\n\nTrademark: a word, symbol, picture, sound, or color used by a business to identify goods\n\nFor establishing a “brand name”\nIf the brand name becomes a common noun (e.g. yoyo, escalator) then the company loses it right to exclusive use\n\n\nPatent: limited period of time to machines, systems, and other inventions\n\nProvides a detailed description of the invention\nLifetime is 20 years, after it expires, anyone has the right to make use of its ideas\n\n\nCopyright: protections for authors with certain rights to original works they have written\n\nThe right to reproduce the copyrighted work\nThe right to distribute copies of the work to the public\nThe right to display copies of the work in public\nThe right to perform the work in public\nThe right to produce new works derived from the copyrighted work\n\n\n\nSoftware\nCopyright protects the original expression of an idea, not the idea itself\nCopyright usually protects the executable program, not the source program. Typically, the source code to a program is confidential, in other words, a trade secret of the enterprise that developed it.\nPatent-holding companies aggressively use the courts to enforce their patent rights; these companies are sometimes referred to by their detractors as patent trolls. The Williamson v. Citrix Online decision sets a precedent for other courts to strike down software patents that are determined to be “too broad and indefinite”\nLimits to IP Protection\nThere is a tension between the need to reward the creators of intellectual property by giving them exclusive rights to their ideas and the need to disseminate these ideas as widely as possible.\nCongress addresses this by only granting exclusive rights for a limited period of time\n\nFair Use\nExamples of fair use include citing short excerpts from copyrighted works for the purpose of teaching, scholarship, research, criticism, commentary, and news reporting.\n\nWhat is the purpose and character of the use?\nWhat is the nature of the work being copied?\nHow much of the copyrighted work is being used?\nHow will this use affect the market for the copyrighted work?\n\nDigital rights management (DRM)\nFor protecting after-market IP\nDRM refers to any of a variety of actions owners of intellectual property may take to protect their rights. As Christopher May puts it, “All DRM technologies are aimed at tracking and controlling the use of content once it has entered the market”\nMany experts suggest that any technological “fix” to the problem of copyright abuse is bound to fail. All prior attempts to create encryption or anticopying schemes have been abandoned or circumvented."},"thoughts/intelligence":{"title":"Can Machines Think?","links":["thoughts/explainability","thoughts/ontology","thoughts/Materialism","thoughts/causality","thoughts/intentionality","thoughts/Chinese-room-argument","thoughts/information"],"tags":["sapling"],"content":"Intelligence as a measure of information conversation ratio. How do we test intelligence of machines vs humans?\nIntelligence can only be contextually based on information available. There are no intrinsically difficult questions, only with respect to inputs\nCan intelligence be artificial? (Dretske)\nThere are two ways of thinking about it\n\nLike money → everyone has it, some have more than others\n\nphilosophers view\n\n\nLike wealth → something possessed by only those who have more than the average amount of money\n\ncomputer scientists view\n\n\n\nThought alone is not enough, the thoughts need to do something, and sometimes explain the doing. Actions that are not governed by/explained by thought are not intelligent\nBiological naturalism (Searle)\nOntologically equivalent to materialism\nTwo main theses:\n\nall mental phenomena from pains, tickles, and itches to the most abstruse thoughts are caused by lower-level neurobiological processes in the brain\nmental phenomena themselves are higher-level features of the brain\n\nThat is, he believes that consciousness is both a cause of events in the body and a response to events in the body.\nEntails that the brain has the right causal powers to produce intentionality\n\nWeak AI → principle value of the computer in the study of the mind is that it gives us a very powerful tool\nStrong AI → the appropriately programmed computer really IS a mind. Computers, given the right programs, can be literally said to understand and have other cognitive states\n\nIt is attempting to refute the claims in the Chinese room argument that\n\nthe machine can literally be said to understand the story and provide the answers to questions\nwhat the machine and its programs do explains the human ability to understand the story and answer questions about it\n\nInformation Processing\nArgument that rests on the ambiguity of what ”information” is. For example, one can say that the programmed computer does not do information processesing, it only manipulates formal symbols.\nIt is important to note that information processing doesn’t imply intentionality.\n\ninformation transformation → taking info at one end, transforming it, and producing different information as output\n\nIt is up to outside observers to interpret the input and output as information the ordinary sense (derived intentionality)\nStrong AI only makes sense given the dualistic assumptions that, where the mind is concerned, the brain doesn’t matter"},"thoughts/intentional-arrangement":{"title":"Intentional Arrangement","links":["thoughts/intentionality","thoughts/organizing-system","thoughts/library","thoughts/desire-paths","thoughts/emergent-behaviour","thoughts/interoperability","posts/digital-gardening","thoughts/latent-factor-model"],"tags":["seed"],"content":"Source: The Discipline of Organizing by Glushko\n\nexplicit or implicit acts of organization by people, or by computational processes acting as proxies for, or as implementations of, human intentionality\n\nThe intentional arrangements of resources in an organizing system are the result of design decisions about what is organized, why it is organized, how much it is organized, when it is organized, and how or by whom it is organized.\nCan be created by top-down authoritative institutions like libraries, museums, businesses, and governments or bottom-up self-organizing systems composed of aggregated interactions of actors with resource or with each other.\nExamples of emergent organization are desire paths, swarm intelligence in local interactions (e.g. ants, bees, fish, etc.), or crowdsourcing (see: emergent behaviour)\nSimilarly, Adam Smith’s “invisible hand” is another example where individuals collectively generate an outcome they did not directly intend but that arose from their separate self-interested actions as they respond to price signals in the marketplace. This is the basis of a lot of multi-agent system reasoning and economics.\nStandardization allows interoperability — especially necessary for information systems that serve many people. No two people organize things the same way. No two people have the same requirements for the same information system.\nDesign questions/dimensional perspectives on the design of organizing systems\nMaintaining organizing systems with long expected lifetimes mean that incremental changes to description vocabularies and classification schemes need to happen over time — even when the categories are not always explicit. (related: digital-gardening)\nExamples\nEnumeration\nPutting the resources into a set without any specification of any properties they might share. Only property that matters is that the resources are in the same set\nCollocation\nPutting resources in the same location without any additional organization. For a small collection, the proximity-to-use organizing principle is the easiest way to satisfy a requirement to minimize the time to find frequently used resources.\nSame concept for latent-factor model"},"thoughts/intentionality":{"title":"Intentionality","links":["thoughts/writing","thoughts/Chinese-room-argument","posts/me-myselves-and-i","thoughts/Brentano's-Thesis","thoughts/Materialism","thoughts/context","thoughts/rationality"],"tags":["sapling"],"content":"Intentionality is the ability to be about something.\nDerived Intentionality\nIntentionality is derived. The same reason why computer-generated art or AI-assisted writing would not be considered ‘novel’ or ‘intelligent’. The intentionality of the user is what gets injected. These tool-assisted generation of artifacts are no more different than fancier pens and paint brushes.\nChinese room argument example tries to show that even programming something with intentionality (a person) with a format program, that formal program carries no additional intentionality\nHow does intention change with time? Are my intentions in the past just as valid as my current intentions?\n\nSmart contracts and blockchains try to permanently embed and codify intentionality\nBut, intentionality doesn’t mean you can’t change though. People aren’t permanent, thoughts aren’t permanent. Why should we try to make them so? (see: meditation on change of self)\n\nDennet\nIntentionality is based off of repeated behaviour rather than internal mechanisms. This accepts Brentano’s definition of the mental, but proposed materialist way to view intentionality (and intelligence)\n\nBehaviour should be understood not in isolation but in context and as part of a consistent pattern of behaviour (holism)\nA consistent pattern of behaviour in context can be construed as rational (interpretation)\n\nRabbit example\nWe infer that a rabbit can tell a fox from another rabbit, always wanting to get away from the one but not the other\nThus, on a given occasion, we attribute to the rabbit intentional states (beliefs and desires) about a particular fox, on the basis not only of its current behaviour but also of the pattern in its behaviour over time."},"thoughts/interaction-design":{"title":"Interaction Design","links":["thoughts/crutch-and-shoe-metaphor","thoughts/telerobotics","thoughts/interaction-failure","thoughts/language","thoughts/notation","thoughts/bandwidth","thoughts/hypertext","thoughts/desktop-metaphor","thoughts/play","thoughts/research-debt","posts/collaborative-thinking","thoughts/ephemereal-content","thoughts/organizing-system","thoughts/knowledge-distillation","thoughts/memory-palace","thoughts/writing","thoughts/symbolic-system","thoughts/representation","thoughts/Mindstorms","posts/agentic-computing"],"tags":["sapling"],"content":"Source: A Brief Rant On The Future of Interaction Design by Bret Victor\nTools address human needs by amplifying human capabilities. Great tools have parts that fit the problem as well as fit the person.\nWe need new mediums for exchange of ideas and active construction of thoughts (crutch and shoe metaphor). Too much telecommunications work will eventually lead us to building crutches rather than shoes. Stop trying to remedy a perceived defect and instead focus on new functionality\n\nGo ahead and pick up a book. Open it up to some page. Notice how you know where you are in the book by the distribution of weight in each hand, and the thickness of the page stacks between your fingers. Turn a page, and notice how you would know if you grabbed two pages together, by how they would slip apart when you rub them against each other.\n\n\nTake a moment to pick up the objects around you. Use them as you normally would, and sense their tactile response — their texture, pliability, temperature; their distribution of weight; their edges, curves, and ridges; how they respond in your hand as you use them.\n\nBeyond Touch\nCurrent technology is very much just Pictures Under Glass. All interaction are glassy and feel like they have no connection with whatever task you were performing. Almost as if it was just under a pane of glass. List of interactions you can do with Picture Under Glass:\n\nSlide\n\nWhen working with our hands, touch does the driving, and vision helps out from the back seat. Moving our limbs and bodies is so well choreographed, its just telerobotics for us. Why should we limit our interactions to a single finger or two? This quite literally just interaction failure\n\nHumanity is using the dynamic medium merely to emulate and extend static representations from the era of paper.\n\nNew representations of thought — written language, numerals, mathematical notation, data graphics — have been responsible for some of the most significant leaps in the progress of civilization, by expanding humanity’s collectively-thinkable territory. Why then, have we been trapped using the desktop/paper metaphor for the last few centuries? Why have we essentially bottle-necked our bandwidth for interaction design?\nTed Nelson, the guy who coined the terms hypertext and hypermedia, called the continued use of paper simulations as “like tearing the wings off a 747 and driving it as a bus on a highway.”\n\nAnd what about screens as a whole? Is the future of computation really just sliding fingers around slabs of glass? — Jason Yuan\n\nDesktop metaphor was originally designed in 1973 to suit a very different need in computation—the need to mirror digital content with its physical equivalent (thus, the need for folders an documents)\nPremise of a lot of the work behind DynamicLand and Jinha Lee\nComputers in Friendlier Forms\nSource: Omar Rizwan on Notion Blog\n\nIf you have objects on your computer, you can have holographic projected versions of them on your desk and then you can suck them into physical objects if you want to physicalize them. Or you can turn them back into holographic objects if you have the physical object\n\nOn making things that are just toys:\n\nI kind of want to make more things that are just toys where it’s fun to interact with the thing, because I feel like that actually sets a very high bar, a game can be fun because it has a story or it has cool characters or there’s a scoring system or something. But a toy, it has to be fun to play with just because it’s fun to play with just from the interactions themselves.\n\nOn physical analogues of software:\n\nI think that they’re all about trying to take stuff in the computer and give them some of the richness and texture and embodiment of things in the real world and scale also. Where everything on the computer is kind of pristine and closed and perfect and you can’t touch it and it doesn’t decay, and it all kind of fits in this 11 inch rectangle.\n\nModes of Human Communication\nBy upgrading forms of external communication, we can reduce research debt and allow us to collaborate better\nConversing (person-to-person)\nFace-to-face, ephemereal, improvised. As it stands today, most of this happens through spoken word, hand-waving, and static sketches. This makes grasping the same idea as another person extremely difficult (low bandwidth communication)\n\nWords are terrible at representing systems\n\nCan we reduce the time to generate models for ideas down from hours to seconds? Is there any way we can integrate the stage into presentations (much like a play)?\nPresenting (person-to-people)\nA lot of this is related to creating good organizing systems\nBlackboards are more flexible than a computer for presentations right now.\n\nWhat’s the point of a living, dynamic speaker, if the presentation itself is completely static?\n\nCan we create the visuals of a well-polished science YouTube live like a blackboard?\nCan we map concept space to physical space and use the stage as an outline of the presentation? Kind of like a book where you can tell how much of it you’re finished by the weight of each side, can the audience see what the presenter has already covered?\nReading/Browsing (media-to-person)\nCan we dynamically create content customizable/personalizable for each user? Are there other channels we can use for communicating information outside just text?\nGetting this right is critical for effective knowledge distillation\nIs it possible to create dynamic spatial media? Virtual museums of information? Are there ways to engage with things other than the hands? Can we storytell as a way to ingest and interact with information?\nThe focus is on spatial representation of usable knowledge.\nCan we create a memory palace for people to walk through to recall information and learn new information?\nWriting (person-to-media)\nAs it stands, writing is just manipulation of symbols. Even for static materials, the symbols just are more literal representations. Coding still, is manipulating indirect symbolic systems and representations\nCan we show multiple representations of dynamic behaviour? Can we transform between different representations easily?\nThe goal is to have manipulation of the behaviour and data itself rather than a structure or set of symbols that ‘represents’ that behaviour/data: related to the idea of turtles and the LOGO language in Mindstorms\nThinking (person-to-self)\nWhy are almost all representations used in intelletual work (both final product and intermediate scratch work) mostly 2D? Mostly paper or pixels\nCan we create dynamic tactile mediums?\n\nPlayfair’s invention of data graphics was transformative because it tapped into capabilities of the human visual system which had gone unused in intellectual work. It may be similarly transformative to tap into the profound capabilities that enable a person to tie a shoelace or make a sandwich, and bring them to bear on more abstract thinking.\n\nWebstrates\nSource\nIn the seventies, Alan Kay introduced the concept of Personal Dynamic Media that let a user “mold and channel its power to his own needs” (see: agentic computing)\nTwo decades later, Mark Weiser envisioned a future of ubiquitous computing, where heterogenous devices of varying sizes and capabilities interact easily with each other and technology disappears into the background.\nSubstrates are software artifacts that embody content, computation and interaction, effectively blurring the distinction between documents and applications.\nCool Experiments\n\nhttps://www.robertxiao.ca/research/lumitrack/\nhttps://www.robertxiao.ca/research/desktopography/\n"},"thoughts/interaction-failure":{"title":"Interaction Failures","links":[],"tags":["seed"],"content":"Interacting with interfaces is complex, and often poorly designed. Most times,\n\nThe tasks are implicit and complex: the machine doesn’t “know” the user’s end goal\nInteraction is unpredictable and complex: coordination between human and machine is complicated, usage can be unexpected and evolved, users can change their minds\n\nInterfaces\n\nFunctionality Problem: what are the functions this object can perform? Will it do what I want?\nVisibility Problem: what mode is this object in? What sequence of controls do I use to get what I want?\nNegative Transfer: what would happen if I do what I usually do?\n\nDesigners\nCan fail to\n\nunderstand the range of users and their limitations\nunderstand contexts of use\ncommunicate what it does, how it works/worked, etc.\nstart with basic usability needs, and might try to make it exciting or beautiful first\n\nMarket Pressures\nUser’s don’t always make good purchase choices\n\nAdding new functionality is relatively easy and cheap whereas adding effective controls/feedback is expensive and costly for time and space\nDesigner time is expensive\nSome consumers value cost or looks over usability\n"},"thoughts/interdependence":{"title":"Interdependence","links":["thoughts/representation","thoughts/emergent-behaviour"],"tags":["seed"],"content":"\nDependent Existence: for a flower to exist and for us to have experienced it requires the existence of many others. The gardener, the sun, the water, the minerals, the soil.\n\nIn cognitive science, concept of connectionist representation\nIn emergent systems, there is no ‘conductor’ there is only a symphony that interdepend on each other\n\n\nIndependent Existence: a separate existence. A flower cannot be by herself or have a self-nature (svabhāva).\n"},"thoughts/internet-computing":{"title":"Internet Computing","links":["tags/CPSC317","thoughts/Internet","thoughts/peer-to-peer","thoughts/security","thoughts/Postel's-Law","thoughts/IP-Address","thoughts/Protocol","thoughts/language","thoughts/Application-Layer","thoughts/HTTP","thoughts/Transport-Layer","thoughts/TCP","thoughts/UDP","thoughts/Network-Layer","thoughts/Link-Layer","thoughts/Physical-Layer","thoughts/NAT","thoughts/DHCP","thoughts/fault-tolerance","thoughts/access-control"],"tags":["sapling","CPSC317"],"content":"Notes for CPSC 317 (see all notes)\nThe Internet is a network of networks. The main goal was to integrate a number of separately administrated entities into a common entity\nSee also: peer-to-peer, security, Postel’s Law\nChanging an entrenched internet\nEnabling a Permanent Revolution in Internet Architecture by McCauley, Harchol, Panda, Raghavan, and Shenker\nThe current Internet architecture is both inherently flawed (so we should explore radically different alternative designs) and deeply entrenched (so we should restrict ourselves to backwards-compatible and therefore incrementally deployable improvements).\nFor example, the decades-long migration effort from IPv4 to IPv6\nTLDR;\n\nInternet architecture is, and will remain, difficult to change — clean-slate research and projects seem increasingly impossible\nCurrent focus is on backwards-compatible designs that have been tested in large-scale operational networks\nOSI model of the internet sees levels below them as ‘logical pipes’ to get something from place A to B\n\nCommunication\nNecessary conditions\n\nA communication medium\nSource(s) and destination(s)\nProtocol (language)\nMessage\n\nCircuit Switching\nDedicated path between source and destination. Path taken is determined when connection is established. Single stream of info per path\nWorks well when\n\nGuaranteed service is valuable\nDemand is steady (unchanging rate)\nStarting a new conversation is rare\n\nPacket Switching\nData is divided in packets that are sent individually where each packet is self contained (contains source, destination, and data) and independently routed.\nWorks well when\n\nStatistically good performance is good enough\nDemand is bursty (rapidly changing rate)\nStarting a new conversation is frequent\n\nMultiplexing\nMultiple input streams must share the medium. It must be possible to “demultiplex” at the destination\nTypes\n\nTime division multiplexing: each person gets a certain amount of time on the channel\nFrequency division multiplexing: each person gets a single frequency band on the channel\nCode division multiplexing: combines all messages using a specific code that can be decoded if code is known\nOrthogonal multiplexing: a combination of techniques\n\nNetwork Protocol Stack (from most abstract to least)\nEach layer takes data from above adds header information to create new data unit passes new data unit to layer below\n\nApplication Layer (HTTP)\nTransport Layer (TCP, UDP)\nNetwork Layer (IP Address) — this is the ‘thinnest’ part of the network stack!\nLink Layer\nPhysical Layer\n\nHubs, Switches, and Routers\n\nHub - broadcasting through cloning bits (physical layer)\n\nSimplest and cheapest way to create a network\nLots of unnecessary traffic\nOther people can see your traffic\n\n\nSwitch - hub but it knows where other hosts are for direct addressing (link layer)\n\nKeeps a switch table mapping interface number to MAC address\nIf table is initially empty, will behave like a hub and broadcast\nCan start populating switch table based off of sender field from frames\nQuicker than a router for internal communication (though some routers have an Ethernet switch built in)\nIf engineered right, can be full-duplex\n\n\nRouter - glue that ties networks together (network layer)\n\nDoes NOT support broadcast\nServes as a bridge between private home network and the network of the internet provider (which can reach the rest of the internet)\nModern routers can also perform\n\nNetwork address translation (NAT)\nAssigning IP addresses to hosts using DHCP\nBroadcast WiFi signal\n\n\n\n\n\nError Correction\nMethods for fault tolerance in data transmission\n\nParity Bit\n\nEven parity is 1 if number of 1s is odd\nCan detect odd number of bit flips\n\n\n2D Parity Bit\n\nAdditional parity bits for each row\nAdditional parity bits for each column\nOne last additional bit in last row of parity bits\n\n\nChecksum\n\nAssume data is a sequence of 16-bit integers\nAddition, 1’s complement sum, carry out added back in\nChecksum is the 1’s complement of the computed value\nCompare with the received data (if same, it is ok)\n\nAlternatively, compute the same function over the data and checksum\n\n\n\n\nCyclic Redundancy Check (CRC)\n\nUses only XOR and shift\nParameterized by constants G and r\nr + 1 is the length of G (some power of 2)\nG is the generator (arbitrary bit pattern)\nSender wants to send D\n\nChooses r CRC bits, R such that &lt;D, R&gt; is exactly divisible by G (mod 2)\n\n\nReceiver knows G, divides &lt;D, R&gt; by G. If the remainder is non-zero an error is detected!\nCan detect all burst errors less than r+1 bits, and burst errors greater than r+1 with probability 1−0.5r\n\n\n\nAccess Control\n\nHalf-duplex - both sides can transmit, but only one at a time\n\nCarrier Sense Multiple Access\n\nListen before sending, only send if no one else is\n\n\nCollision Detection\n\nWhile sending, listen to see if what you are sending is garbled\nIf so, give up\n\n\n“Try again later” uses binary exponential backoff\n\nRandom backoff between 0 and power of 2 (n increases each time)\n\n\nTurn-based access control\n\nControlled by centralized party - polls everyone\nControlled in a decentralized manner - passes a token between senders\n\n\n\n\nFull-duplex - both sides can transmit at the same time without interference/NAT\n\nNetwork Metrics / Peformance\n\nBandwidth: max rate at which data can be sent over a link, usually in bits per second\n\nMeasured in base 10, kilobit is 103 bits, megabit is 106 bits\n\n\nSizes: usually in bytes\n\nMeasured in base 2, kilobyte is 210 bytes, megabyte is 220 bytes\nExcept for disk managers, who use bits to make sizes look larger\n\n\nLatency: how long is it from when something is sent to when it is received\n\nPacket and bit/byte latency: measure start of sending packet/bit/byte to receiving it\nRound trip time: time to send packet and receiving a response\n\n\nJitter: variation in latency — interpacket variance\nThroughput: amount of data moved from one place to another in a given time\n\nUsually measured in bytes not bits\n\n\nGoodput: rate at which useful data arrives\n\nDoes not include headers, encoding costs, etc.\n\n\n\nDelay\n\nAverage Service Time: time taken to put the average packet on the wire\n\nS=Average packet size×Bandwidth8bits/byte​\n\n\nProcessing Delay: figuring out where packet should go\n\nPretty much fixed (almost always variable due to cache hits, network queues, etc. but because of how negligible the times are, we can treat as fixed.)\n\n\nQueueing Delay: waiting time to get access to a link\n\nVariable\nIncrease over service time when idle\nDelayQueueing​=1−US​−S where S is the average service time when no other requests and U is the server utilization (usually traffic intensity)\n\n\nTransmission Delay: time to write packet to medium\n\nFixed for bits, variable for packets (dependent on size)\nDelayTransmission​=BandwidthMessage size×8bits/byte​ for each segment (as each router needs to receive the entire packet before adding it to the queue)\n\n\nPropagation Delay: time to move each bit over transmission medium\n\nFixed for meter, variable depending on actual length traveled\nDelayPropagation​=Link speedTotal distance​\n\n\nEnd-to-end Delay: sum of all sources of delay\n\nTraffic Intensity\n\nDetermined by\n\nNumber of packets arriving per second: a\nAverage packet size: L in bits\nTransmission rate: rate at which bits are disposed of per second: R\n\n\nTraffic Intensity: RLa​\n\nSliding Window\n\n\nWhen we allow multiple packets in flight, modelling this with a finite state machine is less than ideal\n\n\nWe can use a sliding window to represent the ‘window’ of all bits that are in flight\n\nAlthough, better version of this is to just use a proper protocol like TCP\n\n\n\nTimeout: how much longer should I wait for the ack for the first packet in my window\n\n\nBig window costs us memory\n\n\nSmaller window is easier to reason about\n\n\nGo-back-N\nReceiver\n\nWhen packet is received, send ACK for last packet received in order\nDiscard arriving packet if out of order (receiver window is 1)\n\nSender\n\nCan have a specific number of outstanding (unacknowledged) packets in memory: sender’s window\nStart timer on first packet sent\nOn timeout go to last unack’ed packet and resend everything (restart timer)\nReceived ACKs may be cumulative (restart timer on receipt)\n\nSimulation Link\nSelective-Repeat Strategy\nReceiver\n\nEach packet is ack’ed individually\nOut of order packet is stored for later: receiver’s window\nIf a packet arrives whose sequence number is too small to fit in the window it will be ACKed and dropped.\nOnly when a packet whose sequence number is too large to fit in the window will it not be ACKed; such packets will be silently dropped.\n\nSender\n\nCan have a specific number of outstanding (unacknowledged) packets in memory: sender’s window\nEach packet has its own timer and is individually resent if timeout is reached\nACKs received in order move the sender’s window\n\nSimulation Link\nCongestion/Flow Control\n\nAs long as sender’s window size is smaller than the receivers window size, it’s fine\nIf the network can’t handle a full window’s worth of data (packets + ACKs are dropped by routers) then sender can reduce sending window\nIf all packets are ACKed, we can increase the window again (too slow! we can up the pace)\nReceiver will notify the sender about how much data it can handle (usually included in the ACK)\nIf receiver runs out of space, sender will send a packet every once in a while even though its not supposed to just to check if it now has space\n"},"thoughts/interoperability":{"title":"Interoperability","links":["thoughts/social-graphs","thoughts/tools-for-thought","thoughts/credible-exit","thoughts/idea-list","thoughts/interaction-design","thoughts/Unix","thoughts/composable"],"tags":["sapling","pattern"],"content":"\nPortability implies switching platforms, but true interoperability is platform-agnostic.\n\nI think interoperability in the context of the web means being able to transparently understand and share data, agnostic of platform. Closed platforms disallow this as they curate the information they present to end-users/devs rather than letting users control their own data.\nA good standard is if I can hit ‘export all data’ on a platform and I can completely and freely use that elsewhere — no vendor lock-in. I think this is esp hard for social graphs, there is no common/accepted standard for identity let alone connectedness between individuals\nRelated: tools for thought, credible exit\nNoun-based Interoperability\nSource: Riffle: Building data-centric apps with a reactive relational database\n\nSince the introduction of object-oriented programming, most interoperability has been “verb-based”: that is, based on having programs call into each other using APIs. Indeed, new programmers are often taught to hide data behind APIs as much as possible in order to encapsulate state. Unfortunately, verb-based APIs create an unfortunate n-to-n problem: every app needs to know how to call the APIs of every other app. In contrast, data-based interoperability can use the shared data directly: once an app knows how to read a data format, it can read that data regardless of which app produced it.\n\nHow do we create sources of truth that are legible outside of the application, possibly in ways that the application developer never anticipated?\n(see: data lensing in idea list)\nBeyond Windows\nSource: Universal data portability by Alexander Obenauer\nA modern day HyperCard. What other types of interaction design can we facilitate?\n\nA window runs a self-contained program, isolated from other programs that […] run at the same time in other windows. (Wikipedia)\n\nWindows and applications silo data. The only way to ‘share’ data between them is through the file system. What if applications exposed data by default?\nAn application then is a specific view on types of data rather than a standalone thing. Data can be dragged through different applications. Each data is annotated with a type. There is also a most common interface for data that allows basic-level interop with any application. We can then create an open marketplace for views on data.\nApps in this operating system then declare ‘default’ apps for certain data types.\nAbility to not only source local data but remote data too → fetching from URLs, feeds, etc.\nUNIX-level composability — apps should be atomic building blocks. Creating pipeline apps to transform and massage data (possibly using Clay??).\nAuto-grouping of similar data? Set-theory for data?\n”Engelbart-inspired bootstrapping. A system with which you can, as a user, co-evolve both the tool and your methodologies simultaneously to build up better whole systems (those systems which include the human and the tool).”\nSemantic Data/Markers\nAuto-run scripts/data transformations to data.\nMarkers like ‘today’ that point to a new date every day. Auto generate a day-summary for the next day. etc.\nChronological Data Querying\nWhat data was created a few days ago? etc.\nAdverserial Interoperability\nSource: Adversarial Interoperability by Cory Doctorow\n\nWhen you create a new product or service that plugs into the existing ones without the permission of the companies that make them. Think of third-party printer ink, alternative app stores, or independent repair shops that use compatible parts from rival manufacturers to fix your car or your phone or your tractor.\n\nIf a platform chooses to not make a well documented API available, for example, a loyal client could still interoperate with them, taking more adversarial approaches such as reverse engineering their API or downloading content and storing it locally.\nData Lenses\nSource: Ink and Switch on Cambria\nAn organization must balance their desire to change their API against their customers’ reluctance to change something that works for them. The result is strong pressure to preserve backward compatibility over time, often across many versions.\nDevelopers rely on tribal knowledge to inform them which operations are safe—for example, they intuit that they can respond with additional data, trusting existing clients to ignore it, but not require additional data in requests, because existing clients won’t know to send it. Developers also often resort to shotgun parsing: scattering data checks and fallback values in various places throughout the system’s main logic. This can often lead not just to bugs, but also security vulnerabilities.\nHowever, in practice, most interoperability requires a tradeoff between\n\nConsistency: both sides see a meaningfully equivalent view of the world\nConservation: neither side operates on data they can’t observe\nPredictability: the local intent of every operation is preserved\n\nCambria\nOver time, a project using Cambria will accumulate many lenses, each describing the relationship between two versions. Migrations between distant versions are created by composing lenses into a graph where each node is a schema, and each edge is a lens. To translate data between two schemas, Cambria sends it through the shortest available path in the lens graph. These lenses must be kept in a place where even old versions of the program can retrieve them, such as in a database, at a well-known URL, or else as part of the document itself.\n\nCaveats:\n\nImagine a lens that combines a firstName and lastName fields into a single fullName. This lens works reliably in one direction. All names already stored in the system could be combined into a single field, but there are many names which could not be reliably converted back to “first” and “last” names. The result is a so-called lens that can only run reliably in one direction.\n"},"thoughts/interpretive-labour":{"title":"Interpretive Labour","links":["thoughts/group-limits"],"tags":["seed"],"content":"Interpretive Labour\nSource: Research Debt in Distill Pub\nThere is a tradeoff between energy put into explaining an idea, and the energy needed to understand it. This energy differential is called ‘interpretive labour’\nMost research is one-to-many, where there are a lot more people trying to understand a subject than explaining it. As a result, the quality of explanations have an outsized impact for the better.\n\nCost of understanding increases when there are more source to try to understand. This may be why people specialize so that there is less noise. Related: group limits"},"thoughts/intersectionality":{"title":"Intersectionality","links":["thoughts/interdependence","thoughts/Design-Justice","thoughts/bias"],"tags":["seed"],"content":"Intersectionality identifies multiple interdependent factors of advantage and disadvantage. It refers to the interconnected nature of social categorizations such as race, class, and gender as they apply to a given individual or group\nOn Intersectionality\nIn Design Justice\nBlack women workers at GM were told they had no legal grounds for a discrimination case against their employer because antidiscrimination law only protected single-identity categories.\nThe concept of intersectionality provided the grounds for a long, slow paradigm shift that is still unfolding in the social sciences, in legal scholarship, and in other domains of research and practice.\nWhile there is rapidly growing interest in algorithmic bias audits, especially in the fairness, accountability, and transparency in machine learning community, most are single-axis: they look for a biased distribution of error rates only according to a single variable, such as race or gender.\nOne question about [intersectional approaches] is how many identity variables to include because each adds complexity (and, in many situations, time and cost) to audits."},"thoughts/intersubjectivity":{"title":"Intersubjectivity","links":["thoughts/interdependence","thoughts/phenomenology","thoughts/double-consciousness"],"tags":["seed"],"content":"Related: interdependence, phenomenology, double-consciousness\nSubjectivity is the perception or experience of reality from within one’s own perspective\nEdmund Husserl: Intersubjectivity is most simply stated as the interchange of thoughts and feelings, both conscious and unconscious, between two persons or “subjects,” as facilitated by empathy"},"thoughts/interval-scale":{"title":"Interval scale","links":["thoughts/Decisions-under-ignorance","thoughts/Decisions-under-risk","thoughts/Decision-theory"],"tags":["seed","PHIL321A"],"content":"Any interval scale can be linearly transformed (ordinal transformation) between each other and not be affected\nAssign to each outcome x a value v(x) such that v(x)≥v(y)⟺x≥y and v(x)=v(y)⟺x∼y\nvon Neumann-Morgenstern Theorem (vNM)\nWe need interval scales for most of the rules for DUI, and we need them for DUR. vNM tells us how to construct an interval utility scale.\nWe find your utility for x by measuring the risks that you are willing to take to get x. vNM shows that your utility for a lottery is equal to its expected utility.\n\nIf your preferences have enough structure (i.e., if they satisfy the vNM conditions), then they can be represented by a utility function u (unique up to positive linear transformation) which has the expected utility property.\n\nLotteries\n\nWe have a set of basic prizes/outcomes\nWe can compose lotteries (if L1​ and L2​ are lotteries, then so is [pL1​,(1−p)L2​])\n\nThis creates compound lotteries\n\n\n\n\nPeterson argues that vNM is ok for descriptive decision theory but not normative decision theory (vNM does not distinguish the meaning of utility from the measurement of utility)\nNormative decision theory involves prescribing actions which needs a strong version of EU max: acts are rational because they maximize expected utility. This demands a concept of utility that is independent from the measurement of utility itself (or we get a circular argument)\nAxioms\n\nvNM 1, Completeness: A≻B or A∼B or B≻A\nvNM 2, Transitivity: if A≻B and B≻C then A≻C\nvNM 3, Independence: A≻B if and only if ApC≻BpC\nvNM 4, Continuity: if A≻B≻C there ∃p,q such that ApC≻B≻AqC\nvNM 5, Probability: it does not matter if you are awarded prize A if you first roll a die and then roll it again, or make a double roll, provided that you only get the prize if you get two sixes\n\nIf pq+(1−p)r=s then (AqB)p(ArB)∼AsB\n\n\n"},"thoughts/interviews-and-data-recording":{"title":"Interviews and Data Recording","links":["thoughts/quantization"],"tags":["seed"],"content":"Five Key Issues\n\nSettings Goals. Goals will influence the nature of data gathering sessions, the data gathering techniques to be used, and the analysis to be performed.\nIdentifying Participants. Those who fit the profile of types of people from whom data can be gathered are called the study population. Types of sampling are as follows:\n\n\nProbability Sampling: simple random sampling or stratified sampling\nNonprobability Sampling: convenience sampling or volunteer panels\n\nConvenience Sampling: sample includes those who were available rather than those specifically selected\n\n\n\n\nRelationship with Participants: informed consent with a clear and professional relationship between participant and researcher (however, informed consent is generally not required when gathering requirements data for commercial company where a contract usually exist between collector and provider)\n\nTriangulation: the investigation of a phenomenon from at least two different perspectives. This is mostly focused on verification and reliability of data rather than making up for the limitations of another type of methodology\n\nTriangulation of data: data is drawn from different sources at different times/places/people\nInvestigator triangulation: different researchers (observers, interviewers, and so on) have been involved in collecting and interpreting the data\nTriangulation of theories: use of different theoretical frameworks through which to view data\nMethodological triangulation: employ different data gathering techniques\n\nInterviews\n\nA conversation with a purpose\n\nGood for exploring issues, learning more about tasks, and getting inside user’s head.\n\nOpen-ended/unstructured → exploratory and similar to conversation. Can be time-consuming but can also produce rich insights\nSemi-structured → plans basic script with both closed and open questions but probes interviewee until no new relevants info is there\nStructured → predetermined questions like a questionnaire, study is standardized (same questions with each participant)\nGroup interviews → 3-10 people selected to provide a representative sample of the target population. Useful for investigating shared issues rather than individual experiences\n\nPlanning\nWhen developing Interview Questions, keep in mind open questions are best suited where the goal of the session is exploratory; closed questions are best suited where the possible answers are known in advance. Break long or compound questions into separate questions\nA lot of decisions to make:\n\nChoosing a framework\nLevel of participation to adopt\nHow to make a record of the data\nHow to gain acceptance into the group being studied\nHow to ensure that the study uses difference perspectives\n\nEthnography: the description of the customs of people and cultures. A distinguishing feature of ethnographic studies compared with other data gathering is that a situation is observed without imposing any a priori structure or framework upon it, and everything is viewed as “strange”.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTechniqueGood forKind of DataAdvantagesDisadvantagesInterviewsExploring issuesMostly qualitative (some quantitative)Interviewer can guide, encourages contact between researchers and usersArtificial environment might be intimidating, remove them from usual environmentFocus GroupsCollecting multiple viewpointsMostly qualitative (some quantitative)Highlight areas of agreement/conflict, encourages contact between researchers and usersPossibility of dominant charactersQuestionnairesAnswering specific questionsQuantitative and QualitativeCan reach many people with low resource requirementsDesign is key, response rates may be lowDirect observation in the fieldUnderstanding context of user activityMostly qualitativeObservational insightsVery time-consuming, huge amounts of dataDirect observation in a controlled environmentCaptural detail of individualsQuantitative and qualitativeUser can focus on task without interruptionData may be of limited use due to artificial environmentIndirect observationObserving users in natural environment without distractionQuantitative (logging) and qualitative (diary)Can be long due to automative recordingLarge amounts of data implies need for tools to support analysis, participants may exaggerate memories\nRunning the interview\nBefore starting, make sure that the goals of the interview have been explained to the interviewee and that they are willing to proceed. Listen more than talk, repond with sympathy but without bias, and to appear to enjoy the interview.\n\nIntro\n\ninterviewer introduces themselves\nexplain why you’re doing the interview\nreassure interviewee re: ethical issues\nask interviewee if they mind being recorded\n\n\nWarm-up session\n\neasy, nonthreatening questions\n\n\nMain session\n\nquestions presented in logical sequence\nprobing questions at the end\norder may vary in semi-structured interview\n\n\nCooling-off period\n\neasy questions to defuse any tension\n\n\nClosing session\n\ninterviewer thanks interviewee\nswitch off recorder or put notebook away\n\n\n\nObservation\nUsers may be observed directly by the investigator as they perform their activities or indirectly through records of the activity that are studied afterward.\nObservation can also result in a lot of data to sift through and can be complicated to do well than at firs appreciated. As such, a clearly stated goal is important to have focus for an observation session.\nExample frameworks:\n\nThe person: Who is using the technology at any particular time?\nThe place: Where are they using it?\nThe thing: What are they doing with it?\n\n3 common approaches\n\nSimple observation: user is given a task, the evaluator just watches. This gives no insight into users’ decision process\nThink aloud: subjects asked to say what they are thinking/doing. However, its hard to talk while concentrating and thinking may alter the way people naturally perform the task.\nCo-discovery learning: two people work together on a task and normal conversation is monitored.\n\nDegree of Participation\n\nPassive Observer: observer who adopts an approach at the outsider end; does not take part in the study environment at all\nParticipant Observer: adopts an approach at the insider end; becomes a member of the group being studied\n\nCoding Sheet\n\nA data recording instrument in which a list of itemized coding options are structured\n\nThis standardizes observation practices which makes it more objective.\nConsider:\n\nevaluation goals (break it all down!)\nstage of design\nobservation method/types of data\n\nwhat would potentially be an interesting finding from this particular style?\ne.g. for think-aloud, it might be good to record action vs spoken comments\n\n\n\nQuestionnaire\nSurvey vs Questionnaire: the questionnaire is a part of the survey. The questionnaire is just the concrete things you’re asking.\nPros\n\ncheap\ndoes not require presence of evaluator\nmany results can be quantified\n\nCons\n\npreparation is “expensive” → need to design questions well\ncan have low response rate or low quality response\ndifficult to do in-depth “probing”\n\nA questionnaire is good when motivation is high enough without anyone else present. If persuasion is needed, a structured interview is probably better\nDesigning a Questionnaire\nKeep in mind\n\nwhat info is sought?\nhow would you analyze results?\nwhat audience do you want to reach?\nwhat will you do with your analysis?\n\nDon’t use vague questions, pilot the questionnaire before testing.\nQuestions\n\nshould not be transferable to other interfaces, can’t be interpreted in different ways depending on judgment (i.e. domain specific and clear wording)\navoid double-barreled questions and leading questions\nto de-bias: neutral language, can have random order of questions for different participants\nfor validity\n\nuse previously validated questionnaires\ntriangulation (ask multiple questions about the same matter)\npiloting\n\n\n\nTypes of questions\n\nOpen-ended (hard to analyze rigorously)\nClosed (easily analyzed but can be hard to interpret if not well-designed)\n\ncheckboxes and ranges\n\nrange of answers to demo questions is predictable: offer a predefined list\ninterval doesn’t have to be equal in all cases, depends on what you want to know\nmention how many boxes to check, be consistent with ascending/descending order\n\n\nLikert and semantic differential scale\n\nused for measuring opinions, attitudes, and beliefs\nwidely used for evaluating user satisfaction\nLikert: a five, seven, or nine-point agreement scale used to measure respondents’ agreement with various statements\nsemantic differential scale: rely on choosing pairs of adjectives to explore a range of bipolar attitudes about particular opinions\n\n\nranked (closed)\n\nrespondent places ordering on items in a list\nuseful to indicate preferences\nforced choice\n\n\nmulti-choice (closed)\n\noffered choice of explicit responses\n\n\n\n\n\nChecklist\n\nthink about ordering of questions → impact can be influenced by order\nconsider if different versions are needed for different populations\nprovide clear instructions on how to complete questionnaire\n\neg. if answers can be saved and completed later\n\n\nthink about length → avoid questions that don’t address study goals\nconsider allowing respondents to opt out at different stages especially if long → better to have some than none\nthink about layout and pacing\n\nData and Analysis\n\nSubjective: what you were told what happened\nObjective: what you captured using your senses\nQuantitative: data in the form of numbers or data that can be easily translated into numbers. Focuses on ascertain magnitude, amount, or size of something\nQualitative: data in the form of words and images. Focuses on nature of something (themes, patterns, and stories)\n\nNote that quantitative data is not always objective! Subjectivity can come from participants in how they express opinions or from investigators during the data capturing/interpreting/analysis process.\nSimilarly, it is unfair to try to quantize all qualitative data. This needs justification. Also be wary of translating small populatino sizes into percentages.\nWhat to focus on\n\nWhat are the most important needs/tasks to support?\nWhat are the repeated patterns?\nKey issues/areas that could be improved\nWhat surprised you?\nWhat is essential/nonessential in implementation\n\nSteps\n\nInitial reactions or observations (identify patterns, simple numerical analysis like averages, ratios, percentages)\nData cleansing (checking for erroneous entries and anomalies)\nAnalysis\n\nQualitative Analysis\nThematic Analysis\nThemes are a small number of high-level patterns that answer your evaluation questions.\nGoing from codes (descriptive labels) to categories (grouping imposed on codes) to themes (interpretive patterns). Deductive analysis is just the inverse (starting at themes and arriving at codes)\nDo an initial pass to check of internal consistency: make sure themes occur across several or all participants. Then, step back to see if an overarching narrative emerges from the themes. One can them remove themes or look into why there are conflicts.\nOne way of doing this is using affinity diagrams:\n\nrecord each idea/observation/problem/etc on individual card or post-it notes\nlook for notes that seem to be related\nsort notes into groups until all used\n\nsort and resort as necessary\n\n\n\nCategorizing Data\n\nscheme of data: code the data according to categories\n\nif analysis frame is chosen beforehand: deductive analysis\nif study is explanatory and it is important to let themes emerge from data: inductive analysis\n\n\ncan then analyze with appropriate methods like counting averages # of problems or identifying recurring patterns\n\nCritical Incident Analysis\nHelps identify significant subsets of data for more detailed analysis.\nThis is not about summarizing all incidients, more like finding gold nuggets. Incidents need not be bad all the time, can be either desirable or undesirable.\nRisks and Consent\nPotential way conclusions can be flawed:\n\nConstruct validity: are we measuring the right thing? Is this clearly connected to our research question? Did we misunderstand the concepts we are working with?\nInternal validity: What are alternative explanations for the results? Other bias, confounding factors, etc.\nExternal Validity: To what extent are our results and conclusions of our experiment generalizable to our original research question? (how representative are our tasks and users?)\nEmpirical Reliability / Reproducibility: Can the study be reproduced?\n\nRisks and Consent:\n\nIn what ways could your participants could be harmed by the study or its results?\nCould be physical harm (less likely in CS), emotional harm (stress, reputation, etc.)\nEvaluate the likelihood of each potential risk (including unlikely cases)\nAre there ways to mitigate these risks? Potentially: adjust your study design\nWhat would you do if a participant were harmed? e.g. correction, compensation?\n"},"thoughts/introductions":{"title":"Introductions","links":["thoughts/quantization","thoughts/idolization","thoughts/introductions","thoughts/intersectionality"],"tags":["sapling"],"content":"I always dislike introductions because it feels like applying labels to things too early. People have expectations (oh so you’re an artist? show me your work). Both self-imposed and not!\nAnd can open the door to unhealthy idolization. As a side note, this is the same reason why calling yourself a writer vs someone who writes are two very different vibes (i dabble)\nAnd of course, it’s first of many experiences, why do we feel the need to get it right at the start? When people are prompted, “describe yourself”, shouldn’t we be a little alarmed when they routinely describe something else? People who introduce themselves by talking about their job make me feel really weird. You’re just describing the means you take the keep yourself alive — what are the ends? What makes you feel alive?\nIntersectional Identities\n\n“A man may be regarded as the point of intersection of an indefinite number of circles representing social groups” — Cooley, Human Nature and the Social Order (1922)\n\nCooley imagined affiliations and interests as a system of coordinates, with each additional group determining one’s individuality and identity more accurately.\n“These ‘reciprocally constructing phenomena’ that may empower one person with increasing self-actualization, burden others with exponentially debilitating oppression.”\nSee also: intersectionality\nAbout Page\nSource: About Andrew Kortina, Wide-Eyed\n“It strikes me that these labels are more often conversation enders than conversation starters, attempts to reduce a vast complexity into a neat, little word, with the lossiest of compression algorithms.”\nWe cheer for the clothes, not the people. As Feynman said, “honors is epaulets, honors is uniforms”\nIncreasingly online\nWill we ever get to a point where introductions become useless because we already know everything about each other? What about pre-stalking people on social media before meeting them? Is this just people projecting their identities?"},"thoughts/ionization-energy":{"title":"Ionization Energy","links":["thoughts/nuclear-binding-energy"],"tags":["seed"],"content":"Electron binding energy, more commonly known as ionization energy, is a measure of the energy required to free an electron from its atomic orbital or from a solid.\nNuclear binding energy and forces are on the order of one million times greater than the electron binding energies of light atoms like hydrogen and is part of the reason why the energy efficiency of nuclear is much higher than chemical methods."},"thoughts/jazz.tools":{"title":"jazz.tools","links":["thoughts/local-first-software","thoughts/access-control","thoughts/encryption"],"tags":["seed"],"content":"Jazz is an open-source multiplayer data toolkit. Build live, collaborative, multi-device apps with frontend code only.\nEmbodies the principles of local-first software\nComponents\n\ncojson: A collaborative JSON type\nState synchronization and persistence\nCryptographic access control\nGlobal mesh: smart routing transport layer for p2p data. With encryption!\n"},"thoughts/joie-de-vivre":{"title":"Joie de vivre","links":[],"tags":["seed"],"content":"\nok we get it dude u have a jovial light that cant be vanquished (tweet)\n\nA French phrase often used in English to express a cheerful enjoyment of life"},"thoughts/knowledge-distillation":{"title":"Knowledge Distillation + Teaching","links":["thoughts/research-debt","thoughts/teaching","thoughts/game-design","thoughts/constructionist","thoughts/agency","thoughts/idea-list"],"tags":["sapling"],"content":"Source: Patterns in confusing explanations by Julia Evans\nHeavily linked with research debt. What makes for effective teaching and knowledge distillation?\n\nGames + interactive content (a constructionist approach) &gt; just reading\n\nHow can we create worlds for people to explore on their own? How do we give agency back to students?\n\n\nHow do we create content that caters for all levels of understanding? Possible relation to a thing in project list where I thought about creating multi-level blogs\n\nConfusing Explanations\nTop things to avoid in explanations and blog posts\n\nInconsistent expectations of the reader’s knowledge: it might explain in great detail how a for loop works but the next paragraph immediately following implicitly assumes knowledge like how malloc works for example. In this case, nearly zero people will understand how malloc works without understanding how a for loop works. Pick 1 specific person and write for them\nStrained Analogies: don’t try too hard to write a Big Complex Analogy, otherwise more energy will be spent by the user trying to figure out what exactly are the similarities and differences between the two\nJargon without providing context\nUnsupported information and statements\nExplaining the “wrong” way to do something without saying it’s wrong\n“What” without the “why”\n"},"thoughts/language-development":{"title":"Language development","links":["thoughts/constructionist"],"tags":["seed"],"content":"Approaches to study of language development\n\nLanguage socialization: A description of children’s language use in social contexts and an account of the social processes by which children come to use language in the manner of their culture\nLinguistic: the Language Acquisition Device (LAD) must contain some knowledge of the structure of language in order for language acquisition to be possible — this innate knowledge cannot be specific to any particular language thus it is a Universal Grammar (UG)\nLearnability approach: focuses on explaining the fact that language is acquired (i.e., that language is learnable).\nDevelopmental approach: focuses on explaining the course of language development.\n\nNature vs Nurture\n\nIs the development of language in children the result of human innate knowledge (e.g. walking upright) or is it the result of the experiences of children and how they are raised (e.g. learning calculus)?\nEmpiricism: the mind at birth is like a blank slate; all knowledge and reason come from experience\nNativism: knowledge cannot come from experience alone. The mind must have some preexisting structure in order to organize and interpret experience\nInteractionism/constructivist: acknowledges there must be some innate characteristics of the mind that allow it to develop language based on experience but places greater emphasis on accounting for children’s language-learning experiences\n\nCognitivism vs Behaviourism\n\nBehaviourism: change in behaviour occurs in response to the consequences of prior behaviour; behaviour can be fully accounted for in terms of things external to the mind\nCognitivism: we cannot understand behaviour without understanding what is going on inside the mind of the organism producing the behaviour\n\nCritical Period\n\nCritical period hypothesis: language must be learned within a biologically determined window (comparable to birds and imprinting)\nSometimes also called the sensitive period or optimal period (less sensitive than critical period)\n\nMeasuring sound discrimination\n\nProsody: includes learning about the intonation, stress, pitch of a language.\nCategorical perception: when a range of stimuli that differ continuously are perceived as belonging to only a few categories\nPhoneme Boundary Effect: example of categorical perception. e.g. the phonemes /b/ and /p/ differ along a single acoustic continuum (voice onset time), but listeners hear each stimulus as either a /b/ or a /p/, nothing in between\nDistributional learning: learning from simply being exposed to frequency distributions of speech sounds in one’s surroundings\nStatistical learning: learning by counting the frequency with which one stimulus is followed by another\nRule learning: a stronger claim than statistical learning, claims that babies can learn a pattern that must be described in terms of symbols (or variables) that stand for any sound. Babies learn algebraic rules, not just statistical regularities\nPhonological bootstrapping hypothesis: children use phonological cues (e.g. nouns tend to have first-syllable stress whereas verbs have second-syllable stress) to break into grammatical structure\nProsodic bootstrapping hypothesis: pauses and changes in intonation at phrase boundaries\n\n5 stages in early speech production\n\nReflexive Crying and Vegetative Sounds\n\nBurps, sneezes, anything that accompanies biological functions\n\n\nCooing and Laughter (elicited by social interaction)\nVocal Play or Expansion Stage\n\nIn first few months, the only recognizable speech sounds are vowel-like. The first recognizable consonant-like sounds are heard at around 2 to 3 months of age, and tend to be back of the mouth (velars), such as [g] and [k].\n\n\nReduplicated Babbling (e.g. bababa)\n\nDeaf child also babble but at a later time than other children. The number of sounds produced gets smaller (not larger) over time.\n\n\nNon-Reduplicated/Variegated Babbling (e.g. bagiga)\n\nWordless sentences are often referred to as jargon\n\n\n\n\nImpacts of experience in speech production\n\nInput from adults – affects sounds and prosody in babbling. This is also influenced by the language that they hear (babbling drift)\nVocal feedback from own productions\nSocial feedback\n\n\n\nPhoneme Acquisition Time\n\nWhy are some phonemes acquired later than others?\n\nEase of articulation\nFrequency in the input\nMarkedness\nFunctional load – how many words in the language use this sound?\n\n\nEarly: /p/, /b/, /d/, /m/, /n/, /j/, /w/, /h/\nMiddle: /t/, /k/, /g/, /f/, /v/, /tʃ/, /ŋ/, /dʒ/\nLate: /θ/, /ð/, /s/, /z/, /ʃ/, /ʒ/, /l/, /r/\n\nExperimental approaches for studying infant’s perception\n\nCross-language speech perception: see Werker &amp; Tees\nHigh-amplitude Sucking Technique\n\nBabies like to hear sounds\nBabies lose interest in a sound when it is presented repeatedly (habituation)\nBabies who have lost interest in a previously repeated sound will become interested if a new sound is presented (dishabituation)\nWorks best for &lt;0;4\n\n\nConditioned Head Turn Procedure\n\nBabies are interested in moving toys\nUsing the presentation of the moving toy as a reward, babies can be trained to turn their heads when they hear a change in a sound being presented\nWorks best for 0;5-1;0\n\n\nIntermodal Preferential Looking Paradigm\n\nPlacing a child on their mother’s lap in front of two video monitors, which play two different events simultaneously.\nA speaker between the two monitors plays a verbal sentence that matches only one of the videos.\nA hidden observer measures how much the child looks at each screen.\nIf the child watches the correct monitor longer and more quickly than the incorrect monitor, they are deemed to have understood the sentence.\n\n\n"},"thoughts/language-of-thought":{"title":"Language of Thought","links":["thoughts/language","thoughts/computability","thoughts/neural-networks","thoughts/connectionist-networks","thoughts/GOFAI"],"tags":["seed"],"content":"Can we use language as the method of querying memory?\nLanguage of thought hypothesis\nProposes thinking occurs in a mental language called mentalese.\nComputations occur in a classical way (over symbols in mentalese). However, neural networks or connectionist networks challenges any argument for mentalese (as they don’t fall under the GOFAI/classical computation category)"},"thoughts/language":{"title":"Language","links":["thoughts/phonetics","thoughts/phonology","thoughts/semantics","thoughts/morphology","thoughts/syntax","thoughts/pragmatics","thoughts/sociolinguistics","thoughts/language-development","thoughts/language-of-thought","thoughts/linguistic-relativism","posts/new-words"],"tags":["seed"],"content":"\nLanguage is the systematic and conventional use of sounds (or signs or written symbols) for the purpose of communication or self expression (Crystal, 1995 as cited in Hoff 2014:4)\n\nLanguage uses both iconic and symbolic representations\n\nIconic: direct connection between the sound/shape/look of a word and its meaning\nSymbolic: no connection between the sound/shape/look of a word and its meaning\n\nChildren learn language through exposure to language, not through being taught explicit rules. Language must be learned through exposure, and the language you learn is based on the language you hear/see (input)\nThe goal of language is to learn a productive grammar that can generate (or produce) an infinite number of phrases that others can understand.\nInvolves knowledge of\n\n\nSounds (phonetics)\n\n\nWords (phonology, semantics)\n\n\nGrammar (morphology, syntax)\n\n\nSocial and communicative Function (pragmatics, sociolinguistics)\n\n\nLanguage Productivity: you can come up with (or generate) new words and sentences, even if you have never said them before\n\n\nLanguage Comprehension: you can understand infinite combinations of morphemes and words, even if you have never heard them before\n\n\nComponents of Language Knowledge\n\nPhonetics\nPhonology\nMorphology\nSyntax\nSemantics\n\nSemantic organization: organizing the world between cognitive organization and language\n\n\nPragmatics\nSociolinguistics\n\nSee also:\n\nlanguage development\nlanguage of thought\nlinguistic relativism\nNew Words\n"},"thoughts/latent-factor-model":{"title":"Latent-Factor Models","links":["thoughts/change-of-basis","thoughts/outlier-detection","thoughts/visualization","thoughts/gradient-descent"],"tags":["seed","CPSC340"],"content":"Like change of basis but instead of hand-picking the features, we learn them from data.\nPart weights are a change of basis from xi​ to some zi​. The canonical example is Principal Component Analysis (PCA)\nPCA\nPCA is parametric and does not provide unique global optimum.\nTakes in a matrix X and an input k and outputs two matrices such that X≈ZW:\n\nZ is a (n,k) matrix. Each row zi​ is a set of features/part weights\nW is a (k,d) matrix. Each row wc​ is a part/factor/principle component\n\nWe can think of W as rotating data so that the slope is zero\n\n\nApproximation of one x^ij​ is (wjTzi​)=⟨wj,zi​⟩\n\nEach xi​ can be thought of as a linear combination of all the factors\n\n\n\nAssumptions:\n\nAssumes X is centered (each column of X has a mean of zero)\n\nUse cases:\n\nDimension reductionality: Effectively allows us to reduce the dimensionality of X if k&lt;d\n\nActually, it only ever makes sense if k≤d\n\n\nOutlier detection: if PCA gives a poor approximation, xi​ could be an outlier\nData visualization: k=2 to visualize high-dimensional objects\n\nWe minimize\nf(W,Z)​=i=1∑n​j=1∑d​(⟨wj,zi​⟩−xij​)2=i=1∑n​∥WTzi​−xi​∥2=∥ZW−X∥F2​​Approximating xij​ by ⟨wj,zi​⟩Approximating xi​ by WTzi​Approximating X by ZW​\nIf we do alternating minimization,\n\nFix Z and optimize W: ∇w​f(W,Z)=ZTZW−ZTX\nFix W and optimize Z: ∇w​f(W,Z)=ZWWT−XWT\n\nWe converge to a local optimum which will be a global optimum if W and Z are randomly initialized (if you don’t pick a saddle point)\nFor large X, we can also just use SGD and cost per iteration is only O(k)\nChoose k by variance explained\nHow much of the variance can be explained by the choice of factors?\nFor a given k, we compute the variance of the errors over the variable of each given xij​\n∥X∥F2​∥ZW−X∥F2​​\nUniqueness\nOptimal W is non-unique:\n\nScaling problem: Can multiply any wc​ by any non-zero α\nRotation problem: Can rotate any wc​ within the span\nLabel switching problem: Can switch any wc​ with any other wc​\n\nTo help with uniqueness,\n\nNormalization: We ensure ∥wc​∥=1\nOrthogonality: We enforce wcT​wc′​=0 for all c=c′\nSequential fitting, we fit each wi​ in sequence\n\nMulti-Dimensional Scaling\nGradient descent on points on a scatter point; try to make scatterplot distances match high-dimensional distances\nf(Z)=∑i=1n​∑j=i+1n​(∥zi​−zj​∥−∥xi​−xj​∥)2\nNo W matrix needed! However, cannot be done using singular value decomposition (a matrix factoring technique). We need gradient descent.\n\nNon convex\nSensitive to initialization\nUnfortunately, MDS often does not work well in practice; MDS tends to “crowd/squash” all the data points together like PCA.\n\nt-SNE\n\nt-Distributed Stochastic Neighbour Embedding\n\nHowever, using Euclidean (L2-norm) may not be a great representation of data that lives on low-dimensional manifolds. In these cases, Euclidean distances make sense “locally” but Euclidean distances may not make sense “globally”.\n\nt-SNE is actually a special case of Multi-Dimensional Scaling. The key idea is to focus on distance to “neighbours”, allowing gaps between distances to grow\nWord2Vec\nEach word i is represented by a vector of real numbers zi​\nTrained using a masking technique.\n\nTakes sentence fragments and hides/masks a middle word\nTrain so that zi​ of hidden word is similar to zi​ of surrounding words\n\np(zi​)=j∈surrounding∏​∑c=1# words​exp(zcT​zj​)exp(ziT​zj​)​\nGradient descent on for −log(p(zi​))"},"thoughts/learning":{"title":"Learning Frameworks","links":["thoughts/teaching","thoughts/academia","thoughts/social-contracts","thoughts/causality","thoughts/emergent-behaviour","thoughts/In-Over-Our-Heads"],"tags":["sapling"],"content":"\nLearning as an act of becoming really good at exploring terrain — Shaun Martin\n\nSee also: teaching\nFramework for knowledge\n\nSensing → info intake, active reading\nReasoning → asking questions, applying knowledge, critical thinking\nActing → learning by doing\n\nHow does this tie with academia and whether individuals are scared to try new things?\nAs historically human creatures, learning and obeying social contracts is how we’ve survived. As a result, we can distinguish actions as either discovery (child-like curiosity, discovering the world and its causal relations) or ritual (following exact actions because you were taught to fit in).\nThere was an experiment of subjecting young children (~5-6yrs) to a game of Powerball where a ball could be passed back and forth between 3 people. Later in the game, the ball wasn’t passed to one of the children, leading to feelings of social exclusion. This then led the young children to adopt more ritualistic approaches to learning rather than discovery-based.\nKnowledge Structures\nSource\nHow do we build up a set of concepts that make sense together? Some ideas are compatible, others aren’t.\nIdeas become more deeply integrated into our model as larger and larger chunks of ideas begin to form into knowledge structures. To learn, we simply have to expose the surface area of our existing knowledge structures to new ideas.\n\nThis is a naturally emergent process.\nTwo main variables to manipulate\n\nThe structure of our ideas\nThe information environment we expose ourselves to\n\nTraversing our knowledge structures frequently lets us recognise more effective traversal strategies for specific situations. Based on context, we can apply different traversal strategies (maps/lenses) to retrieve appropriate information from these structures\nAs we continue to learn, we move up layers of abstraction through these generalised heuristics:\n\nI have a strategy for X\nI have a nuanced strategy for X\nI have several strategies for X\nI have a meta-strategy to pick the appropriate strategy for X\n\nQuite similar to stages of development in In Over Our Heads"},"thoughts/library":{"title":"Libraries","links":["thoughts/library","thoughts/Internet","thoughts/information-system","thoughts/double-consciousness","thoughts/Arweave"],"tags":["seed"],"content":"The Great Library of Alexandria is one of the largest information systems in modern history. It was built in Alexandria, Egypt, and part of a larger research institution called the Mouseion. The idea behind the library was to be a universal collection of knowledge.\nMany influential philosophers worked at the library in the 2nd and 3rd centuries BC, including Euclid (founder of geometry), Homer (author of epic Greek poems), Plato (founder of the first Western university), and Socrates (founder of Western moral philosophy).\nPublic libraries are great examples of where knowledge goes beyond data and information. They are centers for civic knowledge, trusted knowledge broker, community archives, public digital infrastructure, rather than just a place to borrow books\nDo we need bookstores and libraries anymore? Yes!\n\nThey are the primary point of internet access for underprivileged communities.\nNot just about search and delivery of content, but about curation and serving the body not just the mind\n\nHow can digital communities learn from libraries and librarians to prevent a Library of Alexandria moment?\n\nWhen you’re trying to optimize for ‘hits’ or ‘clicks’ feels very opposite to how libraries value knowledge\nWhat knowledge needs to be protected? What knowledge shouldn’t be universally accessible?\nAlternative networks: what could the Internet have looked like if it was designed by librarians? A writing on the bit: A People’s History of Computing in the US\n\nSee also: information system\nFugitive Libraries\nSource: Fugitive Libraries by Shannon Mattern\nLibraries are not universally welcoming spaces. At least 87% of librarians are white, and stories of discrimination/hostility for their race/class/sexual identity/disability are not uncommon. There is a lot of reinforcement of outdated values embedded in classification.\nThere is a concept called ”double-consciousness” which is essentially the “sense of always looking at one’s self through the eyes of others, of measuring one’s soul by the tape of a world that looks on in amused contempt and pity” especially felt by minorities. Historical library practices can shape contemporary technologies.\nHow neutral should libraries be? Historically, neutrality has been a core value for a lot of librarians, but has often been used to justify “disengagement from crises in urban communities.”\nUndercommons: a place allowing for “ongoing experiment” with informal ways of learning together, of building futures together\nFugitivity then, is the mode of being other than settled, especially recognizing that there are spaces and modalities that exist separate from the logical, the logistical, the housed and the positioned.\nThe permanent library\nArchive nesting in Arweave\n\nIt is our expectation that when eventually a permanent information storage system more suited to the challenges of the time emerges, the Arweave’s data will be ‘subsumed’ into this network.\nThis pattern of ‘nesting’ of archives when they are retired is common across human history. An archive of Gopherspace (a ‘knowledge web’, prior to the HTTP-based web) can be found inside the Arweave’s permaweb. In-side the Gopherspace archive, one can find archives of earlier Telnet and bulletin board-based discussion systems\n"},"thoughts/life":{"title":"Life","links":["thoughts/consciousness"],"tags":["seed"],"content":"Spinoza\nSource: Spinoza — Understanding the emotions Clare Carlisle\nEvery individual thing has conatus: it strives to persevere in its existence. In order to live, we need power, or energy, and because various external influences can diminish our power, we seek not only to sustain this power, but to increase it.\nThe mind’s power of thought, and the body’s power of movement – fluctuates over time. Joy arises with the feeling of an increase in power, and the emotion of sadness when power is diminished.\nOzma of Oz\n\nTin Man vs Tik-Tok\n\nTin Man used to be living (born a real man) but lost his body over time due to a series of wood chopping ‘accidents’\n\nA modern tale of the ship of Theseus\nDoes the original entity persist through the gradual replacement of each of its parts, if it doesn’t, where does it stop being the same entity?\nTwo differences\n\nReplacement pieces are different material from the parts they replace (tin vs flesh) does this matter?\nTin Man tells his own story from when he used to be a creature of flesh and bones → has a memory\n\n\n\n\nTik-Tok has neither of those two differences\n\n“Thinks, Speaks, Acts, and Does Everything but Live”\nBuilt in a tinker’s shop\nTik-Tok is treated as a conscious being but is not treated as being able to live\n\n\n\n\nConclusion from Tik-Tok story\n\nNeither thinking nor consciousness entails being alive\n\n\nHilary Putnam’s ingenious argument (attributed to Paul Ziff)\n\nx is a mechanism → x is ~alive\nby first order logic\n\nif x is ~alive → x is not alive\nif x is not alive → Tik-Tok is not conscious\nthis is a contradiction of Tik-Tok’s label which proclaims he can think\n\n\n\n\n\nTraditional connection between thinking and being alive\nWe suspect the distinction between what is alive what is mechanical is not clear-cut\nTwo criteria of life\n\nstructural (important for plants)\n\nbiochemical definition of life → digestion, growth, reproduction, self-motion, perception, etc.\n\n\nbehavioural (important for animals)\n\nany program that matched some reasonably good psychological theory of how people behave would thereby satisfy the behavioural criteria for consciousness and life\n\n\n\nx is conscious does entail that xis alive. Descartes would agree but posits that we should alter language such that this entailment no longer holds"},"thoughts/linear-algebra":{"title":"Linear Algebra","links":[],"tags":["seed","CPSC340"],"content":"A lot of content summarized from Mark Schmidt’s notes on Linear Algebra\nNotation\nGenerally column major\n\nScalar (1,1): α\nColumn Vector (m, 1): [a1​a2​​]\nRow Vector (1, n): [a1​​a2​​]\nMatrix (m, n): [a1,1​a1,2​​a2,1​a2,2​​]\n\nOperations\nTranspose\n(AT)ij​=(A)ji​\nA matrix is symmetric if A=AT\nVector Addition\nAssociative (brackets don’t matter) and commutative (order independent)\na+b=[a1​a2​​]+[b1​b2​​]=[a1​+b1​a2​+b2​​]\nScalar Multiplication\nAssociative (brackets don’t matter) and commutative (order independent)\nαb=α[b1​b2​​]=[αb1​αb2​​]\nInner Product\nBetween two vectors of the same length, multiply each element together to get a scalar result\naTb=∑i=1n​ai​bi​=γ\nA specific version of this is the dot product which can be expressed as the inner product between two vectors, a⋅b=aTb\n\nCommutative: aTb=bTa\nDistributive across addition: aT(b+c)=aTb+aTc\n\nOuter Product\nBetween two vectors of the same length, create a matrix multiplying each combination of elements in each vector.\nGiven two vectors u=​u1​u2​⋮um​​​ and v=​v1​v2​⋮vn​​​,\nu⊗v=A=​u1​v1​u2​v1​⋮um​v1​​u1​v2​u2​v2​⋮um​v2​​……⋱…​u1​vn​u2​vn​⋮um​vn​​​\nThe resulting matrix A is always rank-1.\nMultiplication\nIn general, we can multiply matrices A and B when the number of columns in A matches the number of rows in B\nIf A is (m, n) and B is (n, p), then AB is (m, p)\n\n\nAssociative: A(BC)=(AB)C\nDistributive across addition: A(B+C)=AB+AC\nGenerally not commutative: AB=BA\nTransposing reverses order: (AB)T=BTAT\nMatrix powers don’t change order: (AB)2=ABAB\nMatrix-vector multiplication always yields a vector: xTAy=xT(Ay)=(Ay)Tx=yTATx\n\nProperties\nVector Norm\nA scalar measure of a vector’s length\n\n\n∥x∥≥0\n\n\n∥x∥22​=xTx\n\n\nEuclidean Norm (L2-Norm): ∥x∥2​=∑i=1n​xi2​​\n\nAlso note that ∥x∥2=∥x∥22​=rTr=⟨r,r⟩\n\n\n\nManhattan Distance (L1-Norm): ∥x∥1​=∣r1​∣+∣r2​∣\n\nHow many ‘blocks’ you need to traverse\n\n\n\nL∞-Norm: ∥x∥∞​=max(∣r1​∣,∣r2​∣)\n\nHow many blocks you have to walk in any one dimensions\n\n\n\nRank\n\nThe dimension of the vector space generated (or spanned) by its columns.\nThis corresponds to the number of linearly independent columns of A.\n\nThis minimal set of vectors that span a space is called a basis\n\n\n\nOrthogonal\nIf for some set of vectors q:\n\nqiT​qj​=0, we call qi​ and qj​ orthogonal\nqiT​qj​=1, we call qi​ and qj​ orthonormal\n\nInner product of square orthogonal matrices is the identity matrix: QTQ=I=QQT\nLinear Dependence\nA vector is linearly dependent on a set of vectors if it can be written as a linear combination of them\nc=α1​b1​+α2​b2​+⋯+αn​bn​\nA set of vectors is linearly dependent if and only if the zero vector can be written as a non-trivial combination of any of the vectors.\nA matrix with fully independent columns has full column rank. If this is the case, Ax=0 implies that x=0\nSpecial Matrices\nIdentity Matrix\n1’s on the diagonal and 0’s otherwise. In​ denotes an (n,n) identity matrix.\nMultiplication by the identity matrix yields the original matrix. Columns of the identity matrix are called elementary vectors.\nDiagonal Matrix\nD=​d1​00​0d2​0​00d3​​​\nSpaces\nRange (Column-space)\nSubspace spanned by the columns of a matrix.\nA system Ax=b is solvable if and only if b is in A’s column-space\nSubspace\nA non-empty subset of vector space that is closed under addition and scalar multiplication\nPossible spaces of R3\n\n0 Vector\nAny line or plane through the origin\nAll of R3\n\nWe say that the vectors generate or span the subspace when you can reach any point in the subspace through a linear combination of the vectors.\nMatrices as transformation\nViewing Ax=T(x)\nA linear transformation can’t move the origin. But, if there are linearly dependent columns, there are non-zero vectors that can be transformed to zero. The set of vectors that can be transformed to 0 is called the null-space.\nNull space: N(A) is all x such that Ax=0\nFundamental Theorem of Linear Algebra\n\nr is the dimension of the column-space which is the same as the dimension of the row-space\nThe null-space is orthogonal to the row-space\n\nInverses\nWe can find the inverses if and only if A is square and doesn’t have null-space outside of the zero vector (otherwise we either lose information to the null-space or can’t get to all vectors)\nIf the inverse exists, it is a unique matrix such that A−1A=I=AA−1\nSome identities\n\n(A−1)T=(AT)−1\n(γA)−1=γ−1A−1\nAssuming both A−1 and B−1 exist, (AB)−1=B−1A−1\n\nSpecial inverses of diagonal matrices\nD=​d1​00​0d2​0​00d3​​​\nD−1=​1/d1​00​01/d2​0​001/d3​​​\nSolving Linear Equations\nGiven A and b, we want to solve for x in Ax=b\nSay, [21​−11​][xy​]=[15​].\nWe can interpret this multiple ways:\n\nBy rows: x is the intersection of the hyperplanes 2x−y=1 and x+y=5\nBy columns: x is the linear combination that yields the RHS in x[25​]+y[−11​]=[15​]\nTransformation\n\nAx=b generally has a solution when b is in the column-space of A. It has a single unique solution if the columns of A are linearly independent.\nIf Ax=b has as solution we say it is consistent.\nBasically, x=A−1b\nWe can solve using Gaussian Elimination"},"thoughts/linear-regression":{"title":"Linear Regression","links":["thoughts/calculus","thoughts/change-of-basis","thoughts/Decisions-under-ignorance","thoughts/regularization"],"tags":["seed","CPSC340"],"content":"Vector dimensions:\n\nw is (d,1) (weights)\ny is (n,1) (targets)\nxi​ is (d,1) (features)\nX is (n,d) each row is xiT​\n\nLinear regression makes predictions y^​i​ using a linear function of xi​: y^​i​=wTxi​\nWe set w to minimize the sum of squared errors: f(w)=∑i=1n​(wTxi​−yi​)2\n\nTake the derivative of f and set it equal to 0 f′(w)=0 gives us w=∑i=1n​xi2​∑i=1n​xi​yi​​\nCheck to second derivative to make sure we have a minimizer (if double derivative is positive). f′′(w)=∑i=1n​xi2​. As xi2​ by definition must always be positive, this is a minimizer.\n\nIn d-dimensions, we minimize\nf(w)​=21​i=1∑n​(wTxi​−yi​)2=21​∥Xw−y∥2=21​wTXTXw−wTXTy+21​yTy=21​wTAw−wTb+c​​​\nwhere A is a matrix, b is a vector, and c is a scalar\nThe generalized version of “set the derivative to 0 and solve” in d-dimensions is to find where the gradient is zero (see calculus). We get\n∇f(w)​=​∂w1​∂f​∂w2​∂f​⋮ ∂wd​∂f​​​=​∑i=1n​(wTxi​−yi)xi,1​∑i=1n​(wTxi​−yi)xi,2​⋮ ∑i=1n​(wTxi​−yi)xi,d​​​=Aw−b=XTXw−XTy​​​\nWe can fit to polynomial equations using a change of basis\nCost\nOf solving equations in the form Aw=b\n\nO(nd) to form vector b\nO(nd2) to form matrix A\nSolving a (d,d) system of equations is O(d3)\n\nOverall cost is O(nd2+d3)\nRobust Regression\nWe minimize the L1-norm of residuals instead of L2-norm\nf(w)=∥Xw−y∥1​\nHowever, as the L1-norm uses the absolute function, it is non-differentiable at 0. We can use a smooth approximation of the L1-norm instead, like Huber loss:\nh(ri​)={21​ri2​ϵ(∣ri​∣−21​ϵ)​∣ri​∣≤ϵotherwise​\nAbsolute error is more robust and non-convex errors are the most robust.\n\nGenerally not influenced by outlier groups\nBut it is non-convex so finding global minimum is hard\n\nBrittle Regression\nYou want to minimize size of worst error across examples. For example, if in worst case the plane can crash or you perform badly on a group.\nWe can instead minimize the L∞​ norm which is convex but non-smooth. This effectively minimizes the highest error (effectively Minimax regret in DUI).\nThe smooth approximation to the max function is the log-sum-exp function:\nmaxi​{zi​}≈log(∑i​exp(zi​))\nPenalizing Model Complexity\nOptimize score(p)=21​∥Zp​v−y∥2+p where p is the degree of the polynomial.\nOther ones also exist which replace the p term with λk where k is the estimated degrees of freedom (for polynomials, k=p+1). λ controls how strongly we penalize complexity.\nλ=1 is called the Akaike information criterion (AIC)\nSee also: regularization"},"thoughts/linearizability":{"title":"Linearizability","links":["thoughts/causality"],"tags":["seed"],"content":"Defined as consistency in the face of concurrent reads/writes.\nInformally: every operation takes effect atomically sometime after it started and before it finished. All operations behave as if executed on a single copy of the data\nNot to be confused with serializability: transactions having the same effect as if they were run in some serial order. Also contrasting with causal relationships, linearizability is defined in terms of real-time whereas causal is defined in terms of message sending and receiving.\nThe consequence/desired property of linearizability is that every operation returns an “up-to-date” value, sometimes called “strong consistency”\nWe can guarantee linearizability of get (quorum read + read repair) and set (blind write to quorum). If events overlap, either order could happen and is ok.\nNot without downsides\n\nPerformance costs: lots of messages and waiting for responses\nScalability limits: leader can be a bottleneck\nAvailability problems: if you can’t contact a quorum of nodes, you can’t process any operations\n"},"thoughts/linguistic-relativism":{"title":"Linguistic Relativism","links":["thoughts/language","thoughts/terminology","thoughts/hermeneutical-injustice","posts/new-words","thoughts/language-of-thought"],"tags":["sapling","pattern"],"content":"See: linguistics\nDoes language dictate how we think about the world? Do we need labels and terminology to discuss things? Yes! Otherwise, we are in danger of being hermeneutically injusticed (see also: new-words)\nProgramming\nSapir-Whorf hypothesis is true — but the effects are far more pronounced for programming languages than for spoken languages.\nThe language you write code in ends up shaping huge parts of you worldview even when you’re not programming (language of thought). The best purpose of language in general, and programming languages in particular is to expand the domain of thinkable thoughts\nThe power of programming languages (and why you may want to learn them, even if not intent on building software) is that they let you get you hands dirty with building and using your own abstractions."},"thoughts/liveness":{"title":"Liveness","links":["thoughts/distributed-systems","thoughts/consistency"],"tags":["seed"],"content":"\nA promise in distributed systems that claims that “something good” will eventually happen. A system will never enter a state such that no progress cannot be made.\n\nOne such example of a liveness property is eventual consistency."},"thoughts/local-first-software":{"title":"Local-first software","links":["thoughts/jazz.tools","thoughts/CRDT","thoughts/Operational-Transform","thoughts/privacy","thoughts/NAT"],"tags":["seed","pattern"],"content":"\nBy centralizing data storage on servers, cloud apps also take away ownership and agency from users. If a service shuts down, the software stops functioning, and data created with that software is lost.\n\nWhy local first?\n\n1-5yr time scale: access benefits, computing literacy\n\nhttp://viznut.fi/texts-en/permacomputing.html\nAny community that uses computers would have the ability to create its own software. A local software would address local needs better than the generic “one size fits all” solutions would.\n\n\n50yr time scale: change in how we view computing (interop, intersubjective, etc)\n5000yr time scale: enable interplanetary collaboration\n\n\nfrom Anselm Eickhoff’s presentation of jazz.tools at Local-first Meetup Berlin\nInk and Switch\nYou own your data, in spite of the cloud\nTo sum up: the cloud gives us collaboration, but old-fashioned apps give us ownership. Can’t we have the best of both worlds?\n\nTraditional web app model: “If it’s not stored in the server database, it didn’t really happen”\nLocal-first model: “The client’s local storage is what matters — the server is just for multi-user sync and backup”\n\nLocal-first ideals include\n\nNo spinners: your work at your fingertips\n\nAll operations can be handled by reading and writing files on the local disk, and data synchronization with other devices happens quietly in the background.\n\n\nYour work is not trapped on one device\nNetwork is optional\nSeamless collaboration\n\nAuto-merging changes using a CRDT or OT\nAsynchronous changes that need review (e.g. suggestions or pull requests)\n\n\nThe long now (optional permanence/digital longetivity)\n\nWhen you do some work with local-first software, your work should continue to be accessible indefinitely, even after the company that produced the software is gone.\n\n\nSecurity and privacy by default\n\nMany professionals cannot use cloud apps due to regulatory compliance and confidentiality obligations.\n\n\nUser retains ownership and control\n\nYou should be able to copy and modify data in any way, write down any thought, and no company should restrict what you are allowed to do.\n\n\n\nA potential digital dark age is looming. The documents created in cloud apps are destined to disappear when the creators of those services cease to maintain them. Cloud services defy long-term preservation. No Wayback Machine can restore a sunsetted web application. The Internet Archive cannot preserve your Google Docs.\nServers have a role to play in the local-first world — not as central authorities, but as “cloud peers” that support client applications without being on the critical path. For example, a cloud peer that stores a copy of the document, and forwards it to other peers when they come online, could solve the closed-laptop problem.\nThe key difference between traditional systems and local-first systems is not an absence of servers, but a change in their responsibilities: they are in a supporting role, not the source of truth.\nActive questions:\n\nCRDTs accumulate a large change history, which creates performance problems. These pile up, but can’t easily be truncated because it’s impossible to know when someone might reconnect to your shared document after six months away and need to merge changes from that point forward.\nNetwork communication remains an unsolved problem. The use of P2P technologies in our prototypes yielded mixed results. On one hand, these technologies are nowhere near production-ready: NAT traversal, in particular, is unreliable depending on the particular router or network topology where the user is currently connected.\nVisualizing document history is important and difficult. How do we communicate this version history to users? How should users think about versioning, share and accept changes, and understand how their documents came to be a certain way when there is no central source of truth?\n"},"thoughts/logical-fallacies":{"title":"Logical fallacies","links":[],"tags":["seed"],"content":"\nSlippery Slope: conclusion based on the premise that if A happens, then B, C, …, X, Y, Z will happen too. So A means Z will happen. But in reality, none of the steps logically entail each other.\nHasty Generalization: conclusion based on insufficient or biased evidence\nPost hoc ergo propter hoc: conclusion that if A occurred after B, then B must have caused A\nGenetic fallacy: conclusion that the origins of a person, idea, institute, or theory determines its nature, character, or worth\nBegging the claim: circular conclusion where the result is validated in the claim\nCircular argument: restates the argument rather than proving it\nEither/or (false dichotomy): oversimplifying an argument by reducing it to two sides or choices\nAd hominem: attack on the character of a person rather than their opinions or arguments\nAd populum: eoptional appeal to speak to positive/negative concepts rather than the real issue at hand (e.g. if you were a true x)\nRed herring: diversionary tactic that avoids key issues by diverting it to another argument\nStraw man: oversimplifies an opposing viewpoint and attacks weakened argument\n"},"thoughts/longest-chain-consensus":{"title":"Longest-chain consensus","links":["thoughts/consensus","thoughts/system-model","thoughts/Public-key-Infrastructure","thoughts/proof-of-work","thoughts/proof-of-stake"],"tags":["seed"],"content":"See also: consensus\nRequires f&lt;2n​\nPrimarily studied in the synchronous system model. Three forms:\n\nPermissioned + PKI\nPermissionless + PoW\nPermissionless + PoS\n\nPseudocode properties (all implementations should satisfy these!):\n\nInitialize a hard-coded “genesis block” B0​ so everyone knows where the chain starts\nIn each round r=1,2,3,…\n\nOne node is chosen as a leader. This leader can prove itself as leader to other nodes, non-leaders cannot pretend to be a leader or manipulate their chances of becoming a leader.\nLeader can create a set of round-r blocks where all of there predecessors are strictly created in some previous round, each with a predecessor block. Blocks form an in-tree rooted at the genesis block.\n\n\n\nHonest behaviour\n\nForm block B using all known pending pending transactions\nSet the predecessor of B to be the current longest chain, break ties arbitrarily\nImmediately broadcast (B,predecessor) to all other nodes\n\nBalanced Leaders\nWe define a sequence l1​,l2​,…,∈{H,A} as w-balanced if in every window li​,li+1​,…,lj​ where j−1≥w and a strict majority of leaders in that sequence are honest.\nOn finality: if f&lt;2n​, we can consider all but the last k blocks in the chain finalized. k is up to the user to determine what parts of the chain they want to consider finalized.\nIf we assume that the rate of block production is slow relative to the maximum message delay Δ then inadvertent honest forks rarely occur.\nWe define Bk​(G) as the last k blocks of the longest chain where G is the current tree of all transactions known. Bk​(G) is potentially ill-defined if there are multiple longest chains.\nTheorem: if a leader sequence is 2k-balanced, then for every possible sequence G0​,G1​,… has\n\nThe common prefix property: ∀i,Bk​(Gi​) is well defined\nFinality: one a block is confirmed, it is always confirmed, Bk​(G0​)≤Bk​(G1​)≤…\nLiveness: if a transaction is known to all honest nodes, it will eventually be included in Bk​(G)\n\nIn the case that we chose completely random leaders:\n\nwe can expect randomly chosen leaders to be reasonably balanced\n\non average, expect 1−nf​&gt;21​ nodes to be honest\nbigger window length w means that we are less likely to see ≥50% Byzantine nodes\nProbability of a given length w window being ≥50% Byzantine is ≤e−cw where c is some constant (exponential is good!)\nProbability of a given length ≥w window being ≥50% Byzantine is ≤T2e−cw where T is the length of the sequence we’re considering\nThus we get a failure probability less than some δ if w≥c2​(lnT+lnδ1​)\n\n\nhowever, there is a small but non-zero chance of balancing failure\n"},"thoughts/longevity":{"title":"Longevity","links":["thoughts/life","thoughts/public-goods","thoughts/money","thoughts/funding","thoughts/research-institutions"],"tags":["sapling"],"content":"How much of human existence is just us trying to create impact beyond our incredibly short lifespans? If we extend life beyond its natural lifespan, do we need to reconsider how we define life?\nOf resources and knowledge\nPermanent land preserves, the Global Seed Vault, the internet itself as the foundational communications technology of a global age. These are not only public goods, they are the cultural practices that maintain such goods over generations.\nMoney and research\nMoney is power, and many are already looking for ambitious and impactful ways to spend their billion-dollar treasuries. Could funding long-term research like creating a new DARPA be the way forward?"},"thoughts/lossiness-as-mutation":{"title":"Lossiness as Mutation","links":["thoughts/emergent-behaviour","thoughts/bandwidth","thoughts/writing"],"tags":["seed","pattern"],"content":"Source: Self-Organizing Ideas by Gordon Brander\nMaybe chaos is necessary for emergent behaviour. Thus, lossy communication in low bandwidth communication helps seed for selection/mutation of new ideas. Examples of this include writing.\nWhen we write, we flatten the cloud of associated ideas in our head into a linearized subset (lossy). The reader then unflattens this linearized subset into their own cloud of associated ideas (lossy). Each lossy step is an opportunity for mutations in understanding to emerge."},"thoughts/lost-knowledge":{"title":"Lost knowledge","links":["thoughts/forgetting","posts/collaborative-thinking","posts/networked-thought","thoughts/digital-permanence","thoughts/search"],"tags":["seed"],"content":"Source: Searching for Lost Knowledge in the Age of Intelligent Machines in The Atlantic\n\nWhat if other objects like the Antikythera Mechanism have already been discovered and forgotten? There may well be documented evidence of such finds somewhere in the world, in the vast archives of human research, scholarly and otherwise, but simply no way to search for them. Until now.\n\nUndiscovered public knowledge: coined by Don Swanson. A problem that occurs when researchers arrive at conclusions independently from one another, creating fragments of understanding that are “logically related but never retrieved, brought together, [or] interpreted,”\nAre better tools for collaborative thinking with networked thought potential ways to counteract this? What about digital permanence? How do we effectively search for information?\n\n“The prime action of use is selection, and here we are halting indeed. There may be millions of fine thoughts, and the account of the experience on which they are based, all encased within stone walls of acceptable architectural form; but if the scholar can get at only one a week by diligent search, his syntheses are not likely to keep up with the current scene.”\n— As We May Think, Vannevar Bush\n"},"thoughts/machine-learning":{"title":"Machine Learning","links":["thoughts/Data-Capitalism","thoughts/data-mining","thoughts/exploratory-data-analysis","thoughts/supervised-learning","thoughts/No-Free-Lunch-Theorem","thoughts/unsupervised-learning","thoughts/gradient-descent","thoughts/hyper-parameter-optimization","thoughts/regularization","thoughts/linear-regression","thoughts/multi-class-classification","thoughts/binary-classification","thoughts/maximum-likelihood-estimation","thoughts/latent-factor-model","thoughts/recommendation-system","thoughts/neural-networks","thoughts/convolutional-neural-networks","thoughts/Autoencoders","thoughts/semantics","thoughts/transformers","thoughts/generative-models","thoughts/GOFAI","thoughts/NFAI","thoughts/linear-algebra","thoughts/probability","thoughts/calculus","thoughts/automatic-differentiation"],"tags":["sapling","CPSC340"],"content":"Theory\nWe produce a lot of data (see: data capitalism)\n\nData mining: automatically extract useful knowledge from large datasets\nMachine learning: automatically detect patterns in data and use these to make predictions or decisions\n\nTypically, AI ⊂ ML ⊂ Deep Learning\n\n\nTypically, data mining is more human-in-the-loop and more application specific whereas machine learning is more hands-off and general\nBoth similar to statistics but more emphasis on larger datasets, predictions instead of descriptions, and more general models\n\nHealthy skepticism is good though:\n\n“The combination of some data and an aching desire for an answer does not ensure that reasonable answer can be extracted from a given body of data”\n\nJohn Tukey\n\n\nMain topics:\n\nExploratory Data Analysis\nSupervised learning\nNo Free Lunch Theorem\nUnsupervised Learning\nOptimization\n\nGradient Descent\nhyper-parameter optimization\n\n\nRegularization\nRegression\n\nLinear Regression\nMulti-class classification\n\n\nBinary classification\nMLE\nLatent-factor models\nRecommender System\nNeural Networks\nCNNs\nAutoencoders\nDeep learning Semantics\nTransformers\nGenerative Models\nPhilosophy\n\nGOFAI\nNFAI\n\n\n\nRelated background:\n\nLinear Algebra\nProbability\nCalculus\n\nAutomatic Differentiation\n\n\n"},"thoughts/maintenance":{"title":"Maintenance","links":["thoughts/infrastructure","thoughts/incentives","posts/paid-open-source","thoughts/tragedy-of-the-commons","thoughts/web3","thoughts/public-goods","thoughts/innovation","thoughts/From-Counterculture-to-Cyberculture","thoughts/Lindy-effect"],"tags":["sapling","pattern"],"content":"\nWe (re)build our computer systems the way we (re)build our cities: over time, without a plan, on top of ruins (Ellen Ullman)\n\nMaintenance: the gritty upkeep work that keeps the infrastructure of the world running.\nOne of my favourite examples is the constant rebuilding of the Ise Grand Shrine. Retaining knowledge through constant revision\nIncentivizing Maintenance\nThe Maintainers: the individuals whose work keeps ordinary existence going rather than introducing novel things. Related to paid open source, how do we incentivize maintenance?\nIs maintenance just the battle against the tragedy of the commons? “Fundamentally, digital infrastructure has a free rider problem. Resources are offered for free, and everybody (whether individual developer or large software company) uses them, so nobody is incentivized to contribute back, figuring that somebody else will step in.” Source\nWho are all the maintainers behind the thousands of libraries we depend on each day? Is maintenance a form of Ghost Work?\nCan we use web3 to ‘codify’ maintenance as a value? Is maintenance inherently centralized (i.e. we need a centralized body to uphold a public good)\nInnovation is overvalued\nSource: Hail the maintainers by Andrew Russell, The Maintainers Organization and their fellowship\n“What happens after innovation, they argue, is more important. Maintenance and repair, the building of infrastructures, the mundane labour that goes into sustaining functioning and efficient infrastructures, simply has more impact on people’s daily lives than the vast majority of technological innovations.”\n“These shuttles brought high-tech employees from hip, pricey urban homes to their lush suburban campuses, without exposing them to the inconvenience of public transportation or to the vast populations of the poor and homeless who also call Silicon Valley their home.” → similar to some concepts in From Counterculture to Cyberculture talking about displacement of people during the ‘back to the land’ movement by the New Communalists\nIt is crucial to understand that technology is not innovation. This preoccupation with novelty is unfortunate because it fails to account for technologies in widespread use, and it obscures how many of the things around us are quite old.\nThe stalest innovation stories focus on well-to-do white guys sitting in garages in a small region of California, but human beings in the Global South live with technologies too. Which ones? Where do they come from? How are they produced, used, repaired?\nRelated: Lindy Effect\nThird, focusing on infrastructure or on old, existing things rather than novel ones reminds us of the absolute centrality of the work that goes into keeping the entire world going. We need to acknowledge and attribute where we are today to the shoulders of the giants we stand on.\n“Feminist theorists have long argued that obsessions with technological novelty obscures all of the labour, including housework, that women, disproportionately, do to keep life on track.”\nMaintenance and Care\nShannon Mattern in Places Journal\n\n“All of the incentives for all the actors are against maintenance. Nobody ever named a maintenance project, nobody ever got recognized for a maintenance project, nobody ever much got blamed for deferring maintenance during the time while they were in office.”\n\nRust: urban infrastructures\nOutsiders sometimes make the mistake of focusing on the rusty bridges and broken pipes — the “defective objects” themselves — whereas local fixers are more concerned with “the social and political relationships in which [those objects are] embedded.”\nAs Nikhil Anand writes in Hydraulic City, “The maintenance of water infrastructures binds residents, plumbers, engineers, and politicians in an (uneven) system of “hydraulic citizenship.””\nWe should always ask: what, exactly, is being maintained? “Is it the thing itself,” Graham and Thrift ask, “or the negotiated order that surrounds it, or some ‘larger’ entity?” Often the answer is all of the above.\nWe should also remember that the preservation of our world — the human one — is sometimes at odds with caring for the ecological context. Perhaps not every road should be repaired. Geographer Caitlin DeSilvey encourages us to embrace entropy within the built world, to ask ourselves for whom we engage in preservation, and to consider cultivating an acceptance of “curated decay” where appropriate.\nDust: domestic maintenance, housework, care work\nMaintaining life — that’s a big job. Joan Tronto and Berenice Fisher define care as “everything that we do to maintain, continue, and repair ‘our world’ so that we can live in it as well as possible.\nWe care for things not because they produce value, but because they already have value.\nAryn Martin, Natasha Myers, and Ana Viseu propose that a critical practice of care would “pay attention to the privileged position of the caring subject, wary of who has the power to care, and who or what tends to get designated the proper or improper objects of care.”\nCracks: repair of objects\n“repair not only extends the use value of objects but becomes a mechanism of social interaction.” People gather around, watch, and chat. The shop is a space of public pedagogy, an “operating theater” where the repairman opens gadgets, demonstrates technical skills, and perhaps encourages observers to mend rather than discard their own broken things\nCorruption: curators who clean and maintain data\nHistorian Nathan Ensmenger reports that “from the early 1960s to the present, software maintenance costs have represented between 50 and 70 percent of all total expenditures on software development.”\nThe internal maintenance work isn’t supposed to be visible to end users, who tend to like the idea that they’re working with “raw” data. Yet “data never come as raw,” Plantin observes. “Multiple interventions are always needed before data can be reused.”"},"thoughts/map-as-territory":{"title":"Map as territory","links":["thoughts/To-Live-in-their-Utopia","thoughts/Seeing-like-a-State","thoughts/LLMs","thoughts/search"],"tags":["seed","pattern"],"content":"\n“Our maps are still maps, approximating the landscape of truth from the territories of the knowable — incomplete representational models that always leave more to map, more to fathom, because the selfsame forces that made the universe also made the figuring instrument with which we try to comprehend it.” Source\n\nSee also: abridged maps (To Live in their Utopia), Seeing like a State\nAggregators instead of Oracles\nSource\n\n“When I first began tinkering with a software program that eventually gave rise to the idea of the World Wide Web, I named it Enquire, short for Enquire Within upon Everything, a musty old book of Victorian advice I noticed as a child in my parents’ house outside London.\nENQUIRE was influenced by a book that claimed to be an oracle - to have the answer for every question, to promise the ‘suggestive magic’ that so excited the young Berners-Lee. But that promise was ultimately unfulfilled. Berners-Lee recognised that what he needed to build was not an oracle but a map - a web of paths and connections linking information and guiding users as they research, not an all-knowing entity that gives us a single answer.\n\n\nThe difference between screen-based search and voice based search is the difference between a map and an oracle - on screens we’re given a list of search results to navigate, whilst in voice based search we’re not read a list of potential links - we’re given an answer.\nTemperature gives us variability, and this is what makes LLMs more of a walk through a map of latent space than a single true answer\n\n0, which produces perfect fidelity to the physics, i.e., always selecting the most likely next word\n0.8, which slightly weakens the gravitational pull in its textspace, so that less common words will be chosen more frequently\n\n\nPredictions are paths within a multidimensional space of knowledge, helping us map out those spaces and understand its hills and valleys\n\nHow do we show the map when the map itself is in thousands of dimensions?\n\n\n\nSee also: search as oracle"},"thoughts/math":{"title":"Math","links":["thoughts/boundary-object","thoughts/distributed-systems","thoughts/decentralization","posts/new-words","thoughts/semantics","thoughts/Raft-Consensus-Algorithm","thoughts/Tendermint","thoughts/PSL-FLM-Impossibility-Result","thoughts/FLP-Result"],"tags":["seed"],"content":"Three pillars of Math\n\nDefinitions: so you know what you’re talking about, the communicate ideas to others, boundary object. In distributed systems talks especially, disagreements over tech are actually just disguised disagreements over definitions (e.g. on what decentralization means, see also word meaning and semantics)\nTheorems\n\nPossibility results (e.g. protocols like Raft or Tendermint) articulate assumptions under which solution has desired properties\nImpossibility results (e.g. PSL-FLM and FLP results) tell you to avoid wasting time trying to design something that cannot exist\n\n\nProofs: arguments of why we know these theorems are actually true statements\n\nCan guide you to what the solution might look like (specifically, in the realm of possibility results)\nCan help you asses whether changes to the solution void the properties it originally had\n\n\n"},"thoughts/maximum-a-posteriori-estimation":{"title":"Maximum a Posteriori (MAP) Estimation","links":["thoughts/maximum-likelihood-estimation","thoughts/probability"],"tags":["seed","CPSC340"],"content":"Maximizes w^∈argmaxw​{P(w∣D)}\nGiven our data, what is the model w is the best model?\nThis is connected to MLE through Bayes’ Rule:\nP(w∣D)=P(D)P(D∣w)P(w)​∝P(D∣w)P(w)\nIntuitively, P(w) is accounting for how ‘likely’ this model is. We can also treat this as a regularizer.\nw^∈argwmax​{P(w∣D)}​≡argwmax​{i=1∏n​P(Di​∣w)P(w)}≡argwmin​{−i=1∑n​log(P(Di​∣w))−log(P(w))}​\nWhere −log(P(w)) acts like the regularizing term. In fact, many regularizers are equivalent to negative log-priors.\nRelation between regularized loss functions\nL2-Regularized Least Squares\nIf we assume a Gaussian likelihood and a Gaussian prior, then MAP estimation is equivalent to minimizing f(w)=21​∥Xw−y∥2+2λ​∥w∥2\nL2-Regularized Robust Regression\nIf we assume a Laplace likelihood and a Gaussian prior, then MAP estimation is equivalent to minimizing f(w)=∥Xw−y∥1​+2λ​∥w∥2"},"thoughts/maximum-likelihood-estimation":{"title":"Maximum Likelihood Estimation (MLE)","links":["thoughts/linear-regression","thoughts/Naive-Bayes","thoughts/No-Free-Lunch-Theorem","thoughts/maximum-a-posteriori-estimation"],"tags":["seed","CPSC340"],"content":"Maximizes w^∈argmaxw​{P(D∣w)}\nSuppose we have a dataset D with parameters w. For example,\n\nWe flip a coin three times and get D={H,H,T}\nThe parameter w is the probability that this coin lands heads\n\nThe likelihood as a probability mass function P(D∣w). MLE is choosing a w^ that maximizes the likelihood (w^∈argmaxw​{P(D∣w)})\n\nIn the case above, w^ is 32​\nNotation\nargmin and argmax return the set of parameter values achieving the minimum and maximum values respectively. For example:\nargminw​{(w−1)2}≡{1}\nargminw​{cos(w)}≡{…,−2π,0,2π,…}\nWe can also show that maximizing the MLE is equivalent to minimizing the negative log-likelihood. That is,\nw^∈argmaxw​{∏i=1n​P(Di​∣w)}≡argminw​{−∑i=1n​log(P(Di​∣w))}\nThis is true because logarithm is strictly monotonic so the location of the maximum doesn’t change if we take the logarithm. Changing the sign flips the max to the min.\nThis is typically easier to compute as it turns a product of probability into a sum.\nGenerative vs Discriminative\n\nDiscriminative maximizes P(y∣X,w)\n\nLeast squares, robust linear regression, logistic regression fall under this category\nWe don’t model X so we can use complicated features\n\n\nGenerative maximizes P(y,X∣w)\n\nNaive Bayes\nNeeds to model X\n\n\n\nRelation between loss functions\nLeast squares (squared L2-loss of residuals)\nIf we let the likelihood function of the labels be Gaussian:\nP(yi​∣xi​w)=2π​1​exp(−2(wTxi​−yi​)2​)\nThen the MLE of w is the minimum of f(w)=21​∥Xw−y∥2\nAbsolute error (L1-loss of residuals)\nIf we let the likelihood function of the labels be Laplacian:\nP(yi​∣xi​w)=21​exp(−∣xTxi​−yi​∣)\nThen the MLE of w is the minimum of f(w)=∥Xw−y∥1​\nLogistic loss\nh is the sigmoid function 1+exp(−x)1​. If we let the likelihood function of the labels be\nP(yi​∣w,xi​)=h(yi​wTxi​)=1+exp(−yi​wTxi​)1​\nThen the MLE of w is the NLL, which we can show to be equivalent to the logistic loss\nNLL(w)=∑i=1n​log(1+exp(−yi​wTxi​)1​)=∑i=1n​log(1+exp(−yi​wTxi​))\nLast part is true because of log rules (−log(x1​)=log(x)).\nOverfitting\nConceptually, MLE is saying that we should find the w that makes D have the highest probability given w. From No Free Lunch Theorem, we know that there is always a model that performs well for some unlikely w. This is overfitting!\nWe actually want to find the w that has the highest probability given the data D. For this, we need MAP"},"thoughts/meaning-laden":{"title":"Meaning-laden","links":["thoughts/terminology","thoughts/plurality","thoughts/truth"],"tags":["seed"],"content":"See also: terminology, plurality\nDefinition overloading\nI think my worry with a ‘standard library’ is that there is no one ‘unified’ interpretation of a single concept, how will it adapt to usage overtime? Wouldn’t this anchor thought patterns to a shared ’truth’ and prevent evolution of language?\n“By changing what we were, you change what we are and what we are going to be.”\nErasure through terminology change, is this essentially just cooptation?\n“Thus potential sources of uncertainty and misunderstanding arise in the form of homonyms (i.e. words that are used to denote more than one concept) and synonyms (i.e. more than one word for the same concept).” Source"},"thoughts/meaning":{"title":"Meaning","links":["thoughts/semantics","thoughts/language","thoughts/terminology","thoughts/quantization","thoughts/praxis"],"tags":["sapling"],"content":"\nIf code is law, then are programmers the governors?\n\nA less philosophical discussion of semantics\nSource: Meaning in Kernel\n“The fight for liberty is not conducted with natural language in the form of political rhetoric: it is hashed out in technical protocols.”\nInteresting quote by Buckminster Fuller (who did a lot of pioneering work on geodesic domes): “You never change things by fighting the existing reality. To change something, build a new model that makes the existing model obsolete”\nWith respect to the argument that ‘the fight for liberty’ being a fight fought with technical protocols and not natural language is interesting. I think natural language is absolutely necessary and that discourse is pretty similar to change itself. Shared terminology and labels for discourse is important for both. Change can happen based off of perceived improvement which is correlated with how well that improvement is communicated.\nSeems similar to praxis debate, too many layers of abstractions on theory that is no longer grounded in practice becomes useless unless applied. The arguments then occur in the abstract definitions of the labels and less so how they actually apply."},"thoughts/mechanism-design":{"title":"Mechanism design","links":["thoughts/incentives","thoughts/economics","thoughts/utility","thoughts/game-theory"],"tags":["seed"],"content":"Distributed Algorithmic Mechanism Design: Recent Results and Future Directions. Joan Feigenbaum and Scott Shenker\nMechanism design asks how one can design systems so that agents’ selfish behavior results in the desired system-wide goals (similar to incentive design and economics)\nThe mechanism designer’s task is to find a formula for the payments that causes agents to be no worse off by revealing their true costs than they would be by lying about their costs\nMechanisms in which agents are asked to directly reveal their utility functions are call direct mechanisms\nA dominant strategy is one where agents only choose strategies that regardless of how other agents play, never result in lower payoffs than any other strategy.\nSee: game theory, economics"},"thoughts/meditation":{"title":"Meditation","links":[],"tags":["seed","PHIL451A"],"content":"A Cognitive Science Approach\n\nFocused Attention\n\nDirecting attention towards an object (e.g. breathing)\nDetecting mind wandering/distractions\nDisengagement of attention from distractors\nLeads to effortless sustained attention and ability to monitor attention and notice mind-wandering\nStudy showing that focussed attention led to extreme increases in perceptual dominance both during and after meditation\n\n\nOpen Awareness\n\nNo explicit focus on objects\nNonreactive\nLeads to acute awareness of phenomenal qualities of experience without ‘grasping’ (approach/avoidance).\n\n\n"},"thoughts/memex":{"title":"Memex","links":["thoughts/hypertext","thoughts/the-garden-and-the-stream"],"tags":["seed"],"content":"Source by Vannevar Bush\n\ndevice in which an individual stores all his books, records, and communications, and which is mechanized so that it can be consulted with exceeding speed and flexibility.\n\nBush envisioned the memex behaving like an “intricate web of trails” similar to the function of the human mind, which he believed works by a method of “association” and not via an alphabetical index. According to Levy (2008, 508), the most “innovative feature” of Bush’s memex system was the establishing of associative indices between portions of microfilmed text—what we now call hypertext links—so that researchers could follow trails of useful information through masses of literature.\n\nTapping a few keys projects the head of the trail. A lever runs through it at will, stopping at interesting items, going off on side excursions. It is an interesting trail, pertinent to the discussion.\n\nThe web is not the memex\nSee also: the garden and the stream\n\nA memex contains both original materials and the materials. Unlike the web, there is no read-only version of the memex. Anything you read you can link and annotate. Not just reply, but change\nLinks are associative (read: backlinks)\nLinks and annotations are made by readers as well as writers. A stunning thing that we forget, but the link here is not part of the author’s intent, but of the reader’s analysis. On the world wide web of course, only an author gets to determine links. What would it be like to have Curius-like annotations by default?\n"},"thoughts/memory-palace":{"title":"Memory Palace","links":["thoughts/interaction-design","thoughts/information-scales"],"tags":["sapling"],"content":"Method of Loci\nSource: Method of Loci in Wikipedia\nCan we create associations between digital/physical space and concept space? Create a digital library of our concepts and ideas?\nBetter ways to browse/discover ideas and concepts\nMore from Bret Victor and DynamicLand\nRelated to better interaction design. How do we better browse information at different scales. One where conceptual connections between pieces of knowledge and branches can be visually seen and explored\nCan we bring together representations across fields so ideas can cross-pollinate?\nSupports\n\nGeneralization: going from specific examples to an abstracted pattern\nInstantiation: going from abstraction to specific examples\nAnalogy: diverse examples of the same pattern\n"},"thoughts/mental-model":{"title":"Mental Models","links":["thoughts/representation","thoughts/context","thoughts/feedback-loops","thoughts/conceptual-model"],"tags":["seed"],"content":"Mental models (cognitive frameworks) help us understand how people reason and react to interface experiences. They provide predictive and explanatory power for understanding user behaviour.\nIt is an inner representation of a system.\n2 types:\n\nInternal frameworks: about the mental process inside user’s head\nExternal frameworks: account for interactions with technologies, environment, and context\n\nCharacteristics:\n\nconstantly evolving\nnot always an accurate representation (can contain errors and uncertainty measures)\nprovide a simple representation of a complex phenomena\n\nModels are runnable. We use Norman’s seven stage model\n\nEstablish the goal to be achieved\nForm the intention for action (what should I do?)\nSpecify the action sequence (how do I do that?)\nExecute the action sequence (let’s see how it goes)\nPerceive the system state (what am I seeing and hearing?)\nInterpret the perceived system state (what’s actually happening?)\nEvaluate the system state (is this right?)\n\n(realistically, this model is only good for exploratory learning when a user is learning a system for the first time or for encountering error cases)\nIf a breakdown occurs on the left (2-4), we call that the gulf of execution: the difference between the intentions and allowable actions\nIf a breakdown occurs on the right (5-7), we call that the gulf of evaluation: the difference between the actual system state and user’s understanding\nSee also: feedback loops\nMental model vs Conceptual model\n\nmental models: something the user has (forms)\n\nusers “see” the system through mental models\nusers rely on mental models during usage\nmental models can support or impede user’s interaction\n\n\nconceptual model: something the designer creates\n\nessentially a high level description of how a system should work\nto foster good mental model formation by the user\n\n\n\n\nInterface Types\nPrimarily concerned with:\n\na function\ninteraction style used\ninput/output device used\nplatform it’s being designed for\n\nE.g.:\n\ncommand\ngraphical\nmultimedia\nvirtual reality\nweb\nmobile\n"},"thoughts/message-broadcast":{"title":"Message broadcast","links":["thoughts/Raft-Consensus-Algorithm","thoughts/State-Machine-Replication-(SMR)","thoughts/HoneyBadgerBFT","thoughts/causality","thoughts/gossip","thoughts/idempotence"],"tags":["seed"],"content":"Ordering\nTotal order broadcast or Atomic broadcast\nGlobally consistent broadcast, agreement from all nodes (hard but can be done with consensus algorithms like Raft!)\nIn state machine replication, total order broadcast assumes the state update function is deterministic. That is, whenever two replicas are in the same state, giving them the same input, they will transition to the same next state. The main limitation is that total order broadcast cannot update state immediately, have to wait for delivery through broadcast\nExamples: HoneyBadgerBFT\nCausal Broadcast\nObeys happens-before (causal) relationships.\nIn state machine replication, assumed state update function is deterministic and concurrent updates are commutative. Replicas can process updates in different orders and still end up in the same state\nFIFO (reliable) Broadcast\nMessages sent by the same node must be delivered in the order they were sent\nAssumes state update function is deterministic + all updates are commutative.\nBest-effort\nNo ordering guarantees.\nAssumes state update function is deterministic + commutative + idempotent + tolerates message loss\nReliability\nNodes can die mid-transmission!\nTwo strategies for mitigating node-death:\n\nEager reliable broadcast: first time a node receives a message, re-broadcast to each other node (reliable but expensive! O(n2) messages for n nodes)\nGossip: first time a node receives a message, forward it to k other nodes, chosen randomly (reliable with high probability)\n\nRetry semantics\n\nAt-most-once: send request, don’t retry, update may not happen\nAt-least-once: retry request until acknowledged, may repeat update\nExactly-once: retry + idempotence / deduplication\n"},"thoughts/metalabel":{"title":"Metalabel","links":["thoughts/idea-machine"],"tags":["seed"],"content":"Source: Metalabel.xyz\n\nA group of people using a collective identity to communicate a perspective through a series of distinct releases that contribute to a greater whole\n\nSee also: idea machine\nElements\n\nA core perspective or mission. That could be advocating for a certain perspective, aesthetic, region, idea, or about solving a problem and changing the world.\nA principle (or group of principles) curating the output. Labels are ultimately trying to communicate an idea, incrementally, with each release. That means they need a consistent vision to successfully put an idea in the mind of the public. This means a level of creative leadership is required to curate what releases and artists are invited to be a part of the project.\nDiscrete releases. A label exists to put culture into the world, whether that’s music, ideas, a way of living, words, or something else. What makes a label unique from a person’s personal creative practice is that different people are invited to release work under the same banner. By constructing an umbrella under which multiple artists can sit, the label can generate more dialogue and “heat” because disparate nodes in the network are reflecting back similar ideals. What makes a label unique from a brand is that all of a brand’s efforts result in the promotion of a single product. In the case of a label, its efforts are always promoting different products that all relate to the same core aesthetics or ideals.\nInformation architecture. Record labels use catalogue numbers to sequence their releases. A similar sequencing or contextualizing of works is a key element of the label. It helps clue t he audience into the larger context of the work. This is already common in the new worlds of drop commerce and Web3, where concepts like “Seasons” have become used to conceptually organize content and community, bringing the language of fashion and television into broader cultural creation. This mixing is a hallmark of meta labeling.\nA scene it participates in. Labels are at their best when they represent or are in dialogue with a thriving, organic scene or community. To truly be valuable, it must create value for the larger scene it’s a member of.\nA source of funding. Without funds to put into projects, any label is limited in its output. Previous labels used sales from their products to fund new releases, or someone’s existing personal wealth. In the world of Web3 and meta labels, these sources of funding are evolving\n"},"thoughts/metaphysics":{"title":"Metaphysics","links":["thoughts/ontology","thoughts/epistemology"],"tags":["seed","PHIL451A"],"content":"\nmeta ta physika (“after the things of nature”)\n\nIdeas that can’t be reached through objective studies of material reality. Common areas of study are ontology and epistemology"},"thoughts/mimetic":{"title":"Mimetic","links":["thoughts/in-group-bias","thoughts/communities","posts/collaborative-thinking"],"tags":["sapling"],"content":"Source: Mimetic by Brian Timar\nHumans inherit convictions mimetically from each other — we learn what to value by imitating our peers. Maybe this is why we tend to want to conform to ingroup expectations in communities so much?\nDangers of groupthink as these feedback loops loop into themselves and repeat:\ncollaborative-thinking\nAmong experimentalists, it’s not hard to find graduate students who can tell you every detail about how a particular machine operates, and almost nothing about why it should be built."},"thoughts/mind-body-problem":{"title":"Mind-Body Problem","links":["thoughts/Materialism","thoughts/idealism","thoughts/epistemic-authority"],"tags":["sapling"],"content":"Concerned with how the mind and body are connected to one another. Are we just entirely made up of matter or is there something more?\nTwo points of doubt\n\nWe know a bit about minds without knowing anything about brains\nAre minds different from brains?\n\nApproaches\n\nMaterialism (physicalism): minds and brains are identical (identity theory)\n\nnothing magical about minds, just the brains activity\n\n\nDualism: minds are distinct from the brain\nIdealism: everything is mental/in the mind\n\nConsidering other minds\nFor own mind, introspection works more or less. However for other minds, we must rely on behaviour of others (what they say and do, see also: epistemic authority)\nTwo possible ways of understanding how this way of knowing other minds works\n\nBehaviourism: behaviour is all there is to mentality\n\nSeems inconsistent with introspection\nWe have thoughts that are never reflected in our behaviour\nWe can have multiple thoughts that correspond with a behaviour → umbrella example\n\nA man looks out of a window, goes to a closet and takes an umbrella before leaving his house\nWhat is he thinking?\nThere are multiple possible thoughts that could’ve lead to his taking the umbrella\n\n\n\n\nCommon-sense psychology (C-SP): understand most likely correct interpretation\n\nCommon-sense knowledge of other minds rests on knowledge of some general principles of the characteristic behaviour of people\n\nHe sees pretty much what we see when we look.\nHe doesn’t like most stuff that people don’t like (getting soaked).\nHe minimizes costs when possible.\nIn general, we attribute normal perceptual and reasoning abilities.\n\n\n“We attribute thoughts to him that it is reasonable for him to have, given those abilities.”\n\n\n"},"thoughts/mindfulness":{"title":"Mindfulness","links":["posts/a-failure-resume","thoughts/digital-mindfulness","thoughts/meditation","thoughts/desire-paths"],"tags":["sapling"],"content":"What do I do when I am stressed or overcome with anxiety about the future? What about a terrible decision I feel like I’ve made or some failure?\nI’ve found regular mindfulness sessions to be extremely helpful in easing my ‘always-overthinking’ brain.\nRelated: how can we be more digitally mindful?\nSelf-guided Mindfulness\nI’ve tried guided meditations like the ones on YouTube and Headspace and such but find they don’t work super well for me. I really like being intentional with what I want out of the meditation session and I find it helps me focus and stay present. Here is the process I find works best for me:\n\nStart with focusing on breathing and bodily sensations. Are you comfortable? Is your head clear? Is your breath and heartbeat regular and relaxed?\nAsk an additional level of ‘why’s if anything feels out of the ordinary. Reflect on life situation + most current feelings.\n\nThen, if I am feeling stressed/anxious about anything in particular:\n\nPerspective and timescales. How will current events play out a month from now? A year? A decade?\nWhat doors can this open? We can choose to believe everything happens for a reason when looking back on it. Everything will play out regardless of what failures happens your way. The best I can do is make the most of my hand and make that the best possible future.\n\nOr if things are going well:\n\nWhat is going well right now?\nWho am I grateful for?\nWhat was beautiful today?\n\nDesigning for busy-by-default\nSource: Fools and their time metaphors by Aaron Z. Lewis\n“The subjective quality of time—how it feels from the inside—is missing entirely. If the idea of a “subjective calendar” sounds too far-out, consider the alternative maps made by creative cartographers. They add new meaning by distorting conventional representations of geography. Why can’t our calendars follow suit?”\nSee also: desire paths"},"thoughts/money":{"title":"Money","links":["thoughts/language","thoughts/decentralization","thoughts/incentives","thoughts/social-contracts","thoughts/utility","thoughts/Internet","thoughts/web3","thoughts/quadratic-funding","thoughts/accountability","thoughts/Protocol","thoughts/catch-22"],"tags":["sapling"],"content":"\nMoney is a form of shared truth\n\nSources: Money Language in Kernel and Shelling Out in Kernel\nMoney is old. “We have yet to discover a civilization that didn’t have money. So, we know it’s at least as old as civilization.”\nMoney is a language through which to communicate value. Then, was does this mean for decentralization? No one should control the expression of economic value in exactly the same way that no one controls the meaning of words: they are arrived at consensually through common use.\nWhat money is, how it is created, and who gets to distribute it goes to the very heart of the ways in which we are all incentivised to act\nIncentives can be thought of as the social, political and neurobiological primitives which define what kinds of behaviours we express.\n\nProgrammable money provides utility which makes it much more than just money for the internet: it turns it into the internet of money.\n\nMoney allows specialization, allowing goods and services to be transformed into a intermediary representation that is accepted by society.\nAmassing Capital\nUnfortunately, billionaries don’t actually do anything. Even if they start with good intentions, they get cynical the more they progress and ‘make it to the top’. After having huge amounts of capital, they are scared to give it out as the people who make their way in front of these people are the fakes. Because of the competition, the people who successfully can present are the ones who spent the time polishing the pitch rather than doing fundamental research and important breakthroughs.\nMoney and Imagination\n“I have a theory, which has not let me down so far, that there is an inverse relationship between imagination and money. Because the more money and technology that is available to [create] a work, the less imagination there will be in it.” Alan Moore\nMoney and Speech\n\nFree as in beer or free as in speech? — Richard Stallman\n\nFreedom of speech means that our ability to speak is free, but that doesn’t mean we can say whatever we want (e.g. hate speech and defamation).\nSimilarly for web3, access to the network is unrestricted (you only need a connection), but saying anything meaningful (e.g. state changing on the shared public record) has an associated cost. Thus certain behaviours we agree to be malicious (like creating fake transactions) are not disallowed, but just economically unsustainable.\nWouldn’t this favour the rich? Well yes, but we can mitigate this by using weighting like funding which values number of unique contributions more than dollar amounts.\nTaxes\nLaffer curve: as the tax rate increases, the rate at which revenue increases slows down due to increased avoidance, evasion, and disincentive to engage in the taxed activity.\nThis can be applied to the rise and fall of empires: “governments that overburdened their taxpayers, such as the Soviet Union and later Roman Empire, ended up on the dust-heap of history, while governments that collected below the optimum were often conquered by their better-funded neighbors.”\n(Although, might not be 100% valid)\nDispute Resolution\n\nMost pre-modern cultures, ranging from the Iriquois in America to the pre-Christian Germanic peoples, decided that payment was better than punishment\n\nCurious if the main reason is that you need to keep track of whether someone has served their punishment or not and memory historically has been unreliable. On the contrary, you don’t need to remember whether you were paid because you can just count your money.\nThe question applied to web3: can we advance the aims of rehabilitative justice using a shared and common historical record for better accountability?\nEngineering Money\n\nMoney-as-a-protocol really allows us to do is program incentives at scales never before possible\n\nMoney, in this context, is not a concrete thing, its an abstraction to communicative values, as a language, and as a technology. It is a classifier for things that exhibit behaviour that lets us use it for\n\nStore of Value\nMedium of Exchange\nUnit of Account\n\nMore importantly, there are tradeoffs in the above properties.\nGold is a great store of value but sucks as a medium of exchange (interesting tangent, the traditional heuristic that money should have inherent intrinsic value which is separate from the money but the truth is almost the opposite. If the medium used to express transactional relationship has its own value, its not a very functional abstraction).\nHistorically, like physical materials, we’ve just accepted properties the way they are and just built what we could with them. However, also like physical materials, we’ve found ways to engineer the fundamental properties that we want.\nHowever, it feels liek we run into a weird catch 22 here where it is debatable whether the new ‘engineered money’ actually does have the properties we claim to have so it holds us back from developing/accepting it any more. I think this is in large part due to money being a combination of both technical protocols and social contracts\nAndreas’ argument is that we can use abstract monetary protocols which we can engineer to change gradually higher-order social contracts."},"thoughts/monism":{"title":"Monism","links":["thoughts/idealism","thoughts/Materialism"],"tags":["seed","PHIL451A"],"content":"Monism holds that ultimate reality is all of one kind and one kind only.\nMore familiar forms of monism:\n\nIdealism\nMaterialism\n\nNeutral Monism\nThe intrinsic nature of ultimate reality is neither mental nor physical (between idealism and materialism)\nMuch like interiors and exteriors, in Russel’s account the mental and physical imply and necessitate each other as reflection of a single (hence monistic) nature"},"thoughts/morality":{"title":"Morality","links":["thoughts/Evolutionary-game-theory","thoughts/ethics"],"tags":["seed"],"content":"Morality is the compass of what people ought to or ought not to do, especially when it can cause benefit or harm to other people.\nOne of the really towering intellectual achievements of the 20th century was understanding the origins of morality in the context of Evolutionary game theory.\nSee also: ethics"},"thoughts/morphology":{"title":"Morphology","links":[],"tags":["seed"],"content":"\nThe system for combining units of meaning\n\n\nMorpheme: the smallest unit of meaning in a language\n\nFree morphemes can occur alone\nBound morphemes cannot\n\n\nOrder of acquisition of phonemes (Brown 1973)\n\nMorphemes come in in a consistent order (within a language)\nThe order is predicted based on a variety of factors, including the salience and consistency of form-meaning mapping\nSame factors can predict differences between languages (in languages with more salient affixes and consistent form-meaning mappings children learn the morphology earlier)\n\n\nThe U-shaped curve\n\nRote memorization (child stores the forms they hear) – higher accuracy\nRule acquired and over-applied – lower accuracy\nRule acquired but children also know there are exceptions – higher accuracy\n\n\n"},"thoughts/move-fast-and-break-things":{"title":"Move Fast and Break Things","links":["thoughts/move-fast-and-break-things","thoughts/software-and-politics"],"tags":["sapling"],"content":"\nTo slow down is to end up default dead; the rhythm and pace of how technology is supposed to be built does not allow for consideration of social consequences. (Jasmine Wang)\n\nEngelbart\nMoving fast but not breaking things\nA activity → core business activity (actual value producing activities)\nB activity → reduce product-cycle time, make faster, smarter, higher-quality A activities (increase velocity)\nC activity → reduce improvement-cycle time, make faster, smarter, higher-quality B activities (increase acceleration)\nThis is exactly what I mean when I talk about “bootstrapping.” It is a very American term – the image is of someone able to perform the wonderful, impossible trick of pulling himself up by pulling up on his own bootstraps – but the idea is one that we put into practice every time that we “boot up” a computer. A small bit of code in a permanent read only memory knows how to go out to the disk to get more instructions, that in turn know how do to even more things, such as getting even more instructions. Eventually, this process of using successive steps to lead to ever bigger steps, building on each other, get the whole machine up and running. You start small, and keep leveraging what you know at each stage to solve a bigger and bigger problem.\nVelocity vs Acceleration\nVelocity vs Acceleration based execution. Insightful read on how orgs can be moving fast but not be innovating. How do we have a high acceleration org rather than just a high velocity one?\nThe Coming Software Apocalypse\nSource: The Coming Software Apocalypse by James Somers\n\noccasionally shift the pile of sand so it settles in a more stable configuration\ntake time to understand the actual tech youre working on and the problems you’re trying to solve\ntalk it out with people\n\n“The problem is that software engineers don’t understand the problem they’re trying to solve, and don’t care to,” says Leveson, the MIT software-safety expert. → software and politics"},"thoughts/multi-class-classification":{"title":"Multi-class Classification","links":["thoughts/probabilistic-classifier","thoughts/K-means","thoughts/regularization","thoughts/SVM"],"tags":["seed","CPSC340"],"content":"One vs All\nSuppose we only know how to do probabilistic binary classification. But we have k classes we want to distinguish between. We can\n\nFor each class c, train binary classifier to predict whether example is a c.\n\nThis creates k binary classifiers\n\n\nOn prediction, apply the k binary classifiers to get a “score” for each class c.\nPredict the c with the highest score (argmax)\n\nThis divides the space into convex regions (much like K-means)\nNotation: wyi​​ denotes a classifier where c=yi​\nLoss\nProblem: how can we define a loss that encourages the largest wcT​xi​ to be wyi​T​xi​?\nBasically, for each xi​ we want\n\nwyi​T​xi​&gt;wcT​xi​ for all c that is not the correct yi​\nWe write wyi​T​xi​≥wcT​xi​+1 to avoid the strict inequality\nThis is the constraint we use!\n\nSum\nPenalizes each c that violates the constraint\n∑c=yi​​max{0,1−wyi​T​xi​+wcT​xi​}\nMax\nPenalizes the c that violates the constraint the most\nmaxc=yi​​{max{0,1−wyi​T​xi​+wcT​xi​}}\nAdding L2-regularization turns both into multi-class SVMs. Both are convex upper bounds on the 0-1 loss.\nMulti-class Logistic Regression\nSimilarly, we can smooth out the max with the log-sum-exp trick to get something differentiable and convex:\nf(w)=∑i=1n​[−wyi​T​xI​+log(∑c=1k​exp(wcT​xi​))]+2λ​∑c=1k​∑j=1d​wcj2​\nWe can rewrite the last term as ∥W∥F2​ (the Frobenius norm of the matrix W).\nThis is equivalent to binary logistic loss for k=2\nSee also: multi-class probabilities"},"thoughts/multiple-realization":{"title":"Multiple Realization","links":["posts/agi"],"tags":["seed","pattern"],"content":"Are there multiple ways of realizing intelligence?\nThe bitter lesson: the less human heuristics we insert and the more we improve the learning capacity of the network, the better they seem to perform.\nDo we necessarily need human intelligence for machine intelligence? Are we constraining solution space with human heuristics? (see: agi)\nFlying\nMen have always been interested in flying. Once upon a time, scientists determined to understand how birds fly. First they watched them, hoping to correlate the motion of a bird’s wings with its upward movement. Then they proceeded to experiment and found that when its feathers were plucked, a bird could no longer fly. Having thus determined that feathers were the organ of flight, the scientists then focused their efforts on microscopic and ultramicroscopic investigation of feathers in order to discover the nature of their flight-giving power.\n“You have no reason to suppose that airplanes and birds work the same way — birds have no propellors, airplanes have no feathers”"},"thoughts/network-effect":{"title":"Network effects","links":["thoughts/pace-layers"],"tags":["seed"],"content":"Source\nSelf-reinforcing flywheels.\nNetwork effects create winner-take-all-dynamics where only one or two firms end up dominating an entire industry and can’t be challenged.\nBroadly divided into three categories:\n\nDirect: when the number of users has a direct impact on the value of a product\nIndirect: two-sided (or multi-sided) products where the size of one user group affects how valuable the product is to another user group\nData: products which become better with more users via the data those users generate\n\nThings that also reinforce network effects:\n\nSwitching costs: how difficult or expensive it is for a user to switch from one product to another\nMultihoming costs: how easy or likely it is to use multiple competing networks simultaneously\n\nBoth of these costs are not necessarily monetary – they can also be psychological or time/effort-based\nOnce a platform reaches a certain level of dominance, it becomes a new default and moves down a layer of the stack:\n\n"},"thoughts/neural-networks":{"title":"Neural networks","links":["thoughts/convolutional-neural-networks","thoughts/change-of-basis","thoughts/latent-factor-model","thoughts/multi-class-classification","thoughts/binary-classification","thoughts/linear-regression","thoughts/gradient-descent","thoughts/fundamental-tradeoff","thoughts/intelligence","thoughts/representation","thoughts/fault-tolerance","thoughts/supervised-learning","thoughts/unsupervised-learning"],"tags":["seed","CPSC430"],"content":"See also: convolutional neural networks\nShallow Networks\nMany domains require non-linear transforms of the features (see: change of basis). Usually not obvious which transform to use.\nNeural network models try to learn good transformations. Whereas latent-factor models train the embedding and model separately, neural networks learn both features and the model at the same time.\nLet k be the number of hidden units. Generally, y^​i​=vTh(Wxi​) (or, with bias, y^​i​=∑c=1k​vc​h(wcT​xi​+βc​)+β)\n\nArtificial neural network:\n\nxi​ is measurement of the world\nzi​ is internal representation of world\n\nEach h(zi​) can be viewed as binary feature: do we care about it or not?\nUse sigmoid as a smooth approximation\n\n\nyi​ is output of neuron for classification/regression\n\nParameters: the (k,d) matrix W, and (k) vector v. To turn this into multi-class classification, we modify v into a (k’, k) matrix (where k’ is the number of classes) and convert to probabilities by computing the softmax of the y^​c​ values\nLosses:\n\nBinary Classification: f(W,v)=∑i=1n​log(1+exp(−yi​vTh(Wxi​)))\nRegression: f(W,v)=21​∑i=1n​(vTh(Wxi​)−yi​)2\n\nTraining\nGenerally non-convex as W and v are both variables. As such, finding the global optimum is NP-Hard. We can use Stochastic Gradient Descent (SGD) but this is not guaranteed to reach a global optimum due to non-convexity.\nImplicit Regularization\nOften, increasing k, the number of hidden units, improves test error. This seems at odds with the fundamental tradeoff, doesn’t it?\nHowever, learning theory (trade-off) results analyze global min with worst test error. The actual test error for different global minima will be better than worst case bound. Among the global minima, SGD is somehow converging to “good” ones! Empirically, using SGD is like using L2-Regularization, but the regularization is “implicit”.\nWith small models, “minimize training error” leads to unique (or similar) global mins. With larger models, there is a lot of flexibility in the space of global mins (gap between best/worst).\nWe get results that look like the following:\n\nDeep Learning\nInstead of a single layer of hidden units, we can stack them.\ny^​i​​=vTh(W(m)h(W(m−1)h(…W(1)xi​)))=vT(Il=1m​h(W(l)xi​))​Where I is repeated function composition​\nVanishing Gradient Problem\nThe gradient of the sigmoid function away from the origin is nearly zero. This is worse when you take the sigmoid of a sigmoid of a sigmoid…\nIf these are numerically set to 0 because of how small they are, Stochastic Gradient Descent (SGD) will not make progress\nThis is partially solved by replacing the sigmoid activation with the ReLU activation. Alternatively, can also use skip connections that ‘shortcuts’ between layers\nPhilosophy of Deep Learning\nMost commentators agree that current deep learning methods fall short of implementing general intelligence, and it remains an open question as to whether some modification of current deep learning methods will be able to do so (more of a question of intelligence)\nSelf-learning algorithms like AlphaZero (which learns from self-play) seem to disprove/vindicate the empiricist approach (need real world experience to learn)\n. The counterargument here is that systems like AlphaGo have built in knowledge about the rules of Go and mechanisms to explore possible outcomes one at a time (e.g. Monte Carlo Tree Search for the solution space)\nBrain-like networks\n\nBiological similarities\n\nCNNs have high sensitivity to spots, edges, and bars in specific orientations\nThis echoes the work of hubel and wiesle (1962) which found similar patterns in the feline visual cortex\n\n\nBoth systems have created a system of internal representations that corresponds to important distinctions and structures in the outside world\nNeural networks have decently high fault tolerance (some redundant neurons)\n\nMay help to explain functional persistence of brains in the face of minor damage. In a large network, a loss of a few neurons will not make a huge impact, but the quality of its computations will progressively degrade (this is why network distillation still works)\n\n\n\nDifferences\n\nReal neural networks aren’t fully connected like ANNs\nReal neural networks have horizontal cell-to-cell connections within a given layer which are not present in ANNs\nReal brains don’t use backprop via generalized delta rule\n\nBack prop requires\n\ncomputing partial derivates to minimize error\npropagating deltas through the network back to relevant connections\n\n\nThere is little empirical evidence for this in biological brains\n\n\nReal brains show a progressive reduction in reaction time as one learns\n\nNot seen in ANNs where error decreases but prediction time remains constant\n\n\nSupervised require a ‘global truth’ or teacher. These ‘perfect’ signals are not present in the real world\n\nSo far, unsupervised learning has been vastly inferior to supervised approaches\n\n\n"},"thoughts/neutrality":{"title":"Neutrality","links":["thoughts/Rhizome-Proposal","thoughts/interoperability"],"tags":["seed","pattern"],"content":"Net Neutrality\nThe Neutrality Pyramid: A Policy Framework to Distribute Power Over the Net by Juan Ortiz Freuler\n\nNeutrality: “It is fundamental to our right to freedom of expression that any and all parties who exercise control over any layer of our channels of communication respect the integrity of the message that is being delivered.”\n\nNet neutrality: separating the content layer from the connectivity layer. The rhetoric was that if we kept layers separate, it creates more value for everyone\n\nMore users mean more clients for ISPs\nBetter quality of internet service provided by ISPs is a win for users\n\nTo protect the virtuous circle, we can and should apply the core principles of net neutrality to other layers. (see Rhizome proposal)\n\nWe currently have device and net neutrality (mostly) but we should push for platform and data neutrality.\nLeveraging the pyramid:\n\nSeeing the pyramid: Users should be aware of who the intermediaries are at each layer. If intermediaries do not respect our rights, we should shift to more decent providers.\nObserving behaviours within each layer: we need to promote enforceable rules to ensure that each level of the pyramid will be kept from abusing its gatekeeping powers. This requires open standards for interoperability\nObserving dynamics between layers: we need to promote enforceable rules that ensure intermediaries do not illegitimately discriminate between the actors operating in the other layers of the pyramid\n"},"thoughts/niche-at-scale":{"title":"Niche at scale","links":[],"tags":["seed"],"content":"“Tokyo, a city where a number of unusual stores exist: Stores that only sell vinyl records from the 1970s, or that only sell whisky from the 1980s.\nPut those stores in a Des Moines suburb and they will obviously fail. But in Tokyo, where 20 to 30 million people can reach the city by train, there are likely to be a few thousand people who love 1970s albums or 1980s whiskey.\nThe internet is Tokyo.” (Source)\n“In the future, everyone will be world-famous for 15 minutes.” — Andy Warhol"},"thoughts/noise":{"title":"Noise","links":["thoughts/hash-function"],"tags":["seed"],"content":"Source\nWhat makes RNG “good”?\n\nFair statistical distribution\nLow degree of repetition (more correct: a statistically correct degree of repetition)\nHigh theoretical maximum (best-case) repeat period\nHigh guaranteed minimum (worst-case) repeat period: crucially should be about ~million-billion range\nSeedable with a nice range of seeds: we don’t want seeds that limit us to only odd numbers or only large primes\nFast warm up: some popular RNGs have pretty terrible initial values!\nPlatform independence: behaviour should be consistent across platforms\nDeterministic: the same seed should lead to the same results\nSpeed: we may want to be able to generate lots of numbers, fast!\nParallelism: is it thread safe?\n\nWhy not use rand()?\n\nOnly gives us 15-bits of random numbers (range is [0,32767])\nNot very fast!\nNot very good, statistically speaking\nGlobal state which is bad for multi-threading!\n\nWhat RNG should we use?\nLehmer/Park-Miller\n\nScale by prime S\nModulus by prime M\n\nuint32_t m_state = 1337; // initial seed\nuint32_t S = 0x0000BC8F;\nuint32_t M = 0x7FFFFFF;\nuint32_t LcgParkMiller::Rand() {\n\tm_state = (m_state * S) % M;\n\treturn m_state;\n}\nProblem: can get stuck at 0 if the seed is bad\nMCGs (Mixed Congruential Generator)\n\nScale by prime S\nAdd bias B\nModulus by prime M\n\nuint32_t m_state = 1337; // initial seed\nuint32_t S = 0x0019660D;\nuint32_t M = 0x3C6EF35F;\nuint32_t M = 0x7FFFFFF;\nuint32_t LcgParkMiller::Rand() {\n\tm_state = (m_state * S + B) % M;\n\treturn m_state;\n}\nXor shifting\n\nBit-shift around and xor with yourself a few times\n\nuint32_t m_state = 1337; // initial seed\nuint32_t xorshift1::Rand() {\n\tm_state ^= (m_state &lt;&lt; 13);\n\tm_state ^= (m_state &gt;&gt; 17);\n\tm_state ^= (m_state &lt;&lt; 5);\n\treturn m_state;\n}\nNoise functions\n\nOrder independent RNG!\nInfinite table: put an index in, get a random float or number back out\n\n1-D function: index is a single number\n2-D function: index is a pair of numbers\nN-D function: index is an n-tuple\n\n\nTotally pure: noise = mungeAndMangleBits(position)\nCan actually use hash functions for this\n\ncrc32, Murmur , Squirrel3, and std::hash are all very good and fast\nmd5 and sha1 are good but slow (cryptographically sound)\n\n\n\nn-D noise from 1D noise function\nBasically munge the coordinates together my multiplying by a large prime number with non-boring bits. Be careful to make sure that the primes are magnitudes apart!\nuint32_t Get3DNoise(int x, int y, int z, uint32_t seed) {\n\tconstexpr int PRIME1 = 198491317;\n\tconstexpr int PRIME2 = 6542989;\n\treturn Get1DNoise(x + (PRIME1 * y) + (PRIME2 * z), seed);\n}"},"thoughts/notation":{"title":"Notation","links":["thoughts/design-goals"],"tags":["seed"],"content":"See also: design goals\nCognitive Dimensions of Notations\n\nAbstraction Gradient (Efficiency)\n\nAbstractions make it hard for first-time programmers to understand it\nAbstractions are powerful for professional software developers to make easy to write, read, and maintain software\nThere should be a gradual increase in complexity \nLanguages with a high abstraction floor are called abstraction-hungry\nLanguages with a low abstraction ceiling are called abstraction-hating\n\n\nConsistency\n\nCoherence across the features of a language. It is easier to learn something if there are few exceptions to learn\n\n\nDiffuseness (Learnability)\n\nHow many things there are to learn about a language\nNumber of keywords is a good approximation for diffuseness\n\n\nError-proneness\n\nBloch: make it easy to do it right, hard to do it wrong\nThe more guarantees you want to make about the program at compile time, the more work the programmer needs to do to get something running\n\n\nSecondary Notation\n\nAnything that is only there to help the programmer but does not affect what the code actually does\n\n\n"},"thoughts/nuclear-binding-energy":{"title":"Nuclear Binding Energy","links":["thoughts/Nuclear-Fusion","thoughts/Nuclear-Fission"],"tags":["seed"],"content":"Nucleons are attracted to each other by the strong nuclear force. The nuclear binding energy is the amount of energy needed to disassemble a nucleus into its constituent parts.\nFor nuclear reactions, the total binding energy of the resulting elements must be greater than that of the starting element.\nMass Defect\nThe mass of an atomic nucleus is less than the sum of the individual masses of the free constituent protons and neutrons. The difference in mass can be calculated by the Einstein equation, E=Δmc2 and the energy is equivalent to the nuclear binding energy. In the equation, Δm is the mass defect.\nBinding Energy\nNuclear Fusion uses lighter elements, such as hydrogen and helium, which are in general more fusible; while the heavier elements, such as uranium, thorium and plutonium, are more fissionable.\nThe inverted curve is due to two opposing forces:\n\nthe nuclear force, a manifestation of the strong interaction, which holds protons and neutrons tightly together in the atomic nucleus; and\nthe Coulomb force, which causes positively charged protons in the nucleus to repel each other.\n\nUnder most conditions, the Coulomb force dominates. However, when accelerated to high enough speeds, nuclei can overcome this electrostatic repulsion and be brought close enough such that the attractive nuclear force is greater than the repulsive Coulomb force.\n"},"thoughts/object-classification":{"title":"Object Classification","links":["thoughts/object-detection","thoughts/KNN","thoughts/Naive-Bayes","thoughts/SVM","thoughts/probability","thoughts/decision-tree","thoughts/Ensemble-method","thoughts/convolutional-neural-networks"],"tags":["seed"],"content":"See also: object detection\nData to Model\nRandom Sample Consensus (RANSAC)\n\nrandomly choose minimal subset of data points necessary to fit model\npoints within some distance threshold of model are a consensus set, the size of the consensus set is the model’s support\nrepeat for N samples, model with biggest support is most robust fit\n\nChoosing number of samples k\n\nlet ω be the fraction of inliers\nlet n be the number of points needed to define hypothesis (e.g. n=2 for a line)\nsuppose k samples of n points are chosen. then\n\nthe probability that all n in a sample are correct is ωn\n\n\nthe probability that all k samples fail is (1−ωn)k, thus we choose a k large enough to keep this below a targe failure rate\n\nAdvantages\n\ngeneral method\neasy to implement and calculate failure rate\nDisadvantages\nonly handles a moderate percentage of outliers without cost blowing up\nmany real problems have high rate of outliers (e.g. noise)\n\nHough Transform\n\nFor each token, vote for all models to which the token could belong\nReturn model with most votes\n\ne.g. for each point, vote for all lines that could pass through it; true lines will pass through many points and thus receive many votes\nTurning image space into parameter space. Rearranging y=mx+b into y−mx=b where b and m are the variables instead of y and x.\nWe can alternative transform it using Book’s Convention: xsin(θ)+ycos(θ)+r=0. Then, xsin(θ)+ycos(θ)=ρ\nAdvantages\n\nCan handle high percentage of outliers: each point votes separately\nCan detect multiple instances of a model in a single pass\nDisadvantages:\nComplexity of search time increases exponentially with the number of model parameters\nCan be tricky to pick a good bin size\n\nClassification\nClassifier is a procedure that accepts as input a set of features and outputs a prediction for the class label.\nStandard Bag-of-Words pipeline\n\nDictionary Learning: learn visual words using clustering\nEncode: build Bags-of-words vectors for each image\nClassify: train and test data using BOW (KNN, naive Bayes, SVM)\n\nBayes Rule\nSee: Bayes’ Theorem\nDecision boundary, the location where one class becomes more probable than the other (e.g the point where the probability classes are equal).\nThe Bayes’ risk is the shaded region where one class’s probability is still non-zero beyond its decision boundary.\n\nSee also: probability\nROC Curve\nTrade-off between true positive rate and false positive rate. A random classifier will always have 1:1 true positive and false positive rate\n\nParametric vs Non-parametric\n\nParametric classifiers rely on a model\n\nfast, compact\nflexibility and accuracy depend on model assumptions\n\n\nNon-parametric classifiers are data driven (rely on comparing to training examples directly)\n\nslow\nhighly flexible decision boundaries\n\n\n\nSpatial Pyramid\nHave multiple scales of the input image to compute histograms across. Train a classifier for each scale along with a combined weight to combine each classifier.\nVLAD (Vector of Locally Aggregated Descriptors)\nInstead of incrementing the histogram bin by a single count, we increment it by the residual vector x−c(x) (diff between cluster center and feature vector)\nDimensionality is Kd where K is number of codewords and d is the dimensionality of the local descriptor (128 for SIFT)\nDecision Tree\nSee notes on decision trees\nClassifier Boosting\n\nTrain an ensemble of classifiers sequentially\nBias subsequent classifiers to correctly predict training examples that previous classifiers got wrong\n\nCNNs\nSee notes on CNNs"},"thoughts/object-detection":{"title":"Object Detection","links":["thoughts/object-classification","thoughts/texture"],"tags":["seed"],"content":"See also: object classification\nTemplate Matching\nLinear filtering is also known as template matching. Convolution/correlation can be thought of as comparing a template (the kernel) with each section of the image.\n\nConsider the filter and image section as vectors\nApplying the filter can be interpreted as computing the dot product between the filter and the local image patch\n\nThe correlation is then normalized to between -1 and 1 using cosine similarity, where 1 is the value when the filter and image region are identical. This process is essentially finding the cosine similarity between template and local image neighbourhood\ncosθ=∣a∣∣b∣a⋅b​=(a⋅a)(b⋅b)​a⋅b​=∣a∣a​∣b∣b​\nThen, we can map over the image and create a correlation map. Thresholding this gives us detections.\nGood:\n\nRobust against noise\nRelatively easy to compute\nBad:\nScaling (we can address this using scaled representations like a Gaussian image pyramid)\nRotation\nLighting conditions\nSensitive to viewing direction and pose (in 3D worlds)\n\nGaussian Image Pyramid\nCollection of representations of an image. Typically, each layer of the pyramid is half the width and half the height of the previous layer. In the Gaussian version, each layer is smoothed by a Gaussian then resampled to get the next layer.\nDetails get smoothed out (are completely lost) as we move to higher levels, only large uniform regions of colours in the original image are left.\n\nLaplacian Pyramid\nTo do this, create a Gaussian pyramid and take the difference between one pyramid level and the next after smoothing but before subsampling.\nAt each level, retain the residuals (difference between smoothed image and normal image) instead of the blurred images themselves.\nConstructing the pyramid, we repeat until min resolution reached:\n\nBlur\nCompute Residual\nSubsample\n\nReconstructing, we repeat until original resolution reached:\n\nUpsample\nBlur\nSum with residual\n\nLocal Feature Detection\n\nMoving from global template matching to local template matching (e.g.edges and corners)\n\nAs differentiation is linear and shift invariant, we can implement it as a convolution.\nThe discrete approximation is ∂x∂f​≈ΔxF(X+Δx,y)−F(x,y)​ where Δx is usually 1. This is equivalent to a convolution F is a 1×2 filter with the first element is −1 and the second element is 1. Note that the derivatives go up for the Y direction and the right for the X direction.\nWe usually smooth the image prior to derivative estimation. Increased smoothing\n\neliminates noise edges\nmakes edges smoother and thicker\nremoves fine detail\n\nWeights of a filter for differentiation should sum to 0 as a constant image should have derivative 0.\nEdge Detection\nThe goal here is to identify sudden changes in image brightness as this encodes the vast majority of shape information.\nAn edge is a location with high gradient.\nMainly caused by\n\nDepth discontinuity\nSurface orientation discontinuity\nReflectance discontinuity (e.g. change in material)\nIllumination discontinuity (e.g. shadows)\n\nAs we usually smooth prior to derivative calculation and convolution is associative, we can combine both steps and use derivatives of Gaussian filters.\nD⊗(G⊗I(X,Y))=(D⊗G)⊗I(X,Y)\nSobel (Gradient Magnitude)\nLet I(X,Y) be an image. Then, we let Ix​(X,Y) and Iy​(X,Y) be estimates of the partial derivatives in the x and y directions, respectively. Then, the vector [Ix​,Iy​] or ∇f=[∂x∂f​,∂y∂f​] is the gradient and Ix2​+Iy2​​ is the gradient magnitude.\nThe gradient points in the direction of most rapid increase of intensity. The direction is then θ=arctan∂x∂f​∂y∂f​​. The strength of the edge is then the magnitude ∣∣∇f∣∣=∂x∂f​2+∂y∂f​2​.\nMarr/Hildreth (Laplacian of Gaussian)\nDesign Criteria\n\nlocalization of space (find where the edge is)\nlocalization in frequency (identify high frequency and low frequency edges)\nrotationally invariant (rotation shouldn’t affect edges)\n\nFind the zero-crossings (intercepts) of the Laplacian of the Gaussian. This is ∇2G(x,y)=2πσ4−1​[2−σ2x2+y2​]exp(−2σ2x2+y2​)=0\nAlternatively, we can say that subtracting the delta function from the Gaussian gives you an approximation of the Laplacian\nCanny (Local Extrema of 1st deriv)\nDesign Criteria\n\ngood detection (reduce missed edges, reduced edges where edges don’t exist)\ngood localization (accurate edge detection)\none (single) response to a given edge\n\nFind the local extrema of a first derivative operator.\nSteps\n\n\nApply directional derivatives of Gaussian\n\n\nComputer gradient magnitude and gradient direction\n\n\nPerform non-max suppression\nNon-max suppression allows us to suppress near-by similar detections to obtain one “true” result. In images, we select the maximum point across the width of the edge (following the direction of the gradient).\nIn implementations, the value at a pixel q must be larger than its interpolated values at p (the next pixel in the direction of the gradient) and r (the previous pixel in the direction of the gradient). Interpolate as needed.\n\n\nLinking and thresholding\nTrying to fix broken edge chains by linking separate edge pixels through taking the normal of the gradient and linking it if the nearest interpolated pixel is also an edge pixel. Accept all edges over low threshold that are connect to an edge over high threshold.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAuthorApproachDetectionLocalizationSingle RespLimitationsSobelGradient Magnitude ThresholdGoodPoorPoorThick edgesMarr/HildrethZero-crossings of 2nd DerivativeGoodGoodGoodSmooths CornersCannyLocal extrema of 1st DerivativeBestGoodGood\nBoundary Detection\nHow closely do image edges correspond to boundaries that humans perceive to be salient or significant?\nOne approach is using circular windows of radii r at each pixel (x,y) cut in half by a line that bisects the circle in half. Then, compare visual features on both sides of the cut and if the features are statistically different, then the cut line probably corresponds to a boundary.\nFor statistical significance:\n\nCompute non-parametric distribution (histogram) for left side\nCompute non-parametric distribution (histogram) for right side\nCompare two histograms, on left and right side, using statistical test\n\nExample features include\n\nRaw Intensity\nOrientation Energy\nBrightness Gradient\nColor Gradient\nTexture gradient\n\nFor this implementation, we consider 8 discrete orientations (θ) and 3 scales (r)\nFeatures\nCorners are locally distinct 2D image features that (hopefully) correspond to a distinct position on a 3D object of interest in the scene.\nCannot be an edge as estimation of a location along an edge is close to impossible (the aperture problem)\nAutocorrelation\nCorrelation of the image (distribution of pixel values) with itself. At each pixel, compute its partial derivative w.r.t. either the x or the y axis, Iy​=∂y∂I​,Ix​=∂x∂I​.\nWindows on an edge will have autocorrelation that falls of slowly in the direction of the edge but rapidly orthogonal to the edgge. Windows on a corder will have autocorrelation that falls off rapidly in all directions.\nHarris Corner Detection\nAs a stats reminder, covariance is the direction of the correlation. The closer the covariance is to 1, the closer it is to a perfect positive correlation. -1 implies perfect negative correlation.\nWhen drawing distrubtion, draw normals to edges going from low values (dark) to high values (white).\n\nCompute image gradients over small region\nCompute covariance matrix [∑p∈P​Ix​Ix​∑p∈P​Iy​Ix​​∑p∈P​Ix​Iy​∑p∈P​Iy​Iy​​] (essentially fitting a quadratic to the gradients over the small image patch P)\nComputer eigenvectors and eigenvalues of the covariance matrix.\nUse threshold on eigenvalues to detect corners (&gt;0 is a corner)\n\nWe can visualize the covariance matrix C as an ellipse whose axis lengths are determined by the eigenvalues and orientation determined by R (the rotation matrix). It tells us the dispersion of the gradients nearby.\nAs C is symmetric, we have the covariance matrix as the ellipse equation\nf(x,y)=[x​y​][10​01​][xy​]=const\nWhere the minor axis is λmax−1/2​ and the major axis is λmin−1/2​\nThen, this is what the eigenvalues tell us:\n\nCase 1 (both λ1​ and λ2​ are close to zero): flat region\nCase 2 (λ2​ is much greater than λ1​): horizontal edge\nCase 3 (λ1​ is much greater than λ2​): vertical edge\nCase 4 (λ1​ are both rather large λ2​): corner\n\nTo threshold, we can pick a function\n\nHarris &amp; Stephens: λ1​λ2​−κ(λ1​+λ2​)2 which is equivalent to det(C)−κtrace2(C). κ is usually 0.4 or 0.6.\nKanade &amp; Tomasi: min(λ1​,λ2​)\nNobel: trace(C)+ϵdet(C)​\n\nLinear Algebra Aside/Review\nGiven a square matrix A, a scalar λ is called an eigenvalue of A if there exists a nonzero vector v that satisfies\nAv=λv\nThe vector v is called an eigenvector for A corresponding to the eigenvalue λ. The eigenvalues of A are obtained by solving det(A−λI)=0\nKeypoint Description\nScale Invariant Features (SIFT)\nDavid Lowe\nInvariant to translation, rotation, scale, and other imaging parameters. (Generally works for about ~20% change in viewpoint angle)\nAdvantages:\n\nLocality: features are local (robust to occlusion and clutter)\nDistinctiveness: individual features can be matched to a large database of objects\nQuantity: many features can be generated (even for small objects)\nEfficiency: fast (close to real-time performance)\n\nDescribes both a detector and descriptor\n\nMulti-scale local extrema detection\n\nUse difference of gradient pyramid (3 scales/octave, down-sample by a factor of 2 each octave)\n\n\nKeypoint localization\n\nWe then remove low constrast or poorly localized keypoints. We can determine good corners by using the covariance matrix! (Threshold on magnitude of extremum, ratio of principal curvatures)\n\n\nOrientation assignment\n\nCreate histogram of local gradient directions computed at selected scale multiplied by the gaussian kernel at the center\nAssign canonical orientation at peak of smoothed histogram (mode)\n\n\nKeypoint description (SIFT Descriptor)\n\nhistogram of local gradient directions\n\n(8x(4x4)) = 128 dims\n4x4 = 16 histograms\n8 orientations each\n\n\nNormalized to unit length to reduce the effects of illumination change\nRobust to affine changes (rotation and scaling)\n\n\n\nHistogram of Oriented Gradients (HOG)\n\nuses 8x8 cells and blocks which consist of 2x2 cells\nthen for each cell, create a histogram of ‘unsigned’ gradients\n\nperform soft binning (adding to one bin also adds to neighbour bins)\n\n\nconcatenate then L2 normalize\n15x7x4x36 = 3780\n\n‘Speeded Up’ Robust Features (SURF)\n\n4x4 cell grid of 5x5 cells\neach cell is represented by 4 values\n\nsum of all x derivatives\nsum of all y derivatives\nabs of 1\nabs of 2\n\n\nuse Haar wavelets filters (simple derivative filters where all black on one side and all white on the other, weighted by gaussian)\n4x4x4 = 64 dims\n\nObject Recognition\n\nMatch each keypoint to the database of keypoints\nTo find out probability of correct match, we can compare the ratio of distance between nearest neighbour and 2nd nearest neighbour. A threshold of 0.8 provides great separation.\nidentify clusters of at least 3 features that agree on an object and pose\nLowe uses a generalized Hough transform\ncheck each cluster found by performing detailed geometric fit of affine transformation to the model\naccept/reject interpretation accordingly\n\nApproximate Nearest Neighbour\n\ngenerally, finding nearest neighbour in high-dimensional data is linear in time (even for KD trees)\n\nTransformations\nDegrees of freedom (DOF)\n\ntranslation: 2\nrigid (euclidean): 3\nsimilarity: 4\naffine: 6\nprojective: 8\n"},"thoughts/observer-expectancy-effect":{"title":"Observer-expectancy Effect","links":["thoughts/Atlas-of-AI","thoughts/bias"],"tags":["seed"],"content":"Clever Hans\nStory from Atlas of AI\nClever Hans was a horse that was claimed to have performed arithmetic and other intellectual tasks. After a formal investigation in 1907, psychologist Oskar Pfungst demonstrated that the horse was not actually performing these mental tasks, but was watching the reactions of his trainer.\n“Their intuition was right: the questioner’s posture, breathing, and facial expression would subtly change around the moment Hans reached the right answer, prompting Hans to stop there.”\n“The story of Clever Hans is compelling from many angles: the relationship between desire, illusion, and action, the business of spectacles, how we anthropomorphize the nonhuman, how biases emerge, and the politics of intelligence.”\nOperant Conditioning\nLearning process where behaviors are modified through the association of stimuli with reinforcement or punishment. In it, operants—behaviors that affect one’s environment—are conditioned to occur or not occur depending on the environmental consequences of the behavior."},"thoughts/ontology":{"title":"Ontology","links":["thoughts/metaphysics","thoughts/urban-planning","thoughts/Heidegger","thoughts/Primacy-of-Consciousness","thoughts/prefigurative-politics"],"tags":["seed"],"content":"\nWays of being\n\nThe nature of being and structures of objects. Ontology seeks the classification and explanation of things. A branch of metaphysics.\nTechnology\nTechnology as it relates to ontology. Artifacts shape and create ways of being. Thus, “design is ontological”\nCivic design + urban planning are also importantly ontological as they are designing spaces of dwelling + being (see: Heidegger)\nThe train changed the way we exist — it expands the vista of existence. We are betting on new means of technology for new ways of being, ones attuned to interdependence, mutuality, etc. Emphasizing the ontology of interdependence over the ontology of separation.\nMy understanding of Heidegger on technology is that it is about a reorganization of the world, both spatially and temporally. It is this same reorganization or framing that “orders” all things we know (e.g. point 2) — see modern theories of God and consciousness referring to quantum microtubules to try to explain them. Technology then, is a way of ‘challenging’ existing means of knowledge in a way that reframes it.\nHeidegger sees technology as potentially dangerous in that it instrumentalizes the human and the subjective and “displaces” things from their unmediated existence.\n\n“how do we reconcile technology with conviviality + care for the web of life?”\n\nSee also: prefigurative politics"},"thoughts/optical-flow":{"title":"Optical Flow","links":["thoughts/aperture-problem"],"tags":["seed"],"content":"How do we determine how objects (and/or the camera itself) move in the 3D world? Difficulty comes as motion is geometric whereas optical flow is radiometric (about an origin)\nSee also: aperture problem\nConstraint Equation\nLet image intensity be denoted by I(x,y,t). Then, applying chain rule, we obtain dtdI(x,y,t)​=Ix​dtdx​+Iy​dtdy​+It​.\nLet u=dtdx​ and v=dtdy​. Then [u,v] is the 2D velocity space.\nIf we set dtdI(x,y,t)​=0, then we get the optical flow constraint equation: Ix​u+Iy​v+It​=0.\nWe assume constant brightness for this, meaning I(x(t),y(t),t)=C.\nWe measure each of the following:\n\nSpatial Derivative: Ix​=∂x∂I​, Iy​=∂y∂I​\n\nForward difference\nSobel filter\nScharr filter\n\n\nOptical Flow: u=dtdx​, v=dtdy​\n\nWe need to solve for this! (the unknown in the optical flow problem)\n\n\nTemporal Derivative: It​=∂t∂I​\n\nFrame difference\n\n\n\nLucas-Kanade\nA dense method to compute motion [u,v] at every location in an image.\nWhere can you see movement that can be effectively computed? A corner!\nSolve for v in v=(ATA)−1ATb where v is the 1-by-2 column matrix of u and v. A is the n-by-2 column matrix of Ix​(qi​), Iy​(qi​) partial derivatives evaluated at point qi​ (A is actually the same matrix C used in Harris corner detection). b is the 1-by-n matrix consisting of the negative of the temporal partial derivative for each point.\nA=​Ix​(q1​)Ix​(q2​)⋮Ix​(qn​)​Iy​(q1​)Iy​(q2​)⋮Iy​(qn​)​​v=[Vx​Vy​​]b=​−It​(q1​)−It​(q2​)⋮−It​(qn​)​​\nLucas-Kanade Method\nAssumptions\n\nMotion is slow enough that partial derivatives Ix​, Iy​, and It​ are well-defined\nThe optical flow constraint equation holds (dtdI(x,y,t)​=0)\nWindow size is chosen so that motion [u,v] is constant in the window\nWindow size is chosen so that the rank of ATA is 2 for the window (required inverse exists)\n\nStereo\nComputing depth from multiple images. Formulated as a correspondence problem: dtermine match between location of a scene point in one image and its location in another.\nDisparity: d=x−x′=Zbf​ where b is baseline, x is distance from O to epipolar line, and x′ is distance from O′ to epipolar line. Z is distance from b to target X.\nSimple stereo algorithm\n\nRectify images (make epipolar lines horizontal)\n\nRectified images have these properties:\n\nImage planes of cameras are parallel\nFocal points are at same height\nFocal lengths are the same\nEpiolar lines fall along the horizontal scan lines\n\n\n\n\nFor each pixel\n\nFind epipolar line\nScan line for best match\nCompute depth from disparity\n\n\n\nNaive approach, pixel-based often lacks content. What we can try is min SSD-error of a window-based approach.\nAnother approach is to match the edges (the zero-crossings) at different scales.\nNote: Sum squared differences (SSD) is the same as Normalized Cross Correlation (NCC)"},"thoughts/optionality":{"title":"Optionality","links":["thoughts/exploit-explore","thoughts/prestige"],"tags":["seed"],"content":"\n“I can never be all the people I want and live all the lives I want. I can never train myself in all the skills I want. And why do I want? I want to live and feel all the shades, tones and variations of mental and physical experience possible in my life.” — Sylvia Plath\n\nOptionality: the nature of accepting the possibilities of the unknown, to give every fractal self a chance to live. I’d much rather know, but then again, not-knowing keeps all the possibilities open. It keeps all the worlds alive. Source\n\n“This individual has merely acquired stamps of approval and has acquired safety net upon safety net. These safety nets don’t end up enabling big risk-taking—individuals just become habitual acquirers of safety nets. The comfort of a high-paying job at a prestigious firm surrounded by smart people is simply too much to give up. When that happens, the dreams that those options were meant to enable slowly recede into the background. For a few, those destinations are in fact their dreams come true—but for every one of those, there are ten entrepreneurs, artists, and restaurateurs that get trapped in those institutions.” — Mihir Desai on Optionality\n\nAs humans, we tend to love ‘exploring’ our possibilities more than ‘exploiting’ and diving deep/committing to what we already have — we lean heavily toward explore in the exploit explore tradeoff curve.\nThis is the optionality fallacy.\nIn a life like the one we live in where cause and consequence is extremely hard to distinguish due to the huge number of confounding variables, trial and error reigns supreme. Though, this doesn’t mean trying everything really quickly and abandoning things if results don’t manifest (ahem, talented kid burnout syndrome)\n\nTinkering and experimenting is a more efficient investment of your time than following a set path of learning which assumes an intrinsic value in specific skills and ignores the non-linear way life works.\n\n“Accumulate optionality through differentiation, not conformity” recommends Torenberg\nMaybe why humans constantly chase prestige: to keep doors open as a means of cross-disciplinary recognition.\nParadox of Choice\nIndividuals are more likely to be stressed from a larger number of options as considering more choices requires more mental effort.\nRequiem for a Dream\nNew Yorker on Aaron Swartz\n“I’ve hired a lot of very talented programmers, and one of the things I discovered was that the people who didn’t graduate from college couldn’t finish projects,” his father says. “Because when you go to college, there’s all sorts of stupid stuff you have to do in order to get through.”\n\nIt is a vertiginous thing to have so much freedom—to be always self-skeptical, always testing the reasons for your beliefs, always prepared to abandon them for something better. If you can do anything you want, then every day becomes an existential problem—an empty space of possibility that has no ceiling but also no walls and no floor.\n\nMaking the Decision Right\nSarah on Substack\nWhat I’ve come to learn over the past year is that there are two types of people: people who make the right decision and people who make the decision right.\nOptimizing for optionality seems like the rational thing to do, until I really looked at my life. I had all these “options,” but I hadn’t done any of them.\nI choose the next thing and make it the best."},"thoughts/organizing-system":{"title":"Organizing Systems","links":["thoughts/intentional-arrangement","thoughts/information-scales","thoughts/context","thoughts/cozy-software","thoughts/research-debt","thoughts/design-requirements","thoughts/collections","thoughts/human-computer-interaction","thoughts/quantization","thoughts/information","thoughts/interaction-design","thoughts/search"],"tags":["sapling"],"content":"Organizing System: an intentionally arranged collection of resources and the interactions they support. It has authoritative description and standard classification.\nWe organize\n\nphysical things (objects/artefacts)\ninformation about physical things\ndigital things\ninformation about digital things (metadata)\n\nGood organizing systems are\n\nSkimmable at different levels. Is there a 5s version? A 60s version? (see: information scales)\nTransformable. Can the user transform the data into different representations without having to explicitly define these ahead of time?\nContext-sensitive. Not one size fits all, can we create unique content for every reader/reading depending on their prior understanding and current needs? (see: personal software)\nInterrogable. Can we get clarifications and answers from the text without having the author having to anticipate those questions?\n\nThe document as an ‘information container’, but also a ‘thing’ which documents a thing. Data can be seen as ‘elementary observations about properties of objects, events, and their environments’\nA set of resources is then transformed by an organizing system when the resources are described or arranged to enable interactions with them.\nTradeoffs\n\nThe effectiveness of a system for accessing information is a direct function of the intelligence put into organizing it (Svenonius, 2000)\n\nThere are inherent tradeoffs in an organizing system, what is the goal? What are we optimizing for?\nSimilar to research debt where there is no effort put into the initial organizational process so the mental burden of retrieving information is much higher.\nGoals\n\nStoring\nRetrieving\nMinimizing effort to find (increased scale means increased difficulty of finding things)\n\nNot all users have the same goals! How do we address this?\nThree Tiers of Organizing Systems\n\nUser interface or presentation components where users or other applications interact with the data\nBusiness logic or functions that use the data\nStorage of data itself\n\n\nDesign Heuristics\nSensemaking: organizing to derive meaning from experience by fitting new events of observations into what they already know\nQuestions to ask\n\nWhat? What is organized, what type of resource or information object?\nWhy? What are the goals of the system?\nHow Much? How many different organization schemes are used?\nWhen? When is the organization of the resource imposed? At the creation of the resource?\nHow, and by whom? Who is doing the organization? Professional indexers? Algorithms?\nWhere? Can be abstract ideas like the cloud or physical like infrastructures and build environments.\n\nSimilar to the questions in the HCI design requirements. When designing an organizing system, it is important to consider domain, scope, and scale.\nWhat\n\nthe scope and scale of the collection\nthe number and nature of users\nthe time span or lifetime over which the organizing system will operate\nthe physical or technological environment in which the organizing system is situated\nthe relationship of the organizing system to other ones that overlap with it in domain or scope\n\nGeneral organizing principles\n\nWe organize collections of resources using the properties that are easiest to perceive, or whose values vary the most among the items in the collection\nWe group together resources that we often use together\nWe put rare or unique resources where we can protect them\n\nExamples of these are 1. alphabetical ordering or 2. chronological ordering\nThis scoping process is similar to the double-diamond design in HCI.\nThe Filing Cabinet\nSource\nThe shift from bound volumes (sets of books) to filing systems was extremely significant in the history of classification.\nThe currently accepted version of the filing cabinet is attributed to the Library Bureau, the Boston-based company founded in 1876 by Melvil Dewey (the guy who invented the Dewey decimal system)\nStill reinforced gendered division of labour though — “female file clerks were expected to handle papers, but not to understand their contents; in contrast, it was male managers and executives who read the files, performing jobs that purportedly required thought”\nMostly made popular by efficiency, exploitation of gendered labour, anxiety over information loss, and granular certainty (wanting to break more and more of life into discrete, quantized, and labelled parts)\nInformation then, was seen as something that could be standardized atomized, and stripped of context.\nNow though, the filing cabinet is “no longer an exemplar of productivity and speed, the file cabinet now embodies the facility of bureaucracies to produce paper, to delay, to leave us waiting” — it can create an alternative paper-based reality to which officials reflexively defer.\nCuriously enough, piles of paper on a desktop can be both symbols of “overwhelmed white collar worker confronted with information processing long coded as clerical” but also “exemplary information management practice”. In fact, a well-kept pile of papers can be more efficient than a linear index (very similar to the ethic of grab what you need and rely on internal organization than some explicit organizational system). Does this apply to the Desktop Metaphor?\nIs Google search the 21st century filing cabinet?"},"thoughts/originality":{"title":"Originality","links":[],"tags":["sapling"],"content":"Is anything ever new? Is everything derivative\nLavoisier once said: “Rien ne se perd, rien ne se crée, tout se transforme” — meaning “Nothing is lost, nothing is created, everything is transformed.” (when applied to fundamental physics)\nIdeas are mostly recombination, does this imply that everything comes from one ‘original’ idea?"},"thoughts/outlier-detection":{"title":"Outlier detection","links":["thoughts/supervised-learning"],"tags":["seed","CPSC340"],"content":"Find observations that are unusually different from the others (aka anomaly detection).\nWhy? We may want to remove outliers, or be interested in the outliers themselves (security)\nGenerally does not work. It can be hard to decide when to report an outlier. There are always new ways to make outliers!\n5 Types of outlier detection\n\nModel-based methods\n\nSee if z-score is past a certain threshold\nUnfortunately, z-score assumes uni-modal data\n\n\nGraphical approaches\n\nLook at a plot, human decides if data is an outlier\nUnfortunately only in max 2-3 dimensions\n\n\nCluster-based methods\n\nCluster the data\nFind points that do not belong to clusters\n\n\nDistance-based methods\n\nHow many points lie in a radius ϵ?\nGlobal outliers\n\nFor each point, compute the average distance to its KNN\nOutliers are points that are far from their KNNs\n\n\nLocal outliers\n\nOutlierness ratio of example i is the average distance of i to its KNN over the average distance of neighbours of i to their KNNs\n\n\n\n\nSupervised-learning methods\n\nUse supervised learning: yi​=1 if xi​ is an outlier, yi​−0 if xi​ is a regular point\nNeeds supervision: we need to know what outliers look like\n\n\n\nLocal vs global outliers\nIt’s hard to precisely define “outliers”\n\n\nIn the first case it was a “global” outlier.\nIn this second case it’s a “local” outlier:\n\nWithin normal data range, but far from other points.\n\n\n"},"thoughts/pace-layers":{"title":"Pace Layers","links":["thoughts/Mangrove-Theory-of-the-Internet","thoughts/Bentoism","thoughts/trust","thoughts/blockchain","thoughts/Extended-Mind-Hypothesis"],"tags":["fruit","pattern"],"content":"The order of civilization. The whole combines learning with continuity.\nCoined by Stewart Brand in 1999.\nSee: Mangrove Theory of the Internet, Cross-time-scale planning (Bentoism)\nYou can’t have a healthy forest of all old-growth and towering canopies. You can’t have a healthy forest of only weeds, bushes, and saplings.\nAll complex systems have different ‘paces’ at which they operate. Some near the top are faster changing, learning with the trends and the movements of the day to day, others are slow and methodical, remembering and holding power.\nThe six Pace Layer levels in descending order from the highest &amp; fastest to the lowest &amp; slowest are Fashion, Commerce, Infrastructure, Governance, Culture, Nature.\n\nWorking down from the fast and attention-getting to the slow and powerful… Note that as people get older, their interests tend to migrate to the slower parts of the continuum. Culture is invisible to adolescents but a matter of great concern to elders.  Adolescents are obsessed with fashion while elders are bored by it.\n\nPhysics (and trust) as a new pace layer\nProposing a layer even below nature: physics. This are the axioms of the universe that seem unchanging (at least in the real world) but are actually configurable in the digital. (see: Lattice)\nTrust is enabled through consistent accessibility\n\nYou can’t trust if you don’t know if something will be available\nThus blockchain is a means of enabling trust, re: trusting objects and Extended Mind Hypothesis\nphysics has infinite availability\nlegitimacy is a pattern of higher order acceptance (vitalik)\nblockchains as digital physics (the bottom layer of trust)\n\nInternet Pace Layering\nGordon Brander on Substack\n\nThe pace layering is partial—evolved layering is relaxed layering—but we can definitely see chronological shearing reflected by the cycle times of different layers of the stack. Hardware and programming languages tend to evolve on the decades timescale. Operating systems tend to release every few years. Apps, every month. Content is instant.\n"},"thoughts/page-layout":{"title":"Page Layout","links":[],"tags":["seed"],"content":"Page layout is the art of manipulating a user’s attention on a page to convey meaning, sequence, and point of interaction.\nVisual\nMechanisms\n\nUpper-left-corner preference\nWhitespace\nContrasting fonts\n\nthe bigger and bolder, the more important the info\n\n\nContrasting foreground and background colours\n\neg. putting white text on a black background\n\n\nPositioning, alignment, and indenting\n\nindented text is subordinate to whatever’s above it\n\n\nGraphics like lines, boxes, coloured bars\n\nthings in a box or group go together\n\n\nFlow\n\nremember tendency to read top-to-bottom and left-to-right (at least, in the west)\nfocal point: spots your eyes can’t resist going to\n\ntend to follow them from strongest to weakest\nbetter pages only have a few — having too many dilutes the importance\n\n\nmeaning and context play a big part in visual flow → perceived meaning of page content will change where the user chooses to look\n\n\n\nPlacement\n\nproximity: put things close together, and viewers will associate them with one another\nsimilarity: if two things are the same shape, size, color, or orientation, then viewers will also associate them with each other\ncontinuity: our eyes want to see continuous lines and curves formed by the alignment of smaller elements\nclosure: want to see simple closed forms, like rectangles and blobs of whitespace, that aren’t explicitly drawn for us\n"},"thoughts/pain":{"title":"Pain","links":["thoughts/OODA","thoughts/taste"],"tags":["seed"],"content":"Belief that pain is the unit of effort\nSource: alkjash on LessWrong\n\nWith this belief, the injunction “actually try” means “put yourself in as much pain as you can handle.” Similarly, “she’s trying her best” translates to “she’s really hurting right now.”\n\nPeople with this belief optimize for the appearance of suffering. Wait until you meet someone for whom telling them about opportunities actively hurts them, because you’ve just created another knife they feel pressured to cut themselves with.\nAntidotes\n\nIf it hurts, you’re probably doing it wrong. That’s just bad form.\n\nSo much of this applies to the physical (e.g. working out) why don’t we apply this to our emotional and mental selves?\n\n\nYou’re not trying your best if you’re not happy.\n\nMotivation in Hard Times\nJohn in vlogbrothers\n“[worked fuelled by resentment and pain] may burn bright, but it also burns dirty”\nOxcart model of pain\nPain is caused by an unwanted difference in the direction in your life from what you want it to be.\nThus, if your life constantly wiggles and deviates between being aligned and not aligned with what you want it to be, it will cause you a lot of pain.\nIt’s like trying to move a cart across a bumpy road. No matter how you try pushing it from the back, it will always snag or get bumped off course and it is up to the pusher in the back to try to course correct the cart to be back on track. This requires constant attention and effort to ensure the direction is right.\nHowever, if you are instead pulling the cart from the front, you find that the cart may move around more moment-to-moment, but guiding the cart to the final destination requires much less active effort.\nThe difference in these two cases is that the goal is at two different levels. One tries to course correct constantly, ensuring that both the position and direction of the cart is right, whereas the other only cares about the direction.\nThe implication of this is that the higher level of abstraction you think about your own goals at, the less day-to-day interruptions and disruptions will actually cause you pain or suffering.\nWayfinding\nThe bigger question then is: how does one figure out the right direction to head in is? What are the right goals at each level of abstraction?\nYou can only start to effectively pathfind when you look up from pushing the cart and start observe what others around you are doing and what people have done in the past. In part, this is why being informed about history is important; avoid what has stopped others’ carts in their tracks.\nOf course, everyone has a slightly different life path, but the general direction can still serve to be useful. Enough iterations of the OODA loop will mean that you develop your own taste about what directions feel right to you."},"thoughts/paperclip-optimizer":{"title":"Paperclip optimizers","links":["posts/agi"],"tags":["seed"],"content":"On LessWrong\nThe paperclip maximizer is the canonical thought experiment showing how an artificial general intelligence, even one designed competently and without malice, could ultimately destroy humanity. The thought experiment shows that AIs with apparently innocuous values could pose an existential threat due to over-optimization."},"thoughts/paratelic-action":{"title":"paratelic action","links":["thoughts/play","thoughts/The-Grasshopper,-Games,-Life-and-Utopia"],"tags":["seed"],"content":"Paratelic Actions are more playful in nature and involve enjoying the process in the moment. This is more akin to reading for enjoyment and for the pleasure of learning.\nSee also: The Grasshopper, Games, Life and Utopia"},"thoughts/pattern-matching":{"title":"Pattern Matching","links":["thoughts/terminology","thoughts/ontology","thoughts/seeing","thoughts/conceptual-model"],"tags":["seed"],"content":"Science as Seeing in Aeon\nIf prior patterns are essential for making sense of things, how can we avoid falling into well-worn channels of perception? And most importantly, how can we learn to see in genuinely new ways?\nTo say that we construct idealised categories is not to say that patterns in the world don’t already exist, but that we must learn how to see them in the world around us.\nIf the brain is a taxonomising (see: terminological anchoring and ontology) engine, anxious to map the things and people we experience into familiar categories, then true learning must always be disorienting. Learning shifts the internal constellation of the firings of our nerves, the star by which we set our course, the spark of thought itself.\nSeeing is a multipartite process, requiring a comparison between noisy signals and idealised models."},"thoughts/peer-to-peer":{"title":"Peer to peer","links":["thoughts/decentralization","thoughts/Network-Theory","thoughts/plurality","thoughts/A-Certain-Tendency-Of-The-Database-Community","thoughts/BitTorrent","thoughts/blockchain","thoughts/Rhizome-Proposal","thoughts/inevitability-of-centralization","thoughts/Prolly-Trees"],"tags":["seed","CPSC317"],"content":"In contrast to client-server models, peer-to-peer systems are decentralized networks with no privileged nodes. Each node performs the same roles as any other.\nA philosophically pluralistic way of computing that recognizes there isn’t only one correct primary site (see: A Certain Tendency Of The Database Community).\nExamples include BitTorrent, blockchains, Rhizome\nDatabases\nSource\n\nLimitations of Operation logs\n\n”… scenarios where somebody is following a lot of users with a lot of history in SSB and needing to wait for all the logs to be processed, or when a new database replica is added and needs to fetch all the existing state (whether it’s a regular DB or a blockchain full node).”\n“One way to get around needing to ‘catch up’ with a writer is to get ‘snapshots’ of the current state of the data from trusted peers”\n\nHowever, the naive approach does not work in peer-to-peer situations as we can’t verify the legitimacy of the whole snapshot\nCan we utilize ZK proofs here?\n\n\nOverall, unsolved. All of this together leads to either a centralization of power in long-lived nodes that do the replication for you due to preferential attachment\n\n\nActual databases use indices\n\n“For example, instead of an application getting a list of every single post in a database and filtering based on the ‘tags’, they can say Get me the first 32 posts with the tag #cats and the database engine will figure out how to do that for them.”\nThis can heavily optimize reads for client applications\n\n\nMauve suggests that we can actually solve these using Prolly Trees\n\nValues\nSource\n\nWe should improve and preserve the Web. The Web is a genuine social accomplishment and we should look after it. Don’t let lesser platforms win out.\nDevops is oppressive! Many people can’t publish websites or apps because they can’t run servers. Publishing should be accessible to all.\n“View source” is critical to an open Web. The more code that users can read, the more code they can review and learn from.\n“Modify source” is the p2p Web’s great power. A Web that can be made and remade by its people can better serve their needs and produce a more diverse &amp; exciting world. The Web should be a truly “live” society.\nMinimize change, maximize impact. The p2p Web should still be the Web. Make it better, don’t remake it.\nDon’t forget resilience. A web based on protocols, not platforms, is a safe web. Don’t put data in silos but have various platforms use the same protocols to interact.\n"},"thoughts/people-as-lighthouses":{"title":"People as lighthouses","links":["thoughts/friendship"],"tags":["sapling"],"content":"written in a note once\nSimilar to A love letter to virtual community care by Theresa Gao\nRelated: friendship\nCommunity Care\nLighthouses are beacons for ships at night, identifying the shoreline when everyone else in the town is sleeping.\nWhat happens when the lighthouse stops working? Do the ships crash against the shoreline, oblivious to the jagged rocks beneath the waves?\nNo, the community comes out at night to light their lamps and bonfires. What was once a singular beacon of light is now a coast awash in light! Ideally, this is what community care — supporting the beacons of our communities until they are well again.\nA goal of life is harmony with others, to see people and understand them, make people feel heard."},"thoughts/petname":{"title":"Petname","links":["thoughts/Zooko's-Triangle","thoughts/Arrow's-Impossibility-Theorem"],"tags":["seed"],"content":"\nA way of mapping human readable names to cryptographically secure names\n\nPetnames are potentially a method of achieving all 3 properties of Zooko’s Triangle by having “names with each of the three pairs of properties, and [building] a naming system involving several of these kinds of names, in order to make use of all three properties.”\nWe are already familiar with this concept actually: the contact list on your phone! Each contact in your phone doesn’t actually represent that person, but rather your relationship with that person. John (neighbour) could be Dad in someone else’s phone, even though they refer to the same physical person.\nSupposes that we have three names:\n\nKeys (global and unique): no one has the same key as you\nPetnames (unique and memorable): specific to a relationship between two entities\nNicknames (global and memorable): self-proclaimed\n\nThen, keys and petnames are interchangeable with each other. Nicknames are suggestions by users for petnames that others can set for them.\nMaybe another version of Arrow’s Impossibility Theorem?"},"thoughts/phenomenology":{"title":"Phenomenology","links":["thoughts/Buddhism","thoughts/emptiness"],"tags":["seed"],"content":"In phenomenology, the source of all meaning and value is through the subjective lived experience of conscious beings.\nThis is the argument against independent origination or svabhāva, the very thing Buddhist texts claim that everything we perceive is empty of."},"thoughts/philosophical-realism":{"title":"Philosophical Realism","links":["thoughts/qualia","thoughts/desktop-metaphor","thoughts/fiction","thoughts/Hard-problem-of-consciousness"],"tags":["seed","PHIL451A"],"content":"\nRadical Realism: qualia are real and cannot be explained by science without radical changes in scientific theory\nConservative Realism: qualia are real and can be explained using current understandings of science or extensions of it\nFrankish’ Illusionism: qualia do not actually exist but seem to exist\n\nWe have limited introspective access to the contents of our mental but not the neural medium of those contents\nMental content misrepresent non-phenomenal, physical properties as phenomenal (qualia) — Frankish refers to these as “quasi-phenomenal”\nThis is like the Desktop Metaphor, a fiction created for the benefit of the user. In reality, there are no actual files, folders, etc. in its hardware representation. Dennet states that we use phenomenal properties as a sort of interface for the underlying reality\n\n\nBlackmore’s Delusionism: extension of illusionism. Looking into consciousness reveals only what it’s like when we look\n\nRelated: Hard problem of consciousness"},"thoughts/philosophy-of-mind":{"title":"Philosophy of Mind","links":["tags/PHIL451A","thoughts/consciousness","thoughts/the-Self","thoughts/awareness"],"tags":["sapling","PHIL451A"],"content":"Notes for Philosophy 451A - Philosophy of the Mind with Professor Evan Thompson (see all notes).\nGuiding Questions\n\nWhat is consciousness?\nWhat is the self?\nHow are consciousness and the self related to the brain, the whole body, and the world?\n\nSubjects of discussion\n\nConsciousness\nAwareness\n"},"thoughts/philosophy-of-science":{"title":"Philosophy of science","links":["thoughts/Microworld"],"tags":["seed"],"content":"Karl Popper\nThe aim of scientific inquiry is to find the truth. Yet, some of the greatest scientific theories in the history of science are actually false (e.g. early models of the atom, Newtonian physics).\nCloseness to the truth is a function of two factors\n\nTruth\nContent\n\nThus, the more truths a theory entails, the closer it is to the truth.\n\nEven two true theories can have differing degrees of verisimilitude, depending on how much true information they deliver. For example, the claim “it will be raining on Thursday next week,” if true, seems closer to the truth than the true yet logically weaker claim “it will either be raining next Thursday or it will be sunny”.\n\nSimilarly, the concept of a lie-to-children: creating a simplified world model to explain a system because the real one is too complex to grasp\nVerificationist View\nRejecting that the idea of certain facts are objective\nFor example, a verificationist about height would say that how tall you are depends on what evidence there is about how tall you are. The only height you can have is a height that in principle, discoverable or verifiable that you have\nI its true from one’s perspective, its their truth → “If all my evidence says that there is a tall mountain there, then in my personal picture of the world there is a tall mountain there. That’s all it can mean, for me, to say that there’s a tall mountain there. The mountain really is there, for me, so long as it appears real, and fits my conception of a tall mountain.”"},"thoughts/phonetics":{"title":"Phonetics","links":[],"tags":["seed"],"content":"\nHow to produce/pronounce sounds or signs, how we hear sounds\n\nSounds\n\nPhones: different sounds a language uses\nPhonemes: meaningfully different sounds in a given language\nAllophones: phones that do not differentiate meaning\nPhonotactic knowledge: knowledge of constraints on the sequencing of sounds\nPhonological idioms: words the child produces in a very adultlike way, while still incorrectly producing other words that use the very same sounds\n\nVariations in phonetics\n\nVoicing: whether vocal folds vibrate (e.g. /s/ is voiceless, /z/ is voiced)\nPlace of articulation: where the vocal tract is closed\nManner of articulation: how the vocal tract is closed\n\nStops: completely stop the airflow\nFricative: not completely stopped\n\n\nArticulatory phonetics: describing speech sounds in terms of how they are produced\nPhonetic features: axes of different features for sounds (e.g. voicing, place of articulation, etc.)\n"},"thoughts/phonology":{"title":"Phonology","links":[],"tags":["seed"],"content":"\nWhich sounds are used in language, how sounds are combined to form words\n\nPhonological processes\n\nSystematic ways in which to alter the sounds of the target language so that they fit within the repertoire of sounds they can produce\nCanonical form: preexisting whole-word sound pattern (e.g. consonant + vowel + /j/ + vowel + consonant)\nArticulatory complexity: types of sounds (or sound sequences) that lead individuals with articulatory deficits to make errors\n\nListening\n\nFunctional load: importance of certain features in making distinctions\nPhonological awareness: awareness of and ability to work with sounds in spoken language\nPhonological memory: the ability to remember a sequence of unfamiliar sounds\nSpeech segmentation problem: how do children find word boundaries in a stream of speech\n"},"thoughts/play":{"title":"Play","links":["posts/play","thoughts/Design-Justice","thoughts/research-institutions","thoughts/Mindstorms","thoughts/constructionist","thoughts/The-Grasshopper,-Games,-Life-and-Utopia","thoughts/boundary-object","thoughts/emergent-behaviour","thoughts/Panpsychism","thoughts/Materialism","thoughts/Gall's-law","thoughts/Jestermaxxing"],"tags":["sapling"],"content":"Full post on play: Play to Win: A Post-Work Society\n\nPlay: the intentional activity of doing the thing you would want to do.\n\nSource: Joyful Subversion in Kernel\n“Play allows us to create and share ownership of spaces in ways which competition cannot. Have as much fun as possible along the way. Turn life into a canvas, rather than a graph with checkpoints. Welcome everyone.”\nIn a world were we seem to be locked behind these little rectangles, how can we escape? Play is adoption of strong rules and beliefs, loosely held.\nCreating spaces not products\n“Creating a space for change does not necessarily mean you’re doing it yourself; you’re just making it possible for others.” (a lot of similar ideas in Design Justice and a new DARPA)\nShifting the nature of work/education\nCan we shift education system away from just assessing students to letting them explore the magical worlds themselves? A more Mindstorms-esque constructionist view on education.\nMagic Circles\nSource: Magic circles by Gordon Brandler\nA “magic circle” is the space in which a game takes place. When we step into the magic circle, the we suspend the rules of ordinary life, and allow the rules of the game to mediate our interactions.\nWe often mark the boundaries of a magic circle through ceremonies:\n\nPlaying the THX deep note before a movie\nSinging the national anthem before a game\nRinging a gong before yoga practice\nWalking down the aisle at a wedding\n\nA lot of concepts similar to The Grasshopper, Games, Life and Utopia, the lusory attitude as a boundary\nAnimals and Play\nGraeber on What’s the point if we can’t have fun?\n\nWhy does the existence of action carried out for the sheer pleasure of acting, the exertion of powers for the sheer pleasure of exerting them, strike us as mysterious? What does it tell us about ourselves that we instinctively assume that it is?\n\n\nAnimal cooperation often has nothing to do with survival or reproduction, but is a form of pleasure in itself\n“Man plays only when he is in the full sense of the word a man” (Friedrich Schiller, 1795)\nBut what would happen if we agreed to treat play not as some peculiar anomaly, but as our starting point, a principle already present not just in lobsters and all living creatures, but also on every level of emergent behaviour of self-organizing systems?\nFree will of electrons (a panpsychist-flavoured approach to play)?\n\n“Is it meaningful to say an electron “chooses” to jump the way it does? Obviously, there’s no way to prove it. The only evidence we could have (that we can’t predict what it’s going to do), we do have. But it’s hardly decisive.”\n“If an electron is acting freely—if it, as Richard Feynman is supposed to have said, “does anything it likes”—it can only be acting freely as an end in itself. Which would mean that at the very foundations of physical reality, we encounter freedom for its own sake—which also means we encounter the most rudimentary form of play.”\nInteresting implications for materialism — consciousness does not arise as it is already present? Potential application of Gall’s law\n\n\nPhilosophy as a form of play :)) see also Jestermaxxing\n\nAgainst Irony\n\nThe playful person is neither dogmatist or ironist, but, as Lugones puts it, an easy traveller between, and an explorer of, different normative worlds.\n\nPlay involves lightness with rules — the ability to lightly step away from but also the ability to lightly adopt.\nTo be serious about a game is to play it under the idea that its goals are really and genuinely important — as an Olympic athlete does.\nAn ironist — a spoilsport — by openly refusing that shared commitment, destroys the communal development of shared moods in play.\nTo be playful about games is neither to be utterly serious, or utterly ironic, but to move easily into and out of commitments to rule-sets. To be playful is to bring oneself to care, for a time, about the specified goals of the game, and to adopt, for a time, a temporary but absolute obedience to a set of rules.\nTo be playful with a game is to wear the game’s cares and norms lightly\n\nConsider, for example, the shared mood of tabletop roleplaying games. The players have to commit, temporarily, to the rules of the game and a kind of (absurd) sincerity of purpose. The players have to really go all-in in pretending to be in character — of really being, say, fantasy elves and dwarves on a quest to save a village. As is often remarked by dedicated role-players, this shared mood is often wrecked by the pure ironist — who mocks the activity, who follows the rules mechanically but without real commitment, who breaks the illusion by calling attention to the arbitrariness of its rules\n"},"thoughts/plurality":{"title":"Plurality","links":["thoughts/Archipelago","thoughts/A-Certain-Tendency-Of-The-Database-Community","thoughts/privacy","thoughts/Seeing-like-a-State"],"tags":["sapling","seed"],"content":"Source: pluriverse.world\nPlurality refers to a state in which multiple alternatives co-exist. Classical pluralism in a political science context takes this and applies it to the distribution of power within a political system.\nSee also: Archipelago\nGlobality versus Globalization\nFrom Archipelago\n\nGlobality does not homogenize culture. It produces a difference from which new things can emerge.\nGlobalization standardizes and dilutes. It reduces communities to a single model, attacking them from the top down, diminishing them.\n\nSee also: A Certain Tendency Of The Database Community\nAtlas instead of map\nAn atlas is an unusual type of book. It is a collection of disparate parts, with maps that very in resolution from a satellite view of the planet to a zoomed-in detail of an archipelago.\nIt is a collection of different ways of viewing, one that acknowledges many specific perspectives rather than claiming objective truth.\nPluralistic Publics\nPlural Publics by Shrey Jain, Divya Siddarth, and E. Glen Weyl\n\nInstead of focusing on “private” v. “public,” we seek to protect and enable the emergence of a rich diversity of “publics.\n\nSee also: privacy\nPolycultures\nIn agriculture, monoculture refers to the cultivation of a singular crop in an area. Though monocultures may be efficient, they are more susceptible to disease and pests.\nThree Sisters, a gardening style of Native peoples, refers to the interdependent relationship between species of corn, squash, and beans. Although each of the species have different needs and growth patterns, they work alongside each other in a holistic ecosystem of growth\nSee also: Seeing like a State"},"thoughts/positive-sum":{"title":"Positive Sum","links":["thoughts/zero-sum","thoughts/public-goods","thoughts/incentives","thoughts/quadratic-funding"],"tags":["sapling"],"content":"As opposed to zero sum\nPositive Sum Worlds: Remaking Public Goods in Other Internetand Gitcoin on Seeking a New Kind of Public Good: Open Call for Proposals\nWe govern, share, and maintain public goods as a society. While imperfect in how they are built or administered, these objects draw us together in dialogue, debate, and common concern\nHow can we incentivize creation of public goods? Maybe through quadratic funding.\nAgalmics\nEconomics is the study of the allocation of scarce goods. We need a new paradigm, and a new field of study. What we need is agalmics.\n\nThe study and practice of the production and allocation of non-scarce goods. Source\n"},"thoughts/potemkin-village":{"title":"Potemkin villages","links":["posts/agi"],"tags":["sapling","pattern"],"content":"A “Potemkin village” signifies any deceptive or false construct, conjured often by cruel regimes, to deceive both those within the land and those peering in from outside.\nIn AI\nAI systems attempt to build a sort of ‘potemkin village’ that “works well on naturally occurring data, but is exposed as fake when one visits points in space that do not have high probability”"},"thoughts/power":{"title":"Power","links":["thoughts/trust","thoughts/identity","thoughts/social-contracts"],"tags":["seed"],"content":"Social Power\n\na practically socially situated capacity to control others’ actions, where this capacity may be exercised (actively or passively) by particular social agents, or alternatively, it may operate purely structurally.\n\nWherever power is at work, we should be ready to ask who or what is controlling whom, and why.\nPower can operate actively or passively. Consider a traffic warden:\n\nActive power is the ability of the traffic warden to actually impose a fine\nPassive power is the ability of the traffic warden to influence the behaviour of drivers due to the risk of a fine\n\nThis leads to a few characteristics of power\n\nPassive power tends to dwindle with the dwindling of its active operation\nPower is capacity; this capacity does not go away even if it is not being realized in action\n\nCounterclaim to Foucault’s “Power exists only when it is put into action”\n\n\n\nTypes of social power\n\nAgential Power: power exercised by a single agent on other agents\nStructural Power: no particular agent exercising power (e.g. algorithmic power)\n\nPower only exists due to social relations and inherent trust in the other to uphold the expectations we place of their role.\nIdentity Power\nSocial stickiness of identities, people tend to do what is expected of their social roles and norms\ne.g. ‘People like us aren’t political’; and so they do not vote. Conversely, part of what encourages many of us to vote is a social self-conception in the collective imagination such that ‘People like us are politically engaged’."},"thoughts/pragmatics":{"title":"Pragmatics","links":[],"tags":["seed"],"content":"\nHow do you use the language in communication and conversation\n\nPragmatic principles are principles about how language is used\n\n\nPrinciple of conventionality: the meaning of a word is determined by convention\n\n\nPrinciple of contrast: different words have different meanings (different from mutual-exclusivity as dog and animal are valid labels here)\n\n\nLinguistic competence: ability to use language in grammatical ways, including in production and comprehension\n\n\nCommunicative competence: ability to use languages in manners that are appropriate for a given conversation, communicative goal, or social setting.\n\n\nIntentionality is actually a bigger question with respect to language acquisition: is the child intending to communicate?\n\nEarly vegetative noises are not communicative acts, but they can be for adults (e.g., an intentional yawn to express boredom).\n\n\n\nThree phases\n\nPerlocutionary Phase (0;0-0;10): what I do/say has an effect on other people\nIllocutionary Phase (0;10-1;0): I can use what I do/say to communicate with other people\nLocutionary Phase (1;0+): I can use language (words used referentially and within well-formed sentences) in my communication\n\n\n\nConversational ability\n\nConnected discourse: communication involving multiple sentences or utterances in a longer time period\nGrice (1957, 1985): two rules to be a good conversationalist\n\nTake turns\n\nVery young children (preschoolers and young school age) have longer pauses between turns and less overlapping speech than adults have. They are not as good (as adults) at using words like and, and then, or um to indicate they are not finished talking.\n\n\nBe cooperative, optimize for\n\nQuantity: Make your contribution as informative as is required; provide neither too much nor too little information\nQuality: Try to make your contribution one that is true; do not say what you believe to be false or that for which you lack adequate evidence\nRelation: Be relevant\nManner: Be perspicuous (i.e., be clear—brief, orderly, unambiguous)\n\n\n\n\n\n\n\nTheories about early conversation\n\nPiaget: child does not have the skill and will for conversation yet\nVygotsky: speech serves a different function for children at early stages\n\n\n"},"thoughts/praxis":{"title":"Praxis","links":["thoughts/computability"],"tags":["seed"],"content":"The process in which ideas and theories are realized or embodied as action.\nThe argument is that all professions are mixtures of theory and practice. If a job is so simple that it is just a series of steps to carry out (algorithmic), then it is not a profession (counterpoint: can everything be reduced into algorithmic steps? How concrete do the steps need to be?).\nBates distinguishes between discipline (theory) and profession (practice)."},"thoughts/prefigurative-politics":{"title":"Prefigurative Politics","links":["thoughts/fiction","thoughts/skyhooks"],"tags":["seed"],"content":"\nHow do we create the world we want to see in the future?\n\nSee also: shared fictions, skyhooks"},"thoughts/prestige":{"title":"Prestige","links":["thoughts/optionality","thoughts/self-confidence","thoughts/Goodhart's-Law"],"tags":["sapling"],"content":"Source: The Prestige Trap by Wes De Silvestro\nIs this why so many people want to join ‘prestigious’ places to work like big tech or consulting? To keep doors open?\nFor lack of a better explanation, a big part of it feels like the go-to cop-out for individuals unsure of their larger life direction. Or maybe just lacking general conviction/self-confidence in themselves.\nPrestige, as Paul Graham defines it, is just fossilized inspiration. It’s what was once new and successful and thus became prestigious. “If you do anything well enough, you’ll make it prestigious.”\nA lot of people mistake prestige for excellence. The assumption is that prestige entails excellence but the reality is the opposite: prestige follows excellence\nIvy League\n“For the majority of Ivy Leaguers, the most impressive thing they’ve accomplished is achieving admission to their university. When you’re deemed successful because you went to Harvard rather than celebrated for what got you there in the first place, you learn to game the system and just focus on the credentials the next time around.”\nThis ‘credentials’ grind within these prestigious institutions is very reminiscient of Goodhart’s Law. They saw that their process of scrambling and playing ‘the game’ for college admissions got them prestige so they continue to optimize for that rather than success.\n\nSome of the lowest hanging fruit remains unpicked because few smart people are willing to venture down the road not taken.\n"},"thoughts/privacy":{"title":"Privacy","links":["thoughts/agency","posts/context-collapse","thoughts/Fishbowl-effect","thoughts/context","thoughts/GDPR","thoughts/walking"],"tags":["sapling"],"content":"Privacy isn’t about shutting out everyone and everything. Instead, privacy gives you the power to choose what and with whom you’ll share. It provides safety, control and the right to grant access.\nPrivacy gives you the ability to express yourself, to be creative, to spend your time and your money in whatever manner you like, without the scrutiny of others. It protects our intimate moments, our most embarrassing ambitions, our radical ideas and the ability to be our true selves.\nPrivacy is freedom, consent, dignity and security.\nDefinitions\n\nRestricting access to information or property to what you wouldn’t willingly give away in a particular context\n\nSpecifically, recognizing that privacy is contextual (Nissembaum)\nThe context of your privacy—what’s being revealed to whom and for what reason—utterly changed and you had no say in it.\n\n\nFrom the point of view of an individual restricting access: privacy is a “zone of inaccessibility” that surrounds a person\nFrom the perspective of outsiders seeking access: violating someone’s privacy is an affront to that person’s dignity\n\nHowever, some people take advantage of privacy to plan and carry out illegal or immoral activities\nThere is also conflicting needs between companies and users\n\nCompanies want to use data to improve their products\nUsers want to protect their privacy\n\nData anonymization isn’t enough. Even if some of the data is scrambled and personally identifiable information is stripped, it is susceptable to linkage attacks (correlating rows of the anonymized dataset to other known datasets)\n~87% of all Americans can be identified using only 3 pieces of information:\n\nzip code\nbirthday\ngender\n\nPrivacy for independent development\nPrivacy is the way in which a social group recognizes and communicates to the individual that he is responsible for his development as a unique person, a separate moral agent\nIt’s valuable because it lets us be ourselves. In order to have different kinds of social relationships with different people, we need to have some kind of control over who knows what about us (see: context collapse, Fishbowl effect)\nDifferential Privacy\ntldr; add randomized noise that maintains distribution of data\nWhen submitting a piece of data:\n\nA fair coin is flipped.\nIf heads: the real data is sent\nIf tails: we generate a random number to encode the result as random noise (e.g. true for heads, false for tails)\n\nThis way, we can’t trust any single record to be accurate (plausible deniability), but the aggregate still remains useful.\nAs we know noise distribution, this can be accounted for the in final calculation.\nNote that this will only work for larger datasets as injecting noise into a small dataset will likely result in inaccurate data\nUsage\n\nApple for error reporting\nGoogle for malware reports and traffic reports in Maps\n\nContextual Privacy\nFrom Antonio García Martínez in The right to never be forgotten\nHelene Nissenbaum’s ‘contextual privacy’\nAn example she draws in her work is imagining your interactions with your physician when dealing with a medical issue. Even in a world where the right to live as a stranger among strangers reigns supreme, we unquestioningly turn over the most intimate medical details to people we barely know.\nNow, let’s imagine you leave your doctor’s office and fire up Instagram to take your mind off the diagnosis he just gave you, which is that you don’t have brain cancer but you simply suffer from chronic migraines and will just have to deal. Scrolling past pictures of friends and celebrities, you see an advertisement for a migraine medication, specifically for the vestibular migraines you suffer from. While two seconds ago you were willing to send images of your brain across the world for medical advice, you now feel horribly violated knowing that everyone from Facebook to a pharma marketing team know about your condition.\nThe context of your privacy—what’s being revealed to whom and for what reason—utterly changed and you had no say in it.\nSee also: GDPR\nProxemics\nProxemics, a term coined by anthropologist Edward T. Hall, defines the relationships between a person and their identity, their surroundings, and the social norms of the community around a person or individual.\nGradients of Intimacy\nThere are four zones in proxemics\n\nthe intimate, the “bedroom”\n\nThe “bedroom,” an equally intimate space where only a few people are invited in. This is like a private DM or a text message between one or two friends or family members. It is a space to share your thoughts. Secrets are welcomed, and comfortably kept.\n\n\nthe personal, the “living room”\n\nIt’s semi-private, but can also host large groups and conversations that are designed to be public, private, or in-between. This setting allows for more intimacy because it allows for a smaller group. This design functions much like a salon or a group gathered for lively debate. The living room is a metaphor for a closed Facebook group or a WhatsApp chat group.\n\n\nthe social, the “park bench”\n\nIt’s like walking down the street and engaging in conversation with a coworker or friend, or having a discussion on the tube or in a pub—is a space where anyone can have a conversation between two or a few people, but that conversation takes place in public. Those in the conversation can control who hears it by lowering their voice or walking to a less populated area.\n\n\nand the public space, the “town hall”\n\nThis is where we shout our thoughts or share things we don’t mind thousands of people seeing. The town hall is a public square for speaking loudly and deliberately. Your thoughts can spread virally; They will be heard, amplified and sometimes misinterpreted.\n\n\n\n\nWeb2: considers identity public but data private\nWeb3: identity is private, but data public\n\nRights to privacy\nDiffering opinions on the status of privacy as a right. General consensus is that privacy is a prudential right. That is, rational agents would agree to recognize some privacy rights because granting these rights is to the benefit of society\n\nWarren and Brandeis\n\nPeople in modern society have a right to privacy and that this right ought to be respected\n“The intensity and complexity of life, attendant upon advancing civilization, have rendered necessary some retreat from the world, and man, under the refining influence of culture, has become more sensitive to publicity, so that solitude and privacy have become more essential to the individual”\nWarren and Brandeis argue, there are no adequate legal remedies available to the victims. Laws against libel and slander are not sufficient because they do not address the situation where malicious but true stories about someone are circulated (especially in cases where consent was not attained ahead of time, like through cameras)\n\n\nThomson\n\nEvery “privacy right” violation is a violation of another right\n\n\nReiman\n\nPrivacy is needed if people are to be autonomous moral agents able to develop healthy personal relationships and act as free citizens in a democratic society\nOur personal information is private to the extent that we can control who has access to it\nHe does not argue that privacy is a natural right, nor does he suggest that a person has complete control over what is held private.\n\n\n\nTaxonomy of Privacy\nProposed by Daniel Solove\n\nInformation collection refers to activities that gather personal information\nInformation processing refers to activities that store, manipulate, and use personal information that has been collected\nInformation dissemination refers to activities that spread personal information\nInvasion refers to activities that intrude upon a person’s daily life, interrupt a person’s solitude, or interfere with someone’s decision making\n\nUS Legislation\nRestricting information collection\n\nThe Employee Polygraph Protection Act of 1988 (EPPA) prohibits most private employers from using lie-detector tests under most situation\nThe Children’s Online Privacy Protection Act (COPPA) states that online services must obtain parental consent before collecting any information from children 12 years old and younger.\nThe Genetic Information Nondiscrimination Act of 2008 prohibits health insurance companies and health plan administrators from requesting genetic information from individuals or their family members, and it forbids them from using genetic information when making decisions about coverage, rates, or preexisting conditions\n\nWhat the US collects on its citizens\n\nCensus Records. In order to ensure each state has fair representation in the House of Representatives, the United States Constitution requires the government to perform a census every 10 years\nInternal Revenue Service (IRS) Records\nFBI National Crime Information Center 2000 includes such categories as wanted persons, criminal histories, people incarcerated in federal prisons, convicted sex offenders, unidentified persons, people believed to be a threat to the president, foreign fugitives, violent gang members, and suspected terrorists\nOneDOJ Database provides state and local police officers access to information supplied by five federal law enforcement agencies: the FBI; the Drug Enforcement Agency; the Bureau of Alcohol, Tobacco, Firearms, and Explosives; the US Marshals Service; and the Bureau of Prisons\nClosed-Circuit Television Cameras (CCTV)\nLicense-Plate Scanners\nPolice Drones. Federal Aviation Administration rules require that drones used by the police weigh no more than 25 pounds, fly no higher than 400 feet, and be flown during daylight within view of the operator\n\nCovert Surveillance in the States\n\nAllowed under the Fourth Amendment to the United States Constitution. But it has changed over time\nOlmstead v. United States that the Fourth Amendment protected tangible assets alone. The federal agents did not “search” a physical place; they did not “seize” a physical item\nIn 1934 the US Congress passed the Federal Communications Act, which (among other things) made it illegal to intercept and reveal wire communications. Three years later the Supreme Court used the Federal Communications Act to reverse its position on warrantless wiretaps\nAfter WWII broke out, President Roosevelt agreed to let the FBI resume wiretapping in cases involving national security, though he asked that the wiretaps be kept to a minimum and limited as much as possible to aliens\nIn 1967, Supreme Court rendered that citizens should also be protected from all electronic surveillance conducted without warrants, including bugs (hidden microphones used for surveillance)\nCongress passed the Title III of the Omnibus Crime Control and Safe Streets Act of 1968. Title III allows a police agency that has obtained a court order to tap a phone for up to 30 days\nOperation Shamrock: The Signal Security Agency (predecessor to the NSA) contacted Western Union Telegraph Company, ITT Communications, and RCA Communications who allowed SSA to make photographic copies of all foreign government telegram traffic that entered, left, or transited the United States. Facing hostile congressional and press scrutiny, the NSA called an end to Operation Shamrock in May 1975\nThe Foreign Intelligence Surveillance Act of 1978 (FISA) allows the president to authorize electronic surveillance of foreign nationals for up to one year without a court order, as long as there is little chance that the surveillance will reveal the contents of communications with any US citizens. This required the government to get a court order from the FISA Court\n\nFISA was amended by the Protect America Act of 2007. This act allows the US government to wiretap communications beginning or ending in a foreign country without oversight by the FISA Court.\n\n\nIn 1986, the ECPA was passed which allows police to attach two kinds of surveillance devices to a suspect’s phone line. If the suspect makes a phone call, a pen register displays the number being dialed. If the suspect gets a phone call, a trap-and-trace device displays the caller’s phone number.\n\nWhile a court order is needed to approve the installation of pen registers and trap-and-trace devices, prosecutors do not need to demonstrate probable cause, and the approval is virtually automatic\nThe ECPA also allows police to conduct roving wiretaps—wiretaps that move from phone to phone—if they can demonstrate the suspect is attempting to avoid surveillance by using many different phones\nUnder the Stored Communications Act, part of the ECPA, the government does not need a search warrant to obtain from an Internet service provider email messages more than 180 days old\n\nMany big technology companies have formed an organized called Digital Due Process which is lobbying Congress to update the ECPA\nThe view of the Digital Due Process coalition is that the government should not be able to obtain an email message, document, or photo from an Internet or cloud service provider without a proper search warrant\n\n\n\n\nCongress passed the Communications Assistance for Law Enforcement Act of 1994 (CALEA)\n\nThis law required that networking equipment used by phone companies be designed or modified so that law enforcement agencies can trace calls, listen in on telephone calls, and intercept email messages\n\n\nThe FBI developed the Carnivore Surveillance System in the late 1990s to monitor Internet traffic, including email messages. Armed with a search warrant, the FBI would set up its Carnivore system at the suspect’s Internet service provider. An Internet service provider questioned the FBI’s authority to do this under the Electronic Communications Privacy Act but a US District Court ruled against it.\n\nIn late 2001 the FBI stopped using Carnivore, replacing it with commercial software capable of performing the same function\n\n\nPost 9/11, President Bush signed a presidential order allowing the NSA to eavesdrop on international telephone calls and international emails initiated by people living inside the United States, without first obtaining a search warrant\nCongress then passed the USA Patriot Act which amended many existing laws. Four main principal categories\n\nProviding federal law enforcement and intelligence officials with greater authority to monitor communications\nGiving the Secretary of the Treasury greater powers to regulate banks, preventing them from being used to launder foreign money\nMaking it more difficult for terrorists to enter the United States\nDefining new crimes and penalties for terrorist activity\n\nAllows courts to authorize law enforcement officers to search a person’s premises without first serving a search warrant when there is “reasonable cause to believe that providing immediate notification of the execution of the warrant may have an adverse effect.”\nOfficers may seize property that “constitutes evidence of a criminal offense in violation of the laws of the United States,” even if that offense is unrelated to terrorism.\n\n\n\n\nThis had averse effects. In November 2003, the ACLU reported that public apprehension about the Patriot Act had led to a significant drop in attendance and donations at mosques\nThe Council of the American Library Association was the first of many to pass anti-Patriot Act resolutions: “urg[ing] librarians everywhere to defend and support user privacy and free and open access to knowledge and information”\n\n\nUS Department of Defence created the Threat and Local Observation Notices (TALON) database in 2003. The purpose of the database was to collect reports of suspicious activities or terrorist threats near military bases. The TALON database was shut down on September 17, 2007\nIn 2013, Snowden leaked PRISM, which is a program that allowed the NSA gained access to the servers of Microsoft in 2007; Yahoo in 2008; Google and Facebook in 2009; YouTube in 2010; Skype and AOL in 2011; and Apple in 2012. It enabled the NSA to access stored information without obtaining search warrants when the NSA had reasonable suspicion that the person being investigated is a foreigner outside the US\n\nEdward Snowden and Glenn Greenwald explained XKeyscore as being a system which enables almost unlimited surveillance of anyone anywhere in the world\n“You could read anyone’s email in the world, anybody you’ve got an email address for. Any website: You can watch traffic to and from it. Any computer that an individual sits at: You can watch it. Any laptop that you’re tracking: you can follow it as it moves from place to place throughout the world. It’s a one-stop-shop for access to the NSA’s information. …”\n\n\n\nCode of Fair Information Practices\nIn the early 1970s, a group convened to recommend a set of policies often dubbed the “bill of rights” for the Information Age\n\nThere must be no personal data record-keeping systems whose very existence is secret.\nThere must be a way for a person to find out what information about the person is in a record and how it is used.\nThere must be a way for a person to prevent information about the person that was obtained for one purpose from being used or made available for other purposes without the person’s consent.\nThere must be a way for a person to correct or amend a record of identifiable information about the person.\nAny organization creating, maintaining, using, or disseminating records of identifiable personal data must assure the reliability of the data for their intended use and must take precautions to prevent misuses of the data.\n\nThe Privacy Act of 1974 represents Congress’s attempt to codify the principles described in the Code of Fair Information Practices. However, in most respects, it has fallen short of the desires of privacy advocates:\n\nThe Privacy Act applies only to government databases. Far more information is held in private databases, which are excluded. This is an enormous loophole, because government agencies can purchase information from private organizations that have the data they want.\nThe Privacy Act only covers records indexed by a personal identifier.\nNo one in the federal government is in charge of enforcing the provisions of the Privacy Act. Federal agencies have taken it upon themselves to determine which databases they can exempt.\nThe Privacy Act allows one agency to share records with another agency as long as they are for a “routine use.”\n"},"thoughts/probabilistic-classifier":{"title":"Probabilistic Classifier","links":["thoughts/Decision-theory","thoughts/multi-class-classification"],"tags":["seed","CPSC340"],"content":"We want a model of P(yi​=important∣xi​) for use in decision theory.\n\nPredictions generally map wTxi​ to labels for classes (for binary prediction, we used sign(x))\nProbabilities we want to map wTxi​ to the range [0,1]\n\nThe most common choice is to use the sigmoid function:\nh(zi​)=1+exp(−zi​)1​\nMulti-class Probabilities\nSee also: multi-class classification\nThe softmax function allows us to map k real numbers zi​=wcT​xi​ to probabilities.\nP(y∣z1​,z2​,…,zk​)=∑c=1k​exp(zc​))exp(zy​)​\nThe alternative ‘harder’ version to softmax is the argmax function which simply finds the maximum value, sets it to 1.0, and assigns 0.0 to all other values.\nIn contrast, the softmax operation serves as a “softer” version of that. Due to the exponentiation involved in softmax, the largest value is emphasized and pushed towards 1.0, while still maintaining a probability distribution over all input values. This allows for a more nuanced representation that captures not only the most likely option but also the relative likelihood of other options."},"thoughts/probability":{"title":"Probability","links":["thoughts/Naive-Bayes","thoughts/Dutch-Book"],"tags":["seed"],"content":"Interpreting small probabilities\nI suspect that there is a human limit to the size of a probability we find meaningful. Just like how the size of certain numbers are incomprehensible to humans, some probabilities are so unlikely that they are nearly meaningless. Similarly, Bernoulli conjectured that people neglect small probability events.\n\nNicholas Bernoulli who can be held liable as the creator of the Petersburg gamble suggested that more than five tosses of heads are morally impossible. This proposition is experimentally tested through the elicitation of subjects‘ willingness-to-pay for various truncated versions of the Petersburg gamble that differ in the maximum payoff. In fact, the experimental data show that all versions of the Petersburg gamble which allow for more than six repeated tosses of tails elicit the same willingness-to-pay. From this evidence it is concluded that subjects neglect those outcomes in the Petersburg gamble which occur with a probability smaller than or equal to one in sixty-four, because, given this level, the alternative explanations seem implausible. (Neugebauer 2010, p. 3)\n\nKolmogorov Axioms\n\nThe probability of an event is a non-negative real number\nThe probability that at least one of the possible events happen is 1\nGiven a set of mutually exclusive events, the probability of all of them happening is the probability of each event happening summed up\n\nAs a result, we get:\n\n0≤P(A)≤1\nP(¬A)=1−P(A)\n\nJoint probability\nProbability of both A and B happening. Intersection of the areas of the two events.\nMarginalization Rule\nFor some random variable X\nP(A)=∑x∈X​P(A∩X=x)\nFor example, to roll some even number,\nP(even)=∑i=16​P(i∩even)=0+61​+0+61​+0+61​\nUnion of Events\nGiven an event A and B, the probability of both occurring is\nP(A∪B)=P(A)+P(B)−P(A∩B)\nConditional Probability\nThe probability of A given B has occurred is\nP(A∣B)=P(B)P(A∩B)​=P(B)P(B∣A)P(A)​\nDeriving from this, we get,\n\nP(A∩B)=P(A∣B)P(B)\nP(A∩B)=P(B∣A)P(A)\n\nIndependence\nP(A∩B)=P(A)P(B)\nor\nP(A∣B)=P(A)\nExpected Values\nIf we have a random variable X that can takes values x∈X, we define the expectation of X:\nE[X]=∑x∈X​P(X=x)x\nAdditionally,\n\nFor functions that depend on a random variable: E[f(X)]=∑x∈X​P(X=x)f(X)\nE[αf(X)+βg(X)]=αE[f(X)]+βE[g(X)]\nE[f(X)g(X)]=E[f(X)]E[g(X)]\nE[X∣Y=y]=∑x∈X​P(X=x∣Y=y)f(X)\nE[E[X=x∣Y=y]]=E[X] (tower property, law of total expectation, iterated expectation rule)\n\nBayes’ Theorem\nSee also: Naive Bayes\nLet c be the class label and x be the measurement (evidence)\nP(c∣x)=P(x)P(x∣c)p(c)​\n\nP(c∣x): the posterior probability is the probability of c given x (after the measurement).\np(c): prior probability\nP(x∣c): class-conditional probability (likelihood of c on x)\nP(x): unconditional probability (a.k.a. marginal likelihood or expectedness of evidence)\n\nAlternate formulations:\nExpanded\nP(c∣x)=P(x∣c)P(c)+P(x∣¬c)P(¬c)P(x∣c)P(c)​\nMultiple Hypotheses\nSuppose c1​,…,cn​ is an exhaustive and mutually exclusive set of possibilities\nP(ci​∣x)=P(x∣c1​)P(c1​)+⋯+P(x∣cn​)P(cn​)P(x∣ci​)P(ci​)​\nInterpretations of Probability\nA = draws from a normal deck, B = draws of a face card.\nP(B∣A)=133​ means:\nObjective interpretations: Probability values are determined by factors independent of our beliefs.\nSubjective interpretations: Probability values reflect individual degrees of belief, and vary from person to person.\nWe can evaluate these interpretations as follows (Wesley Salmon)\n\n\nAdmissible. Probability values must satisfy the axioms of the probability calculus (the Kolmogorov axioms). This is also called coherence.\n\n\nAscertainable. Probability values must be values that we can determine (or else they are useless).\n\n\nApplicable. Probability values must be reliable as a “guide to life”. They must be values that we can justifiably use to make decisions.\n\n\nClassical: number of B over number of A is 3/13\n\nProblem: assumes cases of A are equipossible (equal probability)\nThis seems circular\nFails ascertainability, admissibility, and applicability\n\n\n\nFinite frequency: The proportion of B in a long series of draws is exactly 3/13.\n\nIs admissible and ascertainable\nNot applicable: how does this work for single case probabilities?\n\n\n\nLimiting frequency: The limiting frequency in an infinite series of draws would be 3/13.\n\nIs admissible and applicable\nNot ascertainable: there may be no limiting frequency\nAgain, does not work for single case probabilities\n\n\n\nLong-run propensity: The set-up A has a disposition to produce long sequences in which B happens with frequency 3/13.\n\nAssumes that long-run frequencies have an underlying cause through an experimental arrangement/set-up\nNot ascertainable: no improvement on the limiting frequency interpretation\nNot explanatory: the tendency or disposition adds nothing to our udnerstanding\nNot all probabilities can be interpreted as propensities. (no causal relation)\n\n\n\nLogical: B partially entails A, with degree of entailment 3/13.\n\nP(B/A) measures the “proportion” of A that overlaps with B\n\n\n\nEpistemic: The evidence that A happened provides objective support of degree 3/13 that B happened.\n\nLogical and epistemic probabilities might only exist in some cases\nVery unlikely that we know some of the priors/likelihoods can be computed a priori (from pure logic)\n\n\n\nSubjective (actual degree of belief): Somebody believes with degree 3/13 that A will produce B.\n\nCredences can be measured (or even defined) by studying your actions, especially your betting behaviour\nProblem: actual degree of belief is not admissable, people commit probabilistic fallacies all the time\nThis can lead to bad betting combinations (see Dutch Book examples) in which you are guaranteed to lose money\nKey assumption: EU and EMV are equivalent for small but non-trivial amounts of money\n\n\n\nSubjective (idealized credence): An idealized version of someone – with coherent probabilities – believes with degree 3/13 that A will produce B.\n\nFixes admissibility as we require it\nNot applicable: how can we justify using personal probabilities to make decisions if there are no constraints on one’s prior probabilities?\nAnother problem\n\nThe meaning or concept of probability does not essentially involve desires or preferences\nFor example, an enlightened Zen Buddhist monk can have probabilities but no desires\nThus, by Peterson, any theory that creates a necessary (definitional) link between probability and preference/desire must be wrong.\n\n\n\n\n"},"thoughts/program-analysis":{"title":"Program Analysis","links":["thoughts/symbolic-execution","thoughts/Rice's-Theorem"],"tags":["seed"],"content":"\nConcerns what a program does: analysis aim is to check statically/dynamically something about execution behaviours\nIs flow-sensitive: what the analysis does with a statement is dependent on control flow of the program\n\nSoftware analysis falls into many categories\n\nMetaproperty analysis: anything else except for what the code actually does (e.g. style checkers)\nProgram analysis: analysing what a (part of a) program will do / does / can do\n\nStatic program analysis: without running the program. This requires considering all executions of the code of interest which requires context about the execution up to this point\n\nValue-agnostic\nValue-sensitive (symbolic execution)\n\n\nDynamic program analysis: analyzes the program while running it\nAutomatic (Concolic) test-case generation\n\n\n\n3 main properties about code\n\nHow the code is written (fairly simple static meta-property analysis)\nWhat the code does (program analysis - cannot be done precisely)\nWhat the author intended (impossible for fully automatic analysis, not available from the code)\n\nSee: Rice’s Theorem\nDesigning a Static Analysis\n\nDefine the goal: what is the property (of all executions) of a program? What problem is the analysis supposed to help?\nAbstract states σ: type of information analysis tracks through program\nError/output information E: type of information returned by analysis\nAnalysis function: define a function analyze(σ,s) for analysis steps where\n\nσ is an abstract state\ns is a program state\nreturns a pair (σ′,E) of resulting abstract state plus any errors\ntypically defined per-case of type of (supported) statement s\n\n\nConcretisation function: maps abstract states to sets of program states or sets of program executions - this defines what abstract states mean. What does the analysis “think” is a possible state at this point?\n\n(D, A) maps to set of all states in which at least D are declared and A are initialized\n(D, A) maps to set of all states in which at most D are declared and A are initialized\n(D, A) maps to set of all states in which exactly D are declared and A are initialized\n\n\n(Optionally) termination strategy: for recursive control flow (e.g. loops)\n\nWe can do this by instrumenting (changing) the program. We have two places where this can happen\n\nInstrumenting source code (e.g. grab AST and modify it)\nInstrumenting executable (e.g. grab bytecode and modify it)\nInstrumenting runtime (e.g. modify JVM)\n\nThere’s also a choice to be made about how/when to perform the analysis\n\nOnline dynamic analysis: run the analysis as part of / alongside the program\nOffline dynamic analysis: make the program produce a log; analyse it separately\n\nProgram Slicing Example\nNB: We never delete variable declarations, just their initializations. Similarly, we keep flow constructs if they have at least one line inside of them.\nStatic\n\nDefine abstract state σ as (M,L) where\n\nM is a map from variable names to what line numbers may have affect that variable at that point\nL is a list of control flow dependencies at that point\n\n\nFor assignment of x=e at line n where e is an expression that can contain multiple variables\n\nM[x] becomes the union of\n\nThe line number n\nM[y] for every y in the expression e\nUnions of all sets in L\n\n\nL remains unchanged\n\n\nFor if-then-else statements\n\nPush to L: the union of M[y] for every y in the if-condition check\nCopy the map M to start of the then and else blocks\nContinue normally…\nAt the end of each block, union the Ms at the end of each block\nPop from L\n\n\nFor loops\n\nAdd loop variable to M\nPush to L: the union of M[y] for every y in the loop condition\nContinue normally…\nWhen we hit end of loop body, we go back to start of the loop and rewrite. Stop when neither M nor L update\n\nEach variable in M to be union of its old value and the value at the end of the loop\nHead of L is the union of itself and M[y] for every y in the loop condition\n\n\nPop from L\n\n\n\nDynamic\n\nWe add two extra objects to the program state, m and l which are functionally equivalent to M and L from previously\nFor assignment of x=e at line n, we update m to store\n\nn\nm[y] for each y in e\nl flattened\n\n\nFor if-then-else statements\n\nBefore the statement, we calculate s by taking union of all the dependencies in the condition and push this to l\nWe pop l after the statement\n\n\nLoops are the same as if-then-else statements but we do steps 1. and 2. inside the loop instead of outside the statement\n"},"thoughts/programming-models":{"title":"Programming Models","links":["thoughts/system-model","thoughts/feedback-loops","thoughts/mental-model","thoughts/notation","thoughts/composable","thoughts/plurality","thoughts/Gall's-law","thoughts/complexity","thoughts/declarative-programming","thoughts/CRDT","thoughts/semantics","thoughts/design-goals"],"tags":["seed"],"content":"System models for programming.\nTechnical Dimensions of Programming Systems\nSource\n\n7 clusters\n\nInteraction: how do users manifest their ideas, use the system to get a result, and generate new ideas in response?\n\nHow many layers of feedback loops are present when translating a user’s intention to getting a response?\n\nFor example, statically typed languages have three layers of feedback loops that are nested within each other\n\nTranslating ideas into code\nSatisfying the type checker. Errors here will send programmers back to the first feedback loop (i.e. there is a mismatch between what user thinks the program does and what it actually does)\nRunning the program and evaluates the run time behaviour.\n\n\nThe idea is that static checks can push bugs from cycle 3 to cycle 2, reducing the evaluation gulf at cycle 3 at the cost of increasing the total execution gulf in cycle 2 (and thus cycle 3 as well)\nWhen the total evaluation gulf across all the cycles is minimized as to be imperceptible, we call this immediate feedback. Once the user has caused some change to the system, its effects (including errors) are immediately visible (automatically demanded).\n\nDirect manipulation is a special case of an immediate feedback loop. The user sees and interacts with an artefact in a way that is as similar as possible to real life; this typically includes dragging with a cursor or finger in order to physically move a visual item, and is limited by the particular haptic technology in use.\n\n\n\n\nHow does the system facilitate the construction of abstractions?\n\nA first principles approach involves thinking ahead of time about what the required abstraction will be (in the case of a programming language, perhaps an interface) and then encoding it in the system\nOther systems may actually limit the construction of abstractions in order to encourage end users to work with concrete objects (direct manipulation)\n\nExamples of this are Jupyter notebooks (previews after individual cells discourage creating abstractions, because then you would not be able to look inside your execution such a fine grained level) and spreadsheets\n\n\n\n\nSee also: mental model, feedback loops in interaction design\n\n\nNotation: how are the different textual and visual programming notations related?\n\nWhat notations are used to program the system and how are they related?\nSurface vs internal notation\n\nAll programming systems build up structures in memory, which we can consider as an internal notation not usually visible to the user\nWhat the user interacts with instead is the surface notation, typically one of text or shapes on a screen. Every interaction with the surface notation alters the internal notation in some way\n\n\nNotational geography: do similar expressions in a particular notation represent similar behavior?\nIs it good to have overlapping notations to do the same thing in an application? e.g. a keyboard shortcut and GUI menu?\n\nFor programming systems that use overlapping notations, we need to describe how the notations are synchronized.\n\n\nSee also: notation\n\n\nConceptual structure: how do the parts fit together?\n\nComposability: There exist building blocks which span a range of useful combinations. Composability is, in a sense, key to the notion of “programmability” and every programmable system will have some level of composability\nConvenience: are there ready-made solutions to specific problems? i.e. what is the ‘standard library’ of the system?\nComposability vs Convenience\n\nComposability without convenience is a set of atoms or gears; theoretically, anything one wants could be built out of them, but one must do that work\nConvenience without Composability is a bunch of standalone tools, none of which work with each other. You can do anything that each individual tool but nothing that combines them together. A lot of modern web applications are like this\n\n\nConceptual integrity: a drive exists in the Python programming language, which follows the principle that “There should be one—and preferably only one—obvious way to do it” in order to promote community consensus on a single coherent style.\n\nThe apotheosis of this approach can be found in early Smalltalk and Lisp machines, which were complete programming systems built around a single language.\nEverything was done in one language, and so everything was represented with the datatypes of that language.\n\n\nConceptual openness: can be seen as championing the values of pluralism, compatibility, or conceptual openness over conceptual integrity\nSee also: Gall’s law\n\n\nCustomizability: once a program exists in the system, how can it be extended and modified?\n\nThere are a number of interesting questions related to staging of customization. First, what is the notation used for customization? This may be the notation in which a program was initially created, but a system may also use a secondary notation for customization (consider Emacs using Emacs Lisp).\nAdditive authoring requires that system behaviours can be changed by simply adding a new expression containing addresses—in other words, anything can be overriden without being erased.\n\nOne example of this is CSS on the web. Given a web page, it is possible to modify almost any aspect of its appearance by simply adding additional rules to a CSS file.\n\n\n\n\nComplexity: how does the system structure complexity and what level of detail is required?\n\nThere is a massive gap between the level of detail required by a computer, which executes a sequence of low-level instructions, and the human description of a program in higher-level terms.\nDeclarative vs imperative programming systems\n\nDeclarative programming systems like SQL, Prolog or Datalog, the meaning of a program is still unambiguous, but it is not defined operationally—there is a (more or less deterministic) inference engine that solves the problem based on the provided description.\nWhereas imperative programming systems rely on the programmer explicitly defining the steps and using the right abstractions\n\n\nSee also: complexity\n\n\nErrors: what does the system consider to be an error? How are they prevented and handled?\n\nDistinguish between 4 types of errors\n\nSlip: transient human attention failure (e.g. typos)\nLapse: caused by memory failure (e.g. incorrectly remembered method name)\nMistake: logical error (e.g. bad design of an algorithm)\nFailure: system error caused by the system itself that the programmer has no control over (e.g. a hardware or a virtual machine failure)\n\n\nNormally, error detection occurs in the feedback loops that the system implements.\n\nWhat errors can be detected in which feedback loops and how?\nHow does the system respond when an error is detected?\n\nThe system may try to automatically recover from the error as best as possible (CRDTs take this approach)\nThe system may proceed as if the error did not happen\nThe system may ask a human to resolve the issue. This is common for errors that occur on a semantic level\n\n\n\n\n\n\nAdoptability: how does the system facilitate or obstruct adoption by both individuals and communities?\n\nLearnability: systems can be made easier to learn in several ways:\n\nSpecializing to a specific application domain\nSpecializing to simple small-scale needs\nLeveraging the background knowledge, skills, and terminologies of specific communities\nSupporting learning with staged levels of complexity and assistive development tools\nCollapsing heterogeneous technology stacks into simpler unified systems.\n\n\nAdopting a technology is a costly investment in terms of time, money, and foregone opportunities. Can people be assured that the programming system will still be there in 5 years? A century?\nSee also: design goals, notation\n\n\n"},"thoughts/progress":{"title":"Progress","links":["thoughts/introductions","thoughts/research-institutions","thoughts/creation-vs-maintenance","thoughts/virtual-worlds","thoughts/writing","thoughts/Extended-Mind-Hypothesis","thoughts/teaching","thoughts/social-graphs","thoughts/funding","thoughts/philosophy-of-science","thoughts/epistemology","thoughts/incentives","thoughts/The-ones-who-walk-away-from-Omelas","thoughts/catch-22","thoughts/Collingridge-dilemma","thoughts/hedonic-treadmill"],"tags":["sapling"],"content":"Progress, loosely defined by Tyler Cowen and Patrick Collison, is “the combination of economic, technological, scientific, cultural, and organizational advancement that has transformed our lives and raised standards of living over the past couple of centuries”\nProgress implies direction. Progress towards what? Does this require alignment of values? Is progress enabling us to better express our identities?\nProgress isn’t equal, it’s multiplicative. It’s “a mixed blessing—one that resulted in technologies that have allowed many people to live longer, safer lives, but that has, simultaneously, destroyed global ecosystems, caused the extinction of many living species, facilitated rampant population growth, and wreaked havoc on climate systems, the effects of which will be an increase in droughts, floods, storms, and erratic weather patterns that threaten most global societies.” Source\nThere are no ‘thought leaders’ in bleeding tech, only in established fields. Everyone at the edge of a field is a ‘pioneer’ in a sense.\nAfter a subject gets established, how do we continue to make progress through research? A new DARPA perhaps?\nHistory of Progress\nSource\n\nWhy might people in the past have been hesitant to embrace the idea of progress? The main argument against it was that it implies a disrespect of previous generations.\n\nIs progress a result of a more creation focused lens rather than a maintenance focused one? Finding answers through working on new things rather than what was revealed in the past\nSo many historically ‘truthful’ sources like the Church and classical science were wrong about fundamental aspects of the universe (e.g. Earth being the centre of the universe). “By 1600, much of ancient wisdom had crumbled.”\nSkepticism as the taproot of all knowledge, heavily Cartesian\nHowever, writing, the printing press, and other tools allowed us to extend our mind and conceive of knowledge as cumulative.\nProgress Studies\nThe study of the how and why of progress.\nThere are ecosystems that are better at generating progress than others, perhaps by orders of magnitude. What do they have an in common?\nWhat enables progress? “Why did Silicon Valley happen in California rather than Japan or Boston? Why was early-20th-century science in Germany and Central Europe so strong? Can we deliberately engineer the conditions most hospitable to this kind of advancement or effectively tweak the systems that surround us today?”\nHow can we enable useful progress in the future?\nFrom a more epistemlogical standpoint, how much of progress just comes down to good:\n\npedagogy\nnetworks (re: social graphs)\nchance and circumstance\n\n“Organizations as varied as Y Combinator, MIT’s Radiation Lab, and ARPA have astonishing track records in catalyzing progress far beyond their confines. While research exists on all of these fronts, we’re underinvesting considerably.”\nWhat might a a new DARPA to catalyze progress look like?\nJasmine has a really good potential curriculum outline:\n\nHistory and causes: How do we make progress?\n\nHistory of science and technology: How were useful discoveries made? What was the relationship between funding and knowledge?\nPhilosophy of technology/ technological progress: How, why, and when do particular technologies emerge?\nMeta-science / science of science / social epistemology of science: How do we educate, train, and incentivize scientists?\nMechanism design + incentive design: How do we design incentives to elicit certain behaviours and aggregate specific information?\nCultural components of change-making: How have humans organized change in the past around ideas and processes?\nCause prioritization: How do we decide what to focus on?\n\n\nDefinition and measurement: What sort of world(s) should we be we building towards?\n\nVisions of the future: What type of future/utopia do we want to live in? Is progress to one person necessarily progress to the collective? rel: The ones who walk from Omelas\nProgress definition and measurement: How do we define progress and metrics?\n\n\nDrawbacks of progress: What are the risks incurred by progress? How do we make differential progress?\n\nCosts of progress: Can progress be too fast? How do we mitigate risks (esp those that are irreversible and existential)?\nRobust decision-making and better prediction under uncertainty: How can we better predict the impacts of our actions?\n\n(re: catch 22 and the Collingridge dilemma)\n\n\n\n\n\nWhat type of Progress?\nDo we care more about technological progress or social progress? We can measure social progress via, maybe, Gini coefficient (measuring income/wealth inequality). Technological progress might be measured by our ability to impact the universe, the physical world around us.\nHappiness\nHappiness isn’t necessarily highest in the places with the most technological progress\nIs it wrong to try to measure progress in terms of people’s happiness? What about the hedonic treadmill, will we eventually just regress to a new ‘normal’?\nA humanistic take\nSource: How does progress happen? by Kelsey Piper\n\nProgress is anything that helps human beings live better lives: longer, happier, healthier, in mind, body, and spirit. And more choices about how we want to live our lives: our careers; where we live; if, when, and who we marry; whether to have kids or not. Fundamentally, I judge progress by humanistic standards.\n"},"thoughts/proof-of-stake":{"title":"Proof of Stake","links":["thoughts/trust","thoughts/proof-of-work","thoughts/fault-tolerance"],"tags":["seed"],"content":"Uses ‘stake’ tokens to earn the right to become a validator of the blockchain. Users are chosen to become validators pseudo-randomly depending on various factors like size of stake, age of stake, etc. Validators check for validity of transactions, signing the block, and adding to the chain. Reward for the validator is the transaction fees.\nThe stake is a financial motivator for users not to validate or create fraudulent transactions (i.e. if you care about the chain, you should hope that members of the chain are also honest, mutual trust)\nThis validation is known as attesting. You can think of attesting as saying “this block looks good to me.” If you attest to malicious blocks, you lose your stake. 128 validators are required to attest to a block to achieve finality on it — this 128 is known as the committee. The committee works on 32 blocks or ‘slots’ before disbanding — this duration is known as an epoch.\nSource: Ethereum Wiki\nTLDR; a set of validators take turns proposing and voting on the next block, and the weight of each validator’s vote depends on the size of its deposit (i.e. stake)\nOne of the alternatives to PoW\n\nSecurity comes from putting up economic value-at-loss rather than straight up burning energy (however, this doesn’t take into account collusion)\n\nLosing Stake\nFor example, a user can lose a portion of their stake for things like going offline (failing to validate) or their entire stake for deliberate collusion.\nDisadvantages\n\nGreater chance of 51% attacks\n\nThough this is questionable, 51% means you need to control 51% of the staked ETH which would probably cause ETH’s value to drop significantly. There’s very little incentive to destroy the value of a currency you have a majority stake in.\n\n\nIncentive to hoard tokens and not use them\n"},"thoughts/proof-of-work":{"title":"Proof of Work","links":[],"tags":["seed"],"content":"Miners solve cryptographic problems in order to earn the right to add a new block to the chain where the fastest miner gets the rights to add along with token rewards.\nBasis\nCryptocurrencies use a distributed ledger (blockchain) to track all transactions in a publicly agreed upon manner.\nAll transactions are hashed. Hashing in general is a trivial function. To make it ‘work’, the network sets a difficulty for how much work can be expected to “mine” a new block. Mining is essentially just working to find a valid hash for a batch of transactions. A ‘block’ is a set of transactions.\nBecause the ‘winner’ is randomly-chosen proportional to the work done, it incentivizes everybody on the network to act honestly and record only true transactions.\n\nProof of work makes it extremely difficult to alter any aspect of the blockchain, since such an alteration would require re-mining all subsequent blocks.\n\nFinality\nSource: Ethereum Wiki\n\nA transaction has “finality” on Ethereum when it’s part of a block that can’t change.\n\nFinality in proof of work is probabilistic, meaning you cannot be 100% certain a transaction in a block is legitimate, only statistically certain. Finality, in this probabilistic sense, refers to the time you should wait before considering a transaction irreversible\nDisadvantagaes\n\nOperates on the logic of massive power (hashing power) incentivized into existence by massive rewards (block rewards). Only defense attacks is just scale of the network (attackers of size less than x are discouraged from appearing by having the network constantly spend x every day)\nExtremely electricity intensive\n"},"thoughts/prototyping":{"title":"Prototyping","links":["thoughts/interaction-design","thoughts/mental-model"],"tags":["seed"],"content":"The point is to make ideas real. They are (limited) representations of conceptual designs for users to interact with.\nSketching for interaction design\n\nWhy prototype?\n\nSaves time and money: don’t waste time coding/building the wrong thing\nCommunication: discuss ideas with stakeholders\nEvaluate: interface effectiveness for communicating conceptual model\nFurther develop conceptual and physical design\n\nBefore prototyping\nIdentify:\n\nquestions that your prototype(s) need to answer\nrequirements you need to address\nusers and tasks that your prototype(s) will support\n\nAcquiring mental models\n\nUsing the system (hands-on learning)\nObserving others using the system\nReading about a system (documentation)\n\nInteraction Types\nDeciding upon which of the interaction types to use, and why, can help designers formulate a conceptual model before committing to a particular interface\n\nInstructing: users issue instructions to a system\nConversing: users have a dialog with a system\nManipulating: users interact with objects in a virtual or physical space by manipulating them\nExploring: users move through a virtual environment or a physical space\nResponding: system initiates the interaction and the user chooses whether to respond\n\nFidelity\nFidelity is partly a matter of completeness. As you get more hi-fi it become more close to the actual deployment platform\n6 dimensions to fidelity → fidelity is a spectrum. It is complicated to prototype multiple dimensions at once, so don’t!\n\nvisual realism: how real it looks. polish, graphic imagery\nphysical realism: shape and form for 3D objects; feel\nscope: how many features/functionalities included; horizontal vs. vertical\ndata: operates on real vs. faked data\nautonomy: requires “supervision” vs. operates alone\nplatform: interim vs. final implementation\n\nLo-fi\nRough (but flexible) proof-of-concept of interface design. Useful for generating or narrowing down requirements.\nBenefits\n\ncheap/easy to make → intended to be thrown away\nlack of polish → less intimidating for users (surprisingly important!)\n\navoids nitpick feedback\ninspires more creative feedback\nmore willingness to criticize\n\n\n\nMid/hi-fi\nIncreasing in completeness and detail\n\nhigher degree of functionality\nhigher degree of polish\n\nVertical vs Horizontal\nVertical prototype:\n\nincludes in-depth functionality for only a few selected features\nkey design ideas can be tested in depth\n\nHorizontal prototype\n\nsurface layers only: includes the entire user interface with no underlying functionality\na simulation; no real work can be performed\n\nWizard of Oz\nMethod of testing a system that does not yet exist\n\nHuman simulates system’s intelligence and interacts with user\nUser\n\nUses real or mock interface as expected and is told “pay no attention to the man behind the curtain”\n\n\n“wizard” (sometimes hidden):\n\nInterprets subject’s input according to a preset algorithm\nHas computer/screen behave in appropriate manner\n\n\n\nPossible downside is that the human can over-/under-estimate the quality of the actual technology being simulated."},"thoughts/pseudonymity":{"title":"Pseudonymous Web","links":["thoughts/social-graphs","thoughts/virtual-worlds","posts/context-collapse","thoughts/crutch-and-shoe-metaphor","thoughts/web3","thoughts/NFT","thoughts/Zooko's-Triangle"],"tags":["seed"],"content":"\n“Despite my suburban liberal upbringing in Canada, on VRChat, I have listened to people who want gun rights, who refuse vaccination, and who reject taxation. My worldview has become more open to every strand of human experience.” On Life in the Metaverse\n\nSource: The Future of Social Media is Pseudonymous by Kyle Qian\nAn identity-based social graph is fundamentally limited. Your Facebook identity and social graph are mere digital shadows of their physical counterparts, and little more.\nBy attaching our real-world identities to everything we do, these platforms feel suffocating and unsafe. Is there any way we can construct fully virtual worlds not tethered to reality? I’m not sure we can.\n“In real life and on Facebook, you’re always signaling information about yourself which may not have anything to do with the context you’re in”: context-collapse\nIn this sense, Facebook’s social graph is a crutch that imitates real-world relationships. Platforms which 1) require or encourage using your real identity or 2) rely on existing social graphs (contacts, classmates, mutuals) merely imitate “being there.” By mirroring the real world, platforms like Facebook are ultimately limited by the very physical connections they seek to transcend.\nTogether, these platforms form digitally native social graphs based on what people choose to emphasize about themselves, rather than on legal identity or physical proximity. These social graphs are difficult to create on identity-based platforms, as well as in real life, and are the result of meaningful connections between people who otherwise would never have met.\nDecentralized Identities\nSource: How Identity Emerges in Crypto Networks by Graeme\nIs it possible to have pseudonymous identities in decentralized web3 systems? Or does it rely purely on natural human recognition of user handles? What about wallet addresses?\nWeb3 promises participants ‘persistant pseudonyms’ that have protective functions (e.g. not revealing offline identities, but also allows holders to participate in economic activity and hold goods while allowing them to hide undesirable traits that may lead to oppression)\nWe can then use on-chain transactions to hypothesize and analyze possible affiiliations and cultural attitudes. For example, owning an NFT whose profits will go towards a charity may signal values similar to that charity/cause.\nCan we use these web3 packed pseudonymous identities as a tool for social liberation?\nSee also: Zooko’s Triangle"},"thoughts/public-goods":{"title":"Public Goods","links":["thoughts/communities","thoughts/web3","thoughts/digital-commons","thoughts/tragedy-of-the-commons","thoughts/Making-and-Maintenance-of-OSS","thoughts/infrastructure","thoughts/group-limits","thoughts/positive-sum","thoughts/stone-soup-metaphor","thoughts/funding","thoughts/incentives","thoughts/Design-Justice"],"tags":["sapling"],"content":"Definitions\nDefining ‘public’\n\nThe vernacular term “public” is understood to be something of the people, freely available for use—such as parks, roads, and common lands.\n\n”A public can be identified and classified by shared problems, the extent to which they are aware of such problems, and the extent to which they do something about it.” Similarly, how do we define a community?\nCrypto and web3 overcome a lot of the bariers of web1 and web2, namely the inability to satisfy inalienable access to ‘public’ resources. The internet’s so-called ”public spaces” are nothing like our cities’ public parks. They are merely someone else’s private server, where access can be revoked at will.\nPublic also means creating venues in which people could begin to identify themselves as part of a collective whole.\n\nTo publish a web document or software application is more than making something public: it’s an act of making a public (Warner, 2002)\n\nDefining ‘good’\n”Why are public parks more desirable than public parking lots? This brings us to an important realization: any definition of public goods presupposes a shared understanding of what is in the public’s benefit, and why.”\nThese ‘goods’ exemplify a set of shared values within the ‘public’.\nAre there concepts of ‘free riders’ with public goods? What about tragedy of the commons? “It is clearly in the interest of society to promote the creation and consumption of public goods—be they vaccines, public libraries, or open source code”\nEconomical\nPublic goods are\n\nnon-excludable: it’s extremely difficult to stop someone from using the good, like roads and bridges\nnon-rivalrous: it’s abundant, and one person’s use of the good doesn’t substantially reduce the amount left for someone else (e.g. there’s more than enough air to go around)\n\nStatic open source (code that anyone can copy off the internet) is an example of a public good. Another example of these are public infrastructure like roads (although not necessarily, if you have your drivers license revoked you may not be able to make use of the roads in the first place).\nIt seems only natural that online media archives and open digital infrastructure should qualify as well.\nYet, no matter their claim to universality, instantiations of public goods are always local. The public cannot contain everyone. Locality is created and felt through [shared space, time, or experience]. It requires synchronicity of experience, rel: group limits\nClub goods are excludable but non-rivalrous. A lot of more recent app launches (e.g. Dispo and Clubhouse) used waitlists to create ‘fake scarcity’ to transition from a public good to a club good.\nRelated: positive sum worlds\nIncentives\nSource: Retroactive Public Goods Funding by Optimism\nHow do we incentivize people to contribute to the stone soup that is public goods funding?\nOne important advantage of startups over public goods is the possibility of an exit. Exits create incentives for upfront funding, hiring, motivation and alignment through equity, a share in the exit. However, for nonprofits, FOSS, and public goods projects, this “light at the end of the tunnel” does not exist. What do exits for public goods look like?\nWeb3\nSource\nCrypto protocols are one of the most compelling novel institutional forms, specifically deriving from their “public” qualities\n\nunrestricted membership + participation\nopen APIs\ntransparent allocation of resources and power\n\nYet not perfect\n\nOwnership of these resources are concentrated in a precious few (whales)\nThe ‘public’ that this infrastructure serves is mostly decentralized finance\n\nBoth of these main tokeholders share one common concern: price\n‘Voice’ in web3\n“The equivalence of stake and voice in crypto is reminiscent of early American democracy, in which political representation was conditioned on property ownership. Under this regime, only 6% of the total US population was eligible to vote—a laughably exclusionary idea of the body politic by today’s standards (Ratcliffe, 2013)”\n“The most consequential members of the crypto-public today are those who hold the most tokens, meaning even small holders are effectively removed from the conversation about what’s in their benefit.” Same concepts as design justice, we need to talk with and consider the marginalized peoples under existing systems of power so we can gain a deeper understanding of the public that we are building for and how to best serve them."},"thoughts/quadratic-funding":{"title":"Quadratic Funding","links":["thoughts/tragedy-of-the-commons"],"tags":["seed"],"content":"Sources: RadicalxChange and Vitalik\n\nQuadratic Funding (QF) is a more democratic and scalable form of matching funding for public goods, i.e. any projects valuable to large groups of people and accessible to the general public.\n\nMatching funding is where governments or other institutions match individual contributions to a project.\nIn the QF model, the total funding is the square root of each contribution summed and then squared. This empowers smaller individual contribution and make sure that a broad public benefits. This supposedly solves the tragedy of the commons problem.\n\nYour N-th unit of influence costs you N\n\nHow do we get around anonymous identities (or rapid creation of new identities) that abuse the system? Similar with collusion or vote-buying? How do we get around collusion (if we don’t, QF just collapses into one-dollar-one-vote)? “Quadratic payments do not solve every problem. They solve the problem of governing resources that affect large numbers of people, but they do not solve many other kinds of problems.”\nPairwise QF\nPairwise-bounded QF is a partial solution to collusion.\nPairwise-bounded QF computes the total subsidy to a project by looking through all pairs of contributors, and imposes a maximum bound on the total subsidy that any given pair of participants can trigger (combined across all projects). This also means that projects that otherwise would have gotten less money in QF could gain more because of the diversity of supporters garnered.\nPairwise-bounded QF generally penalizes projects that are dominated by large contributors.\nTraditional Models\nOne-dollar-one-vote\n\nTraditional funding model with platforms like Ghost knowledge really only fund articles that would be published because some individual would pay for it for themselves (essentially just patronage).\n\nPhrased less mathematically, either you value the article enough (and/or are rich enough) to pay, and if that’s the case it’s in your interest to keep paying (and influencing) quite a lot, or you don’t value the article enough and you contribute nothing. Source\n\nBut the problem in this case is that smaller contributors have too little influence and the larger contributors have too much\nOne-person-one-vote\n\nEach person gets a singular vote on whether a good gets produced. There is no ‘incentive’ or ‘room’ to contribute beyond that.\nThe problem here is that smaller contributors or people who have very little stake in the good have an outsized influence relative to those who have large stakes."},"thoughts/qualia":{"title":"Qualia","links":["thoughts/neural-networks","thoughts/machine-learning","thoughts/Goodhart's-Law","thoughts/intentionality"],"tags":["sapling"],"content":"Qualia is the Latin plural for the singular “quale” (quality). Often known as the subjective, conscious experience, or phenomenal properties.\n\nExamples of qualia include the perceived sensation of pain of a headache, the taste of wine, as well as the redness of an evening sky\n\nTraditionally have second-order properties\n\nIneffable (indescribable)\nIntrinsic (atomic, unanalyzable, cannot be split into constituent observations)\nPrivate (no interpersonal comparisons)\nDirectly apprehended (not mediated by thought, inference)\n\nVibes\nSource: Nameless Feeling in Real Life Mag\nNeural networks just assess vibes, they are the literal technical implementation of “no thoughts, just vibes”.\nVibes are very similar to the approximations machine learning systems use. Both suffer from Goodhart’s Law\nOn context-less data: Like vibes, these metrics carry no context or narrative; they can tell you nothing about how or why something might be desirable\nIs intentionality derivative with vibes/qualia too?"},"thoughts/quantization":{"title":"Labels, quantization, and legibility","links":["thoughts/Seeing-like-a-State","thoughts/Do-Artifacts-Have-Politics","thoughts/To-Live-in-their-Utopia","thoughts/terminology","thoughts/ontology","thoughts/Goodhart's-Law"],"tags":["sapling","pattern"],"content":"\n“Legibility is a condition of manipulation. Any substantial state intervention in society — to vaccinate a population, produce goods, mobilize labour, tax people and their property, conduct literacy campaigns, conscript soldiers, enforce sanitation standards, catch criminals start universal schooling — requires the invention of units that are visible”\n“The great advantage of such tunnel vision is that it brings into sharp focus certain limited aspects of an otherwise far more complex and unwieldy reality.”\n(Seeing like a State)\n\nBut yet, most complexities of human life fail to be marshalled into a single regulatory code; any codification would be partly arbitrary and artificially static. To codify local practices was thus a profoundly political act.\nThis process of legibility becomes dangerous when it forcefully shapes users. In the process of being made legible, nuance is excluded. Legibility means that ‘only what matters’ and can be quantified is kept; all else is discarded. This is especially dangerous when that legibility happens without the choice of the users.\nSee also: To Live in their Utopia, Seeing like a State\nLabels\nOur obsession of applying labels to everything extends to even whether a hot dog is a sandwhich or not.\n“Accuracy is more useful in entry-level jobs and for novices, because as skill increases, quantification of skill becomes harder.”\nWhy do we have labels in the first place?\n\nthey help us to communicate complex ideas between each other without having to explain our entire mental models\nthey are attached to societal connotations and perceptions of certain concepts\nthey give legitimacy in the form of social proof to concepts\n\nSee also: terminology, ontology\nConcepts\n\nGoodhart’s Law\n\n\nMcNamara Fallacy: Also known as the quantitative fallacy: making a decision involving purely quantitative observations (ignoring all others) is often wrong. Source\nProcrustean: an arbitrary standard is used to measure success, while completely disregarding obvious harm that results from the effort\n"},"thoughts/questions":{"title":"Questions","links":["thoughts/learning"],"tags":["sapling"],"content":"Source: Asking Better Questions in Kernel\n\nGood questions must come from a sincere desire to learn, rather than as a veiled means of stating your own opinion.\n\nAsking better questions is a skill, which means it can be honed and practiced.\nNo one likes feeling indept so many claim to know more than they actually do. Don’t lie about what you don’t know, and instead celebrate a state of not-knowing, “for therein lies both truth and liberation”.\nThe hope is to genuinely enjoy never knowing what you’re going to learn next\nSocratic Method\nA method of hypothesis elimination, where better hypothesis are found by eliminating ones that lead to contradictions.\nA Socratic Circle is an approach to understanding texts. It is based off of the assumption that all knowledge is connected to prior knowledge, all thinking comes from asking questions, and that one question should lead to asking further questions.\nUsually two ‘circles’ of students, where the inner circle explores and analyzes the text through questioning and answering while the outer circle observes. The outer circle gives feedback upon the first circle finishing and the two swap positions."},"thoughts/quorum":{"title":"Quorum","links":["thoughts/consistency","thoughts/idempotence"],"tags":["seed"],"content":"Read/Write Quorum\nIn a system with n replicas, we can ensure consistent\n\nwrites if a write is acknowledged by w replicas (write quorum)\nreads if we request reads from r replicas (read quorum)\n\ne.g. send 3 requests, only 2 have to come back. Choose most up to date based on timestamp\n\n\n\nKey thing to note is that r+w&gt;n, typically, r=w=2n+1​. This means that quorum is generally majority. Thus, reads can tolerate n−r unavailable replicas and writes can tolerate n−w unavailable replicas.\nThen the read will see the previously written value (as the read and write quorum share ≥1 replica). Client can then ‘repair’ the servers by sending its most up to state to servers that are out of date (with original logical timestamp! this is an idempotent operation, should be fine) — this is called read repair."},"thoughts/quotes":{"title":"Quotes","links":["thoughts/friendship","thoughts/skyhooks","thoughts/Alexandre-Grothendieck"],"tags":["evergreen"],"content":"Some quotes to live by.\n\n”Choose joy. Choose it like a child chooses the shoe to put on the right foot, the crayon to paint a sky. Choose it at first consciously, effortfully, pressing against the weight of a world heavy with reasons for sorrow, restless with need for action.” Source\n“In science, if you know what you are doing, you should not be doing it. In engineering, if you do not know what you are doing, you should not be doing it.” [Richard Hamming: The Art of Doing Science and Engineering]\n“Who are the people, ideas, and books that magnify your spirit? Find them, hold on to them, and visit them often.” (re: friendship)\n“Forgiveness is the alchemy by which the shame transforms into the honor and privilege of being invited into another’s darkness and having them witness your own with the undimmed light of love, of sympathy, of nonjudgmental understanding. Forgiveness is the engine of buoyancy that keeps the submarine rising again and again toward the light, so that it may become a lifeboat once more.” Source\n“If an architect believes for a moment that there are hooks in the sky to hang his creations from, he may be able to conceive of structures that he would otherwise not dare to think about. Once the design starts to take shape, he may then begin to see ways in which the essence of it can still be achieved without the need for sky-hooks at all. Maybe this will work for us, too.” (Source: Steve Grand, Creation: Life and How to Make It) (re: skyhooks)\n“People need to be able to shape, extend, reconfigure, and repair their environment to feel truly at home within it… And when there’s no space to call your own, there’s no opportunity to take refuge in quiet and solitude, and it’s more difficult to share space with others.” Source\n“Even if it’s not your ideal life, you can always choose it. No matter what your life is, choosing it changes everything.” Source\n“We’re all just walking each other home.” ― Ram Dass\n“磨刀不誤砍柴功” — Chinese Proverb. Taking a break to sharpen your saw will not delay you from cutting wood more\nBe humble at the mountaintops, be strong in the valleys, and be faithful in between. Source\n“Anything new is by nature without precedent — meaning, without data to know whether it will work or not. So when we approach building new things, we don’t optimize for metrics. We optimize for feelings.” Source\n“Yet I live earnestly, building the most beautiful sandcastles I can, knowing they will be washed away. And getting others on the beach to build with me, at times even suspending our belief of the fact that it will disappear; letting ourselves be fooled for a moment that it will last.” Source\n“You can’t build railroads before it is railroad time.” (Chuck Thacker)\n“Genius is no more than childhood recaptured at will.” (Charles Baudelaire)\n“In place of occasional experiences of depth that renew and satisfy us, we are simply given an infinite surface upon which to skim indefinitely.” (Source)\n“Many a failure of love follows on the—usually false—opinion that we have exhausted the other person’s inside, that there is no further promise of depth.” (Source)\n“When the winds of change blow — some people build walls, others build windmills.” — Chinese Proverb\n“Find out who you are and do it on purpose” – Dolly Parton\n“We cannot sow seeds with clenched fists. to sow, we must open our hands” (Adolfo Perez Esquivel)\n“The ultimate, hidden truth of the world is that it is something that we make, and could just as easily make differently” — David Graeber\n“Life can only be understood backwards but it must be lived forwards” (Soren Kierkegaard)\n“The invention of the ship was also the invention of the shipwreck.” (Paul Virilio)\n“History is made by those who show up” (Unknown)\n“Imagine you could play a computer” (Doug Engelbart, when inventing the keyboard)\n“Map the regions of your own affinity and interest, across all relevant dimensions: intellectual, aesthetic, moral. The rest, you can ignore freely. Ignore strenuously!” (Robin Sloan)\n“Twenty years from now you will be more disappointed by the things that you didn’t do than by the ones you did do. So throw off the bowlines. Sail away from the safe harbor. Catch the trade winds in your sails. Explore. Dream. Discover.” (Horace Jackson Brown Jr.)\n“Magic happens because you choose for it to - don’t forget it!” (Anson Yu)\n“If you want to build a ship, don’t drum up people to collect wood and don’t assign them tasks and work, but rather teach them to long for the endless immensity of the sea.” (Antoine de Saint Exupéry)\n“It is necessary to any originality to have the courage to be an amateur.” (Wallace Stevens)\nAlexandre-Grothendieck#^cfd6c7\n"},"thoughts/r-K-Selection-theory":{"title":"r/K Selection theory","links":[],"tags":["seed","pattern"],"content":"\nIn ecology, r/K selection theory relates to the selection of combinations of traits in an organism that trade off between quantity and quality of offspring\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr-selectedK-selectedmany offspring, low investmentfew offspring, high investmentthrive in unstable habitatsthrive in stable habitats\nr-selection\nr-selected species are those that emphasize high growth rates, typically exploit less-crowded ecological niches, and produce many offspring, each of which has a relatively low probability of surviving to adulthood\nExamples include dandelions, insects, rodents, etc.\nK-selection\nK-selected species display traits associated with living at densities close to carrying capacity and typically are strong competitors in such crowded niches, that invest more heavily in fewer offspring, each of which has a relatively high probability of surviving to adulthood\nExamples include elephants, humans, whales, etc."},"thoughts/rationality":{"title":"Rationality","links":["thoughts/utility","thoughts/Decisions-under-risk","thoughts/effective-altruism"],"tags":["seed"],"content":"An act is rational (roughly) if it is most likely to maximize satisfaction of the agent’s preferences or utility.\nBounded rationality: the choices that people make attempt to maximize benefit and minimize cost are imperfect: humans have a hard time precisely estimating benefit and cost and thus use imperfect heuristics to pick the most promising choices. (see: Decisions under risk)\nInteresting argumental ties to effective altruism, especially w.r.t. optimizing for benefit and minimizing cost."},"thoughts/react":{"title":"React in 30 minutes","links":["thoughts/declarative-programming","thoughts/composable"],"tags":["fruit","technical"],"content":"React is, as the website says, “a JavaScript library for building user interfaces.” What this means exactly, is often confusing to people.\nMany articles claim to teach you “React in 10 minutes” but rarely go beyond a few code examples. The problem is that this leaves you with an inkling of what React looks like and maybe how to write a bit of it, but no clue why it works and how to think about fixing React bugs.\nThis isn’t some random tutorial which throws you a bunch of example code that you go on to copy-and-paste and then leaves you hanging wondering what it does. The goal of the workshop is to give attendees a good foundation of the concepts within React, why people choose to use React, and how it works behind the hood. Hopefully, this gives you a good enough foundation to hit the ground running and avoid entire classes of bugs when starting out in React!\nThis post assumes that you know basic HTML, CSS, and JavaScript.\nWhat is React?\nThe TLDR; is that it’s a bunch of code you can add to your project that makes building websites and interfaces that react to changes when you do things (e.g. press buttons) really easy.\nReact has two distinguishing features that set it apart:\n\nReact is declarative meaning you tell React exactly what you want the page to look like and it’ll figure out how to do that.\nReact is component based meaning that you can create pieces of the UI that you can reuse across different parts of your application.\n\nHow a website works\nMost likely, you’ve written some code for a website in plain old HTML before. This, as you know, defines the structure for what is inside a webpage. If the webpage is a house, the HTML is the blueprint that tells you what the house should generally look like and what is inside.\nYet, to turn that bunch of HTML into something displayable in your web browser, it needs to parse it into a more intermediate format it can more easily work with: the Document Object Model (DOM).\nThe nested structure of the HTML lends itself very well to a tree-like structure which the browser can then efficiently traverse and make updates to.\nTurning HTML into the DOM and then into the actual site\nHow React works\nThe Virtual DOM\nOf course, you can still manipulate and interact with the regular DOM through JavaScript still but the main gripe that React has with this way of manipulating the DOM (e.g. adding new things to the screen or modifying existing bits on the page) is that it can be realllyyy slow in some cases.\nThis is where the mystical virtual DOM comes in. Modifying the DOM in a declarative way is difficult because modifying the real DOM is imperative by nature, making it difficult to write declarative applications. You could use element.innerHTML = &#039;your html&#039; in a declarative fashion, but an in-memory version implemented in JavaScript would be faster than using innerHTML, which is exactly exactly what React does. React creates a virtual DOM where it keeps track of what it would like the real DOM to look like and then intelligently batches and combines changes together to optimize and squeeze the most performance out of the few expensive writes to the real DOM that it actually does. More info on how DOM diffing and patching works under the hood for those curious.\n\nPlease note that Virtual DOM is not faster than raw, imperative operations on the real DOM (best performance). React uses the Virtual DOM because it is a relatively efficient way of declaratively representing your UI from state, and much faster compared to straight innerHTML calls.\n\nReact’s Virtual DOM in action. ReactDOM.render() tells React to attach your React components to the real DOM\nComponents\nAnother things that React tackles really well is that regular HTML and JS makes it a lot more difficult than it should be to re-use the same UI and logic across different pages on your website.\nWhat if you wanted to reuse this ‘product card’ component?\nThe main philosophy of functional React is that you shouldn’t override and inherit behaviour. Instead, get the behaviour you want by creating and composing reusable components. Each of these components can hold some sort of data of its own, called its state. Each component can also take in a few arguments or parameters called properties or props for short. Each component is composed up of either primative HTML elements (e.g. &lt;div&gt; or &lt;h1&gt;) or other React components. The elements that a component is made up of are called its children.\nReact Component Diagram\nBecause React uses composition to build components, there is a natural downward flow of information, where components pass data to their children. Thus, to change a parent’s state, the parent needs to explicitly pass a callback function that allows that behaviour to the child (e.g. an arrow function that wraps the setState function for the parent). This is detailed more in the ‘Building an App section’.\nJSX\nJSX is a extension to regular JS which lets you write HTML-like code to define React components. It is declarative at heart, meaning you describe exactly what you want the component to look like based off of some given data (props and state in this case) and React will figure out how to get the page to look like that.\nJSX can be just regular HTML.\n&lt;div&gt;Bottom Text&lt;/div&gt;\nIt can also be a user-defined component. In this case, the component has no children or ‘inner HTML’ so we can write its shorthand.\n&lt;ExampleComponent /&gt;\nWe can also pass in properties to a component by adding a tag with that property name. Any string can be passed in regularly, or you can also choose to pass in any JS expression (e.g. number, function, object, etc.) by enclosing it in curly braces.\n&lt;Hello name=&quot;World&quot; someData={2 + 3} /&gt;\nTo ‘compose’ components, you can nest them as children within each other.\n&lt;Dashboard&gt;\n  &lt;Greeting /&gt;\n  &lt;Statistics&gt;\n    &lt;BarChart data={data} /&gt;\n    &lt;PieChart data={data} /&gt;\n  &lt;/Statistics&gt;\n&lt;/Dashboard&gt;\nGreat, so that’s just what JSX looks like, how do we use it in React? The first thing to note is that a component in React is defined as a function which returns some JSX.\nHere’s a simple ‘Hello World’ component in React! We then write a return statement that defines exactly how to render this component (i.e. what children make up this component).\nNote that React passes us all of the props from the parent in a ‘props’ object. Then, in the return statement’s JSX, we can then access the name property by using {} to indicate a JS expression and accessing the ‘name’ field in the props object.\nimport React from &quot;react&quot;\n \nfunction Greeting(props) {\n  return &lt;p&gt;Hello {props.name}!&lt;/p&gt;\n}\nLet’s say we want to create a &lt;HelloWorld/&gt; component that re-uses our &lt;Greeting/&gt; component from above.\nimport React from &quot;react&quot;\n \nfunction HelloWorld() {\n  return (\n    &lt;div&gt;\n      &lt;h1&gt;My Hello World App&lt;/h1&gt;\n      &lt;Greeting name=&quot;World!&quot; /&gt;\n    &lt;/div&gt;\n  )\n}\nThen, at the end, we tell React to finally render our &lt;HelloWorld/&gt; component into the actual DOM by using ReactDOM.render(...). Here, we choose to replace the element with the id ‘root’ in the real DOM with our &lt;HelloWorld/&gt; component.\nimport ReactDOM from &quot;react-dom&quot;\n \nReactDOM.render(HelloWorld, document.getElementById(&quot;root&quot;))\nLife Cycle\nSo far we’ve covered static components using just plain old JSX and data passing using props, but this misses the biggest benefit that React offers: reactivity to data changes. We’ll look a little at the life cycle of React components to see where we can update a component when data changes.\nThere are three main parts to the life cycle to a React component. They are mounted, updated, and eventually unmounted.\nReact Component Lifecycle\nMount\nMounting happens when the component is first added to the virtual DOM. Here, we set the initial state of the component (e.g. to track the number of clicks, a user’s input, etc.) and tell React to update the DOM.\nUpdate\nAn update happens when its parent component is updated or when its state or props changes. Here, we can modify the state of the component, do asyncronous things (like call an API), and rerender the component.\nUnmount\nThe unmount happens when the parent component is no longer rendered to the DOM. Now, we cleanup anything that needs to be cleaned up, destroying all state, and removing the component from the DOM.\nHooks\nNote that when a component updates, React will call the component again with the new props thus clearing all local state. This is why you can’t just declare a new const inside a component for state. Introducing: hooks! They are JavaScript functions that allow us to ‘hook’ into the React lifecycle and do things like fetch data and persist state.\nuseState\nLet’s say we want to track how many times a user has clicked a button. A naive implementation might look something like this, where we define a variable in the scope of the component.\nimport React from &quot;react&quot;\n \nfunction Counter() {\n  let count = 0\n \n  return (\n    &lt;div&gt;\n      &lt;p&gt;You clicked {count} times&lt;/p&gt;\n      &lt;button onClick={() =&gt; count++}&gt;Increment&lt;/button&gt;\n    &lt;/div&gt;\n  )\n}\nHowever, this will not work! Because React doesn’t know about the count variable, incrementing it will not update the component and thus not re-render it onto the screen. To register a component’s state with React, we use the useState hook.\nThis will allow React to preserve the state between re-renders. useState returns a pair: a read-only variable representing the current state value and a function that lets you set a new value for the state.\nThe first parameter to the useState hook is the default value that React should assign to the state when the component is first mounted.\nimport React, { useState } from &quot;react&quot;\n \nfunction Counter() {\n  // Declare a new state variable called count initialized to 0\n  const [count, setCount] = useState(0)\n \n  // When the button is clicked, update the count to be\n  // the previous count + 1\n  return (\n    &lt;div&gt;\n      &lt;p&gt;You clicked {count} times&lt;/p&gt;\n      &lt;button onClick={() =&gt; setCount(count + 1)}&gt;Increment&lt;/button&gt;\n    &lt;/div&gt;\n  )\n}\nNow when the button is clicked, the count is then incremented by one. Because we used the useState hook, React knows about this component state and will then known to re-render the component when the state is updated.\nNote that you cannot directly modify the count variable because it is read-only. If you want to mutate it, use the setCount function that is returned from the hook.\nA note about syntax\nBecause this is just using JavaScript’s destructuring assignment syntax, we can call the variable whatever we want.\nconst [count, setCount] = useState(0)\n \n// is equivalent to\n \nconst state = useState(0) // state is an array of length 2\nconst count = state[0]\nconst setCount = state[1]\nuseEffect\nAsynchronous data fetching, manually changing the window title, and setting recurring events are all examples of side effects. Yet, React won’t let us do that right now because we have no way to do things in between the mount and unmount lifecycle events.\nIntroducing the useEffect hook, which lets us peform side effects within components.\nLet’s consider the previous example of keeping track of amount of times the user has clicked a button. What if we want to set the window title to be the number of times the button has been clicked so far?\nimport React, { useState, useEffect } from &quot;react&quot;\n \nfunction Counter() {\n  const [count, setCount] = useState(0)\n \n  // setup a new effect that runs everytime `count` is updated\n  // this will re-render the component too\n  useEffect(() =&gt; {\n    document.title = `${count} clicks`\n  }, [count])\n \n  return (\n    &lt;div&gt;\n      &lt;p&gt;You clicked {count} times&lt;/p&gt;\n      &lt;button onClick={() =&gt; setCount(count + 1)}&gt;Increment&lt;/button&gt;\n    &lt;/div&gt;\n  )\n}\nuseEffect doesn’t return anything, but takes two parameters. The first is the callback which is the effect we want to run, this callback takes 0 parameters. The second parameter is the array of dependencies for the effect. React uses this array to determine when to re-run our effect (you can optionally omit this entire array to re-run the effect every time the component updates).\nuseSWR\nWhat about making API requests to fetch user data or data from an external third-party? While technically you can do this with useEffect and useState, it gets messy really quickly.\nThis hook isn’t an official React one, but I use it so much in my projects that it might as well be. Introducing useSWR, a hook from Vercel that lets you make asynchronous HTTP requests extremely easily.\nimport React from &quot;react&quot;\nimport useSWR from &quot;swr&quot;\n \n// a function that performs an http request and returns a response\n// this can be reused across all of your requests\nconst fetcher = (url) =&gt; fetch(url).then((res) =&gt; res.json())\n \nfunction Profile() {\n  // example HTTP GET request to /api/user\n  const { data, error } = useSWR(&quot;/api/user&quot;, fetcher)\n \n  // we can use conditional rendering to display\n  // error or loading messages\n  if (error) return &lt;div&gt;failed to load&lt;/div&gt;\n  if (!data) return &lt;div&gt;loading...&lt;/div&gt;\n \n  // if the code reaches this point, we know\n  // that our request will have finished and have no errors!\n  return &lt;div&gt;Hello {data.name}!&lt;/div&gt;\n}\nOrder matters\nOne really common beginner mistake that I made a lot when first starting out is that hooks need to be called in the exact same order every time. This means that you can’t have hooks after any conditional returns, inside if statements, or inside function/callback definitions.\n// DON&#039;T: conditionally run hook\nif (someCondition) {\n  useEffect(() =&gt; {\n    // run effect here\n  })\n}\n \n// DO: put conditional inside the hook\nuseEffect(() =&gt; {\n  if (someCondition) {\n    // run effect here\n  }\n})\nFor more information, read the official React docs on this.\nBuilding an App\nGreat, so now you have all this theory. You’re probably itching to build something and that’s exactly what this section is for! We’ll be building a very basic todo application where you can add and remove from the list. For this bit, a basic understanding of Git and GitHub is required.\nFirst, make you install a version of Node &gt;= 12.x. If you are unsure, you can check your Node version by doing node -v.\n$ node -v\nv12.14.0 # good to go!\nStarter Code\nNormally, most people will tell you to run create-react-app here to create a new React application, but I find that create-react-app has way too much boilerplate (extra code) for my taste so I made my own more stripped down version of create-react-app. It contains everything you need to get started with a new React app and, in my opinion, is a lot less confusing.\nTo get started open your terminal of choice and clone this minimal React template. This will give you all the code you need to get started!\n# clone the repository\n$ git clone https://github.com/jackyzha0/min-react.git\n \n# navigate to folder\n$ cd min-react\n \n# install dependencies\n$ npm i\n \n# run local dev server! (open localhost:3000 in your browser)\n$ npm run start\nThen, let’s remove some of the starter files we don’t need. Delete the src/components folder. We’ll be working src/App.js for the rest of this tutorial.\nCreating the App component\nLet’s delete what’s in src/App.js and start fresh. What do we need in our todo app? Well, we need to track the list of todos the user has. Let’s create a state for that and initialize it with a few default todos. Then, the component should just be a container for all of our todo items. We can just map over the todos state and turn them into &lt;TodoItem&gt; components!\nWe can keep the styling from the template just so it looks a bit nicer.\nimport React, { useState } from &quot;react&quot;\nimport styled from &quot;styled-components&quot;\n \n// custom styling using styled-components!\nconst AppContainer = styled.div`\n  margin: 40vh 30vw;\n`\n \nfunction App() {\n  // setup todos\n  const [todos, setTodos] = useState([&quot;do laundry&quot;, &quot;finish homework&quot;])\n \n  // app is composed up of all the current todos\n  return (\n    &lt;AppContainer&gt;\n      &lt;h1&gt;todos&lt;/h1&gt;\n      {todos.map((item, i) =&gt; (\n        &lt;TodoItem key={i} name={item} /&gt;\n      ))}\n    &lt;/AppContainer&gt;\n  )\n}\n \nexport default App\nThe problem is, we don’t have a &lt;TodoItem&gt; component yet. Let’s make it!\nCreating a Todo component (Component Styling)\nWe need a component to display each of our todo items. The only prop we really need is the name of the todo to display it so we’ll make a simple component; no need to track state or anything.\nfunction TodoItem(props) {\n  return (\n    &lt;div&gt;\n      &lt;p&gt;{props.name}&lt;/p&gt;\n    &lt;/div&gt;\n  )\n}\nLet’s add a hover state so that the item has a strikethrough effect when you hover over it. We can do this using styled-components which is a way for us to add CSS to our components really easily.\n// create a &#039;styled&#039; div that has a strikethrough on hover\nconst TodoItemContainer = styled.div`\n  &amp;:hover &gt; p {\n    text-decoration: line-through;\n  }\n`\n \nfunction TodoItem(props) {\n  // replace the original div with the `TodoItemContainer`\n  // styled div we just created\n  return (\n    &lt;TodoItemContainer&gt;\n      &lt;p&gt;{props.name}&lt;/p&gt;\n    &lt;/TodoItemContainer&gt;\n  )\n}\nGreat! Now let’s see what that looks like.\nOur todo app with a strikethrough effect!\nDeleting a Todo (Passing callbacks)\nHmm, would be great if we could actually delete a todo by just clicking on a todo item. Good thing React has event handlers just like regular HTML elements do! Let’s modify our &lt;TodoItem&gt; to handle a click event. To do that, we just pass a callback to the the desired component using the onClick prop.\nfunction TodoItem(props) {\n  return (\n    &lt;TodoItemContainer\n      onClick={() =&gt; {\n        alert(`You finished ${props.name}!`)\n      }}\n    &gt;\n      &lt;p&gt;{props.name}&lt;/p&gt;\n    &lt;/TodoItemContainer&gt;\n  )\n}\nNow, when we click each todo item, our page will give us an alert saying we completed that item! Not super useful though. Can we just delete the item from the component?\nRecall from earlier that with React, information flows downward in the component hierarchy tree. This means that in order to modify the state of the parent (&lt;App&gt; in this case), we need to pass a callback that helps us modify the information.\nTo do that, we create a deleteTodo function in &lt;App&gt; and then pass an anonymous function to each todo which deletes that given todo. Note that because we can’t directly modify the todos variable as it is read-only, we create a copy of it using the spread syntax ([...todos]), then remove a single element by index using splice.\nfunction App() {\n  const [todos, setTodos] = useState([&quot;do laundry&quot;, &quot;finish homework&quot;])\n \n  // callback to remove a todo\n  const deleteTodo = (index) =&gt; {\n    // copy current todos\n    const newTodos = [...todos]\n    // remove todo at given index\n    newTodos.splice(index, 1)\n    setTodos(newTodos)\n  }\n \n  return (\n    &lt;AppContainer&gt;\n      &lt;h1&gt;todos&lt;/h1&gt;\n      {todos.map((item, i) =&gt; (\n        &lt;TodoItem key={i} name={item} deleteCallback={() =&gt; deleteTodo(i)} /&gt;\n      ))}\n    &lt;/AppContainer&gt;\n  )\n}\nThen, we update &lt;TodoItem&gt; to use this callback on click.\nfunction TodoItem(props) {\n  return (\n    &lt;TodoItemContainer onClick={props.deleteCallback}&gt;\n      &lt;p&gt;{props.name}&lt;/p&gt;\n    &lt;/TodoItemContainer&gt;\n  )\n}\nGreat, so now we have display and deletion. Adding new todos is up next!\nAdding a Todo (Form Inputs)\nFirst, let’s create a short form component that will allow us to accept user input. Let’s setup a state field for the user input and create a form with a single text input. To update the state of the component as the user types, we attach an onChange event handler to the &lt;input&gt; which sets the value of the todo state to whatever the input field is.\nfunction TodoForm(props) {\n  // form state\n  const [todo, setTodo] = useState(&quot;&quot;)\n \n  return (\n    &lt;form&gt;\n      &lt;input type=&quot;text&quot; value={todo} onChange={(e) =&gt; setTodo(e.target.value)} /&gt;\n    &lt;/form&gt;\n  )\n}\nNow, let’s handle what happens when the user submits this form (i.e. presses enter). We add an onSubmit handler to the form to handle this. Note that we specifically tell the browser to not refresh the page (which is the default behaviour for a form submission) and clear the form state.\nBy telling React that the value of the &lt;input&gt; is equal to the component state todo, React knows that this is a controlled component, meaning that the component state is the single source of truth here.\nfunction TodoForm(props) {\n  const [todo, setTodo] = useState(&quot;&quot;)\n \n  const handleSubmit = (e) =&gt; {\n    // prevent form from refreshing page\n    e.preventDefault()\n \n    // show an alert with user input\n    alert(todo)\n \n    // clear form\n    setTodo(&quot;&quot;)\n  }\n \n  return (\n    &lt;form onSubmit={handleSubmit}&gt;\n      &lt;input type=&quot;text&quot; value={todo} onChange={(e) =&gt; setTodo(e.target.value)} /&gt;\n    &lt;/form&gt;\n  )\n}\nLet’s add a bit of styling to make the text box not as ugly and add &lt;TodoForm&gt; to the end of the todos list so it actually gets rendered.\nconst TodoInput = styled.input`\n  padding: 0.7em 0.5em;\n  border: 1px solid black;\n  border-radius: 4px;\n`\n \nfunction TodoForm(props) {\n  const [todo, setTodo] = useState(&quot;&quot;)\n \n  const handleSubmit = (e) =&gt; {\n    // prevent form from refreshing page\n    e.preventDefault()\n \n    // show an alert with user input\n    alert(todo)\n \n    // clear form\n    setTodo(&quot;&quot;)\n  }\n \n  return (\n    &lt;form onSubmit={handleSubmit}&gt;\n      &lt;TodoInput\n        type=&quot;text&quot;\n        placeholder=&quot;Add a new todo...&quot;\n        value={todo}\n        onChange={(e) =&gt; setTodo(e.target.value)}\n      /&gt;\n    &lt;/form&gt;\n  )\n}\n \nfunction App() {\n  const [todos, setTodos] = useState([&quot;do laundry&quot;, &quot;finish homework&quot;])\n \n  const deleteTodo = (index) =&gt; {\n    const newTodos = [...todos]\n    newTodos.splice(index, 1)\n    setTodos(newTodos)\n  }\n \n  return (\n    &lt;AppContainer&gt;\n      &lt;h1&gt;todos&lt;/h1&gt;\n      {todos.map((item, i) =&gt; (\n        &lt;TodoItem key={i} name={item} deleteCallback={() =&gt; deleteTodo(i)} /&gt;\n      ))}\n      &lt;TodoForm /&gt;\n    &lt;/AppContainer&gt;\n  )\n}\nLooking snazzy!\nA nice new ‘Add Todo’ field!\nFinally, let’s link this up back to the main &lt;App&gt; state so that adding a new todo actually modifies the state of the app. We create a addTodo callback and pass this to the &lt;TodoForm&gt; component through using a addCallback prop.\nfunction TodoForm(props) {\n  const [todo, setTodo] = useState(&quot;&quot;)\n \n  const handleSubmit = (e) =&gt; {\n    e.preventDefault()\n    // use the provided &#039;addCallback&#039; prop\n    props.addCallback(todo)\n    setTodo(&quot;&quot;)\n  }\n \n  return (\n    &lt;form onSubmit={handleSubmit}&gt;\n      &lt;TodoInput\n        type=&quot;text&quot;\n        placeholder=&quot;Add a new todo...&quot;\n        value={todo}\n        onChange={(e) =&gt; setTodo(e.target.value)}\n      /&gt;\n    &lt;/form&gt;\n  )\n}\n \nfunction App() {\n  const [todos, setTodos] = useState([&quot;do laundry&quot;, &quot;finish homework&quot;])\n \n  const deleteTodo = (index) =&gt; {\n    const newTodos = [...todos]\n    newTodos.splice(index, 1)\n    setTodos(newTodos)\n  }\n \n  // callback to add a new todo\n  const addTodo = (todo) =&gt; {\n    const newTodos = [...todos, todo]\n    setTodos(newTodos)\n  }\n \n  return (\n    &lt;AppContainer&gt;\n      &lt;h1&gt;todos&lt;/h1&gt;\n      {todos.map((item, i) =&gt; (\n        &lt;TodoItem key={i} name={item} deleteCallback={() =&gt; deleteTodo(i)} /&gt;\n      ))}\n      &lt;TodoForm addCallback={addTodo} /&gt;\n    &lt;/AppContainer&gt;\n  )\n}\nFinished App\nAnd with that, you’ve finished your first React application! To recap, you’ve learned\n\nWhat React is\nHow React works\nThe React Component Lifecycle\nReact Hooks\nand much more!\n\nHopefully this leaves you in a really good position to becoming more comfortable with React. I’ll link a few more resources that I personally found really helpful in my understanding of React.\n\nA reintroduction to JavaScript\nReact in 100 seconds by Fireship.io\nOfficial Intro to React\nReact Intro to Hooks\n10 React Hooks Explained by Fireship.io\nReact Visualized\n\nOur working todo tracker!\n// Full Code\nimport React, { useState } from &quot;react&quot;\nimport styled from &quot;styled-components&quot;\n \nconst AppContainer = styled.div`\n  margin: 40vh 30vw;\n`\n \nconst TodoItemContainer = styled.div`\n  &amp;:hover &gt; p {\n    text-decoration: line-through;\n  }\n`\n \nfunction TodoItem(props) {\n  return (\n    &lt;TodoItemContainer onClick={props.deleteCallback}&gt;\n      &lt;p&gt;{props.name}&lt;/p&gt;\n    &lt;/TodoItemContainer&gt;\n  )\n}\n \nconst TodoInput = styled.input`\n  padding: 0.7em 0.5em;\n  border: 1px solid black;\n  border-radius: 4px;\n`\n \nfunction TodoForm(props) {\n  const [todo, setTodo] = useState(&quot;&quot;)\n \n  const handleSubmit = (e) =&gt; {\n    e.preventDefault()\n    props.addCallback(todo)\n    setTodo(&quot;&quot;)\n  }\n \n  return (\n    &lt;form onSubmit={handleSubmit}&gt;\n      &lt;TodoInput\n        type=&quot;text&quot;\n        placeholder=&quot;Add a new todo...&quot;\n        value={todo}\n        onChange={(e) =&gt; setTodo(e.target.value)}\n      /&gt;\n    &lt;/form&gt;\n  )\n}\n \nfunction App() {\n  const [todos, setTodos] = useState([&quot;do laundry&quot;, &quot;finish homework&quot;])\n \n  const deleteTodo = (index) =&gt; {\n    const newTodos = [...todos]\n    newTodos.splice(index, 1)\n    setTodos(newTodos)\n  }\n \n  const addTodo = (todo) =&gt; {\n    const newTodos = [...todos, todo]\n    setTodos(newTodos)\n  }\n \n  return (\n    &lt;AppContainer&gt;\n      &lt;h1&gt;todos&lt;/h1&gt;\n      {todos.map((item, i) =&gt; (\n        &lt;TodoItem key={i} name={item} deleteCallback={() =&gt; deleteTodo(i)} /&gt;\n      ))}\n      &lt;TodoForm addCallback={addTodo} /&gt;\n    &lt;/AppContainer&gt;\n  )\n}\n \nexport default App"},"thoughts/reading":{"title":"Reading","links":["thoughts/taste","thoughts/value-setting"],"tags":["sapling"],"content":"\nBooks, with some effort, can help you learn to walk, but only people can help you orient where to walk. In other (equally vague yet obvious) words, I can’t know (through books), if I don’t know what to know (through people)\n\nSee also: taste\nI used to think that to be a productive member of society, one just had to put their head down and try to churn out as much output as possible. I very much subscribed to the belief that if I just kept constantly producing new things, I would eventually learn from my mistakes and improve rapidly. I found that while, yes, this approach allowed me to build my technical skills really quickly, I applied it to everything — even things that didn’t need it. I held a metaphorical hammer in my hands and everything seemed like a nail.\nOver the summer, I began to read again. I started with technical write-ups, fiction novels, traversed into self-help, and to memoirs. I started to read more about the state of the world and critically discuss these with family and friends. Reading helped me colour in the lines as to why we need to build in the first place. I realized that the problems we try so hard to solve with technology are not tech problems, but human ones.\nI’ve started to write more about these ideas, at first to help me organize my own thoughts, but eventually segued into an excuse for me to talk to people about interesting ideas and get their perspective. It’s started a sort of chain reaction in a sense, with an observation from a book leading to a conversation with a friend to a blog post ad infinitum — leading me to be a more informed and curious individual."},"thoughts/recommendation-system":{"title":"Recommendation System","links":["thoughts/freedom","thoughts/stone-soup-metaphor","thoughts/quantization","thoughts/attention-economy","thoughts/supervised-learning","thoughts/unsupervised-learning","thoughts/latent-factor-model"],"tags":["seed"],"content":"Captivating algorithms: Recommender systems as traps\nMason’s definition of a trap: ‘an invention for the purpose of\ninducing animals to commit incarceration, self-arrest, or suicide’ (p. 657) — this is exactly what recommender systems get users to do: trap themselves in a viscous cycle.\nTraps operate through ‘scripted roles’ — the ability of the hunter to construct a mental model of its prey. It is not taking its free will to make decisions, but rather manipulating it to its own demise. Recommender systems, Seaver posits, are thought-traps.\nCold Start Problem: when one has no data yet. Without data, data driven recommendations do not work (see: stone soup metaphor)\nTemporarily taking off the veil of abstraction and seeing them for what they really are - pieces of human engineering: “Placing algorithmic systems alongside tripwires and trapdoors not only takes the shine off, reminding us that they, too, are products of ordinary human engineering; it also helps us think about how they work, the ways of thinking they depend on, and how they might be critiqued.”\n“Successful companies like Facebook have become successful, Eyal writes, by becoming ‘first-to-mind’: their users ‘feel a pang of loneliness and before rational thought occurs, they are scrolling through their Facebook feeds’… We can use ‘captology’ to designate this understanding of people in behaviourism inflected terms, as habitual minds with tendencies and compulsions that make them susceptible to persuasion and targets for capture.”\nOptimization metrics (see quantization)\n\nRMSE (root mean squared error) - how accurate the recommender systems were\n\nRMSE just doesn’t work up to a point because user preferences are inherently unstable or ‘noisy’ signals. These vary significantly with time/setting and posed a serious challenge to predictive accuracy\n\n\nTransitioned to ‘captivation metrics’ - ability of a system to capture user attention or ‘engagement’\n\nMoving towards interpreting behaviours (ex. skipping a video, clicking away, watch time, etc.) rather than explicit ratings (ex. asking users to give feedback on accuracy)\n‘Dwell time’: length of individual user sessions\n\n\n\nApproaches\nContent-based Recommendation\n\n“more things like this…”\nCompare the content of an item to user’s preferred items\nA form of supervised learning\n\nCollaborative filtering\n\n“users like you looked for…”\nBased on identification of similar users and their patterns of activity\nA form of unsupervised learning\n\nOne way of doing this is using a technique called matrix factorization, which is a latent-factor model for entries in matrix Y.\nLoss function:\nf(Z,W)=∥ZW−Y∥F2​+2λ1​​∥Z∥F2​+2λ2​​∥W∥F2​"},"thoughts/reflect":{"title":"reflect: NLP Model Explained","links":["thoughts/NLP","thoughts/supervised-learning","thoughts/LSTM","thoughts/latent-factor-model"],"tags":["technical","fruit"],"content":"How do we tell that this is a “valid” intent?\nA (not so) brief exploration of how we tackle classifying intents in reflect. Part 1 will touch on defining the problem we’re trying to solve, the data we have, and how we pre-processed it. Part 2 will focus on the architecture of the model we built, how well it does, and thoughts on improving it for the future.\nThe problem\nClassifying intents is at the core of reflect. When a user inputs an intent, its reflect’s job to figure out whether that intent should let them into the website.\n\nHow do we make sure “do some marketing work for reflect” is classified as productive but “watch cute dog videos” isn’t?\n\nWhy it’s so difficult\nA lot of it comes down to the fact that natural language processing (NLP) is a very difficult task. What does the sentence “learn about physics” mean, and how is it semantically different from “asdflkj I can’t do work”?\nWe can’t just parse for keywords and just allow a user in if we see the word “work” because that word can mean different things in different contexts. For example, “I’m not doing any work right now” would have otherwise been classified as valid. Thus, we can employ the help of a machine learning algorithm to help us capture this deeper meaning.\nSpecifically, the form of machine learning we will be using is called supervised learning, in which we give an algorithm a bunch of labelled data, tell it what it’s doing wrong, and let it ‘learn.’ Through doing this, hopefully the algorithm will be able to generalize and make predictions on unseen data too.\nThe data\nOf course, if we want to perform supervised learning, we’re going to need a lot of data. Where are we going to get all of it? Luckily, we were able to get it through 3 different sources.\nSurvey data (444 entries)\nOur team sent out an interest survey in early January in which we asked 3 questions:\n\nHow would you answer if you were trying to visit a distracting site while trying to focus? (eg. youtube, facebook, etc)\nHow would you answer if you were trying to visit a distracting site to take a quick break from your work? (eg. youtube, facebook, etc)\nHow would you answer if you were trying to visit a distracting site like Facebook but to do work? (e.g. make a marketing post)\n\nWe used these answers to form the basis of our very first dataset. We ended up getting a surprising amount of entries, ending with 148 responses to 3 questions and totalling 444 different observations. Here’s what some of that data looks like after tidying it up:\n\nWe classified any answer to Q1 as invalid and Q2 + Q3 as valid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nintentvalidI am watching a 5 minute video to take my mind off not being productive :/noI’m just borednoI’m bored/tired and trying to relaxno……work is pretty boring, i need to take a quick break I just can’t focus atmyesI finished all my work for today so I’m going to take a break now!yesgetting some advertising done for my clubyesI need to make a post on Twitter for my job announcing the latest update to our siteyesI’m trying to networkyes……\nClosed Beta (790 entries)\nAfter we made a basic dataset, we were able to make our first (admittedly not great) model. But in doing so, this let us create an MVP to which we could use to actually test with. We then deployed this model for use to our closed beta testers and collected their responses (with consent of course!) along with the website it was input on. This was then converted into a .csv file. A few examples are show below:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nintenturlto do some marketingfacebook.com/i cannot focus on my workinstagram.com/fuel my crippling social media addictionfacebook.com/……\nThese were then hand-labelled by our reflect team in a similar format as the survey data we collected earlier.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nintentvalidto do some marketingyesi cannot focus on my worknofuel my crippling social media addictionno……\nClosed Beta Corrections (37 entries)\nFinally, we created a Google Forms through which we directly asked beta testers if a decision made by reflect’s intent classifier was faulty. Specifically, we asked what they input, and what they expected. Here are a few examples:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninputvalidreply to a friend about a labyeslistening to a youtube music playlistyesgoof off as much as possibleyes……\nThis entire section of the dataset was only 37 observations, but it drastically helped us reduce false positives and false negatives by focusing specifically on misclassifications.\nAfter aggregating and combing all our data into one common format, we ended up with a grand total of 1271 observations. They looked something like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninputexpectedfail school by watching youtubenosdlkjasdnomaking a quick product post (should take &lt;10 min)yes……\nSo?\nNow that we have the data, can we just throw it into a machine learning model? Unfortunately, the answer is no, not quite yet.\nData augmentation\nOur dataset is still pretty small, even after aggregating all of our data. It most definitely doesn’t cover all of our bases for all the possible things that future users could possibly input. So, how can we “upsample” our data to get more of it? Well, it turns out that the field of Natural Language Processing has quite a few tricks to augment our data.\nSentence Variation\nEssentially what sentence variation is, is just replacing a few words of given sentences with their synonyms. If we have a sentence like “I’m trying to make a marketing post,” we can swap out singular words with their synonyms and tell our network that it means the same thing.\nIn this example, we could get something like “I’m attempting to fabricate a marketing post”. Adding these additional sentence variations will allow our machine learning model to learn semantic relationships between similar words (e.g. fabricate and make have similar meaning)\nSentence Negation\nAdditionally, if we add a ‘not’ in the sentence, it should flip the meaning of the sentence. An example would be “learning about physics” is valid, whereas “not learning about physics” is not. However, if the sentence already contains ‘not’ (e.g. “I’m not being productive”), adding another ‘not’ would make the sentence too complex and obfuscated, so we just label it as invalid. In essence, we just add ‘not’ to a bunch of sentences and label them as invalid. This helps us to combat intents which use negations in a sort of round-a-bout way to confuse the algorithm.\nShuffled Sentences\nIf we take an existing, valid sentence and completely shuffle the words, the resulting sentence should be invalid. An example would be “to watch a crash course video*” should be valid whereas “video course a watch crash to*” should be invalid. Basically, we’re just shuffling existing sentences and also labelling them as invalid. This helps us to combat intents which grammatically make no sense, but are otherwise valid.\nGarbage Sentences\nWe can also take completely random words from the English language and put them together. The resulting sentences should all be invalid. For example, “*untinkered phalangitis shaly quinovic dish spadiciflorous unshaved” *clearly makes no sense. However, the addition of these gibberish sentences allows our model to be more robust against foreign words and out-of-vocabulary terms.\nVocabulary-mix Sentences\nLast but definitely not least, we can take the most common words in our dataset and mash them together. This should yield us a bunch of sentences which contain “key words” but should be marked as invalid because the context in which they are used makes no sense. For example, “video look watching” and “get research facebook take need want need message watch” are both clearly gibberish sentences, but they have a lot of keywords that one would think would let you in (e.g. video, research, watching, etc.). This lets us be more robust against those who try to get around the algorithm using keywords.\nData preprocessing\nWhat about now? Can we throw it into the neural network yet? Well, no. We may have increased the amount of data we have to work with, but we also need to convert it to a form that is easy to understand for both the computer and for our algorithm. We do that with a series of functions that we apply on our data.\nStrip punctuation\nAs the name suggests, we remove all punctuation from the input phrase. We found that including the punctuation hurt our performance, most likely due to the fact that they offer very little in terms of semantic meaning and obfuscate the real meaning of the sentence.\nstripPunctuation(&quot;I don&#039;t know if I&#039;m being productive! :(&quot;)\n&gt; &quot;I dont know if Im being productive&quot;\nMake everything lowercase\nAdditionally, we found that in this particular setting, capital letters also didn’t matter that much. Because of the nature we collect our data (just a simple textbox), users tend to not bother with capitalization. As such, we remove it to make it consistent across our data.\nlower(&quot;I dont know if Im being productive&quot;)\n&gt; &quot;i dont know if im being productive&quot;\nExpand contractions\nThe English language does this weird thing where we can just smush two words together (e.g. “I am” to “I’m”). Unfortunately for us, these produce extra complexity in our model that we could reduce by making them all consistent. In our case, we chose to expand all of these contractions.\nexpandContractions(&quot;i dont know if im being productive&quot;)\n&gt; &quot;i do not know if i am being productive&quot;\nRemove stop words\nThe English language also has a bunch of these things called stop words — words that do not contribute anything major to the sentence in terms of semantic understanding (usually in the context of natural language tasks such as this). A few examples of them are ‘I’, ‘me’, ‘my’, ‘by’, and ‘on’. By removing them, we once again remove unneeded complexity from the model.\nrmStopwords(&quot;i do not know if i am being productive&quot;)\n&gt; &quot;not know productive&quot;\nTokenization\nHowever, these words are not super friendly for machine learning algorithms who would much rather deal with integers and floating point numbers than words and sentences. How do we fix this?\nWell, one thing we can do is turn words in indices by how often they appear, capped at the most common 1000 words — in essence, creating a vocabulary list mapping common words to numbers. For example, if “the” is the most common word, we convert it to 1. If “work” is the 8th most common word, we convert it to 8. For anything that isn’t in the top 1000, we give it the value of 0.\ntokenize(&quot;not know productive&quot;)\n&gt; [12, 35, 7]\nFixed sequence length\nFinally, we need to ensure that all the inputs are the same length of simplicity sake. If we were to handle dynamic length inputs, it would introduce a whole other level of complexity that we’re really not ready to deal with. As such, we can make sure all the inputs are the same length by adding a bunch of zeros to those who are too short (zero padding) or by slicing those who are too long.\npad([12, 35, 7], 10)\n&gt; [0, 0, 0, 0, 0, 0, 0, 12, 35, 7]\nFinally\nAfter all of these functions have been applied, we have successfully converted a complex sentence into a series of ‘tokens’ that is easy to understand for the machine learning algorithm.\npreprocess(&quot;I don’t know if I’m being productive! :(&quot;)\n&gt; [0, 0, 0, 0, 0, 0, 0, 12, 35, 7]\nWhat’s next?\nNow that we have something ready to feed into our neural network, let’s dive into how the actual model itself works! How do we tell that this is a “valid” intent?\nThe model\nThe type of neural network that we’ll be using is called Long Short-Term Memory (LSTM).\ncredit: Christopher Olah, 2015\nWhat’s so special about these networks is that they are really good at modelling time-series data, making it an ideal candidate for tasks like forecasting or natural language processing.\n# Function to create a RNN model with given parameters\n# max_seq_len: maximum token sequence length\n# vocab_size:  size of tokenizer vocabulary\ndef RNN(max_seq_len, vocab_size):\n    inputs = Input(name=&#039;inputs&#039;, shape=[max_seq_len])\n    layer = Embedding(vocab_size, 64, input_length=max_seq_len)(inputs)\n    layer = LSTM(64, return_sequences = True)(layer)\n    layer = Dropout(0.5)(layer)\n    layer = LSTM(64)(layer)\n    layer = Dense(256, name=&#039;FC1&#039;)(layer)\n    layer = Dropout(0.5)(layer)\n    layer = Dense(1, name=&#039;out_layer&#039;)(layer)\n    layer = Activation(&#039;sigmoid&#039;)(layer)\n    model = Model(inputs=inputs, outputs=layer)\n    return model\nHere’s how we define it in our Keras code, don’t worry if you don’t understand it just yet! We’ll explain it in the next few paragraphs.\ninputs = Input(name=&#039;inputs&#039;, shape=[max_seq_len])\nRight off the bat, you’ll notice that the first layer is the Input layer. Basically, this tells Keras to instantiate a new tensor (a multi-dimensional vector) with a given shape. In this case, we’re creating a one-dimensional tensor that is max_seq_len units long. When we trained our model, this was set to 75.\nlayer = Embedding(vocab_size, 64, input_length=max_seq_len)(inputs)\nNext up, we have the Embedding layer. We could get into a really technical discussion about what this really does, but you can think of it as a layer that helps the neural network to learn semantic relationships between inputs.\ncredit: Rutger Ruizendaal, 2017\nEssentially, it embeds tokens in a higher dimension vector space, where distance between tokens represents its similarity.\nlayer = LSTM(64, return_sequences = True)(layer)\nNow, we get into the meat of the neural network: the LSTM layer. As stated before, these LSTM networks are really good at modelling time series data like language. In this case, our LSTM network has 64 hidden units per cell, and that we’d like to pass these hidden states to the next layer. If you’d like to learn more about the inner workings of the LSTM model, theres a really good resource here!\nlayer = Dropout(0.5)(layer)\nNext, you’ll notice there are a few Dropout layers. These layers help to prevent overfitting by randomly killing off connections between the two layers (a sort of regularization). This makes sure neurons aren’t just “memorizing” the input data. This is especially important because our dataset is relatively small (~2000 observations even after augmentation), so making sure that our machine learning model can generalize outside of this limited dataset is really important.\nlayer = LSTM(64)(layer)\nWe have yet another LSTM layer! By having these two chained right after each other, the first layer can pass all the values of all of its hidden states to the second layer, effectively allowing a sort of ‘deeper’ neural network.\ncredit: Jianjing Zhang 2018\nThis deep LSTM allows our network to learn more abstract concepts, making them well suited for natural language tasks.\nlayer = Dense(256, name=&#039;FC1&#039;)(layer)\nNext, we have something called a Fully Connected layer, or a Dense layer. In a dense layer, each of the input neurons is connected to every output neuron. This kind of ‘glue’ layer helps the network to pick out and discriminate feature output by our previous LSTM layer.\nlayer = Dense(1, name=&#039;out_layer&#039;)(layer)\nlayer = Activation(&#039;sigmoid&#039;)(layer)\nSimilarly, we have one final Dense layer that ‘compresses’ all of the hidden units down to one neuron. However, we want the output value of this neuron to be how confident from a scale of 0 to 1 it is that the intent is valid. We do this by applying something called an activation function.\n\nIn this case, the particular function we chose is the sigmoid activation function, which looks something like the above.\nModel Overview\nPhew, finally got through everything! After putting it all together, we end up with a network that looks something like this:\n# 75 max_seq_len\n# 1000 tokenizer_vocab_size\nmodel = net.RNN(75, 1000)\nmodel.summary()\n \n# _________________________________________________________________\n# Layer (type)                 Output Shape              Param #\n# =================================================================\n# inputs (InputLayer)          (None, 75)                0\n# _________________________________________________________________\n# embedding_1 (Embedding)      (None, 75, 64)            64000\n# _________________________________________________________________\n# lstm_1 (LSTM)                (None, 75, 64)            33024\n# _________________________________________________________________\n# dropout_1 (Dropout)          (None, 75, 64)            0\n# _________________________________________________________________\n# lstm_2 (LSTM)                (None, 64)                33024\n# _________________________________________________________________\n# FC1 (Dense)                  (None, 256)               16640\n# _________________________________________________________________\n# dropout_2 (Dropout)          (None, 256)               0\n# _________________________________________________________________\n# out_layer (Dense)            (None, 1)                 257\n# _________________________________________________________________\n# activation_1 (Activation)    (None, 1)                 0\n# =================================================================\n# Total params: 146,945\n# Trainable params: 146,945\n# Non-trainable params: 0\n# _________________________________________________________________\nTheres a grand total of 150,000 different trainable knobs and parameters in our neural network!\nTraining pipeline\nSo, how does the data we got earlier play a role in helping our machine learning model learn and improve?\nThe first component is the loss function. This component tells the neural network how ‘correct’ its prediction was. In this model, we will use something called binary cross entropy, which is basically a fancy word for log-based error. If the true label is 1, we can then show what the log-loss would be for some given prediction probability.\n\nNext, we need to pick an optimizer. This component tells the neural network how to change its parameters to improve or ‘optimize’ itself. In this model, we chose to use an optimizer called RMSProp with a learning rate of 1e-3 . We aren’t going to cover all the technical details of this optimizer in this blog post, but just know that it is a very fast and effective optimizer.\nRMSProp(black) vs a bunch of other optimizers. credit: Vitaly Bushaev\nOne important hyperparameter we choose is the train-test split. In data science and machine learning, we typically withhold part of our data and set it aside as a test set. The rest of the data will be considered the training set. When training the model, we never feed it the test set. As a result, we can use the test set as a metric to see how well it would perform on real-world, unseen data. In our training, we used a train-test split of 20%.\nAnother important hyperparameter that we can choose is the mini-batch size. The mini-batch size determines how many training examples we feed the machine learning model before updating its parameters. A smaller mini-batch means that we get more frequent updates to the parameters, but it also runs the risk of having outliers that may cause a bad gradient update. A large mini-batch means that we get a more accurate gradient update but it also takes longer. A similar concept is sample size in statistics. We could pick a larger sample to get a better estimate of the overall population, but it is often more expensive to do so. A smaller sample might contain outliers and thus be less robust of an estimate of the overall population, but it very easy to do. So, there’s this tradeoff between accuracy and speed. We found that a good balance between these was a mini-batch size of 128.\nWe then trained our neural network over 10 epochs. A single epoch is one iteration over the entire dataset. If we train it for too many epochs, you run the risk of overfitting (memorizing the training data), but we don’t train it enough, we run the risk of not discovering a better model. One thing we can do it minimize this problem is through the use of cross-validation, which is a technique that lets us ‘test’ on portions of the training set. Essentially, at each iteration during training, we withhold a portion of the training set and use it as a sort of ‘validation’.\ncredit: Raheel Shaikh\nBy seeing when this validation accuracy goes down, we can get a pretty good idea of when our model begins to overfit on our data, and stop the training before this happens. In training our model, we will use 5-fold cross-validation.\nAfter all of this, we end with a training accuracy of 93.60% and test accuracy of 85.95%. Not bad at all!\nServing the model\nGreat! So now we have a trained model. Can we put that in the Chrome extension now? Not quite yet…\nOur model was written and trained with Keras (a Python Deep Learning library). Our Chrome extension is written in TypeScript. How do we get these two to work together?\nLuckily for us, Tensorflow.js exists! This library allows us to run Tensorflow models from within JavaScript. Tensorflow has released a script that lets us to convert a Keras model into something that Tensorflow.js understands, so we can run that to convert our models.\nHowever, we can’t just directly plug-and-play. You may remember that we did all of that data preprocessing before we trained our model. Tensorflow.js doesn’t have any of this built in, so we made our own implementation of it. You can check it out here.\nWe’ll leave all the technical code out (if you’re interested, feel free to peek around the source code!), but we’ve abstracted it enough that classifying an intent is a breeze.\n// declared somewhere earlier\nconst model: nn.IntentClassifier = new nn.IntentClassifier(&quot;acc85.95&quot;) // name of converted model\n \n// send to nlp model for prediction\nconst valid: boolean = await model.predict(intent)\nif (valid) {\n  // let through\n} else {\n  // block page\n}\nFuture improvement\nPossible models\nWe have thought about using something more established and complex like BERT and retraining it on our dataset, however then comes the problem of runtime and memory usage.\nBERT is a huge model. If you thought 150 thousand parameters was a lot, wait till you see BERT’s 110 million parameters. This bad boy takes a few hundred times longer and many times more memory than our current model. While yes BERT may perform really well, we just don’t think it has a place inside of a Chrome extension.\nOur model is decently robust as it is, especially considering the entire model is &lt;2MB and takes less than 200ms to run in browser. For now, we will stick with lightweight models, but we may switch if we find a better match in the future :)\nMisclassifications\nOf course, this algorithm isn’t perfect. It does have a lot of flaws and weaknesses that we find every day, and we’re working to fix those! If there are any misclassifications that you find in the algorithm, we love to hear about it on our feedback form: https://forms.gle/ctypb6FmDT9RQqjv6.\nClosing\nThis NLP model is at the core of reflect. It is this model’s goal to predict whether user intents are valid or not. As a result, we need to make sure this algorithm is accurate, fast, and lightweight. Hopefully, through this blog post, you’ve learned a little about how we went about building a model to fulfill those requirements.\nLearn more about us on our website! ✨ http://getreflect.app/\nIf you have any further questions about reflect or this NLP model, feel free to shoot us an email at hello@getreflect.app"},"thoughts/regularization":{"title":"Regularization","links":["thoughts/complexity","thoughts/Ensemble-method","thoughts/linear-regression"],"tags":["seed","CPSC340"],"content":"A method for controlling complexity. Our main tools:\n\nModel averaging (e.g. ensemble methods)\nRegularization (this)\n\nWhen we have multiple models with the same training models, we should pick models that are more conservative (e.g. in linear regression, pick smaller slope)\nWe should regularize wj​ so that they don’t explode.\nMakes the tangent to the level curves of the gradient point towards the global minimum\n\nL0-Regularization\n\nAdds penalty on number of non-zeros to select features\n\nf(w)=L+λ∥w∥0​\nL2-Regularization (Ridge Regression)\n\nGenerally decreases overfitting\n\nf(w)=L+2λ​∥w∥2\nThis almost always decreases test error. Bigger λ also means gradient descent converges faster.\nTo help with this, we can standardize continuous feature by replacing it with its z-score.\nL1-Regularization (LASSO)\n\nLike L2-regularization, it’s convex and improves our test error\nLike L0-regularization, it encourages elements of w to be exactly zero (though not as sparse)\n\nf(w)=L+λ∥w∥1​\nWe can actually combine this using an Ensemble method + bootstrapping (BoLASSO):\n\nCreate bootstrap samples\nRun feature selection on each sample\nTake the intersection of selected features\nReduces false positives\n\nHow is this different from L2?\nThe penalty stays is proportional to how far away wj​ is from zero. There is still something to be gained from making a tiny value exactly equal to 0.\nWith L2, the penalty gets smaller as you get closer to zero. The penalty asymptotically vanishes as wj​ approaches 0 (no incentive for “exact” zeroes).\nL1-Regularization sets values to exactly 0, basically removing features from the model"},"thoughts/religious-authority":{"title":"Religious authority","links":["thoughts/epistemic-authority","thoughts/meaning-laden","thoughts/terminology","thoughts/intentionality","thoughts/doxastic-partiality"],"tags":["seed","PHIL240A"],"content":"As it pertains to epistemic authority\nDebates from Classical South Asia\nRight and wrong aren’t directly perceptible to humans. However, some traditions in CSA say yes: we trust revelation/sacred texts.\nThe source must be āpta with regard to what’s right and wrong; must have:\n\nperfect knowledge of right and wrong\nthe ability to communicate this knowledge\nat least lack the intention to lie, if not have the intention to communicate honestly\n\nTheoretically, the Vedas are author-less so can have no faults or biases — uniquely trustworthy\nDharmakīrti against authority of the Vedas: even if there was a flawless revealed source, it wouldn’t help as the the revealed source still has to be interpreted and this necessarily happens through human interactions mediated by language\nPartiality makes communication possible, words are meaning-laden (see: terminology) because of conventions and usage\n\nNo one, then, can know the meaning of an ‘authorless’ word: “it is not possible in the case of words that lack an [original] expounder”\nCommon usage is partial and not an independent source of knowledge\n“Since the meaning of authorless words [can] be known neither from tradition, nor from reason, nor from the [ordinary] world, it is [only] proper [to say] that there is no cognition [of the meaning] in this case”\nSee also: derived intentionality, doxastic partiality\n\nCannot trust other humans who are also flawed\n\n“Indeed, a blind [person] does not find the way when led by [another] blind [person]!”\nSince no human being has overcome the confusion which is due to [moral] defects, as an expositor [of the Veda] he does not know the supersensible restriction [of Vedic words] to a particular meaning by himself\n"},"thoughts/rendering":{"title":"Rendering","links":["thoughts/NeRF","thoughts/coordinate-system"],"tags":["seed"],"content":"Projective Rendering\n\nFor each triangle of the object/mesh\n\nProject its vertices onto the screen\nFor each pixel in the triangle on the screen\n\nCompute the colour\n\n\n\n\n\nUtilizes parallelism to take advantage of SIMD (GPUs are fast at this!)\n\nVertex shader: run for every vertex to transform it to normalized screen space\nFragment Shader: run for every pixel to compute the pixel colour\n\nVisibility Methods\nHow do we avoid rendering things that don’t contribute to the final image?\n\nView volume culling, vertex level: Cull iff all vertices are outside w.r.t to a single view volume plane\nView volume culling, object level: Cull iff distance between center of bounding sphere and any view volume plane is greater than the radius of the bounding sphere\nView volume clipping: add/modify vertices so that they are all within bounds\nBackface culling: we never see the backside of the object, cull if Peye​ is below the plane of the polygon\nOcclusion culling, pixel level: use a z-buffer to determine depth at every pixel, only render if what you are about to render is closer (lower z) than what is currently in the buffer\nOcclusion culling, object level: build a bounding box and do a “virtual render” of the box. If no pixels passed the z-buffer then cull the whole object\n\nRaytracing\nClassical\n\nFor each pixel in the image\n\nGenerate a single ray from eye to the pixel\nP := Intersection of closest triangle with ray\ncolour_local :=\n\nshadow if not visible\nPhong(N, L, rayDir) if visible\n\n\ncolour_reflect := raytrace(R) if reflective\ncolour_transmit = raytrace(T) if refractive\ncolour = k_local * colour_local + k_reflect * colour_reflect + k_transmit * colour_transmit\n\n\n\nWe stop ray tracing if\n\nRay hits a diffuse object\nRay exits the scene\nExceeding some maximum recursion depth\nContribution to final pixel colour will be too small\n\nBecause it only uses a single ray, most shadows are pretty hard.\nThis is noticeably different from path tracing, which produces incredibly realistic looking renders that look indistinguishable from photos. Path tracing uses many rays per pixel with the colour averaged across them. At each interaction (bounce/reflection/etc.), the ray direction changes randomly with some distribution. However, this is significantly more expensive to compute.\nPath tracing\nWe can get more effects by adding more rays. Each path can be thought of as a random walk of light through the scene, with each bounce determined probabilistically based on the material properties of the objects and the lighting environment.\n\nAnti-aliasing: multiple samples per pixel\nMotion blur: multiple samples over time\nDepth-of-field (lens blur): multiple samples over lens aperture\nGlossy reflections: multiple reflected rays with random distribution\nSoft shadows: multiple shadow rays for area light sources\n\nCoordinate Systems\nWhich way is up?\n\nY is up\nZ is up\n\nCan also either be (imagine both of the following where X is thumb, Y is pointer, Z is middle)\n\nLeft-handed\nRight-handed\n\nSee also: NeRF, coordinate system\nIntersection Tests\nLine-plane\n\nPlane equation: F(P)=N⋅P+D=0\n\nN=(P2​−P0​)×(P1​−P0​), cross any three points on the plane that are not colinear\nP is any point\nD=−N⋅P1​ or D=−N⋅P2​\n\n\nLine equation: P(t)=Pa​+t(Pb​−Pa​)\nPlug line equation into plane equation and solve for t\n\nRay-Triangle\nRay-Sphere\n\nSphere equation: F(P)=r2−(x−C.x)2−(y−C.y)2−(z−C.z)2\n\nEach of x, y and z are equations of the form x(t)=Pax​+t(Pbx​−Pax​)\nC is the center point of the sphere\n\n\nTake smallest positive t value\n"},"thoughts/replication":{"title":"Replication","links":["thoughts/consistency","thoughts/message-broadcast","thoughts/idempotence","thoughts/State-Machine-Replication-(SMR)","thoughts/blockchain","thoughts/ethereum"],"tags":["seed"],"content":"\nA node that has a copy of the data is called a replica\n\nReplication is the act of ensuring consistency of data across replicas. If one replica is faulty, others are ideally still accessible\nOf course, if data doesn’t change, this is an easy problem: just copy it. Hard problem is when the data changes.\nCan take inspiration from hardware systems! RAID (Redundant Array of Independent Disks) which is used to replicate within a single computer fills a similar role but RAID has a single controlled whereas distributed systems have nodes that act independently.\nAn important concept in replication (and message broadcast) is making sure that we avoid cases where losing an ACK could lead to users doing an action multiple times (e.g. pressing the like button)?\nThis can be done by ensuring idempotence in our actions.\nSMR\nState machine replicationcCan be done by FIFO-total order broadcasting every update to all replicas. Whenever a replica delivers an update message, it applies it to its own state\nThis is what underlies a lot of blockchains, distributed ledgers, smart contracts, etc. (Ethereum is just one big state machine)"},"thoughts/representation":{"title":"Representation","links":["thoughts/context","thoughts/intentionality","thoughts/terminology","thoughts/Brains-in-a-Vat","thoughts/Twin-Earth-Argument"],"tags":["seed"],"content":"Symbolic Representation\nDretske just doesn’t agree that the Pioneer 10 space probe plate symbols will mean anything to any alien life. We find it easy to understand because we have the context and we live on this planet\nWhat sort of things can be representations?\n\nWords/pictures/numbers/diagrams\nState of mind, can represent almost anything at all\n\nWhat sort of things can be objects of representations\n\nAlmost anything (physical objects, sentences, numbers, moods, feelings, emotions, non-existent things)\nPictures represent by resemblance → derivative intentionality. This is the ‘resemblance theory of pictorial representation’, or the ‘resemblance theory’\n\nX represents Y → suggests that representation is a relation between two things\nIs there a basic type of representation underlying everything else? Crane says mental representations are the most basic level.\nWe can’t use pictures as they represent by resemblance. When we try to represent the difference between … and …, if … then …, and either … or … in pictures, we draw a complete blank. There just seems no way of doing it.\nWords don’t work either; they represent by convention (see terminology). Same words in same context mean the same thing to different people\nAll other kinds of representation all depend on mental representations in order to work\nMental representations are either naturalistic or conceptual\n\nnaturalistic → matter of correlation, “these spots mean measles”, spots are a reliable indicator of measles\nconceptual → “this read light means stop”, matter of convention, connection is arbitrary\n\nRelated thought experiments: Brains in a Vat, Twin Earth Argument"},"thoughts/research-debt":{"title":"Research Debt","links":["posts/collaborative-thinking","thoughts/knowledge-distillation","thoughts/teaching","thoughts/interpretive-labour","thoughts/notation","thoughts/attention","thoughts/creation-vs-maintenance","thoughts/incentives","posts/paid-open-source"],"tags":["sapling"],"content":"Source: Research Debt in Distill Pub\nPartly works because we can utilize collaborative thinking. Related: knowledge distillation, teaching\nThinking about accumulated societal knowledge as a mountain and understanding knowledge as climbing a mountain. Every time novel work happens, the mountain gets a little bit taller.\n\nMathematics is a striking example of this. For centuries, countless minds have climbed the mountain range of mathematics and laid new boulders at the top. Over time, different peaks formed, built on top of particularly beautiful results. Now the peaks of mathematics are so numerous and steep that no person can climb them all.\n\nYes, the climb is hard but it could be easier. Let’s build staircases for the mountains.\nHow do we reduce interpretive labour for learners?\nDebt in research\n\nPoor exposition: no good explanation of ideas and concepts\nUndigested ideas: it takes effort to polish ideas, developing the right analogies, language, and ways of thinking\nBad abstractions and notation : abstractions and notation are the user interface of research. To have bad notation is to have bad ways to interact with the underlying knowledge\nNoise: there’s too much new progress being made each day. How do we choose what to focus on? (see also: attention scarcity)\n\nNot just about poorly explained ideas, but rather the lack of ideas being digested and worked through in public (communal messiness of thought). How can we create better abstractions, notations, and visualizations to improve how we interact and interface with ideas?\n\nPart of thinking is having a conversation with ourselves.\n\nDistillation\n“Distillation is also hard. It’s tempting to think of explaining an idea as just putting a layer of polish on it, but good explanations often involve transforming the idea. This kind of refinement of an idea can take just as much effort and deep understanding as the initial discovery.”\nSo who should distill knowledge?\n\nNeeds to be more than one person: too much knowledge to polish every idea from scratch\nCannot be less skilled non-experts: refining and explaining ideas requires creativity and deep understanding\n\nIs distillation a form of maintenance?\nWhere are the Distillers?\nThe research distiller is an integral role for a healthy research community, yet almost no one is filling it right now.\nIs it because people want their work to look hard? Do people not enjoy distillation? While both of these may play a small part, the biggest part is malalignment of incentives (relevant to incentives in open source maintenance)."},"thoughts/research-institutions":{"title":"Research Institutions","links":["thoughts/money","thoughts/funding","thoughts/communities","thoughts/Fishbowl-effect","thoughts/trust","thoughts/infrastructure","thoughts/public-goods","thoughts/academia","posts/hackathons","posts/paid-open-source","thoughts/creation-vs-maintenance","thoughts/idea-list"],"tags":["sapling"],"content":"\nGreat hackers tend to clump together (PG, 2004)\n\nMoney is power. It dictates what type of research and work gets funded, and who gets to do it.\nIt’s important to be aware of the incentive structures in place wherever you work (e.g. academia, industry): 80k hours of work in your life, it matters a lot where that goes (voting with labour).\nOne of the reasons we’re not seeing another Xerox Parc/Bell Labs, it feels like there is too much perception in globalized communities. Intimacy is destroyed when the balance of internal/external balance skews too far towards external (Fishbowl effect). Without intimacy, there is no trust. Without trust, there is no exploration\nAs of now, there are no good spaces to work on long-term (think 10+ years in the future) research to enable the visions of the future. Also, is long-term innovation just infrastructure?\nLimitations of private companies and startups\n\nYou spend less than what you make (for sake of profit)\nTrade-off between intellectually interesting and profitable (? debatable but generally true)\nNot as much funding of public goods. Arguably, “predicting future public goods is as important a social function as predicting future private goods”\n\nCan we move away from depending completely on only one of\n\nGovernment funding\nMarket Influence\nAcademia\n\n\nThe best predictor of success in innovation is the number of other people they come into contact with\n\nSome thoughts. Already have been thinking about this in relation to hackathons, paid open source\n3rd Spaces\nFor people wanting to create change, why do we choose to do that through innovation rather than policy? (is this due to creation vs maintenance approaches to thinking?)\nProcurement services: finding a business application of research (specifically government funded)\nWe’re not building a utopia of research, this is more about finding a relationship/group of people you can be yourself with. It doesn’t need to be permanent, recognizing people have different stages in their lives. Once you’re done, you can move on.\nRelated: reinventing hackathons as 3rd spaces\nPARPA and alternative funding models\nhttps://benjaminreinhardt.com/parpa / https://benjaminreinhardt.com/wddw / https://www.nber.org/system/files/working_papers/w24674/w24674.pdf\nFrom the Atlantic: “In a recent paper, Pierre Azoulay and co-authors concluded that Howard Hughes Medical Institute’s long-term grants to high-potential scientists made those scientists 96 percent more likely to produce breakthrough work. If this finding is borne out, it suggests that present funding mechanisms are likely to be far from optimal, in part because they do not focus enough on research autonomy and risk taking.”\nAnother potential method is research labs becoming sustainable by bringing products to market\nOrgs\nNew Science\nhttps://newscience.org/\n\nNew Science will create a network of new scientific institutes pursuing basic research while not being dependent on universities, the NIH, and the rest of traditional academia and, importantly, not being dominated culturally by academia.\n\nTheir plan is to not replace traditional academic institutions, but to develop alternative/complementary ones to provide ‘competitive pressures’ on existing ones. New Science is to research as Silicon Valley was to entrepreneurship.\nIncentive and organizational structures are not everything. You can copy all of the US’s laws and structures of government and this will absolutely not lead to your country’s GDP per capita suddenly (or ever) jumping to $60k/year. Similar things can be said about research organizations.\n“Instead, I believe that the most promising way to achieve large-scale improvement in the way basic scientific research is organized is to start small, help individual scientists, and to make small steps towards a much better world.” Is this potentially good justification for minigrants?\nSanta Fe\nhttps://www.santafe.edu/\nThemes of research generally surround complex systems and a more system-based approach to analyzing and learning about the world: https://www.santafe.edu/research/themes\n\nComputers were becoming more powerful, and some scientists began to dream of a day when they might simulate highly complex systems, even living systems, in silico.\n\n“We weren’t disillusioned,” [Pines, the SFI Co-founder] says. “But we recognized that universities were ill-equipped to nurture emerging new fields, and we were thinking about how we could help them grow. If we could create an institution where they could flourish, we thought we could make a difference.”\n\nAll we needed was a few million dollars, a building, a staff, and a great deal of luck… The key was simply to create a refuge for brilliant scholars to interact in an environment that was free from boundaries – what one collaborator many years later called “a spa for the brain.”\n"},"thoughts/resource-interaction":{"title":"Resource Interaction","links":["thoughts/organizing-system","thoughts/interaction-design","thoughts/boundary-object"],"tags":["sapling"],"content":"Interactions arise naturally from the affordances of resources or are purposefully designed into organizing systems. Intersects with interaction design\nThe most common interactions are\n\nAccessing resources\nMerging resources\n\nDistinguishing interactions means looking at user requirements, resource properties used, and the legal, social and organizational environment.\nTo enable interactions, it is necessary to identify, describe, and sometimes transform resources.\nOne approach to resource transformation is to use a crosswalk or boundary object which are equivalence tables that relate resources from one organizing system to another\nAnother is to use data mapping, in which descriptions layers are compared and matched using either unidirectional or bidirectional links. This allows us to bridge the vocabulary problem in which different people refer to the same concepts slightly differently.\nAs for evaluation criteria, generally the most important aspects are\n\nEfficiency (timeliness and cost)\nEffectiveness (accuracy and relevance of results)\nSatisfaction (user sentiment)\n"},"thoughts/rhizomatic-vs-arborescent":{"title":"Rhizomatic vs Arborescent Systems","links":["thoughts/A-City-is-not-a-Tree","thoughts/blockchain","thoughts/semilattice","thoughts/Rhizome-Proposal"],"tags":["seed"],"content":"See also: A City is Not a Tree\nArborescent\nHierarchical, tree-like networks.\nComes from the way genealogy trees are drawn: unidirectional progress, with no possible retroactivity and continuous binary cuts.\nWhenever we have a tree structure, it means that within this structure no piece of any unit is ever connected to other units, except through the its parent (meaning the unit as a whole)\nDoes power always need to function top down?\n\n“The enormity of this restriction is difficult to grasp. It is a little as though the members of a family were not free to make friends outside the family, except when the family as a whole made a friendship.”\n\nThe structure of a blockchain follows this very closely. See also: semilattice\nRhizomatic\n\n“a rhizome has no beginning or end; it is always in the middle, between things, interbeing, intermezzo.” (Deleuze and Guattari in A Thousand Plateaus)\n\nA nonlinear and non-hierarchical network with no specific entry or exit points. Defined as a non-arborescent network.\nAlso what Rhizome is named after."},"thoughts/right-to-be-forgotten":{"title":"Right to be forgotten","links":["thoughts/digital-permanence","thoughts/deletion"],"tags":["seed"],"content":"Resistance against digital permanence\nRight to be forgotten\nDenegri Case and the Right to be forgotten\nRight for individuals to “determine the development of their life in an autonomous way, without being perpetually or periodically stigmatized as a consequence of a specific action performed in the past”\n“The public interest, which sometimes stands against the right to deletion and also the right to be forgotten, is the right of the public to know.”\n“By changing what we were, you change what we are and what we are going to be.”\nBarbra Streisand Effect: efforts to hide information result in even greater publicity"},"thoughts/rights":{"title":"Rights","links":[],"tags":["seed"],"content":"Close correspondence between rights and duties. If one has a right, others have a duty not to take that right away. For example, moral obligation to not take away the right to life of another.\nTypes of rights:\n\nA negative right is a right that another can guarantee by leaving you alone to exercise your right. For example, the right of free expression. In order for you to have that right, all others have to do is not interfere with you when you express yourself.\nA positive right is a right that obligates others to do something on your behalf. The right to a free education is a positive right\nAn absolute right is on that is guaranteed without exception\nA limited right is one that may be restricted based on the circumstances\n"},"thoughts/safety":{"title":"Safety","links":["thoughts/distributed-systems","thoughts/liveness"],"tags":["seed"],"content":"\nA promise in distributed systems that claims that “something bad” will never happen\n\nThis is obviously broad, but can have multiple flavours including never returning null fields, dead-locks should never happen, etc.\nInterestingly, all properties can be expressed as the intersection of safety and liveness properties"},"thoughts/scientific-progress":{"title":"Scientific Progress","links":["thoughts/progress","thoughts/academia","thoughts/philosophy-of-science"],"tags":["sapling"],"content":"Heavily attached to societal progress and the concept of defining what ‘forward progress’ even means.\nHas the scientific culture of being wrong now vs a few hundred years ago changed? Would Einstein survive in the modern age? Probably not.\nHypothesizing that this is leading to small incremental changes rather than monumental new theories/original work from being developed. (see: academia)\n\n“Science advances one funeral at a time”\n\nPeople get too attached to their theories. (e.g Planck and Einstein)\nSleeping Beauties\nThe “sleeping beauty” of science papers are papers that are dormant for 10 years then have a lot of citations.\nUsually indicative of work that was too ‘out there’ or didn’t align with existing incentive structures/measures of success when it was first published and got burried, only to be ‘re-discovered’ down the line.\nIterative science and Karl Popper’s Philosophy of science"},"thoughts/search":{"title":"Search","links":["thoughts/information-retrieval","thoughts/questions","thoughts/recommendation-system","thoughts/Bias-in-Search","thoughts/information","thoughts/trust","thoughts/attention-economy","thoughts/LLMs","thoughts/Prolly-Trees","thoughts/internet-computing","thoughts/Internet","thoughts/hermeneutical-injustice","thoughts/meaning","thoughts/reflect","thoughts/democracy","thoughts/censorship","thoughts/transparency","thoughts/privacy","thoughts/Data-Capitalism","thoughts/library"],"tags":["sapling"],"content":"\nSearch has co-opted the citation, vis-à-vis the hyperlink\n\nThe post-WWII ‘information explosion’ meant that we have a lot of info and not great means of looking through it to find what we want. This is a form of information retrieval and way of answering questions.\nInefficient search leads to\n\nknowledge gaps\nerrors\nrepeating research\ntimes spent searching\nknowledge silos\n\nFinding stuff is hard because\n\ntheres a lot of things (large haystack, small needle)\nsemantic meaning and intention is hard to extract/understand\nsearch systems are not complete\nindividuals have different requirements for what they are looking for (is it possible to create personalized search engines?)\n\nGood way to prototype search system is to analyze the typical dimensions that users search for things along and build those in. There should be support the five interaction strategies\n\nbrowsing\nknown item searching\nanalytical searching on one or more of the facets\nempirical searching based on user profiles\nsimilarity searching\n\nSearch is a form of recommendation system\nEssay on Bias in Search\nQueries on information\nHow do we convert a query into subject words and then locate these words in the catalogue if people describe things differently? How do we resolve synonyms?\nIncreasing number of words that describe an item will increase recall at the cost of precision.\nPrecision/recall trade-off\nThe trade-off between recall and precision decides whether a search finds all relevant documents (high recall) or only relevant documents (high precision).\nOn Ads\nWhen you monetize via ads, curation takes a backseat to featuring advertisers - there is just less digital real estate available to curate your own recommendations. This creates trust gaps between users and search engines.\nThe search box versus the feed; intentional browsing vs being served by an algorithm\nIs it possible to move away from the ‘feed’-based model of browsing the social internet?\n\nThe “feed”–an archaic form of content consumption that is effectively just a direct visual manifestation of the data structure that powers it – is a medium that is effectively designed to be consumed alone. –Humphrey Obuobi\n\n“I realized that the experience of research is exactly opposite to the way I usually often encounter information online. When you research a subject, you make a series of important decisions, not least what it is you want to research, and you make a commitment to spend time finding information that doesn’t immediately present itself. You seek out different sources that you understand may be biased for various reasons. The very structure of the library… allows for browsing and close attention. Nothing could be more different from the news feed, where these aspects of information—provenance, trustworthiness, or what the hell it’s even about—are neither internally coherent nor subject to my judgment. Instead this information throws itself at me in no particular order, auto-playing videos and grabbing me with headlines. And behind the scenes, it’s me who’s being researched.”\nBoutique Search Engines\nSource: Re-Organizing the World’s Information: Why we need more Boutique Search Engines by Sari Azout\nWe’ve returned to the curated web with Notion, Airtable, and Readwise collections.\n\nWhat started as a well-intentioned way to organize the world’s information has turned into a business focusing most of its resources on monetizing clicks to support advertisers rather than focusing on the search experience for people.\n\nThe big thing here is that horizontal/‘universal search’ sites like Google use the same interface to search everything, relying on natural language to decipher user intentions. Vertical search players like Yelp/Zillow filled the functionality and relevancy gaps by using more structured search formats appropriate to the medium.\nCuration, when thought of in the context of sharing bite-sized, isolated bits in feed-like architectures, is predominantly about entertainment, not utility. How do we move beyond the search bar in an attention economy?\nBut if we curate, who curates the curators? We then run into a meta-governance problem. Mirror’s Token Race appears to be a good start to answering this question.\nSearch Engine to Oracle\nSource: Language models like GPT-3 could herald a new type of search engine in MIT Technology Review\nTransition from aggregator of information to oracle\n\nPagerank: rank literally whats relevant, asks the user to determine what info they want to actually use\nOracle: just tells you the ‘right’ answer\n\n“The idea is that instead of searching for information in a vast list of web pages, users would ask questions and have a language model trained on those pages answer them directly. The approach could change not only how search engines work, but what they do—and how we interact with them”\nMetzler and his colleagues are interested in a search engine that behaves like a human expert. It should produce answers in natural language, synthesized from more than one document, and back up its answers with references to supporting evidence, as Wikipedia articles aim to do.\nSee also: LLMs\nSearch Engines as Faith\nThey freely provide, it seems, a sorting of the wheat from the chaff, and answer our most profound and most trivial questions. They have become an object of faith. Many view the results of search as objective.\n“Like gods, these mathematical models were opaque, their workings invisible to all but the highest priests in their domain: mathematicians and computer scientists.” (Cathy O’Neil)\nThey are the database of our intentions. We search for things we are hoping to know, hoping to do, and hoping to become\nFederated Search\nFederated search is a technique used to search multiple data sources at once. With federated search, you can retrieve information from many different content locations with just one query and one search interface.\nSee: Prolly Trees\nInternet topology as shaped by search engines\nSource: Critical Atlas of the Internet\n\n\nThe first cone represents the loss of distance. As shown in the picture previous hypothesis, terrestrial space converges at a specific point. On Internet, distance is not relevant. Everything is potentially one click away. The second cone reintroduces the notion of non-physical distances. On the Internet, distance has no relevance, but the notion of space nevertheless remains.\nSee also: internet computing, Internet\nSearch engines create an embedding space for the world that maps from physical to digital. Page ranking is the function that maps and remaps the ‘location’ and relatedness of real concepts.\nThe Vocabulary Problem\nIs this a form of hermeneutical injustice?\n\nLexical Ambiguity: the meaning of human language is not specific\nPolysemy: one word has many meanings\nSynonymy: many words mean approximately the same thing\n\nText Processing\nSimilar to practices in reflect NLP processing actually!\n\nLexical Analysis: break text into tokens\n\nBag of Words\n\n\nTransformations\n\nCase folding: convert all to lower case\nStopwords: remove words that contribute semantic meaning/indexing value\n\ne.g. words that don’t reduce information entropy\nfor example, words that appear in all documents are not very helpful\nzipf’s law, first ~5% are stop words, next ~45% are meaning content words and the last ~50% are long tail rare words\n\n\nStemming: grouping of related words (e.g. cats/cattiness → cat)\nTerm Weighting: assigns weights on basis of importance to document\n\nterm frequency (TF): occurrences in a\ndocument frequenc (DF): # of docs containing the term\nterm weight is usually nTF​DF1​ where n is the total word count\n\n\n\n\nIndex: unique id to a single document\nInverted Index: index which each entry holds list of pointers to all items with a certain property (e.g. title)\n\nGenerations\n\nPre-1998: keyword frequency and boolean operators\n1998-2010: link structure (page rank), keywords\n2010-2015: user data, personalization, NLP\n2015-now: AI, deep learning\n\nEthics\nMajor issues:\n\nAccess to information: value of information and knowledge, democracy, gatekeeping, censorship\nEquity, accuracy (truth) and transparency: search-engine bias and the problem of opacity/non-transparency\nPrivacy and freedom: personal privacy and informed consent; monitoring and surveillance\nProperty rights: how do crawlers/search engines respect property, ownership, free market competition\n\nSearch is a form of data capitalism\nRegulation\nGoogle Books\n\nGoal to scan 1 million books in 3 years\n\nBypassed copyright, which is highly problematic\nTried to fix this by partnering with 20 major libraries through their Publisher Program\nSecretive; tried to tell libraries not to tell each other about this partnership\n\n\nAiming to create a large scale digital library that included historical knowledge\nCritiques\n\nPoor quality of scanning\nPrivacy — Google would gain access into detailed insights about what people were reading\n\nHistorically, librarians focussed heavily on this, sometimes even going to jail due to resisting handing over library records\n\n\n\n\n2005 Class Action Lawsuit by author and copyright holders’ organizations: theft of books\n\n2008 settlement: Google agreed to pay copyright holders; revenue from orphan works; access through libraries\n2011 settlement thrown out over concerns of monopoly, privacy, European opposition\n2013 appealed\n2016 Supreme Court of US ruled that Google Books was legal under the Fair Use exception (claiming it expands public knowledge and understanding)\n\n\nCompetition policies\n\nSherman Act (1890): outlaws “monopolization, attempted monopolization, or conspiracy or combination to monopolize”\n\n\nAaron Swartz\n\nIn 2011, Swartz was arrested MIT police on state breaking-and-entering charges, after connecting a computer to the MIT network in an unmarked and unlocked closet, and setting it to download academic journal articles systematically from JSTOR in the hopes of freer access of information\nFederal prosecutors, led by Carmen Ortiz, later charged him with two counts of wire fraud and eleven violations of the Computer Fraud and Abuse Act\nSwartz was found dead supposedly by suicide in his Brooklyn apartment, though this is still disputed\n\n\n"},"thoughts/security":{"title":"Security","links":["thoughts/trust","thoughts/game-theory","thoughts/internet-computing","thoughts/TLS","thoughts/encryption","thoughts/hash-function"],"tags":["seed","CPSC317"],"content":"Source: Playdough Protocols in Kernel\nIntegrity of information is critical to relationships of trust as we saw in the episode on game theory.\nInternet\nOn security in the internet\nMain excuse: “when x was designed, there were only a few players and they all know and trusted each other”\n\n“Secure” web servers are the equivalent of heavy armoured cars. The problem is, they are being used to transfer rolls of coins and checks written in crayon by people on park benches to merchants doing business in cardboard boxes from beneath highway bridges. Further, the roads are subject to random detours, anyone with a screwdriver can control the traffic lights, and there are no police. — Garfinkel, Spafford, “Web Security and Commerce”\n\nA lot of the internet is based on good faith and relying on users to be good actors\nWhat level of the internet computing stack should be responsible for security? Options:\n\nNew network-layer protocol\nNew transport-layer protocol\nNew ‘pseudo-layer’ between transport and application (TLS, TLS)\nResponsibility of the application (SSH)\n\nTerminology\n\nconfidentiality: only the sender and the intended receiver should “understand” message contents\nauthentication: the sender and receiver want to confirm each other’s identity\nmessage integrity: the sender and receiver want to ensure that the message is not altered without detection\naccess and availability: services must be accessible and available to users\nActors: can be people or entities\n\nAlice, Bob: want to communicate securely\nTrudy: intruder, may\n\neavesdrop: intercept messages\ndelete/add messages\nimpersonation: can fake source\nhijacking: ‘take over’ ongoing connection\ndenial of service: prevent service from being used by others\n\n\n\n\nGiven that Trudy can see all the data, how do we provide confidentiality? Encryption!\n\nBlock Ciphers\n\nMessage is broken into blocks (e.g. 64-bits of data)\nEach block is encrypted/decrypted separately\n264 combinations for a 64-bit block!\nCipher-block chaining\n\nDo an additional operation with the plaintext\nE.g.\n\nXOR first block with arbitrary (randomly chosen) number known by both parties\nFollowing blocks are XOR’ed with previous block\n\n\n\n\nDES\n\n56-bit symmetric key, 64-bit plaintext input\nNo longer considered secure, 56-bit key can be brute forced in &lt;1 day\n3DES is more secure, do it 3 times with 3 different keys\n\n\nAES\n\nAlso symmetric, replaced DES as NIST standard in 2001\n128-bit block cipher\n128-, 192-, or 256-bit key\nWay more secure than DES\n\nBrute force decryption that takes 1 second for DES would take 149 trillion years for 128-bit AES\n\n\n\n\n\nCertification Authorities (CA)\n\nAuthority of who own’s what public keys\nWhen Bob wants Alice’s public key\n\nAlice provides a certificate\nCertificate is signed by CA\nBob applies CA’s public key to confirm certificate’s authenticity\nCertificate contains Alice’s public key\n\n\n\nPreventing Replay Attacks\n\nNonce - value that will only ever be used once (usually derived from clock time)\nA sort of challenge, Alice wants Bob to prove they have received the nonce by sending them back that same nonce\nEnsures this is a new conversation between Alice and Bob\n\nSee also: hash functions"},"thoughts/seeing":{"title":"Seeing","links":["thoughts/attention","thoughts/taste"],"tags":["seed"],"content":"Source\nSee also: attention, taste\nIn her 1984 novel The Lover, Marguerite Duras wrote that “the art of seeing has to be learned.”\nCognitive science now knows that our brains invest a great deal of resources in learning to unsee and tune out irrelevant stimuli, which is why “when you look closely at anything familiar, it transmogrifies into something unfamiliar.”\nPhysical\nReferencing a wonderful and wonderfully obscure 1960 book called Space and Light by a surgeon named Marius von Senden, Annie Dillard relays the numerous case studies of the first generation of patients on whom safe cataract surgeries were performed.\nThe notion of shadow and light was particularly incomprehensible, for shadow is evidence of depth and dimension — something the patients had never experienced and thus something that made no sense at all, that presented them with “the world unraveled from reason.”\nThe newly sighted were suddenly so overwhelmed by the world of light, form, and space that many retreated into their old ways of navigation and sensemaking, choosing to keep their eyes shut and to orient themselves via their familiar senses.\nThis, of course, is a metaphor at once incredibly elegant and incredibly jarring for how we all react to overwhelming new knowledge — especially knowledge about ourselves and ourselves in relation to our formerly familiar surroundings, our suddenly confusing inner world in relation to the suddenly nonsensical outer."},"thoughts/self-confidence":{"title":"Self-confidence","links":["thoughts/prestige","thoughts/optionality","thoughts/In-Over-Our-Heads","thoughts/taste","thoughts/identity","thoughts/interdependence","thoughts/emptiness","thoughts/friendship","posts/nothing-stops"],"tags":["seed"],"content":"\nWe need to have a strong sense of self — of who you are — so you can be unaffected by the way others might define you as\n\nWhen we don’t know what to choose it’s because we don’t know who we are. We outsource so much of our value system to others (see: prestige) because we haven’t yet figured this out for ourselves.\nSelf-knowledge feeds self-trust. Self-trust feeds self-knowledge. And the more you have of both, the less you need the approval of others. This helps us avoid the trap of optionality because with self-trust, we know what we want\nSee also: In Over Our Heads, taste\nIdentity\nEssay by Jenny Odell and in Kopi Chats on Substack\n\n“When I examine my identity, I do see an inalienable spirit grasping for infinity. But in the very same place, I also see an intersection of historical and cultural vectors, held up by a web of countless reliances.”\n\nReminds me of Indra’s Net, that everything is interdependent, empty of dependent origination.\n“Connective vulnerability is so frightening because sometimes we will be let down by others, unmoored from friendships and relationships we thought were our anchors in life. But that doesn’t mean we shouldn’t try. (see: nothing-stops)”\nBeing yourself\nSource\n\nI’ve been thinking about it like this: simply embrace what I actually want and like. Whenever possible, I should try to separate my motivations from the myriad other factors influencing my thoughts and behaviour: other people’s expectations, cultural norms, feelings of jealousy. The good life, says Rogers, comes from confronting your own actual desires, even if those desires are ugly and you do not yourself endorse them, and recognizing that that’s who you are. Deeply understanding your wants and needs is the only place from which you can actually change or grow.\n\nThe best way to know what a person wants to do is simply look at what they actually repeatedly do. I should just dive head-first into the things I actually want, rather than yearning for some set of things I need to force myself to want\nThe most creative people I know are actually just deeply in touch with their own taste, their own feelings, their own aesthetic desires and preferences, their own style, and then turn that into an artifact.\nBecoming an artist, they say, is about learning how to play in this way—the way that you cannot help but play—and turning it into a unique style.\n\n“find what you’re good at, then do that on purpose.”\n\nI find that the stuff that comes super easy to me, the stuff that doesn’t even feel like “work”, is the stuff other people are often impressed by or encourage me to do more.\nOn competition\nI never want to engage in zero-sum thinking—thoughts like “only some small number of us are going to make it”. I never want to feel like I’m entering a race. I do want to feel like I’m pursuing my own authentic goals and have fun.\nMy goal is to focus ruthlessly on being myself, be open to learning what my strengths are through feedback from others, and avoid the temptation to enter crowded fields at all costs. If I focus on being myself, the notion of competition disappears"},"thoughts/self-directed-growth":{"title":"Self-directed Growth","links":["posts/the-fools-who-dream","thoughts/independent-research","thoughts/taste"],"tags":["seed","pattern"],"content":"See also: the-fools-who-dream, independent research\nHow do we give the tools to people so learn for themselves? To treat growth as if it was play? To me, self-directed growth is the continued exercise of taste\nPractical Principles for Self-directed Growth\nFrom Recurse Center\n\ndirecting yourself means doing things that are motivated by your joy and curiosity, instead of by external pressures and fear\n\nWhen you have the freedom to explore things you’re interested in, and to explore them in the ways that make sense to you, you learn them more deeply and retain them for longer. When someone forces you to learn something that you’re not interested in, it doesn’t stick nearly as well.\n\nWork at the edge of your abilities\n\nSome call this flow: \nAlso recognizing that abilities are in flux on a day to day: “On a low day, it’s better to do what you can than to beat yourself up for failing to meet some hypothetical level of productivity that was never actually in reach, and on good days you can push yourself and really get to know the difference between what is impossible and what is just really, really hard.”\n\n\nBuild your volitional muscles\nLearn generously\n"},"thoughts/self-effacing-ends":{"title":"Self-effacing ends","links":[],"tags":["seed"],"content":"An end that cannot be achieved through direct pursuit, but only through pursuit of some other end. For example, meditation and achieving an empty mind, loving another, etc.\nParadox of hedonism: one cannot achieve pleasure by pursuing it directly, but only by devoting oneself to some other end (Sidgwick, 1907)"},"thoughts/self-knowledge":{"title":"Self-knowledge","links":["thoughts/seeing","thoughts/emergent-behaviour","thoughts/composable"],"tags":["sapling","PHIL240A","PHIL451A"],"content":"Reflexivity\nSelf-illumination/reflexivity\n\nEvery conscious experience is directly revealed to itself.\n“In seeing blue, you experience seeing”\nProblems: Reflexivity as a concept is weird though\n\n“A knife cannot cut itself, an acrobat can’t stand on their own shoulders.”\n“How can one and the same awareness be both of an object/content and of itself?”\n\n\n\nReflectivity\nOther-illumination/reflectivity\n\nFor a state to be a conscious state is for it to be the object of a higher-order mental state (an inner perception or inner thought).\n“I see blue, I’m aware that I see blue”\nProblems: how could two states that are nonconscious in themselves come together and make one of them conscious?\n\nCurious how this applies to emergent behaviour\nBy having a higher layer that enables the reference of a lower layer, this is essentially a two-way recursive relation that opens the door to much more composable and perhaps complex behaviour\n\n\n\nNagarjana’s Argument\n\nInfinite regress, you cannot use a fact to establish itself\nSummarized in terms of reflectivity\nOpponents Argument\nCan use recursive definition, a fact can have a ‘base case’\n"},"thoughts/selfish":{"title":"Selfish","links":["thoughts/effective-altruism","thoughts/sleep","thoughts/pain","thoughts/positive-sum","thoughts/the-Self","thoughts/Minecraft-End-Poem","thoughts/self-knowledge","thoughts/agency"],"tags":["sapling"],"content":"\nTo be able to provide care, you need to take care of yourself\n\nSelfcare → more altruism → create a better environment for self → betterment of the self\nIs there a difference between genuine altruism vs egoistic altruism?\nReverse machiavellianism\n\nmachiavellian is doing the good thing for a selfish ends\nopposite would be you do the selfish thing for a good ends\n\nGeneral thoughts:\n\nIn scarce situations - try to align yourself with people’s selfish reasons to get them to do things. It’s ok to be selfish in scarce situations if it enables you to better help others in the future (hill exploration analogy, its ok to be selfish if it means we can reach a better local max)\nWhen possible, try to make situations of abundance for yourself and others\n\nIs there a label for someone who prioritizes themselves so that they can best help others in the long term?\nI think I’m realizing that I am selfish in the sense that I will only help others if it is something I will enjoy doing and obviously that depends on who it is I’m helping, what I’m doing, my state of self, etc.\nThe catch is that I just really enjoy helping others in pretty much all situations (with a few exceptions, like when my own personal well-being is in jeopardy or the person is just an asshole)\n“We tend to wear our ability to get by on little sleep as some sort of badge of honor that validates our work ethic. But what it really is is a profound failure of self-respect and of priorities. What could possibly be more important than your health and your sanity, from which all else springs?” Source\nSee also: pain\nIn Tech\nSource: the technofuturist’s oath by Priya\nIf we don’t make our numbers, we won’t get the next round of funding. And without that funding, we won’t ever fulfill on our positive-sum, idealistic mission that will benefit everyone. is a very common thing to hear people working at startups scrounging at money say.\nDoes competition always kill positive sum worlds?\nIntelligently Selfish\nSource: How to be Intelligently Selfish — Dalai Lama\nHis Holiness skillfully redefines selfishness to mean the healthy pursuit of what’s best for yourself: what would bring you genuine happiness — caring about others.\nPorousness of the self\nSelfishness depends on what the definition of the self is. Expansions on the concept of ‘self’:\n\nnoosphere as the extended self\nThe Egg by Andy Weir + Kurzgesagt\nMinecraft End Poem\nself-knowledge\n\nNice vs Kind\nFrom Joice\nBeing nice is doing something you think will people happy. Being kind is doing something because you want to do it for them. The key is agency and boundary of self :))"},"thoughts/semantics":{"title":"Semantics","links":["thoughts/representation","thoughts/Twin-Earth-Argument","thoughts/convolutional-neural-networks","thoughts/data-distributions"],"tags":["seed"],"content":"\nWhat do words or sentences mean\n\nMeanings are public property: the same meaning be grasped by more than one person and by people at different times. Heavily related to representation\nImplications\nIf this argument is correct, then the content of our thoughts partly depends on what is in the world we live in: content determines object\nConcept of meaning rests on two unchallenged assumptions\n\nUnderstanding a word (knowing its intension) was just a matter of being in a certain psychological state\nMeaning of a term determines its extension (actual physical manifestation / what it is)\n\nThe Twin Earth Argument proves these two assumptions to be false: the extension of the term is not a function of the psychological state of the speaker by itself\nSociolinguistic hypothesis\nDivision of linguistic labour\n\nSome people wear gold rings\nSome people tell the difference between gold and non-gold\nNot everyone needs to tell the difference between gold and non-gold, rely on the judgement of experts\n\nFormally: “every linguistic community exemplifies the sort of division of linguistic labour just described; that is, it possesses at least some terms whose associated “criteria” a re known only to a subset of the speakers who acquire the terms, and whose use by the other speakers depends upon a structured cooperation between them and the speakers in the relevant subsets”\nLexical Development\nMental lexicon: mental dictionary of word knowledge (how it sounds, grammar, definition, etc.)\n\nWord: symbol that refers to something\nSymbol: stands for something without being a part of that something\nContext-bound word: things tied to particular contexts (word use is more specific than actual meaning)\nNominals: names for things\n\nNatural partitions hypothesis: the physical world makes obvious the things that take nouns as labels, whereas the meanings that verbs encode have to be figured out from hearing the verb in use\nRelational relativity hypothesis: possibility that verb meanings will vary from language to language (linguistic work showing that noun meanings are more similar across languages than are verb meanings)\nWord extension: to what extent is a word valid?\n\nUnderextensions: using words in a more restricted fashion\nOverextensions: using words in a more broad fashion (for related study, see Naigles &amp; Gelman 1995 study, results showed that overextensions are mistakes, they don’t indicate incorrect understanding of the words)\nProtowords (also known as phonetically consistent forms — PCFs)\n\nPhonetically consistent: the child uses the same word every time.\n\n\nThings that help with accurate word extension:\n\nTaxonomic extension: words to things are actually taxonomies (they are of the same category)\n\n\nTypes of language use, two ends of a continuum\n\nReferential language style: more object labels\nExpressive language style: relatively fewer object object labels and more personal/social words\n\n\nMapping problem: how do we know what the new word refers to?\n\nFast mapping: initial hypothesis about word meaning\nLexical principles/lexical constraints: guides that limit possible interpretations of new words\n\nWhole-object assumption: words refer to whole objects\nAssumption of mutual exclusivity: different words refer to different kinds of things. No category overlap\n\n\nLexical gaps: Sometimes things are not a one-to-one match – your language may not have a lexical item for something\n\n\nWord spurt: see Choi &amp; Gopnik (1995)\n\nAge at which children learn early words (first 50-100) can vary a lot due to\nEnvironmental Factors\n\nLanguage experience and input\nSocioeconomic status (SES)\nBirth order\n\n\nIndividual Factors\n\nProcessing speed\nPhonological memory\nPersonality and temperament\n\n\n\n\n\nDeep Learning Semantics\nImages\nSemantics in convolutional neural networks\nHidden units often correlate semantically-meaningful concepts.\nInceptionism: what about, instead of weights, use backpropagation to take gradient with respect to xi​. i.e., show me what you think a banana looks like\nStyle Transfer: loss function matches deep latent representation of content image C:\n\nDifference between zi(m)​ for deepest m between xi​ and C\nIntuition, deep layers zi(m)​ capture the semantics/concepts in an image, invariant to actual style\n\nAdversarial Examples: imperceptible noise that changes label/prediction. This is dangerous! We could repaint a stop sign and fool self-driving cars\nUsing semantics means it can learn bad correlations (e.g. correlating grass with cows so when it sees a cow by a beach it has no idea what it is)\nSee also: data distributions"},"thoughts/semilattice":{"title":"Semilattice","links":["posts/networked-thought","thoughts/causality","thoughts/Order-theory","thoughts/A-City-is-not-a-Tree"],"tags":["seed","pattern"],"content":"Visualizing graphs\nThe example the article used was to try and an orange, a watermelon, a football and a tennis ball. How will you keep them in your mind, in your mind’s eye? However you do it, you will do it by grouping them. Maybe two fruits and two sports balls together. In doing so, you construct a tree structure. Maybe you group it by size, two small spheres and two large spheres. This constructs another tree. But try as hard as you like, two trees combined form a semilattice. It is very difficult to hold a semilattice in your head.\nMaybe this is where tools like knowledge graphs come in. How might tools and visualizations allow us to better visually explore these graph relations and allow us to capture more complex relations in things?\n\nRegular note taking like Notion is hierarchical and linear: it’s a tree\nGraph and networked representations like Roam Research, Obsidian allow us to make non-obvious non-hierarchical connections between concepts\n\nAlgorithms\nA join semi-lattice essentially is a topological sort or causal ordering of its elements except all of the elements can be joined (i.e. have a single shared ancestor)\nSee also: Order theory\n\nAs the fabric of living things\nSee also: A City is not a Tree\nMore diagrams in Figma\n"},"thoughts/sequential-games":{"title":"Sequential Games","links":[],"tags":["seed","PHIL321A"],"content":"Represented with a game tree consisting of nodes (representing choices by players) and branches\n(representing different options)\nA player has perfect information if their information set has just one node (e.g., chess); imperfect if more than one (e.g., silent auction where you don’t know the other bids)\nTwo sequential games are equivalent if they have the same game tree."},"thoughts/skyhooks":{"title":"Skyhooks","links":["thoughts/fiction"],"tags":["sapling","pattern"],"content":"We should have nondystopian science fiction — there’s value in reclaiming an optimistic and hopeful future. I love the concept of skyhooks: dream about what you can build as if the sky had hooks you could hang your creations from (a metaphor for unlimited resources) and most of the time, you can still create that with scarce resources.\nSteve Grand in Creation: Life and How to Make It\n\nIf an architect believes for a moment that there are hooks in the sky to hang his creations from, he may be able to conceive of structures that he would otherwise not dare to think about. Once the design starts to take shape, he may then begin to see ways in which the essence of it can still be achieved without the need for sky-hooks at all. Maybe this will work for us, too.\n"},"thoughts/sleep":{"title":"Sleep","links":[],"tags":["seed"],"content":"\n“We tend to wear our ability to get by on little sleep as some sort of badge of honor that validates our work ethic. But what it really is is a profound failure of self-respect and of priorities. What could possibly be more important than your health and your sanity, from which all else springs?” Source\n\n“Sleep is the parentheses of your day; decide how much you need (I recommend 6-8), then be religious about getting that exact amount.” (On how to be a creative person with a job)"},"thoughts/small-technology":{"title":"Small Technology","links":["thoughts/privacy","thoughts/peer-to-peer","thoughts/interoperability","thoughts/colonial-debt","thoughts/digital-mindfulness","thoughts/friction","thoughts/Mangrove-Theory-of-the-Internet","thoughts/Tools-for-Conviviality","thoughts/cozy-software"],"tags":["seed"],"content":"Source\n\nSmall Technology is everyday tools for everyday people designed to increase human welfare, not corporate profits.\n\nPrinciples of small technology:\n\neasy to use\npersonal\nprivate by default\nshare alike\npeer-to-peer\ninteroperable\nzero knowledge\nnon-commercial\nnon-colonial\ninclusive\n\nThoughts: not sure I agree with how they say that the small web is single-tenant. Collaboration is important! We use the web not because its easy to own our stuff, but because it should be easy to do stuff with others.\nSee also: digital mindfulness, friction, Mangrove Theory of the Internet, Tools for Conviviality, cozy software"},"thoughts/social-contracts":{"title":"Social Contracts","links":["thoughts/infrastructure","thoughts/Social-Contract-Theory","thoughts/learning"],"tags":["sapling"],"content":"Social contracts are implicit agreements among members of social groups to cooperate for social benefits. They help us form shared cultures and values\nRousseau: everyone forfeits some rights so that they might also impose selected duties\ne.g. most democracies today, citizens agree to pay taxes in their shared currencies to fund and maintain basic infrastructure like roads, bridges, and electrical grids\nIn his book Leviathan, Thomas Hobbes argues that without rules and a means of enforcing them, people would not bother to create anything of value, because nobody could be sure of keeping what they created. Collaboration (and thus society) is possible only when people mutually agree to follow certain guidelines\nHobbes argues that everybody living in a civilized society has implicitly agreed to two things (collectively known as the social contract):\n\nthe establishment of a set of moral rules to govern relations among citizens (necessary if we are to gain the benefits of social living)\na government capable of enforcing these rules.\n\nSee also: Social Contract Theory\nSocial Rules for learning\nFrom Recurse Center’s Social Rules\n\nFor example, working at the edge of your abilities requires taking emotional risks, and the social rules help create an environment where it’s safe to do that. Letting someone know that they impacted you by breaking a social rule and accepting that feedback gracefully when you’re the one who messed up are important ways to learn generously. This allows everyone to keep working and growing together.\n\n\nNo well-actually’s: correcting someone about something that’s not relevant to the conversation or only tangential to what they’re actually trying to say. Not helpful and break the flow of conversation\nNo feigned surprise: acting surprised when someone doesn’t know something. Makes people feel bad for not knowing things and less likely to ask questions in the future, which makes it harder for them to learn\nNo backseat driving: lob advice from across the room without really joining or engaging in a conversation. Even if your advice is correct, it’s rude to bust into a conversation without asking. If you overhear a conversation where you could be helpful, the best thing to do is to ask to join.\nNo subtle-isms: Subtle-isms make people feel like they don’t belong. We want to create an environment where everyone can focus all their energy on programming.\n"},"thoughts/social-graphs":{"title":"Social Graphs","links":["thoughts/pseudonymity","thoughts/A-City-is-not-a-Tree","thoughts/data-changes-the-application"],"tags":["sapling"],"content":"\nSocial graph: who’s connected to who\n\nHow do social graphs work if users are pseudonymous? If cities are not top-down hierarchies, then are social graphs a better way to think about them?\nDoes directness matter in social graphs? i.e. Facebook is undirected (friend means you are friends with each other) whereas Twitter is directed (you can follow someone but doesn’t mean they follow you back)\nSocial Graph vs Interest Graphs\nInterest graph: online representations of interests\n\nSome social media construct social graphs from interests (e.g. TikTok)\nSome social media construct interest graphs from social graphs (e.g. traditional networks like Facebook)\n\nSee also: data changes the application"},"thoughts/sociolinguistics":{"title":"Sociolinguistics","links":["thoughts/syntax","thoughts/morphology","thoughts/phonology"],"tags":["seed"],"content":"\nHow children learn to use their languages in socially appropriate ways that reflect their status in the community and the social context\n\nVariation\n\nConstrained variation: distribution of a variant is not random or free, there are systematic correlations with independent factors.\nFree variation: accounts for cases where some variants seem to alternate with each other without any reliable constraints in a particular context or when used by a certain speaker\nInterspeaker: variation occurs between different speakers within or across speech communities\nIntraspeaker: variation occurs between within a single speaker, but might depend on interlocutors and context (= inherent variability)\nGroup differentiation: social/regional varieties index (mark) group boundaries\nVariation factors\n\nSpeaker-driven factors\nAudience-driven factors\nTask-related factors\nLinguistic factors\n\n\n\nLanguage vs Dialect vs Accent\n\nLanguage: not mutually intelligible communication systems\nDialect: mutually intelligible, but have differences in sentence structure (syntax), how words are made (morphology), or word choice (lexicon)\nAccent: mutually intelligible, only have differences in phonology (sounds)\n\nRate and Time\n\nOchs (1985)\n\nHow are phonological registers, word order, and ergativity acquired by children learning Samoan (and how does their speech compare to adult patterns)?\n\nChildren use features of both registers even at one word stage, but do not show recognition of appropriate contexts until multi-word stage (2- 2.5 years), children start producing tautala lelei first and then tautala leaga\n\n\nErgativity acquired relatively late, appears to match a sociolinguistic norm in the community\n\n\nTypes of time\n\nReal Time: chronological time based on year\nApparent Time: an estimate of time based on speaker age or date of birth\nWhen we look at a speaker age as a proxy for time, we must make assumptions what things are stable in language and at what ages.\n\n\n"},"thoughts/software-and-politics":{"title":"Software and Politics","links":["thoughts/Do-Artifacts-Have-Politics","thoughts/democracy","thoughts/trust"],"tags":["seed"],"content":"\n“Computers play a fundamental role in making the world — and above all the built structure of the world — alive, humane, ecologically profound, and with a deep living structure”\n— Christopher Alexander to a room of software engineers\n\nDo Artifacts Have Politics? Yes, they do.\nPolitics for Software\nSource: Politics for Software Engineers by Steven Buss\n“Blockchain voting addresses how to vote in a trust-less society, but a trust-less society cannot have a functioning democracy.” (see: trust)\nTrust is more important that provably correct\n“I think where most software engineers fail at politics, it’s in understanding that key point. We tend to over engineer our systems and never need to explain the inner workings to anyone who is non-technical. This isn’t restricted to just software engineers, of course. Any sufficiently advanced technology is difficult to explain in terms a non-expert can understand. But we do, at least, understand that we need to tailor the front-end user experience to the target demographic.”\nCurious how this ties into the [over simplification of user interfaces](/thoughts/books/mindstorms#Microworlds and simplification)\n“Blockchain voting fails at the most basic test of social technology: can you explain how it works to someone skeptical of the people in power in a way that makes them trust the system?”\nValues and Technology\n\n“Technology is the result of human imagination – of human beings envisioning alternatives to the status quo and acting upon the environment with the materials at hand to change the conditions of human and non-human life. As a result of this human activity, all technologies to some degree reflect, and reciprocally affect, human values.” (Friedman and Hendry, 2019)\n"},"thoughts/software-principles":{"title":"Software Principles","links":["thoughts/A-Pattern-Language","thoughts/local-first-software","thoughts/distributed-systems","thoughts/maintenance","thoughts/composable","thoughts/constructionist","thoughts/emergent-behaviour","thoughts/programming-models","thoughts/neural-networks","thoughts/tinkering","thoughts/notation","thoughts/interoperability","thoughts/agency","thoughts/play"],"tags":["sapling"],"content":"\nA Pattern Language for developing software\n\nInspired by Urbit Precepts and the Design Principles Behind Smalltalk\n\nLocal first. Decentralized/distributed systems second. Make self-hosting easy\nThere is a latent cost to new features: maintenance\nHeuristics should only be used where determinism is infeasible\nApps should embrace gradual enrichment, composing smaller features to form more complex ones when necessary\nDump the parts bucket onto the floor. Make it obvious what tools the user has at their disposal to make the most out of the software (constructionist learning)\nIf a system is to serve the creative spirit, it must be entirely comprehensible to a single individual\nAny system should provide a uniform means for referring to the things in its universe. In fact, an even stronger statement is that systems should be designed around a powerful metaphor that can be uniformly applied in all areas. Only from this can complex systems emerge\n\nSee also: programming models\nOn programming advice\nJamie Brandon’s Reflections on a Decade of Coding\nProgramming practices are mostly tacit knowledge. Tacit knowledge isn’t easy to share. An expert will relate some simple-sounding rule of thumb, but then grilling them on specific cases will quickly uncover a huge collection of exceptions and caveats that vary depending on the specific details of the situation.\nIt’s so easy to think that simple solutions exist. But if you look at the history of ideas that actually worked, they tend to only be simple from a distance. The closer you get, the more you notice that the working idea is surrounding by a huge number of almost identical ideas that don’t work. Take bicycles, for example. They seem simple and obvious, but it took two centuries to figure out all the details and most people today can’t actually locate the working idea amongst its neighbours. Even when old niche ideas make a comeback (eg neural networks) it’s not because they were right all along but because someone recognized the limitations and found a new variation on the idea that overcame them (eg deep learning). Finding the idea that actually works amidst the sea of very similar ideas that don’t work requires staying curious long enough to encounter the fine-grained detail of reality and humble enough to recognize and learn from each failure.\nMainstream is mainstream for a reason. The frontier is the place to go mining for new ideas, but it’s 1% gold and 99% mud.\nFor playful software\nSource\n\nExtremely easy to use. “The prime directive. The program should be extremely easy to use. No manual should be needed and program features should ‘explain themselves’ through use. All tasks should be able to be performed in the simplest, most straightforward way. The program should go out of its way to meet the user.”\nSurprising and satisfying. “As long as The Prime Directive is not violated, every opportunity should be taken to make the program surprising and satisfying to use. No opportunity should be missed. The process of making a picture should be as important as the picture produced.” This can be haptics, sound effects, satisfying animations, intermediary feedback, etc.\nLow floor, high ceiling. The program should be extremely accessible by default, but allow for very complex functionality. Toward this, I’ve been trying an expert mode toggle, like taking off the training wheels. (see also cognitive dimensions of notation)\nEncourage messing around. Make actions undo-able, and permanent destruction near impossible. Make it easy and quick to iterate. Unstick people – even random color pickers help.\nReal-world “interoperability.” The more people can bring in their existing knowledge – artistic abilities, drawing, writing styles – and integrate the outputs of the software into their lives – send a friend a link, transferable skills, show a printed poster at home – the better. The “less segregated from ordinary life,” the more natural and frequent its use.\n\nAbove all, people need agency. They need to feel in control. Sometimes, that means designing for subversive behavior. I mean, isn’t the most fun often had when you’re breaking rules? But this is enormously difficult in software, where you must design almost everything from scratch. Unlike life, you don’t get a common repertoire of actions for free – burning, tearing, bringing friends in, etc. How do we design for baseline functionality and unexpected, emergent play?"},"thoughts/soulbound":{"title":"Soulbound","links":["thoughts/game-design","thoughts/games"],"tags":["sapling"],"content":"Source: Soulbound by Vitalik\nNon-transferability\n\nA soulbound item, once picked up, cannot be transferred or sold to another player.\n\nIf you take the proverb that “those who most want to rule people are those least suited to do it” (Douglas Adams, The Restaurant at the End of the Universe) seriously, then you should be suspicious of transferability, precisely because transferability makes governance power flow away from the meek who are most likely to provide valuable input to governance and toward the power-hungry who are most likely to cause problems.\nGame design\nSoulbound in the context of video games (specifically Realm of the Mad God). I think this is such an interesting case study because the game itself is perma-death, meaning that items on characters are permanently deleted/lost on a character death.\nSoulbound is an interesting mechanic that has had a lot of implications for gameplay and in-game economy:\n\nPlayers cannot trade these items. This incentivizes participation for actual gameplay loops + events\n\nFor example, when the game decides to host an event by boosting item drop rates for certain dungeons, people actually just play the game more rather than accumulating wealth and just buying the items off of farmers seeking to make a profit\n\n\nHowever, this enables a secondary (illicit) market that trades accounts rather than items. This only accounts for a very small minority of trades as the average user does not use more than a single account\nThere is certain prestige that comes with owning certain items (e.g. &lt;1% drop rate from one of the most difficult dungeons in the game)\n\nThis is a visual signifier for one of three things: skill, veterancy, or dedication\n\n\nAs a result, organized groups for running hard and dangerous dungeons have emerged, with highly coordinated Discord servers with bots for managing headcount and role distribution (we need 2 healers! 5 people who can boost our damage!)\n\nSome of the more elite groups actually have very strict requirements for entry (need to have 3 characters at the strongest stat cap, activity requirements, etc.)\nAll of these are volunteer run\n\n\n\nBy keeping the most prestigious items untradeable, it incentivizes more community involvement and recognition/prestige around the activities that yield those items. A lot of these items are extremely powerful and enable incredibly unique capabilities for characters. But as the game is permadeath, not a lot of individuals have the guts to use these regularly."},"thoughts/spatial-computing":{"title":"Spatial Computing","links":[],"tags":["seed"],"content":"An infinite canvas perhaps?\nSource\n\nfuck responsive layouts, all my homies hate responsive layouts\n\nThere’s unexpected energy in the space of “being able to lay out elements visually on a 2d plane and know that the browser isn’t going to wrap them somewhere you didn’t mean”.\nSpatial web experiments\n\nhttps://nette.io/\nhttps://www.kosmik.app/\nhttps://sprout.place/\nhttps://www.platz.ooo/\nhttps://twitter.com/azlenelza/status/1418749070336684034\nhttps://twitter.com/raunofreiberg/status/1450479522609090562\n"},"thoughts/state-channels":{"title":"State Channels","links":["thoughts/blockchain","thoughts/bitcoin","thoughts/privacy"],"tags":["seed"],"content":"State channels are a very broad and simple way to think about blockchain interactions which could occur on the blockchain, but instead get conducted off of the blockchain, without significantly increasing the risk of any participant.\nSimilar to the concept of payment channels in Bitcoins’s Lightning Network and Raiden Network on Ethereum, but instead of only supporting payments, they also support general ‘state updates.’\nWork by\n\nLocking up some portion of state into a contract through a deposit of some amount of token\nChannel participants then communicate off-chain to sign valid transactions without submitting them to the chain. Each new update “replaces” the old one\nParticipants choose to close the channel and submit the state back to the blockchain, unlocks state\n\nProperties\n\nNear-instant finality: after all parties sign a state update, it can be considered final. Not instant because of dispute window\nStrong privacy properties: every intermediate transaction happens ‘within’ the channel and doesn’t need to be published to chain (which isn’t true for sidechains for example)\nRequires all parties to be available\nRequires all participants to be hardcoded in the contract\n\nPreventing fraud\nIf a party attempts to fraudulently close a channel, other parties in the channel have a period of time in which they can submit a more recent state, proving that fraud was attempted. Once an infraction is proven, the contract handles the resolution process, (e.g punishing the guilty party by slashing their deposited funds)"},"thoughts/stone-soup-metaphor":{"title":"Stone Soup metaphor","links":[],"tags":["seed","pattern"],"content":"Stone Soup Metaphor\nSource: Stone Soup in Wikipedia\n\nSome travelers come to a village, carrying nothing more than an empty cooking pot. Upon their arrival, the villagers are unwilling to share any of their food stores with the very hungry travelers. Then the travelers go to a stream and fill the pot with water, drop a large stone in it, and place it over a fire. One of the villagers becomes curious and asks what they are doing. The travelers answer that they are making “stone soup”, which tastes wonderful and which they would be delighted to share with the villager, although it still needs a little bit of garnish, which they are missing, to improve the flavor.\nThe villager, who anticipates enjoying a share of the soup, does not mind parting with a few carrots, so these are added to the soup. Another villager walks by, inquiring about the pot, and the travelers again mention their stone soup which has not yet reached its full potential. More and more villagers walk by, each adding another ingredient, like potatoes, onions, cabbages, peas, celery, tomatoes, sweetcorn, meat, milk, butter, salt, and pepper. Finally, the stone (being inedible) is removed from the pot, and a delicious and nourishing pot of soup is enjoyed by travelers and villagers alike. Although the travelers have thus tricked the villagers into sharing their food with them, they have successfully transformed it into a tasty meal which they share with the donors.\n\nProperly seeding new initiatives to set the expectations for future potential contributors.\nPreventing the cold-start problem"},"thoughts/supervised-learning":{"title":"Supervised learning","links":["thoughts/decision-tree","thoughts/Naive-Bayes","thoughts/KNN","thoughts/Ensemble-method","thoughts/linear-regression","thoughts/fundamental-tradeoff","thoughts/regularization","thoughts/Decision-theory"],"tags":["seed","CPSC340"],"content":"\nInput: take features of examples and corresponding labels as inputs\nOutput: a model that can accurately predict the labels of new examples\n\nGenerally, the most successful machine learning technique (with the exception of games)\nExamples:\n\nDecision trees\nNaive Bayes\nKNN (to fit an appropriate k)\nEnsemble Methods\nlinear regression\n\nTradeoffs:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecision treesNaive Bayes# Features usedSequences of rules based on 1 featureAll featuresTrainingO(dn), one pass per depthO(n), just countingNew dataMay need to recompute treeJust update countsAccuracyGood if simple rules based on individual features workGood if features almost independent given labelInterpretabilityeasy to see how decisions are madeeasy to see how each feature influences decision\nNotation\n\nX is the input\ny are the class labels\nn is the number of examples (generally idnex is denoted by subscript)\nd is the number of features (generally index is denoted by superscript)\ny^​ are the predictions\n\nParametric vs Non-parametric\n\nParametric models: have fixed number of parameters w.r.t. n\nNon-parametric models: number of parameters grows with n\n\nGeneral Rules\n\nWe care far more about testing error than training error\nGolden Rule: the test data cannot influence training the model in any way\nIndependent and Identically Distributed (IID) assumption\nFundamental trade-off between getting low training error and having training error approximate test error\n\nWe can mitigate this by penalizing model complexity (e.g. for Penalizing Model Complexity)\nSee also: regularization\n\n\nOptimization bias\n\nHow biased is an “error” that we optimized over many possibilities?\nIs large if you compare lots of different models, small if you only compare a few.\n\n\n\nDecision Theory\nAre we equally concerned about each potential outcome? Usually not! Sometimes, false negatives or false positives have outsized impact — the cost of mistakes might be different\n\nLetting a spam message through (false negative) is not a big deal.\nFiltering a not spam (false positive) message will make users mad\n\nWe can look to decision theory to help us here. Denote cost(y^​i​,y~​i​) as the cost of predicting y^​i​ instead of the actual label y~​i​.\nThen, instead of predicting the most probable label, compute all possible actions and take the action with the lowest expected cost: E[cost(y^​i​,y~​i​)]\n\nWe assign the probability of each state to be the confidence of the predicted class (e.g. predicting ‘spam’ with 0.6 likelihood means we set the probability of true ‘spam’ to 0.6)\n"},"thoughts/symbolic-execution":{"title":"Symbolic Execution","links":["thoughts/program-analysis"],"tags":["seed"],"content":"Helps with analysis where there are infinitely many initial states / executions. A (terminating) program analysis can’t precisely explore them all!\nAll-executions property: are there any executions of the program which violate the desired condition? Equivalently, is there any way to reach a statement such that its Failure Condition is true?\nQuestions that value-dependent all-executions correctness analysis tries to answer:\n\n\nassert x == 42; Can assert statements in this code ever fail?\n\n\nx = a[y]; Can execution of this code ever raise exceptions?\n\n\nthrow new RuntimeException(...); is this ever reached?\n\n\nWill my function always satisfy its intended postcondition? etc.\n\n\nWe will input/unknown values as symbolic values; we give names to these unknowns.\nWe track a symbolic state in our analysis, mapping program variables to symbolic expressions.\n\n\nWe will additionally track a set of path conditions: constraints representing the conditions which must be true in order to reach the current program point.\n\n\nWe can do something similar for loops by doing bounded unrolling.\nProgram Reachability Example\n\nFor loops\n\nFind the set of variables assigned to in the loop body\nCopy symbolic state before the loop, updating each assigned-to variable to map to a fresh symbolic value\n\nDo this once for the start of the loop and once for right after the loop\n\n\nInside the loop, set the path condition to the loop condition\nOutside the loop, set the path condition to the inverse of the loop condition\nContinue both analyses\n\n\n"},"thoughts/symbolic-system":{"title":"Symbolic Systems","links":["thoughts/intelligence","thoughts/frame-problem","thoughts/incremental-view-maintenance"],"tags":["seed"],"content":"Convergence of CS and Philosophy\nNewell and Simon claimed that both digital computers and the human mind could be understood as physical symbol systems. They both use strings of bits or streams of neuron pulses as symbols representing the external world (formal symbol manipulation)\nIntelligence (as claimed by Newell and Simon) requires making the appropriate inference from these internal representations\nTurning rationalist philosophy into a research program\n\nHobbes → reasoning was calculating\nDescartes → mental representations\nLeibniz → “universal characteristic” — a set of primitives in which all knowledge could be expressed\nKant → concepts are rules\nRussell → logical atoms as the building blocks of reality\n\nSymbolic AI as a degenerating research program\nProblem of representing significance and relevance → how do you transfer the learnings to the real world\nCommonsense knowledge problem: how do we represent ‘common sense’ in a way that is accessible to AI systems that use natural language\nThe problem isnt curating those facts, it’s knowing which facts are relevant in any given situations (the frame problem). We should be able to ignore something without having to figure out that it should ignore it\nIf the computer is running a representation of the current state of the world and something in the world changes, how does the program determine which of its represented facts can be assumed to have stayed the same, and which would have to be updated?\nIf a certain proposition is true (e.g. there are no empty spots in a parking lot) will it stay true? for how long?\nSee also: incremental view maintenance"},"thoughts/syntax":{"title":"Syntax","links":["thoughts/declarative-programming"],"tags":["seed"],"content":"\nHow are words combined, is a sentence grammatical?\n\nChild-directed Speech\nMotherese/infant-directed speech (IDS)/child-directed speech (CDS), when talking to babies, adults use a higher-pitched voice, a wider range of pitches, longer pauses, and shorter phrases. Vowels are not only longer in duration, but also are more prototypical examples of the particular vowel being produced\n\nGenerally more straightforward - focused on here and now, directs joint attention, and connects to real world referents\nLots of repetition for new lexical items\nHelps with language acquisition\n\nLearning Syntax\nSyntactic bootstrapping hypothesis: knowledge of language structure is generally useful for learning new verbs. If we know what the syntactic structure of the language is, it is easier to figure out the meaning of a new word\nNegative evidence: correcting overgeneralizations. There is no negative evidence in the speech stream without explicit feedback\nCross-situational learning: computing correlations between hearing a word and experiencing its referent (see Smith &amp; Yu 2008 for related study on whether cross-situational learning is a possible explanation for how children learn words)\nMeasurables\n\nLongest utterance\nAverage utterance (mean length of utterance or MLU)\nKinds of sentences the child can produce\n\nStages of development\n\nStage I (MLU 1.01 to 1.99): earliest word combinations\nStage II (MLU 2.00 to 2.49): grammatical morphemes start showing up\nStage III (MLU 2.50 to 2.99): varied simple sentences\nStage IV (MLU 3.00 and up): varied complex sentences\nStage V: new complex sentences\n\nTypes of Grammars\n\nPrescriptive grammar: dictates how people should speak, gives rules and grammar based around a “standard” or “correct” way of speaking, ignoring variation between dialects or between different demographics of people\nDescriptive grammar: we want to describe how people actually speak, not how we think they should speak\nOpen-class words: nouns, verbs, adjectives (can be added to)\nClosed-class words: auxiliaries, prepositions, complementizers, determiners (usually fixed in language)\nSentence types\n\nDeclarative (statement): “that’s a doggie”\nImperative (command): “look at the doggie”\nInterrogative (question): yes/no questions\nNegative sentences: contain negation\nComplex sentences: contain more than one main verb\n\n\nHirsch-Pasek and Golinkoff (1996): word order comprehension study. Do children know about subjects vs. objects and word order?\n\nTwo theories of syntax\n\nInnate/generative: children do not “learn” anything specific for a given language, they already have a universal grammar (UG), they learn language-specific parameters to the UG\nExperience-based/constructivist: grammatical knowledge comes completely from experience\n"},"thoughts/system-model":{"title":"System model","links":["thoughts/distributed-systems"],"tags":["seed"],"content":"How do we capture assumptions in a system model for distributed systems?\nNetwork behaviour (e.g. message loss)\n\nReliable: message is received if and only if it is sent, messages may be reordered\nFair-loss: messages may be lost, duplicated, or reordered. A message eventually gets through if you keep retrying (can be upgraded to reliable using retry + packet deduplication)\nArbitrary: active adversary, may interfere with messages (can be upgraded to fair-loss using TLS)\nNetwork partition: some links dropping/delaying all messages for an extended period of time\n\nNode behaviour (e.g. crashes)\n\nCrash-stop: node is faulty if it crashes. After it crashes, it stops executing forever\nCrash-recovery: node may crash at any moment, losing in-memory state. It may resume executing sometime later (sometimes call omission fault)\nByzantine: a node is faulty if it deviates from the algorithm. Faulty nodes may do anything, including crashing or malicious behaviour\nCorrect: not faulty\n\nTiming behaviour (e.g. latency)\n\nSynchronous: message latency no greater than a known upper bound\nPartially synchronous: asynchronous for some finite (but unknown, possibly arbitrarily large) periods of time, synchronous otherwise\n\nLike synchronous model, assumes a shared global clock with bounded drift Δ\nThere is an unknown transition point GST (global stabilization time) where the system goes from asynchronous to synchronous.\n\nAll messages sent in an asynchronous period t≤GST are delivered by time GST+Δ\nAll messages sent in the synchronous period t≥GST arrive by time t+Δ\n\n\nThe key difference is that we can wait for a sufficiently long delay (Δ) after the start of a round that if the network has reached synchrony you’re guaranteed to receive all messages from all non-Byzantine nodes\n\n\nAsynchronous: messages can be delayed arbitrarily, no timing guarantees\n\nIdentity and Messages\n\nAuthenticated: a Byzantine node cannot forge a message or change the contents of a received message before it relays the message to other nodes\nNon-authenticated: nodes have no way of verifying the authenticity of a received message\n\nPermissioning\n\nPermissioned: all nodes in the cluster are known ahead of time\nPermissionless: anyone can join the cluster\n"},"thoughts/systems-design":{"title":"Systems Design","links":["thoughts/system-model","thoughts/DHT","thoughts/NAT"],"tags":["seed"],"content":"Software Systems\nSystem design in this context means defining the architecture, product design, modules, interfaces, and data for a system according to given requirements. The purpose of system design is to architect a system that can effectively support the functionality of a product or service.\nSee also: system model\n\nRequirements\n\nFunctional: what does the system need to do?\nNon-functional: what properties does it need?\n\n\n4 things to ask clarifications on\n\nUsers: who will use the system? how will they use it?\nScale: how will our system will handle a growing amount of data?\nPerformance: how fast must our system be?\nCost: what are our budget constraints?\n\n\n\nLatency Numbers\nLatency Comparison Numbers\n--------------------------\nRegister reference                           0.1 ns\nL1 cache reference                           0.5 ns\nBranch mispredict                            5   ns\nL2 cache reference                           7   ns                      14x L1 cache\nMutex lock/unlock                           25   ns\nMain memory reference                      100   ns                      20x L2 cache, 200x L1 cache\nCompress 1K bytes with Zippy            10,000   ns       10 us\nSend 1 KB bytes over 1 Gbps network     10,000   ns       10 us\nRead 4 KB randomly from SSD*           150,000   ns      150 us          ~1GB/sec SSD\nRead 1 MB sequentially from memory     250,000   ns      250 us\nRound trip within same datacenter      500,000   ns      500 us\nRead 1 MB sequentially from SSD*     1,000,000   ns    1,000 us    1 ms  ~1GB/sec SSD, 4X memory\nHDD seek                            10,000,000   ns   10,000 us   10 ms  20x datacenter roundtrip\nRead 1 MB sequentially from 1 Gbps  10,000,000   ns   10,000 us   10 ms  40x memory, 10X SSD\nRead 1 MB sequentially from HDD     30,000,000   ns   30,000 us   30 ms 120x memory, 30X SSD\nSend packet CA-&gt;Netherlands-&gt;CA    150,000,000   ns  150,000 us  150 ms\n\nNotes\n-----\n1 ns = 10^-9 seconds\n1 us = 10^-6 seconds = 1,000 ns\n1 ms = 10^-3 seconds = 1,000 us = 1,000,000 ns\n\nBased off of the above:\n\nRead sequentially from HDD at 30 MB/s\nRead sequentially from 1 Gbps Ethernet at 100 MB/s\nRead sequentially from SSD at 1 GB/s\nRead sequentially from main memory at 4 GB/s\n6-7 world-wide round trips per second\n2,000 round trips per second within a data center\n\nReal-time\n\nWebRTC\n\nFast message propagation (no relaying)\nEncryption and authorization over untrusted signaling servers\nNo setup required, public signaling servers are available\nNot suitable for a large number of peers (quadratic explosion of complexity)\n\nBrowser have a max on connectivity: ~100 diff connections but varies per browser\n\n\n\n\nWebSockets\n\nClients connect to a single endpoint over Websocket. The server distributes document updates and awareness information among clients.\nSolid choice if you want a central source that handles authentication and authorization\nAlso send header information and cookies, so you can use existing authentication mechanisms with this server\n\n\nHypercore/Dat\n\nUses a DHT + NAT traversal utilities\nLow latency\nMedium reliability\n\n\n\n4+1 Architectural Model\n\nLogical view: The logical view is concerned with the functionality that the system provides to end-users (usually uses UML)\nProcess view: The process view deals with the dynamic aspects of the system, explains the system processes and how they communicate, and focuses on the run time behaviour of the system (usually sequence diagram, communication diagram, activity diagram)\nDevelopment/Implementation view: The development view illustrates a system from a programmer’s perspective and is concerned with software management. (usually package/component diagram)\nPhysical/Deployment view: The physical view depicts the system from a system engineer’s point of view. It is concerned with the topology of software components on the physical layer as well as the physical connections between these components\n(+1) Scenarios: The description of an architecture is illustrated using a small set of use cases, or scenarios. They also serve as a starting point for tests of an architecture prototype\n"},"thoughts/task-centered-design":{"title":"Task-Centered System Design","links":["thoughts/prototyping","thoughts/interviews-and-data-recording"],"tags":["seed"],"content":"Articulate concrete descriptions of real-world people doing their real-world tasks and use these description to determine which users and what tasks the system should support\nPrototypean interface that satisfies these requirements and then evaluate the interface by performing a task-centred walkthrough (see: interviews and data recording)\nStrengths\n\npractical way to ground designs to real user tasks\nfinding requirements at pre and early design stages\n\nLimitations:\n\ntasks almost always embody a process even if they are not specific to a specific technological implementation\nmay not encourage consideration of alternate ways to do tasks\nmay be hard to produce pure system- or process- independent tasks\n\nUnderstanding Tasks\nEstablishing requirements starts with identifying and understanding task examples (users and their tasks).\nPersonas\nPersonas are rich descriptions of typical users of the product under development on which the designers can focus on in designing products.\nBased off of user profiles, they include description of user’s behaviour, attitudes, activities, environment\nA persona has 2 goals:\n\nto help the designer make design decisions\nreminds the team that real people will be using the product\n\nScenarios\nScenarios show how tasks are handled by design:\n\nwhat the user would see / do step-by-step when performing the task\ntask + design = scenario\n\nScenario is design-specific; task is design-independent\nNatural way to explain what people are doing and stakeholders relate easily"},"thoughts/taste":{"title":"Taste","links":["aesthetics-and-taste","thoughts/attention","thoughts/self-confidence","thoughts/independent-research","thoughts/iconic-space","thoughts/walking","posts/the-fools-who-dream"],"tags":["sapling","pattern"],"content":"See the blog post version of this note: aesthetics-and-taste\n\nThat’s all life is really—a giant sequence of decisions. Life is the expression of taste\n\n\nTaste is built over time through repeated applications. It requires intention, focus, and care. Taste is a commitment to a state of attention.  It’s a process of peeling back layer after layer, turning over rock after rock. It is the manifestation of self-confidence in what we find beautiful.\nCuriosity and an inherent observatory nature toward the world serve as constant opportunities to test your taste. The process of cultivating taste is a lot like the writing and editing process. George Saunders on the revision process:\n\n“The way I revise is: I read my own text and imagine a little meter in my head, with “P” on one side (“Positive”) and “N” on the other (“Negative”)… This involves making thousands of what I’ve come to think of as “micro-decisions.” These are instantaneous, intuitive – I just prefer this to that… I just have a feeling and react to that feeling, in the form of a cut phrase, or an added word, or an urge to move this whole section, and so on. And then I do that over and over, for months, sometimes years, until that needle stays up in the “P” zone for the whole length of the text…With each choice, even the smallest ones, the story becomes more and more…well, it becomes more her, you could say. There’s more of her essential nature in it, more of what will distinguish her from all of those other writers out there. And gradually, the story starts to become something she couldn’t have foreseen when she started out – bigger, more complex, smarter, funnier, whatever.”\n\nCreation and Research\nIt is this taste that is so important in independent research. Without taste, one cannot carve out their own iconic space.\n\nCreation is the act of bringing something nonexistent into reality. You have to have strong opinions about how the world should work. You have to think that things should be a certain way, that you prefer this color over that, this form of walking over another, this sense of beauty over theirs, this way of approaching the world over some other. Taste gives you the capability and the urgency to imagine.\n(Spencer, creative seeing)\n\nTaste is the ability to know what the good questions to ask are.\n\nWhen [Grothendieck] writes that his peers were more brilliant than him, he is referring to their ability to answer questions. It was just that their questions were unoriginal. (Henrik Karlsson, Good Ideas)\n\nFailure\nSource\nAs keen as some of us are to do life “right”, sometimes the only way to do it right is to make your own decisions. Try. Fail.\n\nThere’s only one person who bears the responsibility of your life and it’s the same person who shrugs that responsibility off when it feels too heavy\n\nSee: the-fools-who-dream"},"thoughts/teaching":{"title":"Teaching","links":["thoughts/In-Over-Our-Heads","thoughts/constructionist","thoughts/game-design","thoughts/Mindstorms"],"tags":["seed","pattern"],"content":"Kierkegaard’s instructional advice in his Journals (from In Over Our Heads)\n\n“If real success is to attend the effort to bring a person to a definite position, one must first of all take pains to find him where he is and begin there. This is the secret of helping others… In order to help another effectively I must understand what he understands. If I do not know that, my greater understanding will be of no help to him… Instruction begins when you put yourself in his place so that you may understand what he understand and in the way he understands”\n\nConstructionist Teaching Style\nThe fundamental issue may then be that there’s too many tools and too much space: the large space should start small and widen (scaffolding), rather than having everything available at game-start.\nHowever, in a siloed world, communication is not exercised. A major component of a lot of subjects seems to be able to communicate effectively with another programmer. If the novice comes out of this experience unable to verbalize and communicate, is it even of use?\nA lot of similar questions to game design. Important learnings in Mindstorms"},"thoughts/telepresence":{"title":"Telepresence","links":["thoughts/bandwidth","thoughts/agency","thoughts/telerobotics","thoughts/feedback-loops"],"tags":["seed"],"content":"Means remote presence\nOne example is the telephone line, a thin, narrow bandwidth of aural telepresence. The far end is Dennet’s experiment which leaves the brain and body joined and intact, but wrap the body in a kind of additional sensory cocoon\nIf this is achievable, would seemingly require a high-bandwidth multi-sensory bath of information with local sensory stimulation\n\n“But as soon as a distant camera responds to your controls, and especially if the mode of control is either ‘natural’ (the helmet rig) or highly practiced (a video-gamer with a joystick), you begin to feel relocated, as if you are in the distant scene”\n\nOne example is the upside-down world. We can put people inside headgear that inverts their vision. It turns out that people can readapt to upside-down world after a period of sustained use. However, if it is passive (wheeled around in a wheelchair), the adaption does not occur\nThe notion that our perceptual experience is determined by the passive receipt of information is misleading. The whole point of seeing and perceiving our world is so that we can intervene and act upon it\nSee also: agency, telerobotics\nOn telerobotics\nIn principle, telerobotics systems need not feel more alien than teleoperated systems to the conscious users, however this is not the case in practice. Closely coupled teleoperated systems seem to much better induce the feeling of actual telepresense more effectively\nWhat seems to be important is the presense of some kind of local, circular feedback loop in which neural commands, motor actions, and sensory feedback are closely and continuously correlated. Temporal disruptions (like lag) may break this ‘illusion’\nThe brain uses a special piece of neural circuitry known as a motor emulator. This is a little circuit that takes a copy of the motor signal to the hand (say) and feeds it into a neural system which has learned about the typical responses from the bodily peripheries which are likely to ensue. The emulator is thus like a little local scale model of the real circuit. It rapidly outputs a prediction of the signals that should soon be arriving from the bodily peripheries, and these are then used instead of the real thing. This emulator-based feedback is then used for ongoing error-connection and smoothing"},"thoughts/telerobotics":{"title":"Telerobotics","links":["thoughts/telepresence","thoughts/Brains-in-a-Vat"],"tags":["seed"],"content":"See also: telepresence\nDennett’s 1981 “where am I” story\nSecret experiment, Dennett’s brain is removed, kept alive in a tank of nutrients, and equipped with a multitude of radio links by means of which it executes all of its normal bodily control functions (very related to Brains in a Vat example)\nDennett’s body is then equipped with receivers and transmitters so that it can use its built-in sensors (eyes, ears, etc.) to relay info back to Dennett’s brain. This is effectively a stretching of the nerves.\n\n“here I am, suspended in a bubbly fluid, being stared at by my own eyes …” — still can’t manage to convince himself to place himself in the tank\n\nDennett’s body is then trapped under a rockslide and as the radio links slowly give away, a shift in point-of-view occurs.\nIs Dennett really in the tank of nutrient, really trapped beneath the soil, or really no-place at all (or both places at once)?\nPoint of view is a construct grounded in the brain’s experiences of control, communication, and feedback. This leaves it open to rapid and radical reconfiguration\nTelerobotics\nCommunication on a higher level of abstraction in which the human communicates goals and the robot comes up with the plan to achieve it\nExample of going to a shop:\n\nWe never think of each individual muscle movement or thought\nIt’s more of a general “I need a soda, I’m gonna go to the store”\nMost of what ‘I’ did, ‘I’ seemed to have very little to do with the vast bulk of neural activity leading both to, and away from, this tip of the metaphorical ‘I’berg is unconscious.\n\nOur conscious high-level decisions thus serve as the impetus for the other systems to do their stuff, while still devolving substantial sub-problems to other internal agencies.\nThis is what T. Sheridan (1992) originally dubbed “Supervisory Control”: a type of control in which only goals and high level commands are communicated to the slave robot."},"thoughts/telic-action":{"title":"Telic Action","links":[],"tags":["seed"],"content":"Telic Actions are more serious in nature, motivated by wanting to achieve something or reach a goal. This would be like writing an essay to get a good mark. Most telic actions are a means to an end rather than the actual end themselves."},"thoughts/terminal":{"title":"Terminals","links":[],"tags":["seed"],"content":"TTY\nSource\nTeletypewriter. Fundamentally sends a stream of input characters and displays output\nYou’ll sometimes hear these apps referred to as “terminal emulators” instead of simply “terminals”. Since the term “terminal” used to refer to a dedicated piece of hardware, we consider these apps as emulating that device.\nIn these emulators, the wires of the traditional TTY are replaced with pairs of file descriptors known as the PTY, short for pseudo-TTY\nCreating a PTY\n\nThe terminal emulator asks the kernel to open a PTY\nThe kernel returns the PTY as a file descriptor. This is the ‘leader’ which is intended for the terminal to interface with (read and write data)\nIt also creates a ‘follower’ PTY which is used by the shell and other processes in the session\nThe terminal emulator then spawns the shell which reads and writes from the follower PTY (the shell’s stdin, stderr, and stdout) is set to the follower PTY\n\n\nNote, there only exists a difference between the leader and follower PTYs for the purpose of line-discipline, which is a sort of intermediate buffer.\nThis is where ‘special’ character behaviour happens:\n\nDon’t send every character that I’ve typed to the shell, please wait until I press ‘Enter’\nErase the last character from the buffer when I press ‘Backspace’\n\nShells\nTo service the full spectrum of use-cases, we want a full, interactive, interpreted programming environment.\nThe work of running other programs as processes and interpreting the commands you write is done by a shell.\nExamples include bash, zsh, and fish\nMost shells also display a prompt (e.g.)\n~/projects\n❯\nThis is normally stored in a variable PS1 which supports dynamic information (e.g. current working directory %d)\nLogin vs non-login shells\n\nLogin shells are shell sessions wherein this shell process is the one the user is logging into the system with\nNon-login shells are shells started when you are already logged in\n\nEscape-sequences\nAll of the escape sequences in this output have the format ESC[c1;...;ckm. These are called Select Graphic Rendition (SGR) escape sequences and are used to assign attributes, like text decorations, to characters. The ci’s are the codes that control the rendering. For example, the code 1 is used to turn on bolding.\nSee also: x-term spec\nBash\n\na | b : pipe the output from program a into the input of program b\na &gt; b: redirect output of program a into a file descriptor b, overwriting\na &gt;&gt; b: redirect output of program a into a file descriptor b, appending\n\nWarp and Blocks\nSource\nMost shells provide hooks for before the prompt is rendered (zsh calls this precmd) and before a command is executed (preexec). Zsh and Fish have built in support for these hooks. Though bash does not have built in support for these hooks, scripts like bash-preexec exist to mimic the behavior of other shells. Using these hooks, we send a custom DCS (Device Control String) from the running session to Warp. This DCS contains an encoded JSON string that includes metadata about the session that we want to render. Within Warp we can parse the DCS, deserialize the JSON, and create a new block within our data model.\nWe also create a separate grid for each command and output based on the precmd/preexec hooks we receive from the shell. This level of grid isolation ensures we can separate commands and their output without having to deal with the output of one command overwriting that of another."},"thoughts/terminology":{"title":"Terminology","links":["thoughts/consensus","thoughts/context","thoughts/plurality","posts/new-words","thoughts/semantics","thoughts/feedback-loops","thoughts/language-of-thought"],"tags":["sapling"],"content":"Orwell on Language\nSource: Orwell on Politics and the English Language\n“In prose, the worst thing one can do with words is surrender to them. When you think of a concrete object, you think wordlessly, and then, if you want to describe the thing you have been visualising you probably hunt about until you find the exact words that seem to fit it.”\nA tangent on language\nLanguage as an abstraction for concepts. Generally, we need it to communicate and refer to objects that are not spatially/temporally local.\nAbstraction involves compression and thus detail is lost if it is lossy. The way we make up for that detail is through having a shared consensus over meaning and intent\nThis meaning and intent is baked into the meaning of the word through how the word is used\n‘In most cases, the meaning of a word is its use’, Wittgenstein claimed, in perhaps the most famous passage in the Investigations. It ain’t what you say, it’s the way that you say it, and the context in which you say it. Words are how you use them.\nYet usage changes over time. No one person or institution decides how language works: it is one of the few logically decentralized aspects of humanity. Old meanings may no longer make sense. new groups try to co-opt old terminology. We accrue a plurality of definitions\nSee: new-words, semantics\nTerminological anchoring\nNatural meaning ‘drift’. I’m hesitant to ‘root’ new terms, terminology evolves in a decentralized way, centralizing meaning often concretizes meaning\nNaming as power, but naming also anchors meaning\nTerminological Feedback Loops\n\nPractice creates new terminology as a way to communicate complex ideas without needing to rexplain each time\nTerminology then shapes how we think about the world: Sapir-Whorf\nSome terminology becomes outdated as practice changes\nArguments ensue over updating shared terminology (esp as language is decentralized, this can cause fracturing)\nTwo camps emerge\n\nPeople who want to use terminology mainly as a means to more efficiently communicate practice\nPeople who like the theory behind terminology or those who are attached to tradition\n\n\nCamps argue over who has the ‘right’ definitions but for their own goals\n\nCamp 1 repeats this loop\nCamp 2 continues to argue over definitions\n\n\n"},"thoughts/testimony":{"title":"Testimony","links":["thoughts/trust","thoughts/Reductionism","thoughts/Nyāya","thoughts/epistemology","thoughts/consensus","thoughts/epistemic-authority","thoughts/Jestermaxxing","thoughts/intentionality","thoughts/power","thoughts/epistemic-injustice"],"tags":["seed","PHIL240A"],"content":"Forming a belief based on the trust of another’s written or spoken word.\nThree main positions of validity of knowledge gained through testimony\n\nSkepticism (e.g. John Locke): one cannot gain knowledge on the basis of testimony alone. Cannot ensure reliability of other actors\nReductionism: one can gain knowledge through testimony, but only if one has independent, inductive reason for believing that the speaker is reliable\nNon-reductionism (e.g. Nyāya): one can gain knowledge through testimony simply by trusting the speaker (provided that the speaker knows what they assert). Testimony is a trust-worthy epistemic instrument, knowledge is communal\n\nRelated: consensus, epistemic authority\nTestimonial Injustice\nTwo main kinds of testimonial injustice\n\nReceiving more credibility than they otherwise would have (credibility excess)\nReceiving less credibility than they otherwise would have (credibility deficit)\n\nE.g. speaker’s accent — indicating certain educational/class/regional background\nAlthough can be beneficial in some cases, see the Jestermaxxing\n\n\nSpecifically, this is in the context of the knower being wrongly judged in their capacity to be accurate\n\nInjustice means it must be harmful but also wrongful\nInjustice also carries a connotation of intentionality to it: it is very hard to believe that one who accidentally misjudges another is committing an injustice against someone (personal belief of Fricker here)\n\n\n\nTwo modifiers for testimonial injustice\n\nPersistent: repeated frequently, for example when the injustices occur in the context of their professional life\nSystematic: centered within a system of power, fundamental to the predominant social, economic, or political practice\n\nSee also: epistemic injustice"},"thoughts/texture-mapping":{"title":"Texture Mapping","links":["thoughts/texture","thoughts/object-detection","thoughts/imaging"],"tags":["seed"],"content":"Hides geometric simplicity by using images convey illusion of geometry. Usually maps points on a 3D surface to a 2D point on a texture to modify some property of the 3D surface (e.g. displacement, colour, reflectance, etc.)\nMipmapping\nSimilar to Gaussian Image Pyramids\nMipmaps (also MIP maps) or pyramids are pre-calculated, optimized sequences of images, each of which is a progressively lower resolution representation of the previous.\n(fun fact: MIP stands for multum in parvo — many things in a small place)\nThey are intended to increase rendering speed and reduce aliasing artifacts. A high-resolution mipmap image is used for high-density samples, such as for objects close to the camera; lower-resolution images are used as the object appears farther away.\nInterpolation within a MIPMAP level is done either by using the nearest texel or interpolating between neighbouring texels. Interpolation between MIPMAP levels can be done by doing linear interpolation of the values of the location at the two levels.\nFiguring out which MIPMAP level to use\nOn a pixel, we calculate the area that it takes up on the texture (the pixel footprint area). By projecting the boundaries of the pixel onto the texture, the area of the pixel footprint area can be computed using the magnitude of the cross product A=∥r1​×r2​∥\nSee notes on sampling"},"thoughts/texture":{"title":"Textures","links":["thoughts/texture-mapping","thoughts/computer-graphics","thoughts/noise"],"tags":["seed"],"content":"Detail in an image that is at a scale too small to be resolved into its constituent elements and at a scale large enough to be apparent in the spatial distribution of image measurements\nSometimes thought of as patterns composed of repeated instances of one or more identifiable elements called textons\nCan be used for\n\nobject identity (tell what it is from the textures)\nobject’s shape (based on spatial deformation of texture)\n\nShape from texture: Estimating surface orientation or shape from texture\nTypically, texture is a property of a region, not a point. But, then we run into a boundary segmentation problem. We compromise by using a local window to compute a texture and assign it to a point.\nSee also: texture mapping\nSynthesis\n\nInpainting (filling in holes)\nProduce large quantities of texture for computer graphics\nWave function collapse\n\nRandomness parameter is actually the size of the patch (less randomness means sample is larger thus more accurate)\nGrainy Film Texture\nSee also: noise\nNormally done with simple white noise SVG applied to background with pass through, max exposure and contrast, and add some opacity\nSVG noise generator"},"thoughts/the-Self":{"title":"The self","links":["thoughts/games"],"tags":["seed","PHIL451A"],"content":"A formal definition\n\nMe (self-as-object)\n\nSome feature is the object of my awareness\nI recognize this feature as mine or ascribe it to myself\n\n\nI (self-as-subject)\n\nI experience myself as subject of perception/cognition/emotion\nBody anchored\n\n\n\nValid in dreams, but also games?\nAutoscopic Phenomena\n\nAutoscopic Hallucination: you see a double of yourself but do not feel like you have ownership or agnecy over it\nHeautoscopy: visual perspective switches back and forth between yourself and double and you feel ownership of both\nOut-of-body experience: you identify the body seen from the outside as yours but have no feeling of agency for it. Direct electrical stimulation at the temporoparietal junction induces out-of-body experiences\n"},"thoughts/the-garden-and-the-stream":{"title":"The Garden and the Stream","links":["thoughts/hypertext","thoughts/search","thoughts/walking","thoughts/time"],"tags":["seed"],"content":"Source\nThe Garden\nA garden is a metaphor for a lot of things: growth, persistence, and the constant battle against entropy.\nThe Garden is an old metaphor associated with hypertext.\n\nThe Garden of Forking Paths from the mid-20th century. The concept of the Wiki Gardener from the 1990s. Mark Bernstein’s 1998 essay Hypertext Gardens.\n\nThe Garden is the web as topology. Every walk through the garden creates new paths, new meanings, and when we add things to the garden we add them in a way that allows many future, unpredicted relationships.\nThere is no right or canonical way to view it\nA hard part of this is The Navigation Problem: how do we give web users just enough guidance to freely explore the web, without forcing them into pre-defined browsing experiences?\nThe Stream\nIn the stream metaphor you don’t experience the Stream by walking around it and looking at it, or following it to its end. You jump in and let it flow past. You feel the force of it hit you as things float by.\nIn other words, the Stream replaces topology with serialization. Rather than imagine a timeless world of connection and multiple paths, the Stream presents us with a single, time ordered path with our experience (and only our experience) at the center.\nWe live in a shallow web. Not of actual trails, but of sign posts.\n\nA web of “hey this is cool” one-hop links. A web where where links are used to create a conversational trail (a sort of “read this if you want to understand what I am riffing on” link) instead of associations of ideas… A web seen as a tool for self-expression rather than a tool for thought.\n"},"thoughts/time":{"title":"Time","links":["thoughts/Order-theory","thoughts/A-Certain-Tendency-Of-The-Database-Community","thoughts/causality","thoughts/clocks","thoughts/trust","thoughts/colonial-debt","thoughts/in-group-bias","thoughts/desire-paths","thoughts/git","thoughts/Antimatter","thoughts/system-model","posts/nothing-stops"],"tags":["seed"],"content":"\n“Time is not linear, but a series of concentric circles, like the rings of a tree. Ourselves today, our newest selves, are the outermost rings of a tree. We are comprised also of every person we were in the years leading up to today, even if those layers have compressed into the past.” Source\n\nThe main insight from Einstein’s special relativity is that time is relative. And as such, points in space-time are not totally ordered but rather partially ordered (see: Order theory).\nTwo observers of the same event might perceive it to have occurred at different times, depending on their relative motion. Or, in distributed systems land, two nodes may receive the same messages in different ordering, depending on network conditions.\nHow do your order the events in the Universe? The answer, as Lamport noted, is you order events in terms of messages that could be sent between them.\nSee also: A Certain Tendency Of The Database Community, causality, clocks\nAt your own speed\nSource\n\nI think this is a shame — and love is my shining example. Call me crazy, but maybe it’s a good sign when things feel remarkably simple and wordlessly right. And when they do, it’s interesting to look around and notice how incredibly irrelevant speed is. Certainty means you’re moving at the speed of trust — a personal pace that ultimately has no record to beat or even road to follow\n\nSaving Time, Discovering a Life Beyond the Clock\n\nExploring the social and material roots of the idea that time is money\n\nSource\n\nChronos vs Kairos time\n\nChronos: quantitative, calendar time\nKairos: qualitative time, memorable moments\n\n\nHorizontal vs Vertical time (Josef Pieper): the point of leisure should not be to work better\n\nHorizontal time: work and refreshment/recovery for work\nVertical time: things that remind you of your mortality and the sort of fragility of your life, but also this deep sort of awe and gratitude about the fact that you exist at all. He describes it as sort of cutting through that horizontal plane; it feels in that moment almost like it invalidates the horizontal time\n\n\nIndustrial Time\n\nTime as money (equal interchangeable hours)\nEmerged from the need to measure others’ labour\nThe colonisation of time (Giordano Nanni)\n\nConflict between British colonial sense of time and the existing understanding of time of people living in colonized places\nTaylorism and factory labour (measuring and systematizing work to make it go faster)\nThis is not much different than the working class today (e.g. Amazon warehouse workers scanner gun, UPS truck with GPS sensors, etc.)\n\n\n\n\nUnfungible time: pick a point in space and time and just keep watch. A branch, a yard, a webcam. A story is being written there\n\nTo consider something as inhabiting time with you is to consider that it has experience\nLesser minds problem: human tendency to see non-human or even outgroups as not as complex as our own\n\nBy simply asking whether they would like a certain vegetable, those neural regions became active\nThe question presumes a person with preferences and desires. Desires and attitudes toward the future only exist in time: the time inhabited by that person\n\n\nIt is a deeply political question about who does and does not inhabit time\nUnfreezing in time: focus on a specific point or feature to help make something’s aliveness more accessible to you\n\n\nTime gardening\n\nI had a friend who always was giving away bags of cabbage to people. Apparently you are supposed to get rid of the outside leaves of the cabbage so the inside leaves of the cabbage can grow to maturity\nNot everything is transactional. “I’ve sort of forgotten that a lettuce keeps growing, assuming that more lettuce leaves for me mean less lettuce leaves for her”\nChronodiversity: a garden invites the human subject into conversations with different modes and speeds of life\n“Time is not money, time is beans”\n\nBeans are not just commodities. Sure, you could eat them, but they weren’t end points and they weren’t dead. At least some of them contained something: the possibility of future beans.\n\n\nSometimes the best way for me to get time is to give it to you\n\n\nDifference between someone who truly has no time versus one who feels like they have no time\n\nThe latter are people whose problems would be solved by going to a cabin with no reception (people who have internalized ‘business is good’)\nFor someone who structurally does not have control over their own time, the answer does not lie in using their own time better\n\n\nOn capturing time: “Time is slipping through your fingers and you want to grab onto that. What could be more human than that?”\n“Like me, they have aged”\n\n\nFools and their time metaphors\nSource\nGcal has turned private calendars into a shared commons. Anyone can easily pull up your schedule, and the blankness-by-default is an open invitation to take away your time. (My mom was particularly surprised to learn that tech workers often don’t use their words to set up meetings. They just “throw time” on the calendar and wait for an RSVP.)\nDigital calendars misrepresent the default state of your time. It’s far from empty. You’re working, thinking, talking, problem-solving, Being. Blankness shouldn’t be an invitation to interrupt. It’s yours, it’s sacred!\nThe UX is additive, rather than reductive. We’re always “putting time on” calendars, never “taking it off.”\nSome particularly desperate people would invent fake events and strategically place them throughout the day, making it difficult for would-be time thieves to find enough “empty” time. These protective mechanisms are band-aids. They’re what designers call desire paths\nVirtual Time\nVirtual Time (Jefferson, 1985)\nA git rebase-like approach to resolving conflict\n\nSimilar to Antimatter and netcode rollback\n\nProgrammers can write correct software without paying any attention to late-arriving messages, and even with no knowledge of the possiblity of rollback, just as they can write without any attention to, or knowledge of, the possibility of page faults in a virtual memory system.\nFor every message there exists an antimessage that is exactly like it in format and content except in one field, its sign. Whenever a message and its antimessage occur in the same queue, they immediately annihilate one another.\n\n\nUses a global virtual time (GVT) watermark\n\nSimilar to GST in timing behaviours in system models\nMin of\n\nall virtual times in all virtual clocks at time r\nthe virtual send times of all messages that have been sent but not yet processed at time r\n\n\nAvoids indefinite queue/buffer growth by trimming all messages with virtual times less than GVT\nHowever, this algorithm is bounded by the slowest connection, which makes it infeasible for an asynchronous latency model (e.g. local first)\n\n\n\nThe Fish of Lijiang\nSource\nA translated sci-fi fantasy short story on time.\n\n“They discovered that those suffering from the side effects of time sense dilation and those suffering from the side effects of time sense compression can help each other, be each other’s cure.”\n“Time flies past the laborer, the poor, the “third world”; time crawls for the rich, the idle, the “developed world”; time stays still for those in charge, the idols, the gods…”\n\nNothing stops\nSee also: nothing stops\nOriginal source from griefbacon\n\nThis is the process of history, and of crisis, of disease and of love. We may try sometimes to stand still, but we are standing on a moving walkway.\nAll people want is for nothing to happen; all anybody wants is another day of our soft, stupid little lives, to be allowed the vulnerabilities we have built into them. We clutter up our houses with useless objects that mean something to us; we adopt pets who would slow us down in a crisis. All this is a way of ignoring the truth that nothing stops, which is to say it is a form of love.\nWe are making a declaration that it is worth it to choose the losing side.\nBut I choose all that anyway; I would rather try and fail to stand still with you than to be fast and sleek without you.\n\nIn stories\nFrom: Stories that use time to hurt you\n\n“Despite how many ticks of the clock are left, there is still meaning”\n\n\nClick (2006)\n\nThe main character Michael Newman receives a remote that controls reality the same way a remote might control a television. Notably, it can pause or fast-forward time.\nIn the second half of the film, Michael is under the impression he has received a promotion at work but later finds out that the actual promotion won’t actually happen for a month or two. Fast-forwarding him to his promotion unwittingly moves him forward a full year. He realizes how much of his life he has just forfeited. A year where he is on autopilot and the world around him has changed: his children no longer like the same TV shows, his dog has passed away, his wife has entered them into marriage counseling.\nBecause the remote learns from his previous actions, it involuntarily drags him forward through gaping chunks of his life. The next skip is a whole decade. He is now obese, his children are now teenagers, and his wife is now remarried.\n6 more years and he has had cancer, a heart attack, and his father has died but this is all news to him.\nIt turns from a story about a funny man with a remote into a terrifying story of regret and the fragility of time. What will the unwelcome passage of time do to the human psyche?\n\n\nThe Stanley Parable (Ultra Deluxe expansion)\n\nThere exists an unextraordinary room. No windows, no doors, just some miscellaneous office decor and supplies tossed about\nIn the middle of the room is a skip button placed there by the narrator. If you get tired of him talking, you can just press the button to jump ahead and skip the monologue.\nThere is a shift in the tone and the situation becomes dire. Every skip gets longer and the narrator is beginning to get desperate. However, the game is designed such that skipping is the only way to progress the story, so naturally-\nAfter enough skips, the narrator will stop speaking and you will remain trapped in the room. The potted plant has died, some lightbulbs burn out. At some point, there is a leak in the ceiling, the ceiling collapses. Plants and other wildlife grow. The plants die.\nIt is about the uncanny endlessness of time and of infinity\nThe thought of what might occupy that space after we are gone is frightening and unknowable\n\n\nBefore Your Eyes (2021)\n\nA short indie game that sees you take the role of Benny. You experience his life in a series of colourful sequences. The first trip to the beach with his mother, his father filming his first birthday, books before bed, his first math assignment, his first friend.\nIt paints these delicate little pictures of moments that might feel strangely familiar, and it does this so well that everything else in the real world just sort of melts away. You’re here and you’re present and you don’t want to leave so you’re careful not to… blink\nBlink and time skips forward. The game is controlled via webcam and it watches you as you play and tracks your blinking. Closing your eyes is how time moves forward, sometimes seconds, sometimes years\nAnd becomes sometimes blinking occurs without conscious effort, there are times that you unintentionally rip yourself away from one of those tender scenes before the dialogue is done.\n“There are some lines in the game that very few people are going to catch and they were designed that way… I personally find that exciting — you can’t catch everything”\nIt captures the sensation of regret, wishing you had said those words you wanted to say\n\n\nManifest (2018-2023)\n\nBecause of an overbooking, Ben Stone, his son Cal, and Ben’s sister Michaela all take a later flight than the rest of the family. Notably, they all have loose ends:\n\nCal’s leukaemia isn’t responding to treatment\nBen is desperately looking for a solution to help his son\nMichaela is on the fence about a marriage proposal\n\n\nAs their plan prepares to land, ground control is confused to be receiving flight 828. Upon landing, no one has phone service. Instead of being greeted by loved ones, they are met with sirens and ambulances. The passengers are informed that although their plane left on April 7th 2013, the current date is November 4th, 2018.\nHow long is long enough to move on after your lover dies?\nAlthough they haven’t lost any of their lives, the world has moved on without them leaving them in a state of stasis\n\n\nInterstellar (2014)\n\nEvery hour on Miller’s planet is 7 years back on Earth. However, a quick 15 minute errand tumbles in chaos. Now an over hour long endeavor, a full 60,000 hours have passed back on the ship.\nWhen the crew finally return, they are informed that their calculation were wrong… 23 years 4 months and 8 days have passed on earth. Without a word, Cooper goes to check for messages from his family and in a single setting, he will see half a lifetime of memories be relayed to him\n\nHis son Tom, a teenager has found a girlfriend\nMoments later, now a grown man, Tom introduces his son, they’ve named him Jesse\nIn the next message, he finds out that his new grandson would not survive Earth’s declining conditions. They buried him out back next to Grandpa, who was alive before the mission\n\n\nIt is because the audience knows the weight of time in this scene that the intensity becomes almost combustable\n\n\nGunbuster (1988-1999)\n\nIn the middle of a space mission on the verge of defeat, Kazumi is frozen, her tears floating in zero gravity\nShe knowingly yet helplessly squanders seconds that are precious days with the man she loves back on earth simply because she deal with how cruel the flow of time is, killing him faster than her\nIn the final episode, to ensure Earth survives any future invasions, Noriko and Kazumi must ignite a human-manufactured black hole. Being there warps their time exponentially. Jung warns them that if they use this, it is effectively death. Not because they’ll die but because they’ll be plunged so enormously far into the future that everyone they know and love will be gone.\nBut there is no other option and they do it. The two girls are lost to time. When they finally return to Earth a full 12000 years in the future, is it the same Earth they left behind?\nDespite the 12000 years passing, the people remember the two lost girls who would one day return. The pilots cannot make contact with anyone nor see any sign of habitation on the planet, suggesting that human civilization is long gone. However, their despair is instantly dispelled when a massive light pattern suddenly appears on the planet saying “WELCOME HOMƎ!” spelled out in simplified Japanese (“オカエリナサイ”. The final letter “イ”, however, is reversed, which indicates the current civilization was mimicking the bygone language)\n\n\nYour Name (2016)\n\nThroughout the film, Mitsuha and Taki are swapping bodies, occasionally slipping into each other’s lives. In doing so, they accidentally get to know each other, not only from leaving notes between every swap, but also by taking the other’s life for a joyride and doing things that maybe the other doesn’t have the courage to do.\nAt some point, Mitsuha goes radio silent. Taki, confused, wants to meet her but his only clues to her location are sketches that he has drawn from his time in her shoes. And so he takes a train to the countryside where he thinks she is. Here, he finds out that the town he is looking for is called Itomori and it was tragically levelled by a comet 3 years ago.\nMitsuha was swapping lives with Taki from another time. But because you assume that because both of their lives are being presented in tandem with each other, that they happen at the same time. When in reality, the events of her life are 3 years behind his.\nAt some point, Mitsuha actually goes to the city to see Taki but he has no idea who she is. In her timeline, she is 3 years too early and he has not started swapping bodies with her yet.\n\n\n\nChronological Snobbery\n\nthinking, art, or science of an earlier time is inherently inferior to that of the present, simply by virtue of its temporal priority or the belief that since civilization has advanced in certain areas, people of earlier periods were less intelligent\n\nThe term was coined by C. S. Lewis and Owen Barfield, and first mentioned by Lewis in his 1955 autobiographical work, Surprised by Joy."},"thoughts/tinkering":{"title":"Tinkering","links":["thoughts/complexity","thoughts/play","thoughts/constructionist","thoughts/cozy-software","thoughts/software-principles"],"tags":["seed"],"content":"Playful Software\nSource\n\nThis early discovery-by-play keeps recurring, in history and across disciplines. Richard Feynman talks about tinkering with radios as a kid. James Cameron, J.J. Abrams, Martin Scorsese, and countless other filmmakers credit Super 8 with their starts.\n\nTinkerability and accessibility. As Feynman notes, “Radio circuits were much easier to understand in those days because everything was out in the open.” The Slate article on Super 8 notes how cheap the film was, “You didn’t have to worry about experimenting with it.” These are the preconditions to play.\nBut the inconvenient fact of technological progress is that it inherently trends toward complexity and obfuscation… Similarly, if I were a kid today, there’s no way I could self-teach HTML by inspecting source.\nPlayful software that let people experiment with their own spaces and forms of expression are a precondition to a more enlivening internet\nSee also: play, constructionist learning, cozy software\nsoftware principles"},"thoughts/tools-for-thought":{"title":"Tools for Thought","links":["thoughts/Extended-Mind-Hypothesis","posts/networked-thought","thoughts/interaction-design","thoughts/memex","thoughts/information","thoughts/research-debt","thoughts/git","thoughts/lost-knowledge","thoughts/public-goods","thoughts/infrastructure","thoughts/organizing-system","thoughts/A-City-is-not-a-Tree","thoughts/desire-paths","thoughts/workflows","thoughts/context","thoughts/memory-palace","thoughts/spatial-computing"],"tags":["sapling"],"content":"\n“When designing new tools for thought, let’s think not just in terms of features, but materials – what software laws of physics do we want embodiments of our thoughts to obey?”\n\nRelated: Extended Mind Hypothesis, Networked thought, interaction design, memex\nQuestions:\n\nHow can we create tools that aid our thinking? How can we extend the range of thoughts we can think?\nAs an extension, how do we organize the abundance of information we have today so that it is both accessible and useful?\nHow do we distill all the research debt so that our ideas are easily useful to others?\nHow can we make mixed media first-class? Allow drawing, images, and video? Bring back the overhead projector!\n\nCan we do this on a browser-level rather than just a standalone app? Building in curius.app-esque support, annotations, note-taking, etc. Minimize context switching\nCapture-organize-synthesize loop\nSource\n\nWhy do file systems force you to name a file (synthesize), and place it in a folder (organize) before you can write in it (capture)?\nWhy do Word Processors present you with a blank page (synthesize) instead of offering scratch notes (capture) that are relevant (organize) to your writing goals?\nWhy do we expect ourselves to create good ideas from nothing (synthesize)? It’s much easier to generate ideas when you have lots of material (capture) clustered by themes and relationships (organize)\n\nMixing up the order of capture-organize-synthesize causes friction. It forces us to make decisions before we’re ready.\nSee also: reverse outlining\nThought as a technology\nSource\n\nHow can we invent new elements of cognition?\nHow can we reify internal mental models?\nNot all visualization is good\n\nThere is a cargo cult mentality which embraces visualization for the sake of visualization. In fact, there is no a priori reason a visual approach is superior\n\n\n\nWord Charcuterie Board\nIs there any way to rearrange the fuzzy cloud of ideas in our head easily on paper/screens?\nHow can we ‘cut’ text without being afraid of it being lost forever? Even with tools like the undo button and Git revision history, the actions of the past are by default hidden. Time is not first class.\nOn memory and lost knowledge\n\nIf a tree fell and nobody remembered, did it really happen?\n\nVery 1984-line-of-thought but those who control the past, how we remember and what history is, controls the present and thus controls the future.\nOur tools of memory should be democratized and public. These tools are a public good. They are, quite literally, the infrastructure for mental representations and operations. These are the patterns the dictate our very mental processes.\nKernel\nSource: Remember in Kernel\nWe live in an information age. The amount of data we produce far outweighs what we consume, so much so that its extended far beyond our ability to make meaningful use of it.\n\n“There is a growing mountain of research [but we] cannot find time to grasp, much less to remember, all the conclusions of others as they appear. Yet specialization becomes increasingly necessary for progress, and the effort to bridge between disciplines is correspondingly superficial […]\n\nA record should be able to be\n\ncontinuously extended\nstored\nconsulted (e.g. through search)\n\nRight now, most organizing systems are really good at 1) and 2) but suck at 3).\nWe’re trying to build tools that allow association between ideas easily, relying on relations and relatedness without forcing hierarchies or classification straight away. Then, the path through these ideas can be thought of as ‘user trails’.\nChunking\nWe naturally group things into patterns and chunks and allow us to think on and rationalize at higher levels of abstraction, descending into lower levels only when we need to.\nThis was demonstrated by chess masters in the 1970’s: “Players learn to recognize somewhere between 25,000 and 100,000 patterns of chess pieces. These much more elaborate ‘chunks’ are combinations of pieces that the players perceive as a unity, and are able to reason about at a higher level of abstraction than the individual pieces”\nIs it possible to automatically pick up repeated digital desire paths of user trails and pull them into workflows? take advantage of natural spaced repetition to identify saliency\nThe focus of modern information systems is moving from “data-processing” towards “concept-processing”, meaning that the basic unit of processing is less and less an atomic piece of data and is becoming more a semantic concept which caries an interpretation and exists in a context with other concepts.\nSource: Automated knowledge discovery in advanced knowledge management\nAre there ways to ‘breadcrumb’ how we navigate? Even something as simple as option+click to leave a crumb and when finishing a session, you can save parts of the trail as a flow.\nSpatial\nCan we create spatial reminders that adhere to their contexts? Sticky notes for our mind? A reminder in certain notes to come back later and add to it, etc. Can we build memory palaces? Spatial representations of information?\nEmotion\nVideo is incredibly expressive. You can feel the passion of people who really care about the things they study, build, and work on.\nIs it possible to create mediums for thought that convey ‘awe and mystery and surprise and beauty’? I really hope so.\nExecution\nCan we embed computation into our notes? Create APIs out of our thoughts and concepts? Compose theorems just as easily as we chain function calls?\nActivism\nSource: On Gathering, Mindy Seu\nMemory work of this sort is also a form of activism. Whose memories are saved and retold to future generations?\nAfter a conversation, all parties maintain ownership of what transpired, and they continue to hold ties to one another. This form of storytelling is not predetermined, but develops through its unfolding.\nTasting Notes\nRobin Sloan shares his recipe for taking notes that spark novels\n\nBut the interesting thing about Robin is he doesn’t look at these things as bricks exactly. They don’t combine together in predictable, linear ways.\nInstead, Robin’s notes are more like ingredients—deep yellow saffron, Ceylon cinnamon, black garlic, and white truffle—bits of the world that he throws together into a pot and covers with a heavy lid. He turns up the heat, he adds salt to taste—until out comes a story.\n"},"thoughts/traditional-knowledge":{"title":"Traditional Knowledge (TK)","links":["thoughts/colonial-debt","thoughts/boundary-object"],"tags":["seed"],"content":"Strategic Translation\nStrategic translation: pollution, data, and Indigenous Traditional Knowledge by Sarah Blacker\nWe look to ‘Water is a living thing’ as a case study. Here, two First Nations communities ”chose to partially translate their knowledge into data because doing so enabled them to enter into dialogue with policy-makers — with data as the lingua franca - to participate in science, and to retain control over their own data.”\nThis is an act of protest against the settler colonial state. Data as a form of partial translation which serves as a boundary object\nScience is not apolitical, nor measurement neutral.\n\n“The practices through which data is collected are informed by the social and political context where the science is being carried out as well as by the assumptions and positionality of the scientists themselves.”\n“Because government metrics of contamination are framed as objective and politically neutral (Hoover 2013), they are difficult to challenge, particularly for economically marginalized and racialized communities that are disproportionately affected by environmental contamination”\n\nThis is also not always true either (e.g. lobbying, strategic funding of specific outcomes in research, etc.)\ntldr; Who is to say the government is objective?\ne.g. ‘science muzzling’ (read: censorship) under the Harper government\n\n\n\nMcLachlan’s three-track method\n\nused in a collaboration between the two First Nations communities and non-Indigenous scientists in order to produce a document that could be circulated in the form of data\nTracks\n\nNarrative: Articulates TK about environmental environmental contamination\nNumerical: Provides measurements of contamination levels using current industry standards\nSynthesis: combination of the above\n\n\n\nSome argue that “local knowledge is altered when it is removed from ‘its embeddedness in a holistic cultural and political context’ so that it can be made comparable, classifiable, and commensurable”\n\n[S]cientists look at very thin slices of stuff. They don’t look at the whole book, they look at one word on a page and try to define. Somebody’s got to put the book together. But if you can’t see the whole book, you can’t do it. That’s the trouble with scientists. Where the traditional knowledge is like you have the whole book. You may not be able to say exactly why, what causes this, what causes that. But you can sure see the changes.\n\nSeems to mutual distrust between First Nations groups and government + industry\n\nFirst Nations groups don’t trust govt + industry as the data they collect is not holistic and doesn’t incorporate TK\nGovernment + industry doesn’t trust First Nations as they doubt their scientific abilities and incapability of producing ‘objective’ data\n\nDid not end up working under Harper government, but First Nations communities were undeterred.\nOpen Access\nFrom Kimberly Christen’s “Does Information Really Want to be Free?” and Salomé Viljoen’s “A Relational Theory of Data Governance”\nContests over access to knowledge arise because of the historical conditions that meant that indigenous people lost control over how and what knowledge was to be circulated\nData and information should only be common use after voluntary communication to others. This is not the case for the vast majority of traditional knowledge.\nPublic domain instead violates indigenous peoples’ rights by defining their collective works as “folklore” and excluding their protection via copyright system"},"thoughts/tragedy-of-the-commons":{"title":"Tragedy of the Commons","links":["thoughts/funding","posts/paid-open-source","thoughts/Kant","thoughts/game-theory"],"tags":["sapling","pattern"],"content":"\nThe more accessible a good is, the less people want to pay for it. —Scott Moore\n\nA government only has interest in funding its own roads, companies like Facebook only have interest in funding their own infrastructure. Very prevalent in open-source, some of the most used software in the world often goes unpaid and unfunded.\nCommons in the modern economic context is any open-access and unregulated resource (e.g. atmosphere, ocean, rivers, etc.)\n\nHow we might focus less on speculation and more on participation?\n\nWould be solved if people acted according Kant’s Categorical Imperative\nWithout Tragedy\nElinor Ostrom defines 7 requirements to govern a commons without tragedy:\n\nClear boundaries (private)\nManaged by locals\nCommunity makes its own rules\nCommunity can monitor behavior\nGraduated sanctions for those who violate community rules\nCheap, accessible means of conflict resolution\nSelf-determination\n\nIn a small community, everybody knows everybody, and can keep track of what they do. This makes small groups iterated games which rewards trust and penalizes sociopathic behaviour"},"thoughts/transformers":{"title":"Transformer Models","links":["thoughts/LLMs","thoughts/LSTM","thoughts/regularization","thoughts/gradient-descent","thoughts/semantics","thoughts/NLP","thoughts/probabilistic-classifier"],"tags":["seed"],"content":"See also: LLMs\nAt a high-level, we can think of a transformer model as taking an input sequence of tokens of length n and predicting the next token at index n+1.\nMost implementations of transformers are autoregressive, meaning that it predicts future values (index n+1 to ∞) from past values (index 0 to n).\nInference\nMainly derived from Brendan Bycroft’s amazing LLM visualization\nEmbedding\nThe smallest unit of understanding for a transformer is a token. This is usually a common sequence of characters like ‘at’ or ‘qu’.\nThe collection of all the tokens the model understands is its vocabulary. The vocabulary maps the token to its index:\n\nToken A: index 0\nToken B: index 1\nToken C: index 2\n\nThe first step of a transformer is turning the input text into the appropriate index in the vocabulary table.\nThen, we use the token index to select the associated column in the token embedding matrix (e.g. the 3rd token index corresponds to the 3rd column of the token embedding matrix). The values of the token embedding matrix are vectors which we call the token embeddings. The token embedding matrix is [Cembed​,nvocab​] where Cembed​ is the dimensionality of this embedding.\nThen, based on the index of the token in the input, we use it to select an appropriate column of the position embedding matrix. The dimensionality of this is the same as Cembed​. We need position embeddings because, unlike LSTMs which operate sequentially, Transformers operate over the whole input sequence at once so it loses information related to token order.\n\n\n                  \n                  Why can&#039;t we just use a column vector of what index the token is? \n                  \n                \nA few good criteria for a positional embedding function:\n\nIt should output a unique encoding for each time-step (word’s position in a sentence)\nDistance between any two time-steps should be consistent across sentences with different lengths.\nOur model should generalize to longer sentences without any efforts. Its values should be bounded.\nIt must be deterministic.\n\nWe try to regularize our weights to ensure they stay close to zero (but not zero exactly!). This would disproportionately distort the embeddings of later tokens! Even if we embed it as a fraction of the total sequence length (so that the position embedding for the first token is a column-vector of 0 and the 1 for the last token), that wouldn’t work either as for different lengths of inputs, we would get different position embeddings for the same token position.\nThe people who wrote the original Transformer paper “Attention Is All You Need” came up with a very smart way of representing position by using:\n\nPE(pos,2i)​=sin(pos/100002i/Cembed​) for even dimensions and\nPE(pos,2i+1)​=cos(pos/100002i/Cembed​) for odd dimensions\n\nThe key part for avoiding collisions was the fact that the wavelength changes depending on i, the dimension. Even if the positional encoding of the first dimension of position x and position y are the same (due to periodicity in sinusoidal functions), it is very unlikely that given a reasonably high Cembed​, we get the same positional embedding for different i.\nThe intuition for this comes from how we represent numbers in binary.\n\n0000\n0001\n0010\n0011\n0100\n0101\n0110\n0111\n1000\n1001\n1010\n1011\n1100\n1101\n1110\n1111\n\nBits in different positions have ‘differing rates of change’. The LSB flips every number, the second-LSB flips every other number, etc. and we get a unique encoding of every number. Using discrete values isn’t great for gradient descent so what’s the continuous version of what’s happening here? Sinusoidal functions.\nThe paper authors also experimented with learned positional embeddings and found similar performance but ultimately chose the sinusoidal version as it meant that the model can extrapolate to sequence lengths outside ones encountered in training.\n\nToken embeddings are learned during training whereas positional encodings can either be fixed or learned. As both embeddings have the same dimensionality, we simply perform an element-wise addition to get the input embedding.\nRunning this for all T of the input tokens gives us the input embedding matrix of size [Cembed​,T]. This corresponds to a column vector [Cembed​,1] for each token.\n\n\n                  \n                  Conceptual Intuition \n                  \n                \nWe are mapping each token to some coordinates in embedding space so the model can learn and understand the semantics of each token.\n\nLayer Norm\nNormalization is an important step in the training of deep neural networks, and it helps improve the stability of the model during training.\nWe do this for each column of the input embedding matrix separately. The goal is to make the average value in the column equal to 0 and the standard deviation equal to 1. To do this, we find both of these quantities (mean μ and standard deviation σ) for the column and then subtract the average and divide by the standard deviation. Finally, we multiply by a learned weight γ and a learned bias β.\nThat is, for each  x∈X where X is a [Cembed​,1] column of the input embedding matrix, then we do\nxnorm​=σ+ϵx−μ​⋅γ+β\nWe add an additional small ϵ=10−5 to prevent dividing by zero. This produces the layer norm matrix LN of size [Cembed​,T].\nTransformer Block\nAs is common in deep learning, it’s hard to say exactly what each of these layers is doing, but we have some general ideas: the earlier layers tend to focus on learning lower-level features and patterns, while the later layers learn to recognize and understand higher-level abstractions and relationships.\nIn the context of NLP, the lower layers might learn grammar, syntax, and simple word associations, while the higher layers might capture more complex semantic relationships, discourse structures, and context-dependent meaning.\nSelf-attention\nThe first step is to produce three vectors for each of the T columns from the normalized input embedding matrix. These vectors are the:\n\nQ: query vector [A,T]\nK: key vector [A,T]\nV: value vector [A,T]\n\nA is the dimensionality of the Q/K/V vectors (it’s convention to set A=Cembed​/n where n is the number of attention heads). For each of Q, K, and V, we have associated learned values for the bias [A,1] and the weights [A,Cembed​].\nTo compute Q, for example, we do QWLN+QB. Note that this matrix-vector addition isn’t normally mathematically valid as we are adding a matrix of [A,T] to a vector of [A,1] but we treat it QB as an [A,T] matrix where each column is the original [A,1] vector.\nWe can think of this as each self-attention block as a graph with A nodes. Then,\n\nK corresponds roughly to ‘what do I have’\nQ corresponds roughly to ‘what am I looking for’\nV corresponds roughly to ‘what information do I share with others’\n\nWe can think of ‘attention’ as some node A asking some node B for information:\n\nWe do QA​KBT​ to get the [T,T] self-attention matrix and then divide by A​.\nThen, we normalize the self-attention matrix with softmax which scales them into probabilities so that each row adds up to a probability of 1.\nWe finally multiply the normalized self-attention matrix with VB​ to get our attention output of size [A,T].\n\nAttention(Q,K,V)=softmax(A​QKT​)V\nThe main goal of self-attention is that each column wants to find relevant information from other columns and extract their values, and does so by comparing its query vector to the keys of those other columns. We also add restriction that it can only look in the past (i.e. causal self-attention).\nThis self-attention step is run in parallel (multi-headed self-attention). To combine the outputs of the attention heads, we simply stack them on top of each other to get the attention output of size [Cembed​,T].\n\n\n                  \n                  Conceptual Intuition \n                  \n                \nThe animal didn&#039;t cross the street because it was too tired\nWhat does ‘it’ in this sentence refer to? Is it referring to the street or to the animal? When the model is processing the word ‘it’, self-attention allows it to associate ‘it’ with ‘animal’. As the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.\n\nWe can also think of attention as communication in a directed graph of vectors: source from ‘Introduction to Transformers w/ Andrej Karpathy’\n\nProjection\nFinally, we perform the projection to get the output of the layer. This is a simple matrix-vector multiplication on a per-column basis, with a bias added.\nResidual=ProjWAttnOutput+ProjB\nInstead of passing this output directly to the next phase, we add it element-wise to the input embedding. This process is called the residual connection or residual pathway.\nMLP\nLike with self-attention, we perform a layer normalization before the vectors enter the MLP.\nEach MLP block has\n\nweights W (a [Cembed​,4Cembed​] matrix)\nbias b (a [4Cembed​,1] column vector)\nprojection weights WProj​ (a [Cembed​,4Cembed​] matrix)\nprojection bias bProj​ (a [Cembed​,1] column vector)\n\nEach [Cembed​,1] vector x from the layer-normed attention residual then, individually:\n\nWTx+b to produce a [4Cembed​,1] column vector\nGELU element wise to produce x~\nWProj​x~+bProj​ to produce a [Cembed​,1] column vector\n\nGELU example\nThis is assembled into the MLP result of size [Cembed​,T]. Like in the self-attention + projection section, we add the result of the MLP to its input, element-wise to produce the MLP residual.\nThis marks the end of the transformer block and the output is ready to be passed to the next block.\nOutput\nFinally, at the end of all the transformer blocks, we perform one final softmax, which helps convert the output into probabilities.\nprobabilistic-classifier#multi-class-probabilities\nWe then take this [Cembed​,T] output block and do a final matrix multiply with another set of learned weights called the language modelling head weights (LM weights) which is a [nvocab​,Cembed​] matrix.\nThis produces the logits of size [nvocab​,T]. The name “logits” comes from “log-odds,” i.e., the logarithm of the odds of each token. Finally, we softmax this again to exponentiate the log-odds to normal odds/probabilities.\nNow, for each column, we have a probability the model assigns to each word in the vocabulary. Then, we can ‘decode’ the final probability back into a token. For example if we’ve supplied six tokens into the model, we’ll use the output probabilities of the 6th column.\nWe do this by “sampling from the distribution.” That is, we randomly choose a token, weighted by its probability. For example, a token with a probability of 0.9 will be chosen 90% of the time.\nWe can also control the “smoothness” of the distribution by using a temperature parameter. A higher temperature will make the distribution more uniform, and a lower temperature will make it more concentrated on the highest probability tokens."},"thoughts/transitive-closure":{"title":"Transitive Closure","links":[],"tags":["seed"],"content":"\nLet:\n\nX be the set of elements\nR be the set of relations\nR2 be the set of relations which can be reached from two hops of relations in R\n\nFor example, (a,c) is reached by going through (a,b) -&gt; (b,c)\n\n\nR∗ is the entire transitive closure over R\n"},"thoughts/transparency":{"title":"Transparency","links":["thoughts/black-box"],"tags":["seed"],"content":"Source: What’s in a Category: Definitions of Authenticity, Transparency, and the Social-Bot by Eseohe Ojo\nThe process of identifying users is how social media platforms monetize their operations\nBig companies avoid fully detailing how they identify and categorize uses, claiming that the information is “proprietary”.\n“They argue, this surveillance allows them ways to minimize the gamification of the social media system by disruptive actors, and by segmentation and differentiation, they may better protect the authentic user from the inauthentic.”\nAlgorithmic Transparency in the News Media\nPDF in Digital Journalism\n\nDeuze (2005, 455) defines transparency as the “ways in which people both inside and external to journalism are given a chance to monitor, check, criticize and even intervene in the journalistic process.”\nSimilarly, defines algorithmic transparency: what we define as the disclosure of information about algorithms to enable monitoring, checking, criticism, or intervention by interested parties.\nEven though transparency is not a new concept for holding governments and institutions accountable, its recent renaissance has been accompanied by changes in communication technologies (Fung, Graham, and Weil 2007)\n\nDigital technologies have changed the access to and scrutiny of information by anyone with internet access, which Meijer (2009) broadly described as “computer-mediated transparency.”\n\n\nTransparency is generally considered a means to see the truth and motives behind people’s actions (Balkin 1999) and to ensure social accountability and trust (Breton 2006). On a very basic level, transparency allows access to more information which can influence power relationships between governments and citizens, business and customers, and in our case between news outlets and audiences (Bennis 2013). The access to more information also reduces uncertainty in social relations and theoretically increases trust (Cotterrell 1999), which is crucial in the maintenance of a functional society (Fukuyama 1995).\nBut while transparency can be seen as beneficial to engendering trust, seeing the inner workings of a government, business, or newsroom can result in negative implications such as undermining competitive advantages or creating costs without concomitant gains (Granados and Gupta 2013).\nChallenges\n\nWhat would motivate a news or media organization to disclose details about their algorithms? Costs identified in producing transparency information included: data preparation, documentation writing, source code polishing, and benchmark testing. “How does being transparent offset loss of revenue?”\nParticipants also suggested that disclosing aspects of how a proprietary algorithmic system works may hurt an organization’s technological competitive advantage, or open the system to manipulation by third parties. Yet it was recognized that people will game the system no matter what, and that by disclosing information publicly it would level the playing field or, as one participant (CS2) put it, “everybody has the same chance now because we all know the rules of the game.”\n\n\n\nCited as:\n\nDiakopoulos, N., &amp; Koliska, M. (2017). Algorithmic transparency in the news media. Digital journalism, 5(7), 809-828.\n\nExplanations as Mechanisms for Supporting Algorithmic Transparency\nPDF in CHI 2018\n\nTransparency involves encountering non-obvious information that is difficult for an individual to learn or experience directly, about how and why a system works the way it does and what this means for the system’s outputs.\nGreater transparency allows people to question and critique a system in order to develop appropriate reliance, rather than blind faith\nMethods for transparency\n\nalgorithm audits, which investigate both how an algorithmic decision making system works, and what its impacts are\n\nsome have argued that platforms are intentionally opaque regarding details about their operation as a form of self-protection from competitors or others who attempt to “game” the system\n\n\nproviding explanations, a common approach in recommender systems that may help solve problems caused by lack of transparency in algorithmic decision-making systems\n\n\nHow explanations are “white box descriptions”\n\nThey provide information about how a system produces a recommendation, particularly focusing on the system’s reasoning and data source\n\n\nWhy explanations are ”black box descriptions”\n\nproviding justifications for a system and its outcomes and explaining the motivations behind the system, but not disclosing how the system works.\nThese explanations fill an intention gap between a user’s needs and interests and the system’s goals, but do not provide any visibility into how the system works\n\n\nTransparency of mechanism vs outcome\n\nCited as:\n\nRader, E., Cotter, K., &amp; Cho, J. (2018, April). Explanations as mechanisms for supporting algorithmic transparency. In Proceedings of the 2018 CHI conference on human factors in computing systems (pp. 1-13).\n\nACM Principles for Algorithmic Transparency and Accountability\n\nAwareness: stakeholders of analytic systems should be aware of the biases + risks + potential harms their design, implementation, and use could cause\nAccess and redress: encourage the adoption of mechanisms that allow questioning by and remediation for groups adversely affected by algorithmic decision making systems\nAccountability: institutions should be held responsible for decisions made by their algorithms, even if how the model arrived at its results is inexplicable\nExplanation: systems are encouraged to produce explanations of model output (regarding both procedures followed by algorithm as well as decisions made)\nData Provenance: describe how the data was collected and should be maintained, along with an exploration of the potential biases induced by the data gathering process\nAuditability: models, algorithms, data, and decisions should be recorded so that they can be audited in cases where harm is suspected\nValidation + Testing: institutions should routinely use rigorous methods to validate their models (against discriminatory harm for example) and documents those methods.\n"},"thoughts/tribe-flourishing":{"title":"Tribe Flourishing","links":["posts/collaborative-thinking","thoughts/communities","thoughts/friendship","posts/context-collapse","thoughts/group-limits","thoughts/interdependence","trust","thoughts/play","thoughts/friction"],"tags":["sapling"],"content":"Related: collaborative-thinking, communities, friendship, and context-collapse\nWhy are we more ourselves in small group chats than our posts? Why are we more comfortable trying new things and working on projects in these smaller groups?\nIs it because of bureaucratic friction at scale? Group limits?\nHow do we move away from the ‘other’ and towards the ‘us’? Towards collectivism and interdependence rather than individualism?\nOn Building with a Squad\nBy Jon Borichevskiy\n\nIt is not a startup, nor an organization with a mission statement, nor a non-profit, nor a consultancy… Our calls are space we hold in which we invite one other to explore and create what wants to be built at the intersection of all our interests and diverse perspectives and past experiences. A squad is a collective identity in which I can participate to create something more intricate, comprehensive, and wonderful than with just myself.\n\nIt’s trusting everyone else in the group is committed to upholding a high-openness, high-trust, playful mode of co-creating and exploration. It’s having the confidence that whatever comes up – we will make space for the collective group wisdom to work through it and come out on the other end stronger, together.\nTakeaways\n\nShared knowledge graph tooling is a common pain point and one no one has solved well\nFriction-ful onboarding can be good! “What does work is having people join our open calls, playing with ideas and tasks they resonate with, and progressively getting more involved”\n\nSquad Wealth\nSource: Other Internet on Squad Wealth\n“Squad culture is the antithesis of neoliberal individualism. Millennials are healing from decades of irony poisoning, rediscovering what it’s like to have generative, exploratory relationships with one another.”\nThe squad economy primarily yields non-monetary forms of value\nSmall Group\nSource: James Mulholland\n\nThe SMALL GROUP offers a private, close-knit environment in which members can share ideas freely.\n\nBenjamin Franklin had the Junto Club, Tolkien and C.S. Lewis had The Inklings, Jobs and Wozniak had Homebrew. The Bloomsbury Group was integral to the success of Virginia Woolf, Clive Bell, and John Maynard Keynes, while MIT’s Model Railroad Club spawned much of modern hacker culture.\nAround a dozen members is the sweet spot of social motivation: small enough to know everyone, yet large enough that the group won’t collapse if one or two members’ enthusiasm wanes; small enough that you are not daunted by competing with the whole world, yet large enough that you still need to be on your toes to keep up.\nAn ongoing relationship provides more effective advice, allowing the use of shorthand for concepts and a two-way conversation that autodidactic education lacks.\nThe goal here is not to invest more in the skills you use at work. Instead, it is to be truly exploratory for no immediate purpose. It is to waste time (yet to savour it), to wander off in the wrong direction (and to find an exciting new path). Indirection and exploration should not come at the cost of doing and building. Doing and building should not come at the cost of having fun.\nAgainst fully online communities\nFirst, they shift the emphasis towards consumption, not creation. How many tweets do you write versus how many do you read, for example? Communicating in real-life shifts the ratio of creation to consumption far closer to 1:1, thus forcing you to fully develop your ideas."},"thoughts/trust":{"title":"Trust","links":["thoughts/philosophy-of-science","thoughts/Descartes'-Meditations","thoughts/game-theory","thoughts/epistemic-authority","thoughts/intentionality","thoughts/Extended-Mind-Hypothesis","thoughts/blockchain","thoughts/accountability"],"tags":["sapling"],"content":"\nIn an empire of lies, telling the truth is a revolutionary act. In a fearful society; love and trust are the primary tools of resistance.\n\nSource: Kernel on Trust\n\nTrust is only meaningful once we have fully understood how people can lie.\n\nHaving clearly defined and encoded rules means that there is an implicit shift from trusting those who own the medium to those who are transacting. In essence, trusting peers rather than a regulatory power.\nCurious whether this has relations to philosophy of science and Cartesian skepticism\nRelated: game theory and trust, epistemic authority\n\nTo dream up important ideas you must think like an idealist; to build systems that will live up to those dreams, you must think like an adversary.\n\nTrust as an Unquestioning Attitude\nC. Thi Nguyen\n“We inhabit trust like we inhabit the air, and we only notice it when it has departed.”\n\nMost theories of trust presume that trust is a conscious attitude that can be directed at only other agents. I sketch a different form of trust: the unquestioning attitude. What it is to trust, in this sense, is not simply to rely on something, but to rely on it unquestioningly. It is to rely on a resource while suspending deliberation over its reliability\n\nBaier’s Goodwill Theory\nColloquial use of “trust” blurs together two very distinct concepts\n\nMere reliance: depending on something\nAttitude of trust: depend in some more normatively loaded manner\n\nE.g. failing to show up after having promised to do so: “I had trusted you and you let me down”\nGenerally applies to things/people within our integrative stance — things we take to be part of us and towards things with which we are supposed to be integrating to form some larger whole\nThe normativity here arises, not from there being any moral commitments in play, but from teleological integration: “The external objects that evoke the strongest sense of betrayal are those whose functions are most tightly integrated into our own thinking and functioning”\n\n\n\nQuestion: can objects be trusted in the normative sense? Can we feel betrayed by objects?\nThis involves ascribing goodwill to the trusted and the sense of betrayal comes from discovery that there is no such goodwill after all.\nResponsiveness Theories\nThinking that the fact that you trust in them/it will give a reason to fulfill that trust.\n\na trustworthy person “takes the fact that they are counted on to be a reason for acting as counted on” (Jones 2012, 66)\n\nBetrayal of trust here is failure to be properly responsive\nHawley’s definition of trust is that “to trust somebody is to take them to have made a commitment to do something and to rely on them to fulfill that commitment. Hawley’s account grounds the sence of betrayal in the trusted person’s failure to live up to their commitments”\nNon-agent based theories\nYet all of these theories share the presumption of agent-directedness or intentionality.\nRelated, agential gullibility and the Extended mind Hypothesis\n\n“The veteran also suffers from a problem of trust, a building block on which all of social life is erected. The everyday, taken-for-granted reality of civilian life ignores much; civility assumes the nonlethal intentions of others. In war, however, all such assumptions evaporate: one cannot trust the ground one walks on, the air one breathes, nor can one expect with full assuredness that tomorrow will come again.” (Kearl 1989, 353)\n…\nThe fact that many philosophers find it odd to speak of being betrayed by their environment is perhaps best explained by the fact that most philosophers have lead, by and large, pretty cushy lives.\n\nInteresting to distinguish between what we are trusting when we trust designed (e.g. search engines, devices, websites, etc.) and non-designed objects (the ground, physics, etc.)\nTo lose trust is to shift from the unquestioning state to the endlessly skeptical and suspicious mood.\nWeb3\n“Because blockchains allow us to define succinctly our shared truths, and because the record itself is shared across all participants, there is a whole new “trust space” we can explore, searching for more valuable kinds of transactions impossible within merely legal fictions.”\nTrust Circles\nFrom Buzzard\n\nTrust between human and non-human systems\nTrust has historically distinguished from mere reliance (Baier, 1986, p. 242) through an attitude of trust or extra factors which distinguish genuine trust from mere reliance (Hawley, 2014, p. 1) that we take to inanimate objects. However, algorithms and computerized decision making systems are beginning to play larger roles in our society — deciding jail time for criminals, giving medical diagnoses, and many more. How should we weigh the epistemic authority or trustworthiness of human versus non-human expert systems?\nI posit that, until these algorithmic systems are able to reliably be held accountable for their decision making, they should not be epistemologically load-bearing. These systems should supplement human decision making rather than be considered an epistemic authority in and of itself. Let us construct a case study to examine this in more detail.\nSuppose you are a hiring manager at a tech company. There is a potential candidate in the pipeline for you company that you are very on the fence about whether to hire or not. She has an incredibly strong ‘yes’ recommendation from a more senior hiring manager. You don’t know this higher up very well but you know that her and this candidate are close friends already. On the other hand, the company uses an internal AI-powered candidate ranking system. This system is quite complex and the original engineers who designed it have since long left the company. This system gives this candidate a strong ‘no’ hire recommendation.\nIn this situation, both systems are ‘authorities’, having been approved by the company for use in the hiring process. The more senior hiring manager is clearly an expert, having been working in this company and hiring many stellar employees in the past. The algorithm can also be considered an expert here, having scored extremely highly on tests of accuracy in predicting based off of historical data whether candidates will do well in the company. It has been vetted for internal use.\nHowever, it is important to note here that in the case example, while both systems are potentially biased, it is far more likely that the algorithmic system is biased.\nThe senior hiring manager could potentially be doxastically partial towards her friend but not because it is normative to be always partial to our friends. Notably, Crawford defines being a good friend constitutively involving forming attitudes about one’s friends that are appropriately responsive to the features that one’s friends have that appear to warrant those attitudes (2019, p. 1). It is unknown to you whether the senior hiring manager has any state-given reason to highly recommend her friend, so we cannot assume this to be the case as it is an unbased claim (as we have no evidence to believe so). Thus, we have solid reason to assume that the manager’s friend actually does have those features that she believes makes them such a good candidate.\nHowever, there is one clear detail in this case that makes the algorithmic expert far more likely to be biased: it is trained on historical data that has been sanitized and decontextualized. In fact, historical data shows that in the past there have been more men in the women in the workforce. The forbidden base rate (Gendler, 2011) here is the statistical information about the relative number of male and female employees in the tech industry. If the data was sampled at random, then it is statistically optimal for the algorithm to prefer male applicants to female applicants rather than purely on the basis of qualification for the position. This, while epistemically rational, may not be the correct choice of action for moral reasons.\nLastly, I put forth the concept of epistemic accountability, a measure of whether there are ways to holding the agent in question accountable for their doxastic claims. Accountability here refers to the ability to reduce the epistemic trust in an authority after violating an epistemic norm (e.g. being incorrect). I argue that the algorithmic authority cannot be held accountable for its actions as it does not have capacity as an epistemic agent on its own — it cannot be held accountable for its decisions. As the algorithm itself is a designed object we can instead attribute it forms of derived trust whereas the trust is not only in the algorithm itself, but its designer (the engineers who created the algorithm, the data engineers who sourced and cleaned the data) or experts who know how to operate it. As both types of progenitors of this type of trust are absent, it would be epistemically irrational to trust this algorithm.\nIn conclusion, it is clear that despite potential biases from both parties, the algorithmic authority has clear flaws in its ability to be held accountable as an epistemic agent and highly likely to be partial against the female candidate due to the forbidden base rate in this case study. It is much more likely that the senior hiring manager is a trustworthy epistemic agent.\nReferences\n\nBaier A. 1986. Trust and antitrust. Ethics 96:231–60.\nHawley, K., 2014. Partiality and prejudice in trusting. Synthese, 191(9), pp. 2029-2045.\nCrawford, L., 2019. Believing the best: on doxastic partiality in friendship. Synthese, 196(4), pp. 1575-1593.\nGendler, T. 2011. On the Epistemic Cost of Implicit Bias. Philosophical Studies 156(57), pp. 33-63\n"},"thoughts/truth":{"title":"Truth","links":["thoughts/verisimilitude","thoughts/Descartes'-Meditations"],"tags":["sapling"],"content":"See also: verisimilitude\nIs there a hard truth to anything in the universe? Can we truly know anything?\n“The word “axiom” means self evident truth. All of mathematics are models based on frameworks of axioms and postulates. We develop proofs and theorems from these. All facts and axioms in mathematics are fundamentally flawed.\nWhat are the universal axioms of the universe? Descartes for example believes in solipsism (we can only know that we exist)\nGödel’s incompleteness theorem\nA great explanation video\nThere will always be statements that are true that cannot be proven\nIt basically states that if math is consistent, it cannot be complete. And if it is not complete, it must not be consistent.”\nRelated to Conway’s Game of Life and whether we can tell a program can halt or not."},"thoughts/types-of-goods":{"title":"Types of goods","links":["thoughts/public-goods"],"tags":["seed"],"content":"Main types of goods\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExcludableNon-excludableRivalrousPrivate Goods (e.g. cars, domain names)Commons (e.g. forests, online privacy)Non-rivalrousClub Goods (e.g. cable, subscriptions like Netflix or Spotify)Public Goods (e.g. air, open source code)\nSee also: public goods"},"thoughts/unsupervised-learning":{"title":"Unsupervised learning","links":["thoughts/supervised-learning","thoughts/outlier-detection","thoughts/clustering"],"tags":["seed","CPSC340"],"content":"In supervised learning, we have features xi​ and class labels yi​. Write a program that produces yi​ form xi​\nIn unsupervised learning, we only have xi​ values, but no explicit target labels. We can\n\nOutlier detection: is this a normal xi​?\nSimilarity/Clustering: which examples look like this xi​\nWhich xi​ occur together\nLatent-factors: what ‘parts’ are the xi​ made from\nData visualization: what does the high-dimension X feature space look like?\nRanking: what are the most important xi​?\n"},"thoughts/urban-planning":{"title":"Urban Planning","links":["thoughts/housing"],"tags":["seed"],"content":"Brand argues that many great buildings achieved their greatness by gradual stepwise evolution over time. New buildings need to be designed with the expectation that they will evolve, because they usually outlive their initial use. Even if a building does not change its use (e.g. it remains the same university department), the needs of its users will change and the building will need to adapt. This should be done, for example, by making sure that changing the space layout in the building is possible without changing the structure.\n\nAlmost no buildings adapt well. They’re designed not to adapt; also budgeted and financed not to, constructed not to, administered not to, maintained not to, regulated and taxed not to, even remodelled not to. But all buildings (…) adapt anyway, however poorly, because the usages in and around them are changing constantly.\n\nSee also: housing"},"thoughts/user-involvement":{"title":"User Involvement","links":["thoughts/Design-Justice","thoughts/human-centered-design","thoughts/design-requirements"],"tags":["seed"],"content":"The underlying aim is to involve members of the public in helping them make a change in their lives where technology is often viewed as an integral part of the process. This is the point of participatory design.\nThe main principles of user centered design (Gould and Lewis) are as follows:\n\nEarly focus on users and tasks: Understanding who users will be (cognitive, behavioural, anthropomorphic, and attitudinal characteristics) and observing users doing normal tasks, studying nature of normal tasks, involving users in design process.\nEmpirical measurement: early in development, get reactions and performance of intended users to scenarios/manuals/etc. Then, later in development, get reactions and performance of users to simulations and prototypes. Where possible, specific usability and UX goals should be identified, clearly documented and agreed upon at the beginning of the project which can help designers choose between alternative designs and check on progress.\nIterative design: when problems are found in user testing, they are fixed, then more tests and observations carried out to see effects of fixes. This allows the design to be refined based on feedback\n\nBenefits:\n\nExpectation management: ensuring that the users’ expectations of the new product are realistic and no surprises for users when the product arrives.\nOwnership: users who are involved and feel that they have contributed to a product’s development are more likely to feel a sense of ownerships toward it and support its user\n\nThe process of this is the double diamond design.\nParticipation\nWide participation helps bring different perspectives to the process, which enhances design itself, produces more user satisfaction with the final product, and engenders a sense of ownership.\nParticipatory/cooperative/co-design is an overarching design philosophy that places stakeholders as central actors in creation activities. A stakeholder is anybody who is affected by the item"},"thoughts/utility":{"title":"Utility","links":[],"tags":["seed","PHIL321A"],"content":"Utility is the tendency of an object to produce happiness or prevent unhappiness for an individual or a community.\nHow can we assign utilities to represent preferences?\nInterval Scales\n\nAssign to each outcome x a value v(x) such that v(x)≥v(y)⟺x≥y and v(x)=v(y)⟺x∼y\nTransformation is linear\nCalled an ordinal transformation\n\nOrdinal Scales must satisfy the following properties:\n\nCompleteness: x≻y or x∼y or y≻x\nAsymmetry: if x≻y then it is false that y≻x\nTransitivity: if x≻y and y≻z then x≻z\n\nInfinite Utility\nAn agent values A infinitely relative to B and C if we deny Continuity: [λA,(1−λ)C]≻B for any λ&gt;0\nThe agent is willing to trade B for any gamble that offers a positive chance of A, when the ‘losing outcome’ is C.)"},"thoughts/utopia":{"title":"Utopia","links":["thoughts/Utilitarianism","thoughts/utility","thoughts/The-ones-who-walk-away-from-Omelas","thoughts/fiction"],"tags":["sapling"],"content":"What is a utopia?\nUtilitarian perspective: a society where everyone can fully maximize their utility function\nIs one even possible?\nIn a scarce world with infinite wants, no. Our concept of utopia is related to our concept of abundance, and most believable forms of utopia depend on being able to leave Earth, thereby providing us with near-infinite resources.\nUtopia might only be a useful concept for people in higher resourced countries, it’s very difficult for people who are worried about day-to-day survival to imagine a utopia that is the same as first-world countries. Maslow’s hierachy of needs?\nDoes the concept of utopia requires a common definition of ‘good’? If so, how do we decide what good is in a maximally beneficial way? Utilitarianist approach of maximizing everyone’s utility function? Related to thoughts in The Ones Who Walk From Omelas\nProtopia\n\nI think our destination is neither utopia nor dystopia nor status quo, but protopia. Protopia is a state that is better than today than yesterday, although it might be only a little better (Kevin Kelly)\n\nWe don’t have much desire for life one hundred years from now. Many dread it. That makes it hard to take the future seriously. So we don’t take a generational perspective. We’re stuck in the short now instead of the long now."},"thoughts/value-setting":{"title":"Value Setting","links":["thoughts/taste"],"tags":["sapling"],"content":"\n“I’ve noticed that many people compete in games they don’t understand because they are modeling the behavior of people around them. Most common is the competition for wealth as a proxy for happiness.” —Michael Seibel\n\nTook on a large number of commitments in the past year, not everything was something I enjoyed. Been thinking about what makes work enjoyable\nAsk yourself, and listen closely to your answer: “What do I like about this world? What pains me about this world?” Spend time developing your taste for what is beautiful, as well as your sensitivity to what is discordant, unjust, ugly, wrong.\n\nAim to be fulfilled—not excessively rich. There’s a reason why lottery winners are just as miserable as they were before. Hedonistic adaptation is inescapable.\n\nSome insightful pieces of writing\n\nhttps://thesephist.com/posts/play/\nhttps://www.julian.com/blog/life-planning\nhttps://paukowee.wordpress.com/2021/03/06/values-setting/\n"},"thoughts/value":{"title":"Value","links":[],"tags":["sapling"],"content":"Source: Kernel on Value\nInteresting approach to taking a negative-space approach to defining values: the negation of what destroys value is then what must give it value.\nI’m not sure I fully agree, I don’t think values follow the contrapositive (e.g. A implies B is true does NOT necessarily mean that not A implies not B)"},"thoughts/verisimilitude":{"title":"Verisimilitude","links":["thoughts/philosophy-of-science","thoughts/Descartes'-Meditations"],"tags":["seed","pattern"],"content":"Truth is not binary, but rather a spectrum. Some propositions are closer to being true than other propositions.\nVery related to the work of Karl Popper and his philosophy of science.\nSee: Descartes’ Meditations"},"thoughts/virtual-worlds":{"title":"Virtual Worlds","links":["thoughts/pseudonymity","thoughts/digital-commons","thoughts/vr","thoughts/positive-sum","thoughts/incentives","thoughts/accountability","thoughts/Dunbar's-Number"],"tags":["sapling"],"content":"Bridging the real and the virtual\nNo matter how hard we try, our virtual selves will inevitably be tethered to our physical selves. Interface with these virtual worlds through looking at our screens, typing on our keyboards, etc. even with a pseudonymity that is not supposedly tied to anything in the physical realm.\n\nThe web is not only an intangible space; it is also a physical space made of brick, mortar, metal trailers, electronics containing magnetic and optical media, and fiber infrastructure. (Safiya Noble)\n\nRelated to: digital commons and VR\nScarcity\nIn a world where scarcity is constructed, do we even need to use it? Can we rely on manipulating abundance (agalmics) through incentives?\n\nI realized I had become used to better digital goods being gated and inaccessible to the public when I felt weird taking all the digital goods without pay. (Nancy)\n\nPhysical Touch\n\n“One concern, though, is that some social skills may not develop as effectively when so many interactions exist online. We learn how we are and aren’t supposed to act by observing others and then having opportunities to act out our observations ourselves.”\n\n“On the internet, you can pull the plug and walk away. There’s no forcing mechanism that makes us have to learn”: how do we keep people digitally accountable?\nDunbar and his colleagues demonstrated that very light touch triggers a cascade of endorphins that, in turn, are important for creating personal relationships. Can this be replicated virtually?"},"thoughts/virtue-ethics":{"title":"Virtue ethics","links":[],"tags":["seed","CPSC430"],"content":"The notion of virtue or excellence refers to reaching one’s highest potential. According to Aristotle, happiness derives from living a life of virtue.\nThere are two kinds of virtues:\n\nintellectual virtues: associated with reasoning and truth\nmoral virtues: habits and dispositions formed through repetition (e.g. telling the truth)\n\nnot simply a disposition to act in a particular way, it is also a disposition to feel in a particular way\n“We may even go so far as to state that the man who does not enjoy performing noble actions is not a good man at all”\n\n\n\nA morally right action then, is a right action is an action that a virtuous person, acting in character, would do in the same circumstances.\nVirtue ethics pays particular attention to the agent as well as the action and the consequences of the action. A good person does “the right thing at the right time for the right reason”\nWhat are the virtues humans need in order to flourish and be truly happy? Commonly, they are\n\nhonesty\njustice\nloyalty\n\nA vice is the opposite of a virtue. It is a character trait that prevents a human being from flourishing or being truly happy."},"thoughts/visualization":{"title":"Visualization","links":["thoughts/colour","thoughts/time","thoughts/causality"],"tags":["seed","book"],"content":"\nAt their best, graphics are instruments for reasoning about quantitative information\n\nMostly from the books Envisioning Information and The Visual Display of Quantitative Information by Edward R. Tufte\nHigh density visualization\n\nHigh-density designs allow viewers to select, to narrate, to recast and personalize data for their own uses.\n\nThe control of information is given over to viewers, not to editors, designers, or decorators\nThin data prompts suspicions: “What are they leaving out? Is that really everything they know? What are they hiding? Is that all they did?”\nIt is not how much information there is, but rather how effectively it is arranged.\nClutter and confusion are failures of design, not attributes of information\nColour\nFundamental uses of colour in information design\n\nTo label (colour as noun)\nTo measure (colour as quantity)\nTo represent or imitate reality (colour as representation)\nTo enliven it decorate\n\nTime\nSee also: time\nThe problem with time-series is that the simple passage of time is not a good explanatory variable: descriptive chronology is not causal explanation. However, time-series plots can be moved toward causal explanation by smuggling additional variables into the graphic design\nOne of the deepest and most powerful ideas in mathematics is the relationship between a differential formulation (such as a step-by-step process, like our “draw” function) and its integrated form (such as a function of time, or plot over time). (from Learnable Programming)\nLinks, Causal Arrows, Networks\nSource and catalogue\nServing simultaneously as images, equations, and verbal summaries, Feynmann diagrams are multimodal and thus, in practice, often modeless\nGeneral rules of thumbs:\n\nFocus on causality. Uncertainties in causal links can sometimes be shown graphically; the SARS diagram indicates uncertain transmission routes with dotted arrows\nMultiple sources and levels of data\nAnnotated linking lines. Links and arrows should have an appropriate specificity: when and how the link operates, strength and persistence of the link, credibility of evidence supporting the link\nAnnotated nouns. Things in diagrams should be appropriately described\n"},"thoughts/vr":{"title":"VR","links":["thoughts/Brains-in-a-Vat","thoughts/representation","thoughts/embedded-AI","thoughts/The-ones-who-walk-away-from-Omelas"],"tags":["sapling"],"content":"Physical bodies perceive the senses that are input from the VR machine. Brains in a Vat experiment? question of representation and whether we need embedded systems\nMore on this from Anthony Tan in their piece Life in the Metaverse.\nExpression of Self\nLGBT community – exploring gender identity. Super easy to express this online, and flexibility to do this more safely in an online space.\n\nWhen form is fluid, stereotypes, beauty, body standards, and gender norms become less restrictive. It’s harder to judge people’s appearances when you can choose exactly how you look.\n\nReality privilege\nWe can explore things and get fully immersed in a virtual world safely\nIs the concept of utopia only a reality for those who can afford to not worry about physiological health? Maslow’s hierarchy of needs\nProteus Effect\nThe Proteus effect describes a phenomenon in which the behavior of an individual, within virtual worlds, is changed by the characteristics of their avatar."},"thoughts/walking":{"title":"Walking","links":[],"tags":["seed"],"content":"Source\n\nWe are, after all, “the small bipeds with the giant dreams”\n\nNietzsche saw the link between walking and creativity. “There is nothing more revealing than to see a thinking person walking,” wrote Thomas Bernhard, “just as there is nothing more revealing than to see a walking person thinking.” A passionate walker herself, Rebecca Solnit has defined the act as “a state in which the mind, the body, and the world are aligned.”\nOn Why We Walk"},"thoughts/web3-critique":{"title":"Web3 Critique","links":["thoughts/infrastructure","thoughts/decentralization","thoughts/blockchain","thoughts/web3","thoughts/interoperability","thoughts/transparency","thoughts/Vanilla-Ice-Cream-effect","thoughts/trust","thoughts/Degraded-Blockchain-problem"],"tags":["sapling"],"content":"Blockchains only really make sense for a very narrow subset of applications where\n\nPeer-to-peer operation: software that can be run by anyone, and messages are passed directly between them\nStrict global consensus: all peers must agree on exactly the same results\n\nTill date, there are only two applications where both criteria are necessary: money &amp; identity (and technically not strictly necessary either)\nMore in Polynya on Blockchain Apps\nMoxie\nReally good piece by Moxie on web3\n\nWeb3 lacks infrastructure\n\nBy definition, infrastructure does not need to be rebuilt every time they are used. To ask people to throw away their infrastructure is rather stupid.\nServers are infrastructure! Nodes are infrastructure! People won’t want to run these themselves. Until we reach a point where enough web3 platforms are able to provide this same level of infrastructure, we’re going to still end up with centralization.\n“If there’s one thing I hope we’ve learned about the world, it’s that people do not want to run their own servers. The companies that emerged offering to do that for you instead were successful, and the companies that iterated on new functionality based on what is possible with those networks were even more successful.”\n\n\nDecentralization is not always good. The more I get involved with the space, the more I am certain that the main value add of blockchain and web3 is not decentralization but rather interoperability and transparency.\n\nDecentralization also ends up making progress very difficult. See: Vanilla Ice Cream effect\n“This isn’t a funding issue. If something is truly decentralized, it becomes very difficult to change, and often remains stuck in time.”\n\n\nBlind trust. Crypto folks don’t necessarily trust the people but some just blindly trust the medium. If people don’t understand how the medium works to facilitate transactions, how can they trust it? Transparency also involves transparency into its inner workings.\n\nSimilarly, “Almost all dApps use either Infura or Alchemy in order to interact with the blockchain.”\nWe just rely on these two pieces of critical (centralized!) pieces of infrastructure to produce the correct results and not be bad actors.\n\n\nDegraded Blockchain problem\n\nObessions with profit\nWeb3 supposed allows us to ‘codify’ the set of values that a public goods represents. Yet, in practice in web3, “little space has been made for different values to be discussed or enacted. Which is why, in the absence of ways to enact our shared values, we default to the lowest common denominator: profit.”\nThe United States Dollar does not have a responsibility to profit its holders. A cryptocurrency is a monetary instrument, not a business."},"thoughts/web3":{"title":"Web3","links":["thoughts/cryptography","thoughts/social-contracts","thoughts/decentralization","thoughts/web3-critique","thoughts/blockchain","thoughts/public-goods","thoughts/dao","thoughts/quadratic-funding","thoughts/proof-of-stake","thoughts/proof-of-work","thoughts/consensus","thoughts/play","thoughts/NFT","thoughts/Arweave","thoughts/Kernel-Curriculum","thoughts/Internet","thoughts/Protocol","thoughts/peer-to-peer","thoughts/privacy","posts/paid-open-source","thoughts/funding","thoughts/decentralized-marketplace"],"tags":["sapling"],"content":"\nWeb3 is the open-source ethic extended beyond code as static artifacts but rather as live organisms.\n\nCrypto systems are interesting, they are “decentralized, jurisdictionless entities that exist entirely in cyberspace, maintained by a combination of cryptography, economics and social consensus” However, blockchain as a technology only enables decentralization, it doesn’t guarantee it.\n\n“The point isn’t ‘web3’, it isn’t ‘decentralization’ for the word’s sake. The point is developing tools that allow us to leverage computers to collaborate more effectively. It’s about accountability and choosing for ourselves what to trust. It’s about withholding trust from strangers on the internet but holding out the belief that we can build a better way.” (Dan Finlay)\n\nThe cyberpunk spirit: the basic property of which adversarial conflict continues to heavily favour the defender. It should be much more expensive to destroy or disrupt than they are to use and maintain — Vitalkik Buterin\nOf course, this is not perfect technology. I do have a few critiques on it as well.\nIndex\n\nBlockchain\nDecentralization\nPublic Goods\nDAOs\nQuadratic Funding\nProof of stake\nProof of work\nConsensus\nPlay\nNFTs\nArweave\n\nNotes on Kernel curriculum: Kernel\nWeb History\nEthereum article\nWe are in the process from changing platforms to protocols; the difference being that, in protocols, there is no central appeal.\nWeb1, Information Economy [1980s to 2000s]\nInternet services were built on open protocols that were controlled by the internet community. Develops could generally rest assured that these protocols wouldn’t change much because they were commonly agreed on by the community.\nWeb2, Platform Economy [2000s to 2020s]\nInternet as we know it today, companies that provide services in exchange for your personal data. Web2 enabled us to enjoy P2P interactions on a global scale, but always with a middleman: a platform acting as a trusted intermediary between two people who do not know or trust each other.\nGeneral migration from open services to more sophisticated, centralized services\nEffects include frequent incursions on personal privacy, the spread of misinformation, and the ‘attention economy,’ all of which have fundamentally changed how we relate to one another, and destabilized civic engagement and labor across industries. Source\nWeb3, Token Economy [2020s to present]\nDecentralized apps (usually running on the blockchain).\nMajor benefits:\n\nAnyone on the network can use the service (no one can block/deny you access to the service)\nPayments are built in (ether or ETH)\nTuring complete\nRequire no personal data\nHigh reliability: runs on decentralized network rather than a single backend\n\nLimitations\n\nScalability/Speed: change to state need to be processed and propagated\nUX: lots of information, not a lot of good ways to interface with it\nFracturability: can happen when there are disagreements about protocol changes\nCost: gas fees\n\nVerifiability is the atom of web3. It is what the hyperlink was for Web1.\nGitcoin\nHow do we incentivize open source software? Gitcoin approaches this through decentralizing funding (away from corporations and toward individuals).\nNot just providing funding, 4 main value adds\n\nEarn → get paid to do maintenance work and upkeep of public goods!\nLearn → mentorship from industry leaders and peers\nConnect → creating a community of builders and support network\nFund → making it easy for developers to support each other\n\nSee also: marketplaces"},"thoughts/website-inspiration":{"title":"Website Inspiration","links":[],"tags":["evergreen"],"content":"Fun interactions\n\nhttp://lynnecarty.info/: blobs! Fun way to visualize overlapping tags in a personal portfolio\nhttps://www.boboland.xyz/: sick use of spatial canvas\nhttps://lfe.org/: telescopic text\nhttps://christophlabacher.com/notes/ethnographic-research-on-dynamicland: really awesome sidenotes\nhttps://tomasp.net/techdims: good non-linear layout of information\nhttps://www.samanthahunt.net/: body as map/navbar\nhttps://ezekielaquino.com/: generates a unique impromptu that is playable on page load\nhttps://www.helloshivam.com/: fun colourful mouse swirly thing\nhttps://thesevenvirtuesproject.com/: slick animations and transitions\nhttps://www.son-la.co/: colourful game of life\nhttps://langworth.com/: film grain and interactive terminal that looks like a video\nhttps://demo.marpi.pl/codeology/: ASCII 3D renderer (feels super smooth)\nhttps://ralphammer.com/: lovely and cute little animations next to each blog post\nhttp://pressanykey.today/: draws a stroke between what keys you press on your keyboard\nhttps://tatianabilbao.com/: drawing/illustration as map/navbar\nhttp://luckysoap.com/andbyislands/: just a big map\nhttp://universalthirst.com/: fun spinny reveal animation\nhttps://river.maxbittker.com/: sfx\nhttps://opalcamera.com/opal-tadpole: spicy onscroll product description\nhttps://taliacotton.com/: hover over letters\nhttps://jessmart.in/: map site\n\nAesthetics\n\nhttps://shapefarm.net/: clean integration of animation and 3D and reactive cursor\nhttps://www.epic.net/: awesome use of typography, 3D, and scroll effects\nhttps://kernel-mag.vercel.app/post/take-back-the-future: love the noise and gradient\nhttps://rauno.me/: just clean, what else is there to say rauno is a legend\n\nhttps://rauno.me/craft/nextjs: a great blog post breaking down the Next.js site design\n\n\nhttps://www.thesolarmonk.com/posts/a-spacebar-for-the-web: super cool use of dropshadow and gradients on text to add depth\nDistinct retro personality and feel\n\nhttps://linear.app/change\nhttps://retool.com/visual-basic/: also distinct retro personality and feel (cool mouse and background interaction too)\n\n\nhttps://oxide.computer/: clean product site with seamless image integration\nUtilitarian sites that do a good job of presenting information cleanly\n\nhttps://sholis.com/\nhttps://paco.me/\n\n\nhttps://neuralink.com/: I love the diagrams and colour choices\nhttps://negative.sanctuary.computer/: love the page layout and the use of monochrome+dithered images\nhttp://dajistudio.com/: clean but incredibly information dense page (very Japanese)\nhttps://ertdfgcvb.xyz/: animated ASCII page\nhttps://www.mergrim.net/en/: clever audio-focused website\nhttps://www.clemzer.dev/: lovely pixelated header font\nhttps://newdesigncongress.org/en/: bold zine-like personality\nhttp://www.scrnprnt.ca/: silly diagrams\nhttps://garden.bradwoods.io/: grunge\nhttps://surrealdb.com/: 3d illustrations and use of transparency/blur are clean\nhttps://www.socratica.info/: fun use of mixed fonts and print screen graphics\nhttps://nicochilla.com/\nhttps://jacobleech.com/\n\nPoetic/Conceptual\n\nhttps://thecreativeindependent.com/welcome/: love the use of reader messages (both audio and textual)\nhttps://sashaportis.com/: fun use of text display\nhttps://www.briewolfson.com/: clean, love the colourway\ncomputing that is tied to the real world through sensors\n\nhttp://feral.earth/\nhttp://solarprotocol.net/\n\n\nhttps://chia.design/: a portal/index into many worlds (a hubworld)\nhttps://handwritten.blog/2022-10-01-hyperlinks-in-handwriting.html: handwritten blog posts with links\nhttps://old.mark-beasley.com/peer-peer-sunset/index.html: peer-to-peer sunset experience\nhttps://tiffanyq.github.io/sunrise/: animated digital sunset\nhttps://tilde.town/~troido/cadastre/town.html: everyone has a little ASCII plot of land\n\n3D graphics\n\nhttps://chartogne-taillet.com/en: styled hand-drawn feel in 3D\nhttps://ventureworks.io/: smooth scroll 3D background\nhttps://henryheffernan.com/: full 3D desk setup???\nhttps://pouria.dev/unknown-pleasures: song visualizer but its a whole cinematic experience\nhttps://hyper.leuys.com/point-of-entry/1#10758717: 3D web browsing\nhttps://bbycroft.net/llm: informative explainer\nhttps://www.stripe.press/poor-charlies-almanack: just fun + great info presentation\nhttps://buttermax.net/\n\nPortfolios\n\nhttps://work.nicochilla.com/\nhttps://liuleslie.github.io/\nhttps://www.trudy.computer/\nhttps://www.theyanktank.com/\nhttps://shashwathsantosh.com/\n\nResources\n\nhttps://www.logo-archive.org/\nhttps://icons8.com/\nhttps://fontsource.org/\n\nhttps://www.ingofonts.de/ingofonts/en/iF_BiroScript/iF_BiroScript.html\n\n\n"},"thoughts/websites-as-homes":{"title":"Websites as homes","links":["thoughts/plurality","thoughts/agency","thoughts/website-inspiration","thoughts/digital-commons","thoughts/cozy-software","thoughts/the-garden-and-the-stream","thoughts/identity"],"tags":["seed"],"content":"\nConsider the pluralities and multiplicities of web-weaving: the more websites and worlds there are, the more environments we can choose to immerse ourselves in. The act of creating a website is an act of agency and expression.\n(Chia)\n\nSee also: website inspiration, digital commons, cozy software, the garden and the stream\nOn Website Redesigns\n\nIn other words, a website creator becomes both author and architect simultaneously. There are endless possibilities as to what a website could be. What kind of room is a website? Or is a website more like a house? A boat? A cloud? A garden? A puddle?\nWhatever it is, there’s potential for a self-reflexive feedback loop: when you put energy into a website, in turn the website helps form your own identity.” —Laurel Schwulst\n\nAbout every half a year, I get an intense urge to redesign my website. Some years I overhaul everything, ripping out the content and gutting the divs and p tags. Other years, I make only minor changes, giving it a fresh coat of paint and changing out old typography for new ones.\nI find it hard to place a finger on exactly why I feel this way. The colours, font spacing, and content — they don’t feel ‘me’. How did something that used to feel so perfect and intimate feel so alien and off-kilter?\nOur digital artifacts and spaces are reflections of our real selves. We feel like we outgrow digital spaces just as we change, learn, and grow in real life.\nIn part, this is why I think having your own little plot on the internet to change and modify at your own whim is worth protecting. This is our last little bit of land in an era where we leave our wizardly powers to build worlds of our choosing at the door of digital giants.\n\n\nThink about it: there’s no way to make a web page or a blog that is not an act of playing with its form at the same time as you’re creating its content. So it just seemed natural: the world was always telling me that you worked on those two things – the container and its contents – together. (Robin Sloan on websites and notes)\n\nSee also: cozy software\nHundertwasser flavour of design\nSource\n\nThe resident has access to the same tools as the architect.\nEverything is writeable, everything is rewriteable.\nPeople can solve their own problems.\n\nAs shared journey\nSource\nWhat if we could create and share journeys we’ve taken across the internet, with the sights we’ve seen and the rocks we’ve collected attached?\nAs location\nSource\nLocated at a point in physical space and cyberspace, a website posits a place of being designated for dwelling and use. We encounter a website at a physical site, engaging it at its own site.\nAs home\nSource\n\nStumbling upon these websites was a bit like walking into their physical house, leaving your shoes by the door and, glass in hand, taking a peek at the spines of their books, their record collection, the picture frames on the wall, or the tidbits they displayed on the sideboard.\n\nAs for the website itself, it focuses on the person first, the occupation second (a philosophy Nick Cave explained much more beautifully)."},"thoughts/wood-wide-web":{"title":"Wood wide web","links":["thoughts/Mutual-Aid","thoughts/library"],"tags":["seed"],"content":"Source\nIn Suzanne Simard’s 1980s field experiments, she found that trees communicate, sending chemical warning signals to one another and passing sugar, water, carbon, nitrogen, and phosphorus between species. Wild forests operate, Simard wrote, as “an intelligent system, perceptive and responsive.”\nThe wood wide web has been a powerhouse metaphor for popularizing the mutualistic relationships of healthy forests.\nMother Trees\nThe eldest trees — having survived storms, droughts, ravenous insects, and the damage wrought by capitalism and colonialism — serve as the central hubs of the wood wide web. They are the strongest, the most resource-rich, with taproots stretching far beneath the earth. Suzanne Simard calls these elders “Mother Trees,” as do many indigenous people.\nMother Trees serve as hubs in a decentralized network. A single Mother can be connected to hundreds of other trees, but in a healthy forest, multiple Mother Trees with overlapping connections ensure that a single elder isn’t responsible for the continuity of the forest as a collective organism.\nTo build resilient decentralized networks, let us create “Mother nodes” — sites in the network bearing a responsibility of care. We’ve built institutions like these before: consider public libraries, which serve both as bearers of cultural memory and as generous sources of nutrients for our minds and communities"},"thoughts/workflows":{"title":"Workflows","links":["thoughts/interaction-design","thoughts/Design-Justice","thoughts/digital-permanence","thoughts/Extended-Mind-Hypothesis","thoughts/attention-economy"],"tags":["seed"],"content":"Creating modular tools for users to build their own workflows and interactions\n\n“Each person’s mind works a little differently, and each person remembers and processes information a little differently. I think we all work at our best when we work with tools that fit how our minds work.”\n\nWhen other people build tools for us to use, they either design tools after their own workflows and mental models, or worse, they design it for a mass market of millions of people who all sort-of-but-not-really work and think in similar ways.\nBuilding tools for others usually means the creator builds for their own workflow/mental models: universal design. We create tools that work OK for the majority, but great for no one.\nTools you build yourself can grow and change as your workflow changes over time: less of a sense of digital permanence\nAre our tools extensions of who we are? At what point does it just become a part of our mind? Relevant: the extended mind hypothesis\nCool Approaches\nMercury\nDefining everything around workflows rather than applications.\nVerb-noun-modifier approach (e.g. Find mentions of Dogs in my notes)\nExtensive use of the command palette, smart context-based dropdown filling\n\nIn conventional App-driven operating systems, functions are segregated within different Apps. The process of moving from App to App generates friction that takes you out of flow, and distracts you from your intentions.\n\nNotifications are off by default unless you specific your availability in the rules of the space (really valuable esp in an attention economy)"},"thoughts/writing":{"title":"Writing","links":["thoughts/knowledge-distillation","thoughts/bandwidth","posts/networked-thought","thoughts/language","thoughts/terminology","thoughts/contact-language","thoughts/context","thoughts/fiction","thoughts/autopoiesis","thoughts/idea-list","thoughts/art","thoughts/logical-fallacies","thoughts/Internet","thoughts/mimetic","thoughts/building-in-public"],"tags":["sapling"],"content":"Why write?\nWriting as crystallized thought, a way of expressing the labyrinth of interconnected, messy, and many time incoherent ideas in my mind. It is a form of knowledge distillation.\n\nThe thing I like about writing is that it’s quite literally thinking—a way for me access my own interiority and construct something from it. What I write is all mine, it’s a living thing, it’s an extension of me that wanders out into the world. (Ava in How To Avoid Half-Heartedness)\n\nIt is a form of lossiness as mutation, a way to re-interpret and adapt the thoughts into a new form — to breathe it new life. Whether networked or linear, molding it into new forms through language and terminology can give it a new perspective. A mental unflattening.\n\nWhat I am doing right now, writing this essay, is, technically, a linear walk through the network of my ideas. That is what writing is: turning a net into a line. (Henrik Karlsson, Reader-generated Essays)\n\nIt is the form almost universally understood by all, a sort of contact language that enables people from vastly different backgrounds and contexts to build shared fictions.\nIt is the contribution of the radical intellectual, a sort of gift and offering. From David Graeber, ‘Fragments of an Anarchist Anthropology’:\n\nOne obvious role for a radical intellectual is to do precisely that: to look at those who are creating viable alternatives, try to figure out what might be the larger implications of what they are (already) doing, and then offer those ideas back, not as prescriptions, but as contributions, possibilities — as gifts […] Such a project would have to have two aspects: one ethnographic, one utopian, suspended in a constant dialogue.\n\nWriting is a form of autonomous knowledge, something that is autopoetic, self-contained, and self-spreading.\nSee also: writing idea list\nArt\nWriting as art\n\nI write like the 12 dollar desk salad, the bar that packs 20 grams of protein and plastic into one 200-calorie brick. But good writing, like a good meal, needs fat. It should indulge readers, is meant to be chewed and enjoyed, affording a generous escape from the prosaic and mundane. — Jasmine Sun\n\nHow much time should we spend producing great writing, and how much trying to prove it to the world? Can we write as if we were Hanya Yanagihara in “A Little Life”? To please only ourselves?\nAs Thinking\nTed Chiang on the power of written language in Truth of Fact, Truth of Feeling:\n\nWriting was not just a way to record what someone said; it could help you decide what you would say before you said it. And words were not just the pieces of speaking; they were the pieces of thinking. When you wrote them down, you could grasp your thoughts like bricks in your hands and push them into different arrangements. Writing let you look at your thoughts in a way you couldn’t if you were just talking, and having seen them, you could improve them, make them stronger and more elaborate.\n\nAs Claims\n\nWe write not only to state what we have think but also to show why others might agree with it and why it matters. We also know that whatever it is we think, it is never the entire truth. Our conclusions are partial, incomplete, and always subject to challenge. So we write in a way that allows others to test our reasoning: we present our best thinking as a series of claims, reasons, and responses to imagined challenges, so that readers can see not only what we think, but whether they ought to agree.\n— Writing in College, by Joseph M. Williams and Lawrence McEnerney\n\nThesis can contain 4 main types of claims\n\nClaims of fact or definition: argue about what the definition of something is or whether something is a settled fact\nClaims of cause and effect: argue that one person, thing, or event caused another thing or event to occur\nClaims about value: what something is worth, whether we value it or not, how we would rate or categorize something\nClaims about solutions or policies: argue for or against a certain solution or policy approach to a problem\n\nThese aim to get the reader to say “that’s interesting, I’d like to know more”\n\nLogos: logic (See also: list of logical fallacies)\nEthos: reputational appeal of the writer\nPathos: emotional appeal\n\nAs query\nSource\nA blog post is a search query. You write to find your tribe; you write so they will know what kind of fascinating things they should route to your inbox.\nSee also: niche at scale and the internet\n\nWriting for a general public, you need to be broad and a bit bland. I didn’t want a general public. I wanted a specific set of people, the people who could help me along as a human being obsessed with certain intellectual problems. I didn’t know who these people were. I only knew that they existed. Hence my writing was a search query. It needed to be phrased in such a way that it found these people and, if necessary, filtered others.\n\nTwo opposing forces to this:\n\nHaving idiosyncratic interests that grow in complexity means that if you pursue them too far you will end up obsessed with things that no one else around you cares about.\nHumans tend to mimic the interests of those around them (see: memetic thinking)\n\nWe can reach a sort of equilibrium by writing and doing things in public."},"thoughts/zero-sum":{"title":"Zero-sum","links":["thoughts/positive-sum","thoughts/group-limits"],"tags":["sapling"],"content":"As opposed to positive sum\nGroup Limits\nIn a local context, relationships may be positive sum. But in the context of all social relationships, they are zero-sum. Social capacity is limited.\nRivalrous in nature. Your ability to acquire something excludes someone else from acquiring it."}}